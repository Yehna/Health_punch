document,summary,id
" Task-oriented dialogue systems are designed to help users achieve predefined goals, such as booking restaurants or movie recommendations via natural language interactions. These systems are deeply connected with external Knowledge Bases  since the system responses are guided by the output from the KB and the dialogue history.   The current state-of-the-arts are end-to-end pipelined systems that rely on Dialogue State Tracking  and Speech Act  annotations. Aside from the annotation cost, which is knowingly high, these pipelined systems must predict a valid DST for querying the KB, execute the query, generate a response template, and finally fulfill it with the retrieved information. The resulting systems are usually overly complicated, and they require multiple steps, including a direct interaction with the KB.   On the other end of the spectrum, there are end-to-end trainable models that use both the KB and the dialogue history as input, and they directly generate system responses. Most of the implementations use either the Gold KB as input or an intermediate API call to retrieve part of the KB . These systems require at least the DST annotation for generating the API calls or to select the gold KB. Moreover, even with the most advanced transformer architecture, end-to-end models struggle when the input becomes too large. For example, in MWOZ, there are 22K entities just for one of the domains. Interested readers can refer to Appendix C for an overview of different task-oriented methodologies.  On the other hand,  discovered a simple yet effective way to query factual knowledge from BERT. Later on,  fine-tuned a pre-trained language model, T5, on just question-answers pairs, without letting the model access any external context or knowledge. These results suggest that the actual knowledge is stored in the model parameters. However, in task-oriented dialogue systems, KB entities do not appear in news articles or Wikipedia, e.g., hotel addresses or postcodes, and thus the aforementioned methods cannot be straightforwardly applied, especially when the KB dynamically changes .  In this paper, we propose a method to store the KB directly into the model parameters using a novel Knowledge Embedded  approach. The resulting model does not use any DST or template responses, nor a KB as input at the inference time, and it can be used in dynamically changing KBs via fine-tuning. The KE approach consists of a newly defined user goal query that generates equivalents KE dialogues from the KB  using minimal annotation effort. Figure shows a high level overview of our approach. To verify the effectiveness of our proposed methodology, we extensively experiment, using both automatic and human metrics, in five task-oriented datasets with small, medium, and large KBs. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all five datasets.  % Additionally, we show that end-to-end models can perform as well as pipelined modularized systems that uses both DST and S-ACT.   
"," %Task-Oriented Dialogue Systems are either modularized with separate dialog %state tracking  and management steps, or end-to-end trainable. In either case, %, and they can be very large. Task-oriented dialogue systems are either modularized with separate dialogue state tracking  and management steps or end-to-end trainable. In either case, the knowledge base  plays an essential role in fulfilling user requests. Modularized systems rely on DST to interact with the KB, which is expensive in terms of annotation and inference time. End-to-end systems use the KB directly as input, but they cannot scale when the KB is larger than a few hundred entries. In this paper, we propose a method to embed the KB, of any size, directly into the model parameters. The resulting model does not require any DST or template responses, nor the KB as input, and it can dynamically update its KB via fine-tuning. We evaluate our solution in five task-oriented dialogue datasets with small, medium, and large KB size. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all evaluated datasets\footnote{Code available in \url{https://github.com/HLTCHKUST/ke-dialogue}}. % The resulting model do not access any external resource during the user interaction, and do not require any KB as input. % to learn to embed structured knowledge of any size directly with model parameters. % % We propose to fine-tune large pre-trained models for task-oriented dialog system with our approach to learning task-specific structured knowledge.   %This has the advantage of  as part of the input nor as an external source during the user interaction.",0
" Open domain question answering~ involves finding answers to questions from an open corpus. The task has led to a growing interest in scalable end-to-end retrieval systems for question answering. Recent neural retrieval models have shown rapid improvements, surpassing traditional information retrieval~ methods such as BM25.   When QA is formulated as a reading comprehension task, cross-attention models like BERT have achieved better-than-human performance on benchmarks such as the Stanford Question Answering Dataset . Cross-attention models are especially well suited for problems involving comparisons between paired textual inputs, as they provide early fusion of fine-grained information within the pair. This encourages careful comparison and integration of details across and within the two texts.   However, early fusion across questions and answers is a poor fit for retrieval, since it prevents pre-computation of the answer representations. Rather, neural retrieval models independently compute embeddings for questions and answers typically using dual encoders for fast scalable search. Using dual encoders results in late fusion within a shared embedding space.  For machine reading, early fusion using cross-attention introduces an inductive bias to compare fine grained text spans within questions and answers. This inductive bias is missing from the single dot-product based scoring operation of dual encoder retrieval models. Without an equivalent inductive bias, late fusion is expected to require additional training data to learn the necessary representations for fine grained comparisons.  To support learning improved representations for retrieval, we explore a supervised data augmentation approach leveraging a complex classification model with cross-attention between question-answer pairs.  Given gold question passage pairs, we first train a cross-attention classification model as the supervisor. Then any collection of questions can be used to mine potential question passage pairs under the supervision of the cross-attention model. The retrieval model training benefits from additional training pairs annotated with the graded predictions from the cross-attention model augmenting, the existing gold data.   Experiments are reported on MultiReQA-SQuAD and MultiReQA-NQ, with retrieval models establishing significant improvements on Precision at   and Mean Reciprocal Rank  metrics.   
"," Neural models that independently project questions and answers into a shared embedding space allow for efficient continuous space retrieval from large corpora. Independently computing embeddings for questions and answers results in late fusion of information related to matching questions to their answers. While critical for efficient retrieval, late fusion underperforms models that make use of early fusion . We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate classification model with cross-attention between questions and answers. The accurate cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at $N$  and Mean Reciprocal Rank .",1
" Topic models, such as Latent Dirichlet Allocation  , aim to discover underlying topics and semantic structures from text collections. Due to its interpretability and effectiveness, LDA has been extended to many Natural Language Processing  tasks . Most of these models employ mean-field variational inference or collapsed Gibbs sampling  for model inference as a result of their intractable posteriors. However, such inference algorithms are model specific and require dedicated derivations.  To address such limitation, neural topic models with black-box inference have been explored, with more flexible training schemes. Inspired by variational autoencoder  ,  proposed Neural Variational Document Model which interprets the latent code in VAE as topics. Following this way,  adopted the logistic normal prior rather than Gaussian to mimic the simplex properties of topic distribution. Logistic normal is a Laplace approximation to the Dirichlet distribution . However, logistic normal can not exhibit multiple peaks at the vertices of the simplex as the Dirichlet distribution. Therefore, it is less capable of capturing the multi-modality which is crucial for topic modeling .  To overcome such limitation,  proposed Adversarial-neural Topic Model , a topic model based on Generative Adversarial Networks   and sampling topics directly from the Dirichlet distribution to impose a Dirichlet prior. ATM employs a generator transforming randomly sampled topic distributions to word distributions, and an adversarially trained discriminator estimating the probability that a word distribution came from the training data rather than the generator. Although ATM was shown to be effective in discovering coherent topics, it can not be used to induce the topic distribution given a document due to the absence of a topic inference module. Such limitation hinders its application to downstream tasks, such as text classification. Moreover, ATM fails to deal with document labels which can help extract more coherent topics. For example, a document labeled as  more likely belongs to topics such as  or  rather than  or .  To address such limitations of ATM, we propose a novel neural topic modeling approach, named Topic Modeling with Cycle-consistent Adversarial Training . In ToMCAT, topic modeling is cast into the transformation between topic distributions and word distributions. Specifically, the transformation from topic distributions to word distributions is used to interpret topics, and the reverse transformation is used to infer underlying topics for a given document. Under such formulation, ToMCAT employs a generator to transform topic distributions randomly sampled from the Dirichlet prior into the corresponding word distributions, and an encoder to reversely transform documents represented as word distributions into their topic distributions. To encourage the generator/encoder to produce more realistic target samples, discriminators for word/topic distributions are introduced to enable adversarial training. Additional cycle-consistency constraints are utilized to align the learning of the encoder and the generator to prevent them from contradicting each other. Furthermore, for documents with labels, we propose sToMCAT that introduces an extra classifier to regularize the topic modeling process.  The main contributions of the paper are:      
","   Advances on deep generative models have attracted significant research interest in neural topic modeling.   The recently proposed Adversarial-neural Topic Model models topics with   an adversarially trained generator network   and employs Dirichlet prior to capture the semantic patterns in latent topics.   It is effective in discovering coherent topics but unable to infer topic distributions for given documents   or utilize available document labels.   To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training    and its supervised version sToMCAT.   ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics.   Adversarial training and cycle-consistent constraints are used to   encourage the generator and the encoder to produce realistic samples that coordinate with each other.   sToMCAT extends ToMCAT by incorporating document labels   into the topic modeling process to help discover more coherent topics.   The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and   text classification.   The experimental results show that our models can produce both coherent and informative topics,   outperforming a number of competitive baselines.",2
"  Probabilistic topic models  are tools for discovering main themes from large corpora. The popular Latent Dirichlet Allocation   and its variants  are effective in extracting coherent topics in an interpretable manner, but usually at the cost of designing sophisticated and model-specific learning algorithm. Recently, neural topic modeling that utilizes neural-network-based black-box inference has been the main research direction in this field. Notably, NVDM  employs variational autoencoder   to model topic inference and document generation. Specifically, NVDM consists of an encoder inferring topics from documents and a decoder generating documents from topics, where the latent topics are constrained by a Gaussian prior.  argued that Dirichlet distribution is a more appropriate prior for topic modeling than Gaussian in NVDM and proposed ProdLDA that approximates the Dirichlet prior with logistic normal. There are also attempts that directly enforced a Dirichlet prior on the document topics. W-LDA  models topics in the Wasserstein autoencoders  framework and achieves distribution matching by minimizing their Maximum Mean Discrepancy  , while adversarial topic model  directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network  .  Recently, due to the effectiveness of Graph Neural Networks   in embedding graph structures, there is a surge of interests of applying GNN to natural language processing tasks . For example, GraphBTM  is a neural topic model that incorporates the graph representation of a document to capture biterm co-occurrences in the document. To construct the graph, a sliding window over the document is employed and all word pairs in the window are connected.  A limitation of GraphBTM is that only word relationships are considered while ignoring document relationships. Since a topic is possessed by a subset of documents in the corpus, we believe that the topical neighborhood of a document, i.e., documents with similar topics, would help determine the topics of a document. To this end, we propose Graph Topic Model , a neural topic model that a corpus is represented as a document relationship graph where documents and words in the corpus are nodes and they are connected based on document-word co-occurrences. In GTM, the topical representation of a document node is aggregated from its multi-hop neighborhood, including both document and word nodes, using Graph Convolutional Network  . As GCN is able to capture high-order neighborhood relationships, GTM is essentially capable of modeling both word-word and doc-doc relationships. In specific, the relationships between relevant documents are established by their shared words, which is desirable for topic modeling as documents belonging to one topic typically have similar word distributions.  The main contributions of the paper are:      
","   Graph Neural Networks    that capture the relationships between graph nodes via message passing   have been a hot research direction   in the natural language processing community.   In this paper, we propose Graph Topic Model , a GNN based neural topic model   that represents a corpus as a document relationship graph.   Documents and words in the corpus become nodes in the graph and   are connected based on document-word co-occurrences.   By introducing the graph structure,   the relationships between documents are established through their shared words   and thus the topical representation of a document is enriched by   aggregating information from its neighboring nodes using graph convolution.   Extensive experiments on three datasets were conducted   and the results demonstrate the effectiveness of the proposed approach.",3
" % {jiaqi: outlines}  % \ys{Need to put more Covid information here. Logic is that we need to do so for covid 19 rather than we have the information and then we can do so. }   In this work, we report the system architecture and results of the team TEST\_POSITIVE in the competition of W-NUT 2020 sharred Task-3: extracting COVID-19 event from Twitter.   Since February 2020, the pandemic COVID-19 has been spreading all over the world, posing a significant threat to mankind in every aspect. The information sharing about a pandemic has been critical in stopping virus spreading. With the recent advance of social networks and machine learning, we are able to automatically detect potential events of COVID cases, and identify key information to prepare ahead.  %   % Users share a wide range of information on social networks. Large platforms, such as Twitter and Facebook, provide sufficient user-generated content for natural language processing applications. For example, massive tweet data posted by users have nourished a variety of applications, e.g. sentiment analysis ~, disaster monitoring ~, event extraction ~ and etc.  We are interested in COVID-19 related event extraction from tweets. With the prevalence of coronavirus, Twitter has been a valuable source of news and information. Twitter users share COVID-19 related topics about personal narratives and news on social media . The information could be helpful for doctors, epidemiologists, and policymakers in controlling the pandemic. However, manual extracting useful information from tremendous amount of tweets is impossible. Hence, we aim to develop a system to automatically extract structured knowledge from Twitter.  % \ys{According to Chieh-Yang, using global model solved the issue of limited annotation, while using the various types of tasks to use all event data to do the training.} Extracting COVID-19 related events from Twitter is non-trivial due to the following challenges: \\  How to deal with limited annotations in heterogeneous events and subtasks?. The creation of the annotated data relies completely on human labors, and thus only a limited amount of data can be obtained in each event categories. There are a variety types of events and subtasks. % Due to the sparsity of the positive samples,  %  the annotation cannot scale up properly and thus only a limited amount of data can be obtained. % The training dataset relies on manual annotation. Hence, we can only obtain a limited number of training data.  Many existing works solve these low resource problem by different approaches, inlcuding crowdsourcing , unsupervised training , or multi-task learning . Here we adopt multi-task training paradigm to benefit from the inter-event and intra-event  information sharing. In this way, \ours learns a shared embedding network globally from all events data. In this way, we implicitly augment the dataset by global training and fine-tuning the language model.    % because our events and subtasks share similarities % to make use of the fundamental relations across different subtasks and events in learning a global embedding network. %  Heterogeneous types of events and subtasks.  Existing work  did not encode the information of different subtask types into the model, while it could be useful in suggesting the candidate slot entity type. In order to make type-aware predictions, we propose a NER-based post-processing procedure in the end of \ours pipeline. We use NER to automatically tag the candidate slots and remove the candidate whose entity type does not match the corresponding subtask type. For example, as shown in Figure, in subtask ``Who'', ``my wife's grandmother'' is a valid candidate slot, while ``old persons home'', tagged as location entity, would be replaced with ``Not Specified'' during the post-processing.   % UK閳 will not be a valid slot for the subtask 閳ユ辅ho閳,as 閳ユ辅ho閳 would require a human-related descrip-tion but 閳ユ矾K閳 is tagged as location-related entityby NER. %   %   % and trains   % tackles each event separately and trains multiple models for different events.   % .}  % To tackle the aforementioned challenges, we propose \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model.  % Built upon a joint event multi-task learning framework, \ours benefits from the training data across all the event types.  % In this way, we implicitly augment the dataset by global training and fine-tuning the embedding parameters.  % Furthermore, we design a type-aware post-processing step to automatically remove the predictions whose entities do not match the corresponding subtask types by leveraging the named entity recognition  .  % For example, ``UK'' will not be a valid slot for the subtask ``who'', as ``who'' would require a human-related description but ``UK'' is tagged as location-related entity by NER. %  % For example, if a predicted slot for subtask ``who'' is tagged with a location related entity, we invalidate the prediction by ``Not Specified''.  %  In summary, \ours is enabled by the following technical contributions:\\ % [itemsep=0mm]    With the unified global training framework, we train and fine-tune the language model across all events and make predictions based on multi-task learning to learn from limited data. \\ %   In this way, \ours benefit from annotated training data of different events and subtasks.\\   We leverage NER tagging on the model predictions and filter out wrong predictions based on subtask types. In this way, \ours benefits from subtask type prior knowledge and further boosts the performance. %     %  %     :Twitter provides sufficient content for NLP tasks. Event extraction is useful in multi application scenarios. COVID-19 situation. %     :  To automatically extract structured knowledge on events. Accurate analysis of COVID-19 tweets can help know the true situation and control the spread.      %     :  Limited annotated data; ;  Type-aware prediction %     : it can be introduced based on the challenges %     :  noisy text data processing;  Joint event multi-task learning;  A well-packaged systematic framework to deal with similar tasks. %    % covid-19 wide spreading  % To automatically extract structured knowledge on events % related to COVID-19 from Twitter is useful for epidemiologists, journalist or policymakers.    % Challenges:  Noisy text in Twitter;  Limited training data;   % In this work, we propose a joint event multi-task learning model for noisy text slot filling tasks with limited training data.  % Our Contributions: %  %        % s 
","  The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important questions . To tackle these challenges, we propose the \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language model.  Moreover, we implement a type-aware post-processing procedure using named entity recognition  to further filter the predictions. \ours outperforms the BERT baseline by $17.2\%$ in micro F1.\footnote{\url{https://github.com/Chacha-Chen/JOELIN}}    % Extracting structured knowledge from Twitter is non-trivial because:  Limited annotated data: structured knowledge needs to be annotated manually;   Various types of tasks: there are different types of slot filling tasks for different events and subtasks.   % To tackle these challenges, we propose \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language embedding parameters.  Moreover, we implement a type-aware post-processing procedure using NER-based techniques to further filter the predictions.\footnote{\url{https://github.com/Chacha-Chen/JOELIN}} %  % \jq{Thanks! Kenneth}",4
"  In the era of digitization, most businesses are turning towards leveraging artificial intelligence  techniques to exploit the information contained in business documents. Traditional information extraction  approaches utilize Natural Language Processing  methods to process the information from documents expressed in the form of natural language text . However, documents contain rich multi-modal information that includes both text and the document layout. The document layout organises the textual information into different formats such as sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues are also indicated through figures/charts/logos etc. and the overall document page appearance. In general, information in a document spans over multiple pages which gives rise to a variety of complex document layouts that can be observed in scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing and understanding these documents is a challenging endeavor and requires a multi-disciplinary perspective combining NLP, computer vision , and knowledge-representation to learn a generic document representation suitable for different downstream applications .  Recent approaches towards document analysis have explored frameworks that utilize information from document text, document layout and document image in different capacities  for specific document tasks.  have proposed joint training of document text and structure for the task of IE from form-like documents, while  combine text and image information for the task of semantic segmentation of documents. Their proposed frameworks optimize the network performance with respect to downstream task which are not suitable for other tasks. To address this limitation,  proposed a pre-training technique based on the BERT transformer architecture , to combine text and layout information from scanned documents. They showcase applicability of their pre-trained network on different downstream tasks further utilizing the image information during fine-tuning for each task. Although  presents a pre-trained framework to learn document representation, there are two limitations to their approach -  the framework only allows for single page documents and  proposed pre-training tasks cannot utilize image information for learning document representation. In the real-world scenario, multi-page documents are common with different pages potentially containing different information across text, layout, and image dimensions. Also, the page image captures the overall layout beyond the appearance of text tokens in the document. Thus, for serving different documents tasks, a unified pre-training framework that learns a generic document representation from all three modalities and works on multi-page documents is necessary.  [t] .     In this paper, we propose such a generic document representation learning framework that takes as input the document text, layout, and image information applicable to different document tasks. Specifically, we encode the multi-modal document information as -  text and position embeddings similar to BERT   text token 2D position embeddings to capture the layout,  text token image embeddings to capture their appearance, and  document page image and position embeddings to learn the document representation capable of handling multi-page documents. In order to handle large token sequences courtesy of multi-page documents, we utilize the Longformer model proposed by  as the backbone of our framework which introduces an attention mechanism that scales linearly with the sequence length. Following the work of , we utilize the Masked Visual Language Modelling  task and a document classification task that enforces the joint pre-training of all the input embeddings. To further ensure the network learns from the image embeddings, we introduce two additional self-supervised pre-training tasks in our framework -  document topic modeling  and  document shuffle prediction . Similar to the work of , we mine the latent topics from the document text and train our framework to predict the topic distribution using only the document page image embeddings for the task of DTM. On the other hand, DSP involves shuffling the page image order while keeping the other embeddings intact for randomly sampled documents during training to identify if the document is tampered with. While DSP task enforces the joint pre-training of the image embeddings with the text and layout embeddings, DTM task helps to learn richer page image embeddings. As explored by different approaches in prior art , we employ a multi-task learning framework to simultaneously train multiple objectives of the different pre-training tasks to learn shared representations across the text, layout, and image modalities of the documents. We train our network on the publicly available ArXiv dataset  which contains millions of research articles spanning a variety of STEM domains such as mathematics, physics, computer science, etc.  Fig.  signifies the applicability of our pre-trained embeddings for different document tasks. We evaluate the performance of our framework on the following tasks and datasets -  Form Understanding and IE from scanned forms    Document Classification    Table Token Classification  and  Document Retrieval . We conduct an exhaustive set of experiments to analyze the performance of our pre-trained embeddings against state-of-the-art  baselines and ablations of our framework. We're able to beat the SOTA baselines trained on comparable dataset size and network parameters for most of these tasks. In summary, the main contributions of this work are:  % We're able to beat the SOTA performance for certain tasks and achieve comparable performance in other cases utilizing only the pre-trained embeddings for fine-tuning on each task. In summary, the main contributions of this work are    
"," In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, layout, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and state-of-the-art baselines. We discuss the current limitations and next steps for our work and make the code available to promote future research in this direction.   % In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, structure, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, document table structure detection, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and SOTA baselines.  % To the best of our knowledge, this is the first approach in which multiple pages \& token-level visual information is encoded along with text and layout during pre-training.  % Our model outperforms existing SOTA baselines pre-trained on comparable dataset sizes across various downstream tasks. We discuss the current limitations and next steps for our work and make the code available to promote future research in this direction.",5
"  Discourse coherence has been the subject of much research in Computational Linguistics thanks to its widespread applications . Most current methods can be described as either stemming from explicit representations based on the Centering Theory , or deep learning approaches that learn without the use of hand-crafted linguistic features.  Our work explores a third research avenue based on the Rhetorical Structure Theory  . We hypothesize that texts of low/high coherence tend to adhere to different discourse structures. Thus, we pose that using even silver-standard RST features should help in separating coherent texts from incoherent ones. This stems from the definition of the coherence itself - as the writer of a document needs to follow specific rules for building a clear narrative or argument structure in which the role of each constituent of the document should be appropriate with respect to its local and global context, and even existing discourse parsers should be able to predict a plausible structure that is consistent across all coherent documents. However, if a parser has difficulty interpreting a given document, it will be more likely to produce unrealistic trees with improbable patterns of discourse relations between constituents. This idea was first explored by  , who followed an approach similar to   by estimating entity transition likelihoods, but instead using discourse relations  that entities participate in as opposed to their grammatical roles. Their method achieved significant improvements in performance even when using silver-standard discourse trees, showing potential in the use of parsed RST features for classifying textual coherence.       Our work, however, is the first to develop and test a neural approach to leveraging RST discourse representations in coherence evaluation. Furthermore,  only tested their proposal on the sentence permutation task, which involves ranking a sentence-permuted text against the original. As noted by , this is not an accurate proxy for realistic coherence evaluation. We evaluate our method on their more realistic Grammarly Corpus Of Discourse Coherence , where the model needs to classify a naturally produced text into one of three levels of coherence. Our contributions involve:  RST-Recursive, an RST-based neural tree-recursive method for coherence evaluation that achieves 2\% below the state of the art performance on the GCDC while having 62\% fewer parameters.  When ensembled with the current state of the art, namely Parseq , we achieve a notable improvement over the plain ParSeq model.  We demonstrate the usefulness of silver-standard RST features in coherence classification, and establish our results as a lower-bound for performance improvements to be gained using RST features.  
"," This paper evaluates the utility of Rhetorical Structure Theory  trees and relations in discourse coherence evaluation. We show that incorporating silver-standard RST features can increase accuracy when classifying coherence. We demonstrate this through our tree-recursive neural model, namely RST-Recursive, which takes advantage of the text's RST features produced by a state of the art RST parser. We evaluate our approach on the Grammarly Corpus for Discourse Coherence  and show that when ensembled with the current state of the art, we can achieve the new state of the art accuracy on this benchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive accuracy while having 62\% fewer parameters.  %This paper explores the impact of silver-standard Rhetorical Structure Theory  trees and relations on discourse coherence evaluation. We show that incorporating discourse features benefits the previous state of the art model and also propose three models based on Recursive Neural Networks. We evaluate our models on the Grammarly Corpus for Discourse Coherence , showing promising results with one model achieving new state of the art performance on discourse classification, and another nearing previous state of the art accuracy. In addition, we provide valuable insights with respect to the application and behaviour of RST relations and trees in discourse analysis, and motivate future work in this area.",6
"    Medical code assignment categorizes clinical documents with sets of codes to facilitate hospital management and improve health record searching~.  These clinical texts comprise physiological signals, laboratory tests, and physician notes, where the International Classification of Diseases  coding system is widely used for annotation. Most hospitals rely on manual coding by human coders to assign standard diagnosis codes to the discharge summaries for billing purposes. However, this work is and error-prone~.  Incorrect coding can cause billing mistakes and mislead other general practitioners when patients are readmitted. Intelligent automated coding systems could act as a recommendation system to help coders to allocate correct medical codes to clinical notes.   Automatic medical code assignment has been intensively researched during the past decades~. Recent advances in natural language processing  with deep learning techniques have inspired many methods for automatic medical code assignment~.   incorporated structured knowledge into medical text representations by preserving translational property of concept embeddings. However, several challenges remain in medical text understanding. Diagnosis notes contain complex diagnosis information, which includes a large number of professional medical vocabulary and noisy information such as non-standard synonyms and misspellings.  Free text clinical notes are lengthy documents, usually from hundreds to thousands of tokens.  Thus, medical text understanding requires effective feature representation learning and complex cognitive process to enable multiple diagnosis code assignment.   Previous neural methods for medical text encoding generally fall into two categories.  Medical text modeling is commonly regarded as a synonym of recurrent neural networks  that capture the sequential dependency. Such works include AttentiveLSTM~, Bi-GRU~ and HA-GRU~.  The other category uses convolutional neural networks  such as  CAML~ and MultiResCNN~.  These methods only capture locality but have achieved the optimal predictive performance on medical code assignment.    Inspired by the generic temporal convolutional network  architecture~, we consider medical text modeling with causal constraints, where the encoding of the current token only depends on previous tokens, using the dilated convolutional network. We combine it with the label attention network for fine-grained information aggregation.    The MultiResNet is currently the state-of-the-art model. It applies multi-channel CNN with different filters to learn features and further concatenates these features to produce a final prediction.  In contrast, our model extends the TCN to sequence modeling that uses a single filter and the dilation operation to control the receptive field. In addition, instead of weight tying used in the TCN, we customize it with label attention pooling to extract relevant rich features.    We contribute to the literature in three ways.  We consider medical text modeling from the perspective of imposing the sequential causal constraint in medical code assignment using dilated convolutions, which effectively captures long sequential dependencies and learns contextual representations in the long clinical notes.   We propose a dilated convolutional attention network , coupling residual dilated convolution, and label attention network for more effective and efficient medical text modeling.    Experiments in real-world medical data show improvement over the state of the art. Compared with multi-channel CNN and RNN models, our model also offers a smaller computational cost.        
"," Medical code assignment, which predicts medical codes from clinical texts, is a fundamental task of intelligent medical information systems. The emergence of deep models in natural language processing has boosted the development of automatic assignment methods.  However, recent advanced neural architectures with flat convolutions or multi-channel feature concatenation ignore the sequential causal constraint within a text sequence and may not learn meaningful clinical text representations, especially for lengthy clinical notes with long-term sequential dependency. This paper proposes a Dilated Convolutional Attention Network , integrating dilated convolutions, residual connections, and label attention, for medical code assignment. It adopts dilated convolutions to capture complex medical patterns with a receptive field which increases exponentially with dilation size. Experiments on a real-world clinical dataset empirically show that our model improves the state of the art.",7
"  The Transformer translation model , which has outperformed previous RNN/CNN based sequence-to-sequence models, is based on multi-head attention networks. The multi-head attention mechanism, which computes several scaled dot-product attention in parallel, can be more efficiently parallelized at sequence level than RNNs , while addressing the drawback of CNNs  which can only model contexts inside a fixed window.  Even though the advantages in parallelization of the multi-head attention mechanism, recent studies  suggest that the computation the scaled dot-product attention is not sufficiently efficient, especially when handling very long sequences, due to the quadratic increasing size of the attention matrix.  In this paper, we study to accelerate the inference of the scaled dot-product attention in another perspective. Specifically, we propose to learn a hard retrieval attention which only attends to one position in the sequence rather than all tokens to simplify the computation of the scaled dot-product attention. Since the hard attention mechanism only attends to one token, the matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can be achieved by a simple and efficient retrieval operation.  Our contributions are as follows:   	  
"," The Transformer translation model that based on the multi-head attention mechanism can be parallelized easily and lead to competitive performance in machine translation. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation subspaces at different positions. Though its advantages in parallelization, many previous works suggest the computation of the attention mechanism is not sufficiently efficient, especially when processing long sequences, and propose approaches to improve its efficiency with long sentences. In this paper, we accelerate the inference of the scaled dot-product attention in another perspective. Specifically, instead of squeezing the sequence to attend, we simplify the computation of the scaled dot-product attention by learning a hard retrieval attention which only attends to one token in the sentence rather than all tokens. Since the hard attention mechanism only attends to one position, the matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can be replaced by a simple and efficient retrieval operation. As a result, our hard retrieval attention mechanism can empirically accelerate the scaled dot-product attention for both long and short sequences by $66.5\%$, while performing competitively in a wide range of machine translation tasks when using for cross attention networks.",8
" Neural Machine Translation  has opened up new opportunities in transfer learning from high-resource to low-resource language pairs . While transfer learning has shown great promise, the transfer between languages with different scripts brings additional challenges. For a successful transfer of the embedding layer, both the parent and the child model should use the same or a partially overlapping vocabulary . It is common to merge the two vocabularies by aligning identical subwords and randomly assigning the remaining subwords from the child vocabulary to positions in the parent vocabulary .   This works well for transfer between languages that use the same script, but if the child language is written in an unseen script, most vocabulary positions are replaced by random subwords. This significantly reduces the transfer from the embedding layer.  argue that romanization can improve transfer to languages with unseen scripts. However, romanization can also introduce information loss that might hurt translation quality. In our work, we study the usefulness of romanization for transfer from many-to-many multilingual MT models to low-resource languages with different scripts. Our contributions are the following:          
"," Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary.   This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.",9
" Machine learning  models used in practice today are predominantly supervised models and rely on large datasets labeled for training. However, the cost of collecting and maintaining labeled training data remains a bottleneck for training high-capacity supervised models. Data programming aims to address the difficulty of collecting labeled data by using a programmatic approach to weak supervision by heuristics, where domain experts are expected to provide data programs  incorporating their domain knowledge. Prior work on data programming focuses on modeling and aggregating labeling functions written manually or generated automatically to denoise labeling functions.  % However, little is known about user experience  % in writing labeling functions and how to improve it.   Writing data programs can be, however, challenging and time consuming.  Most domain experts or lay users have no or little programming literacy, and even for those who are proficient programmers, it is often difficult to convert domain knowledge to a set of rules by writing programs.       %  By extending data programming with programming by example, we bridge the gap between scalable training data generation and domain experts. To address these challenges, we introduce data programming by demonstration ,  a new framework that aims to make creating labeling functions  easier by learning them from users' interactive visual demonstrations. DPBD moves the burden of writing labeling functions to an intelligent synthesizer while enabling users to steer the synthesis process at multiple semantic levels, from providing rationales relevant for their labeling choices to interactively filtering the proposed functions. DPBD draws from two lines of prior research; programming by demonstration  or example , e.g.,, which aims to make programming easier by synthesizing them based on user interactions or input and output examples, and  interactive learning from user-provided features or rationales .    We operationalize our framework with }.   %  along with the materials and anonymized results of the user study %  \documentclass[sigconf]{acmart} \usepackage[moderate]{savetrees} \usepackage{booktabs} % For formal tables \usepackage{listings} \usepackage{latexsym} \usepackage[sets]{cryptocode} \usepackage{amsmath} \usepackage{amssymb} \usepackage{graphicx} \usepackage{setspace} \usepackage{fullpage} \usepackage{xspace} \usepackage{xcolor} \usepackage{caption} \usepackage{subfigure} \usepackage{courier} \usepackage{enumitem} \usepackage[font=normal,skip=2pt]{caption} \usepackage{times} \usepackage{microtype} \usepackage{balance} % to better equalize the last page \usepackage{xcolor} \usepackage[hang,flushmargin]{footmisc} {8pt plus 2pt minus 2.0pt}  \renewcommand % where to search for the images % Copyright  {}  %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%  %% %% The majority of ACM publications use numbered citations and %% references.  The command  switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%  %% %% end of the preamble, start of the body of the document source. %Conference  % %   % %  %% These commands are optional %% % % % \definecolor{tomato}{rgb}{1,0.2,0} \definecolor{turqoise}{rgb}{0.03, 0.91, 0.87} \definecolor{grey}{rgb}{0.4,0.4,0.4} atay:}\textcolor{tomato}{#1}]}\fi} \DeclareRobustCommand{\textcolor{turqoise}{#1}]}\fi} \DeclareRobustCommand{} \DeclareRobustCommand{\xspace} \DeclareRobustCommand{\ruler}{\mbox{{\mbox{{\mbox{ \DeclareRobustCommand{\thenum}{ten\xspace} } } }} \DeclareRobustCommand{}         {0pt}       {1.5em}       {1em}       {0.5em} } }        \renewcommand{     \title[]{Data Programming by Demonstration:\\A Framework for Interactively Learning Labeling Functions}  \author{Sara Evensen} \affiliation{%    } %  \author{Chang Ge} \authornote{Work done during internship at Megagon Labs.} \affiliation{%    } %  \author{Dongjin Choi} \authornotemark[1] \affiliation{    } %  \author{a\u{g}atay Demiralp} \affiliation{    } %   % \renewcommand{  % \renewcommand{    %     \pagestyle{plain}                       
"," % problem & importance   Data programming is a programmatic weak supervision approach to efficiently curate large-scale labeled training data. Writing data programs  requires, however, both programming literacy and domain expertise. Many subject matter experts have neither programming proficiency nor time to effectively write data programs. Furthermore, regardless of one's expertise in coding or machine learning, transferring domain expertise into labeling functions by enumerating rules and thresholds is not only time consuming but also inherently difficult.  % proposed solution  Here we propose a new framework, data programming by demonstration , to generate labeling rules using interactive demonstrations of users. DPBD aims to relieve the burden of writing labeling functions from users, enabling them to focus on higher-level semantics such as identifying relevant signals for labeling tasks.  We operationalize our framework with \system, an interactive system that synthesizes labeling rules for document classification by using span-level annotations of users on document examples.  % evidence that it works  We compare \system with conventional data programming  through a user study conducted with 10 data scientists creating labeling functions for sentiment and spam classification tasks.  We find that \system is easier to use and learn  and offers higher overall satisfaction, while providing discriminative model performances comparable to ones achieved by conventional data programming.",10
"      Deep neural networks are typically trained on a large amount of a single task data through a time-consuming optimization phase. This assumes that the distribution over data points is fixed. However, such neural models do not scale to complex, realistic environments and are prone to distributional shifts and adversarial data points. Online learning on the other hand does not make any distributional assumption and naturally involves an adversarial scenario. However, due to the larger number of training parameters and non-convex optimization landscape, the deep neural networks are hard to train in online settings.     % where the data points are made available over time in an streaming fashion.          {r}{0.6\textwidth}                         $ is exposed to two types of update: the loss gradient and the fast-weights updates. The fast-weights are generated by a meta-learner network based on the gradients w.r.t the slow-weights. The fast-weights maintain information at a time scale that is longer than the network activations, but shorter than the slow-weights, enabling a fast adaptation.         }                     \vskip -0.45in               Meta-learning  has emerged as a promising technique for fast training of deep neural networks by acquiring and transferring knowledge across different tasks through a learned learning algorithm. This work proposes a meta-learning approach to learn sequential adaptation algorithms for deep neural networks. We introduce a sparse variant of Meta Networks to perform an online and continual fast adaptation of deep neural networks over a data stream with non-stationary distribution.           In Sparse Meta Networks , fast-weights are generated sparsely at each step by a meta-learner and accumulated across multiple steps. When the sparse fast-weights are accumulated in this way, across different tasks, they all together act as an mixture of multiple experts in a single Sparse-MetaNet model. Such sparsely generated recurrent fast-weights are not only computationally efficient; and thus can be applied with large scale deep neural networks, but also crucial to maintain a far past memory over the streaming data.           To demonstrate the effectiveness of our approach, we introduce a new vision based benchmark called Online Cifar. In the Online Cifar setup, Sparse-MetaNet shows a better flexibility and a less catastrophic interference, and achieves the best classification accuracy compared with gradient based baselines. We also evaluate Sparse-MetaNet on Wisconsin Card Sorting Test , a simple online reinforcement learning problem adapted from the human cognitive test and large scale language modelling benchmarks. When used along with Transformer-XL for adaptive language modelling, our Sparse-MetaNet achieves 1.00 bpc on enwik8 and 22.67 perplexity on WikiText-103 datasets, improving upon the original Transformer-XL result of 1.06 bpc and 24.0 perplexity, respectively.           {r}{0.5\textwidth}                                              \vskip -0.45in           
","     Training a deep neural network requires a large amount of single-task data and involves a long time-consuming optimization phase. This is not scalable to complex, realistic environments with new unexpected changes.      Humans can perform fast incremental learning on the fly and memory systems in the brain play a critical role.     We introduce Sparse Meta Networks -- a meta-learning approach to learn online sequential adaptation algorithms for deep neural networks, by using deep neural networks.      We augment a deep neural network with a layer-specific fast-weight memory. The fast-weights are generated sparsely at each time step and accumulated incrementally through time providing a useful inductive bias for online continual adaptation. We demonstrate strong performance on a variety of sequential adaptation scenarios, from a simple online reinforcement learning to a large scale adaptive language modelling.",11
"  The advent of open-source software and question and answering websites contributed for improving the way developers produce code. Nowadays, code search permeates the development activities. Developers can spend 15\% of their time searching online for how a piece of code works, how to fix a bug, and how to use an API . According to , at Google, developers search for code 12 times a day, clicking on 2 to 3 results in average per search session.    Most developers use general-purpose search engines  to look for code , which uses page rank and other indexes tactics that are not optimized for searching code. Then, general-purpose search engines do not adequately find code snippets unless they have accompanying descriptions. According to , developers spend more time, visit more pages, and change queries more often when they are doing code-related searches. In particular, newcomers to a project can greatly benefit from semantic search since they face a variety of entrance barriers .   GitHub, a popular source code hosting platform, has attempted to build a semantic code search. They extracted millions of lines of code from its repositories and matched each code snippet to a docstring. The final results were not satisfactory as the tool could find a relevant code snippet only if the user provided a query that matched the docstring description . According to , users' intents were better matched to questions collected from question-answering sites related to programming, e.g., Stack Overflow. Those sites allow users to ask a question and approve the best answer for it. Other users vote for the most helpful answer and mark the wrong or not helpful ones. Those collective actions curate and organize information.  Initial code search studies were based on deductive-logic rules and manually extracted features . The recent success of artificial neural networks has shifted recent works to a machine learning-based approach.  coined a name, neural code search, i.e., code search based on neural networks.  Recent works applied neural networks to summarize and retrieve code snippets.  proposed a neural network with attention mechanism and  presented a recurrent neural network. Our novel approach is based on Convolutional Neural Networks . For the best of our knowledge, CNNs have not yet been used to search for code, but have achieved good results in selecting answers . CNNs prioritize local interactions  and its translation invariant, which are important traits for our task.   In our study, we answer the following research questions:          
"," Software developers routinely search for code using general-purpose search engines. However, these search engines cannot find code semantically unless it has an accompanying description. We propose a technique for semantic code search: A Convolutional Neural Network approach to code retrieval . Our technique aims to find the code snippet that most closely matches the developer's intent, expressed in natural language. We evaluated our approach's efficacy on a dataset composed of questions and code snippets collected from Stack Overflow. Our preliminary results showed that our technique, which prioritizes local interactions , improved the state-of-the-art  by 5\% on average, retrieving the most relevant code snippets in the top 3  positions by almost 80\% of the time. Therefore, our technique is promising and can improve the efficacy of semantic code retrieval.",12
" In recent years, deep learning methods have become the standard for solving information retrieval tasks. These methods can effectively map words and phrases to vector representations. These representations can facilitate better matching between phrases that have similar meanings. Phrases closer in meaning will be represented closer to each other in a vector space. In information retrieval, many ways to develop relevance scores have been used, such as counting word overlap between query and document. Recently, more complex machine learning models use human-verified datasets to train models to assign similarity scores used for rankings. Applying deep learning to Natural Language Processing problems has given rise to new approaches that can better represent a sentence閳ユ獨 meaning using neural networks. For instance,  models with an attention mechanism allow for word relationships to be constructed between different sentences and thus for words to be better placed in context, rather than just by examining the words closest to them. A breakthrough development in Natural Language Processing, the  architecture, extracts word and consequently sentence representations by masking words throughout a sentence and predicting the omitted words, using self-attention to encode the entire sentence at once. Within the BERT framework, the model can also be trained to predict the next sentence out of a few choices, given an input sentence. \\  Even with these advances, deep learning methods still struggle with some inherent difficulties in IR tasks. These challenges result from discrepancies in query and document vocabulary, limited size of data used for training, and weaknesses in a given human-generated query. In an effort to mitigate these effects, our team閳ユ獨 approach was inspired by an existing method, , which for a given input document uses a transformer model architecture to predict plausible queries leading to that document. Although it was shown that the expanded documents indeed allowed improved retrieval performance by a downstream ranking model, this approach requires that all documents in the collection of interest are first ``pre-indexed'' by feeding them as input to the transformer model, which is not practical. Instead, we propose a  method that takes a given query as input and generates several queries similar in meaning. The hope is to create a more powerful query by augmenting the generated queries and the given query into a single representation, which is used to match a desired passage. To complete our architecture, we then feed the expanded queries to a pre-trained BERT model which can predict similarity scores between queries and documents and produce a final ranking. The goal of our approach is to reduce surface form 閳ユ笜oise閳 within a certain query by generating other queries that ask for the same information, but in different ways. By having different representations of the 閳ユ笩ame閳 query, we hope to create more holistic queries and as a result obtain an end-to-end method which can generalize better and potentially reduce the problems which modern IR faces.    
"," This paper describes Brown University's submission to the TREC 2019 Deep Learning track. We followed a 2-phase method for producing a ranking of passages for a given input query: In the the first phase, the user's query is expanded by appending 3 queries generated by a transformer model which was trained to rephrase an input query into semantically similar queries. The expanded query can exhibit greater similarity in surface form and vocabulary overlap with the passages of interest and can therefore serve as enriched input to any downstream information retrieval method. In the second phase, we use a BERT-based model pre-trained for language modeling but fine-tuned for query - document relevance prediction to compute relevance scores for a set of 1000 candidate passages per query and subsequently obtain a ranking of passages by sorting them based on the predicted relevance scores.  According to the results published in the official Overview of the TREC Deep Learning Track 2019, our team ranked 3rd in the passage retrieval task , and 2nd when considering only re-ranking submissions.",13
" % Background Collecting a sufficient amount of electronic health records is a challenging task with various factors . Due to this problem, researchers in the medical field are often provided with only a small amount of data given. Owing to the fact that deep learning techniques perform better on large amounts of data, a number of studies using machine learning techniques have been conducted to solve specific medical problems, regarding a limited number of data . Dementia is also one of many medical symptoms facing this situation.  % Alzheimer's Dementia Dementia, a syndrome in which there is deterioration in cognitive function beyond what might be expected from normal ageing, is mostly affected by Alzheimer閳ユ獨 Disease . % Although studies with Dementia also faces the problem with lacking dataset,  There were previous researches with various approaches to recognize Alzheimer's Dementia , which has shown excellent performance. % However, the dataset used in these works were more adequete with quantity than the one used in this paper. However, datasets used in these works were sufficient with quantity than the one used in this paper.  % The ADReSS challenge The ADReSS challenge  at INTERSPEECH 2020 hosts two tasks: Alzheimer閳ユ獨 Dementia  classification and Mini Mental Status Examination  regression, while providing a refined dataset. The dataset is equally balanced of AD and non-AD participants with the metadata of age and gender. % Each data is a conversation between a participant and an investigator composed of acoustic and textual information. % Each data is a conversation between a participant and an investigator where a participant spontaneously describes the picture given by an investigator. % Each data is a conversation where a participant spontaneously describes the picture given by an investigator with acoustic and textual modality. Each data is a conversation in which participants, in both audio and text modalities, spontaneously describes the picture given by the investigator. % proposing work Participants of the challenge are suggested to solve hosted tasks using only the given data, where the numbers of train and test data are 108 and 48, respectively.  For recognizing AD with small amounts of data, we determined it would be beneficial to use both acoustic and textual features. % why? % we thought it would be best to use as many information as possible for recognizing AD 闉氭帾鐓遍瀬婵庢簜鎼 姘氭棄闈栨棶? Furthermore, we leverage models pre-trained on large scale datasets as feature extractor to get better representation. To this end, this paper focus on exploiting various multi-modal features, and design suitable network architecture. % 闇嬨倢妫 闆﹥妲 闆尗姣勯湆 鑷ф粚娈 鑷у嫴鐏ラ爟姗佺煀 闈广倠鐛忛爟姗佽荡 We compare 3 and 4 different acoustic and textual features, respectively, and use the hand-crafted  feature and part-of-speech  tagging as additional inputs. The usage of POS and HC is influenced by previous research, which has approved that using these features gained from transcript can improve the performance . The proposed network is a modified version of Convolutional Recurrent Neural Network ; capable of computing conversations with variable lengths, and implemented with methods to fit with a small amount of data. Also, the model is able to compute using the acoustic feature only, without any metadata, which can be efficient considering the real-world situation. Our experimental results show using features of the pre-trained network leads to performance gain than that of raw, and regression results imply the potential of network classifying classes of cognitive impairment based on MMSE score.    
"," % The ADReSS Challenge at INTERSPEECH 2020 regards to discern patients suspicious of Alzheimer闁炽儲鐛 Dementia by providing acoustic and textual data. Since the given training dataset only comprised of 108 conversations, leveraging pre-trained models is effective than fitting from scratch. Therefore, this paper aims to recognize Alzheimer闁炽儲鐛 Dementia by exploiting various multi-modal features from pre-trained networks. With the given dataset of conversational form, we modify a Convolutional Recurrent Neural Network based structure to compute input modalities. Our model performs classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. For the classification task, the best test accuracy using only acoustic input is 72.92\%, while using both modality results in 81.25\%. For the regression task, we achieved an RMSE score of 3.7749 . Additionally, our 5-fold cross-validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment, categorized by the MMSE score, with an accuracy of 78.70\%.   %We use 5-fold cross-validation for measuring model performance. For the classification task, the best F1 score using only acoustic input is 86.28\%, while using both modality results in 94.54\%. For the regression task, the best RMSE score is 3.3493.   Collecting and accessing a large amount of medical data is very time-consuming and laborious, not only because it is difficult to find specific patients but also because it is required to resolve the confidentiality of a patient's medical records. On the other hand, there are deep learning models, trained on easily collectible, large scale datasets such as Youtube or Wikipedia, offering useful representations. It could therefore be very advantageous to utilize the features from these pre-trained networks for handling a small amount of data at hand. In this work, we exploit various multi-modal features extracted from pre-trained networks to recognize Alzheimer's Dementia using a neural network, with a small dataset provided by the ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern patients suspicious of Alzheimer闁炽儲鐛 Dementia by providing acoustic and textual data. % With the given dataset, we assess features extracted from the pre-trained networks using a neural network. With the multi-modal features, we modify a Convolutional Recurrent Neural Network based structure to perform classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. % Our model performs classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. % For the classification task, the best test accuracy using only acoustic input is 72.92\%, while using both modality results in 81.25\%. For the regression task, we achieved an RMSE score of 3.7749 . Additionally, our 5-fold cross-validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment, categorized by the MMSE score, with an accuracy of 78.70\%. Our test results surpass baseline's accuracy by 18.75\%, and our validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment with an accuracy of 78.70\%.",14
"   Transformer  is one of the state-of-the-art approaches for Neural Machine Translation , and hence, being widely accepted. For example, in WMT19 machine translation tasks, it is reported that 80\% of submitted systems have adopted the Transformer architecture . Note that high translation quality of Transformer models entails a large number of parameters. Moreover, the Transformer model is inherently much slower than conventional machine translation approaches  mainly due to the auto-regressive inference scheme  incrementally generating each token. As a result, deploying the Transformer model to mobile devices with limited resources involves numerous practical implementation issues.  To address such implementation challenges with little degradation in translation quality, we study a low-bit quantization strategy for Transformer to accomplish high-performance on-device NMT. We note that most previous studies to compress Transformer models utilize uniform quantization . While uniform quantization may be effective for memory footprint savings, it would face various issues to improve inference time and to maintain reasonable BLEU score. For example, even integer arithmetic units for inference operations present limited speed up  and resulting BLEU score of quantized Transformer can be substantially degraded with low-bit quantization such as INT4 .  While determining the number of quantization bits for Transformer, it is crucial to consider that each component of Transformer may exhibit varied sensitivity of quantization error toward degradation in translation quality . Accordingly, a mixed precision quantization can be suggested as an effort to assign different numbers of quantization bits depending on how each component after quantization is sensitive to the loss function. In addition, as we illustrate later, even assigning different quantization bits for each row of an embedding block can further reduce the overall number of quantization bits of the entire Transformer model. Our proposed quantization strategy, thus, provides a finer-grained mixed precision approach compared to previous methods, such as  that consider layer-wise or matrix-wise mixed precision.  % One important aspect is that each block in Transformer contributes to the inference computation and the translation accuracy differently. Transformer consists of three major blocks: embedding, encoder, and decoder. The embedding block has a huge number of parameters due to its dependence on the vocabulary size, easily in scale of tens of thousands. On the contrary, the matrices in encoder and decoder are relatively small since they are independent of the vocabulary size.  As a result, embedding block causes a major memory and latency consumption. Since the decoding steps are not parallelizable at inference time, it also contributes largely to the inference computation.  % In consideration of these, we propose a mixed precision quantization strategy for Transformer quantization with efficient inference computation and reasonable accuracy loss. Accommodating distinguished implementation properties  of each component in Transformer, we propose the following methodologies to decide precision of a block: 1) in the case of embedding block, statistical importance of each word is taken into account and 2) for encoder and decoder blocks, sensitivity of each quantized sub-layer is considered. The main contributions of this paper are as follows:  [noitemsep]     %   
","  The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits . For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8$\times$ smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3$\times$ reduction in run-time memory footprints and 3.5$\times$ speed up  such that our proposed compression strategy enables efficient implementation for on-device NMT.",15
" The rapid progression of generative models in both computer vision  and natural language processing  has led to the increasing likelihood of realistic-looking news articles generated by Artificial Intelligence . The malicious use of such technology could present a major societal problem.  report that humans are easily deceived by its AI-generated propaganda. By manipulating such technology, adversaries would be able to disseminate large amounts of online disinformation rapidly. While it is promising that the pretrained generative models themselves are our best defense , it is often challenging to be aware of the models utilized by adversaries beforehand. More importantly, it ignores the fact that news articles are often accompanied by images with captions .   %We argue that such visual context provides vital clues for discriminating against machine-generated articles.    In this paper, we present the first line of defence against  with images and captions. To the best of our knowledge, we are the first to address this challenging and realistic problem. Premised on the assumption that the adversarial text generator is unknown beforehand, we propose to evaluate articles based on the semantic consistency between the linguistic and visual components. While state-of-the-art approaches in bidirectional image-sentence retrieval  have leveraged visual-semantic consistency to great success on standard datasets such as MSCOCO  and Flickr30K , we show in Appendix they are not able to reason effectively about objects in an image and named entities present in the caption or article body. This is due to discrepancies in the distribution of these datasets, as captions in the standard datasets usually contain general terms including  or  as opposed to named entities such as  and a , which are commonly contained in news article captions. Moreover, images are often not directly related to the articles they are associated with. For example, in Figure , the article contains mentions of the British Prime Minister. Yet, it only contains an image of the United Kingdom flag.   To circumvent this problem, we present DIDAN, a simple yet surprisingly effective approach which exploits possible semantic inconsistencies between the text and image/captions to detect machine-generated articles. For example, notice that the article and caption in Fig. actually mention different Prime Ministers. Besides evaluating the semantic relevance of images and captions to the article, DIDAN also exploits the co-occurrences of named entities in the article and captions to determine the . The  can be thought of as the probability that an article is human-generated. We adopt a learning paradigm commonly used in image-sentence retrieval where models are trained to reason about dissimilarities between images and non-matching captions. In this instance, negative samples constitute articles and non-corresponding image-caption pairs. Not only is this a reasonable approach when the adversarial generative model is unknown, we show empirically that it is crucial to detecting machine-generated articles with high confidence even with access to machine-generated samples during training. More importantly, this means that DIDAN is easily trained on the abundance of online news articles without additional costly annotations.  To study this threat, we construct the NeuralNews dataset which contains both human and machine-generated articles. These articles contain a title, the main body as well as images and captions. The human-generated articles are sourced from the GoodNews  dataset. Using the same titles and main article bodies as context, we use GROVER  to generate articles. Instead of using GAN-generated images which are easy to detect even without exposure to them during training time , we consider the much harder setting where the articles are completed with the original images. We include both real and generated captions which are generated with the SOTA entity-aware image captioning model . We present results and findings from a series of empirical as well as user study experiments. In the user study experiments, we use 4 types of articles including real and generated news to determine what humans are most susceptible to. The insights derived from these findings help identify the possible weaknesses that adversaries can exploit to produce  and serve as a valuable reference for defending against this threat. Last but not least, our experimental results provide a competitive baseline for future research in this area.  In summary,  our contributions are multi-fold:     
"," Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against , they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset composed of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. In addition to the valuable insights gleaned from our user study experiments, we provide a relatively effective approach based on detecting visual-semantic inconsistencies, which will serve as an effective first line of defense and a useful reference for future work in defending against machine-generated disinformation.",16
" Code completion has become an essential feature of Integrated Development Environments . It speeds up the process of software development by suggesting the next probable token based on existing code. The main goal of most existing code completion systems is to suggest accurate variables, arguments, or APIs to developers. Recently, along with the development of deep learning technologies and easy-to-acquire open-source codebases, researchers have started to tackle code completion by learning from large-scale code corpora.  In this paper, we define a new code completion task: full-line code completion. Given a partially completed code snippet, full-line code completion requires predicting the next line of code, different from traditional code completion which only predicts the next code element. Figure 1 shows a motivating example for our task. To complete the last line in Figure 1, traditional code completion needs to predict at least six times separately, and each time the developer needs to choose the correct token. But if we generate the entire line simultaneously, even if the prediction is only partially correct, the developer can correct the code line with fewer operations.   Currently, the most popular technique in the research area of code completion is language models, especially neural language models. Neural language model is a powerful tool for predicting the next token given a token sequence, which naturally fits the scenario of code completion. Recent researches have shown that large-scale neural language models like GPT-2  are capable of generating long text, which brings the potential of code sequence generation.   One of the key challenges in full-line code generation is to guarantee the syntactical correctness of the generated code.  To tackle this challenge, we draw lessons from past researches on semantic parsing. We adopted a widely used framework for syntax-based code generation, which converts the generation of a code snippet into generating its abstract syntax tree  with a sequence of construction actions.  We conduct experiments on two public Python datasets that contain Python files crawled from Github repositories. One dataset is in Python2, and the other one is in Python3. We evaluate the performance of the state-of-the-art approach for traditional code completion, along with a group of neural language models. Our results show that on both datasets, Transformer language models outperform RNN-based models, which is consistent with past researches in language modeling. We also find that syntax-based approaches do not outperform token-based approaches, indicating that directly applying techniques in syntax-based code generation into full-line code completion can be ineffective.  [h]     The main contributions of this paper are summarized as follows:  1) We propose a novel code completion task: full-line code completion and build datasets for this task.  2) We evaluate state-of-the-art models used in traditional code completion and a group of neural language models on our datasets.  3) We analyze the performance of plain token sequence-based language models versus syntax-based language models, and discussed the effectiveness of incorporating syntax information in full-line code completion and some possible improvements in the future.    
"," A code completion system suggests future code elements to developers given a partially-complete code snippet. Code completion is one of the most useful features in Integrated Development Environments . Currently, most code completion techniques predict a single token at a time. In this paper, we take a further step and discuss the probability of directly completing a whole line of code instead of a single token. We believe suggesting longer code sequences can further improve the efficiency of developers. Recently neural language models have been adopted as a preferred approach for code completion, and we believe these models can still be applied to full-line code completion with a few improvements. We conduct our experiments on two real-world python corpora and evaluate existing neural models based on source code tokens or syntactical actions. The results show that neural language models can achieve acceptable results on our tasks, with significant room for improvements.",17
" As neural networks are being adopted to solve real-world problems, while some parts of the network may be easy to develop, other unknown aspects such as hyperparameters, have no clear method of derivation. Ongoing research focuses on developing new network architectures and training methods. When developing neural networks, the question at hand is how to set the hyperparameter values to maximize results and set the training configuration. For network architecture design, important hyperparameters include the type of network, the number of layers, the number of units per layer, and unit type. For training configurations, important hyperparameters include learning algorithm, learning rate, and dropout ratio. All these hyperparameters interact with each other and affect the performance of neural networks. This interaction between hyperparameters can be referred to as epistasis. Thus they need to be tuned simultaneously to get optimum results.\\  The motivation behind this research is to replace tedious manual tuning of hyperparameters with an automatic method performed by computers. Current methods of optimization are limited to trivial methods like Grid search. Grid search is a simple method for hyperparameter optimization. However, as the number of hyperparameters increases, Grid search becomes time consuming and computationally taxing. This is because the number of lattice points increases in an exponential way with an increase in the number of hyperparameters . For example, if there are ten hyperparameters to be tuned and we only try five values for each parameter, and this alone requires more than 9 Million evaluations: 5^{10} = 9765625. For this reason, the grid search is not feasible for certain applications. To solve this, we look to a GA for a higher-performing and less computationally taxing solution. The use of a GA for neural network hyperparameter optimization has been explored previously in . \\   We present an empirical study of GAs for neural network models in machine translation of natural language specifically Japanese to English. We describe the experiment setup in Section 2, our GA method in Section 3,  and results in Section 4. The preliminary findings suggest that a simple GA encoding has the potential to find optimum network architectures compared to a random search baseline.  
","  With neural networks having demonstrated their versatility and benefits, the need for their optimal performance is as prevalent as ever. A defining characteristic, hyperparameters, can greatly affect its performance. Thus engineers go through a process, tuning, to identify and implement optimal hyperparameters. That being said, excess amounts of manual effort are required for tuning network architectures, training configurations, and preprocessing settings such as Byte Pair Encoding . In this study, we propose an automatic tuning method modeled after Darwin's Survival of the Fittest Theory via a Genetic Algorithm . Research results show that the proposed method, a GA, outperforms a random selection of hyperparameters.",18
"  In the past few decades, knowledge graph construction and applications have been rapidly developed and achieved significant outcomes.  For better relevancy in web search, Google has been leveraging knowledge graph that represents real-world entities and their relationships to one another since 2012. %, there are also a large amount of publicly available knowledge graphs, such as freebase, Dbpedia, YAGO that have been constructed and used to many real-world intelligent applications. To identify those entities from text, named entity recognition  techniques have been extensively studied and applied in many areas  including e-commerce search . Such NER systems usually work with a well defined ontology to classify tokens in a sequence of words . A comprehensive and domain-specific PT ontology is beneficial to product search and discovery in an e-commerce platform . At The Home Depot , PT ontology has been used tremendously by the online search to improve query understanding and product retrieval. For example, Figure  shows a snippet of our PT ontology that consists of known PT classes. The PTs in the ontology serve as the entity reference for the NER task  as well as the classes for SKU-PT mapping  on the catalog side that facilitates the retrieval of relevant products.  %. Kutiyanawala et al. also proposed an product ontology framework created specially for e-commerce search and retrieval .  %comprehensive and domain-specific Ontology is required in order to better understand customers閳 intent and account for the expanding catalog. The Ontology enrichment has been proved effective to boost search relevancy. For example, given the customer query ""shower curtain hook"", the system would also return some ""shower curtain"" products since it failed to infer the proper product type due to the lack of knowledge. By introducing a new product type ""shower curtain hook"", the system is able to remove the noise and provide more relevant results. %  %  %  % &\overbrace{whirlpool}^{brand} \quad \overbrace{7.4\:cu\:ft}^{dimension} \quad \overbrace{gas\:dryer}^{product}% \quad\overbrace{gas}^{attribute} %   \\ %   &\overbrace{milwaukee}^{brand} \quad \overbrace{cheap}^{other} \quad \overbrace{drill}^{product}% %  %   %     \[ %   z = \overbrace[1pt][5pt]{ge}^{brand}\ \overbrace{7.3\:cu\:ft}^{dim} \quad\overbrace{dryer}^{product}\quad\overbrace{gas}^{attribute} %   \] %In the domain of e-commerce, a strong and well-structured knowledge graph also plays pivotal roles for both business to business  communications and customer search and navigation experience.   %A structured and standardized product ontology which define product description, catalog formats and business documents support electric data exchange between vendors and buyers.  %The Home Depot  is a world leading home improvement retailer for customers and business. Orange Graph  is the repository and access point for THD domain-specific knowledge, which includes rich product information, project information and their relationships. By adopting well-structured knowledge graph, a high-level of search quality, project-based buying features, marketing and customer services can be offered at THD e-commerce and enterprise systems.  % [htbp!] % {!}{ % {c||c} %  \\ %  vs. Induction Ranges  \\ %  vs. Wood Glue  \\ % } %  %  %  Discovering valid PTs is a key task to build or expand a PT ontology with a fundamental challenge regarding the definition of a PT. % given it's a concept instead of fact. A PT can be defined from the demand side as atomic keywords/phrase that describes what customers look for  or from the supply side as a semantic tag/label that uniquely identifies a product. Within THD, we also have practical guidelines to distinguish between valid and invalid PTs like  %Product type  is an essential component of a PT ontology.  %it is widely used in e-commerce domain to group the similar products together. For instance, consider Appliances category, our goal is to discover distinct types of refrigerators which in this case it could be: ""Side By Side"", ""French Door"", etc.  %Although there are different definitions for a valid PT, In this paper, we define a valid PT as a leaf-level description of an entity.   no common attributes like color, brand, material, style etc in PTs  and  it requires significant differences in the form, functionality or usage location to make a new PT comparing to existing ones . %Another determiner for whether adding a token to a product type makes it a new product type  is if the addition of the new token changes the form, function or usage location. In our example, cordless doesn't change it for drill, while utility does for sink.   Obviously, neither the definition is definite nor the guidelines are exhaustive enough and there are always complicated cases and exceptions in which human judgement based on knowledge in merchandising, customer preference or just common sense is required. %without involving human knowledge which is usually expensive in term of time and monetary cost. %automatically determine if a candidate .  %Although aforementioned definition would generally help to distinguish between valid and invalid PTs, there are several challenges in this task  %as depicted in Table.  %First and foremost, it is crucial to determine a right level of granularity for discovered PTs. Very generic PTs are generally ambiguous as they could be attributable to a broad set of products with different use cases. For example, PT chairs can be ambiguous as it can comprise outdoor chairs, office chairs, dining chairs and each of these chairs types has a different usage location. %Specifically, domain experts have great advantage in  For example, a generic PT range can be broken down into more granular ones by fuel type like gas range, electric range or by other attribute like induction range, convention range. The word ""wood"" is material in wood rolling pin while is about usage in wood glue.   % Moreover, it is often subjective to determine in what level of granularity PT discovery should be stopped and based on what criteria a generic PT should be broken down into more granular PTs. For instance, given a generic PT ranges we can break it down by fuel type  or features . In this example, we can consider one of them as the PT and the other one as an attribute; alternatively they can be combined and construct a more granular PT.  % Another challenge is to automatically identify if a token in a PT is an attribute or not. As an example, consider wood rolling pin and wood glue; token wood in the latter change the use case of the glue, while in the former is a material.     However, leveraging human knowledge in large scale problems is usually timely and expensive.  To reduce such cost, this paper proposes  %The main contribution of this paper is as follows: proposing  an active learning framework that minimizes human effort in PT discovery by 1) identifying high quality candidates using phrase mining and user behavior. 2) limiting number of PT candidates for human validation.  %%%%%%%  
"," Entity-based semantic search has been widely adopted in modern search engines to improve search accuracy by understanding users' intent. %behind the search terms.  %In e-commerce domain, product type  is a central concept in intent understanding as well as catalog organization. %indicating customers' intent in their search queries.  %be identified from customers' queries for understanding  In e-commerce, an accurate and complete product type  ontology is essential for recognizing product entities in queries and retrieving relevant products from catalog.  However, finding product types  to construct such an ontology is usually expensive due to the considerable amount of human efforts it may involve.  In this work, we propose an active learning framework that efficiently utilizes domain experts' knowledge for PT discovery.  We also show the quality and coverage of the resulting PTs in the experiment results.",19
" Distributional word representations trained on large-scale corpora are widely used in modern natural language processing  systems, which aims to describe the meaning of words and sentences with vectorized representations . Recent studies  addressed the state-of-the-art word embedding performance on various NLP tasks, where start to focus on how to evaluate the performance between different word embeddings accurately. However,  and  have demonstrated that even for the same word embedding, most of the existing evaluation methods do not provide the constantly correlative results between intrinsic evaluation and extrinsic evaluation. Therefore, evaluating the performance of word embeddings with a unified metric is challenging in NLP tasks.   proposed a new evaluation framework called CogniVal, which applied traditional neural networks for regression and considered both intrinsic and extrinsic measurements based on collected human natural language processing-related cognitive data sources across three modalities: electroencephalography , functional magnetic resonance imaging , and eye-tracking. CogniVal is potentially identified as a pioneer of multi-modal cognitive word embedding evaluation framework, which conducts vectorized word embeddings evaluation by predicting how much they reflect the semantic representations against cognitive data sources that recorded when human processing natural language.   However, CogniVal framework ignored to measure some characteristics of human physiological signals. Specifically, all three modalities  of cognitive data used in their experiment featuring with non-stationary and non-linear motions . Inspired by , we assume that neural networks and fuzzy systems as computational intelligence methods are suitable tools for modelling expert knowledge and dealing with uncertain non-linear processes or non-stationary time series in a dynamic system, because approximate reasoning characteristics of fuzzy systems could present a practical model to handle uncertainty and disturbances in real data for complex hybrid non-linear or non-stationary problems . For this reason, we proposed a fuzzy-based neural network  framework for evaluating word embeddings with cognitive datasets, name CogniFNN, which expects to enhance the quality of evaluating the performance of word embeddings with cognitive data sources , and achieve a higher ratio of significant results with random word embeddings as well.    The main contributions of our study are shown as follows:       
"," Word embeddings can reflect the semantic representations, and the embedding qualities can be comprehensively evaluated with human natural reading-related cognitive data sources. In this paper, we proposed the CogniFNN framework, which is the first attempt at using fuzzy neural networks to extract non-linear and non-stationary characteristics for evaluations of English word embeddings against the corresponding cognitive datasets. In our experiment, we used 15 human cognitive datasets across three modalities: EEG, fMRI, and eye-tracking, and selected the mean square error and multiple hypotheses testing as metrics to evaluate our proposed CogniFNN framework. Compared to the recent pioneer framework, our proposed CogniFNN showed smaller prediction errors of both context-independent  and context-sensitive  word embeddings, and achieved higher significant ratios with randomly generated word embeddings. Our findings suggested that the CogniFNN framework could provide a more accurate and comprehensive evaluation of cognitive word embeddings. It will potentially be beneficial to the further word embeddings evaluation on extrinsic natural language processing tasks.",20
" As a key step in constructing a knowledge graph, relation extraction is a task to extract the relation between the entities expressed in a sentence.  Previous work has largely focused on intra-sentence binary relation extraction, where the goal is to extract the relation between an entity pair in the sentence.   However, some relations require more than two entities and may span multiple sentences, which is defined as n-ary cross-sentence relation extraction. As the example shown in Table, the relation ``educate'' includes four entities, the person's ""name``, ""academic degree``, ""academic major`` and ""school``. In addition, this relation spans in four sentences in the example. Some prior works have applied a supervised learning approach to tackle this task, but they require large-scale labeled training data. [ht] , which has the ``educate'' relation. ``edu'' denotes that the sentence represents the ``educate'' relation and ``--'' denotes it does not.} {\linewidth}{ c X c c} \toprule Pos. & Sentence & DS & R\\ \midrule 3   &   Alan Turing worked on hyper computation in Princeton University.& edu & --  \\ 4   &  He obtained his PhD in 1938.  & edu &edu   \\ 18  & Alan Turing studied logic and computer science in Princeton.  & -- & edu   \\ 20  &  His PhD advisor is Alonzo Church & -- & edu  \\    To obtain large-scale annotated data, some work assumes that if the consecutive sentences  contain the entities that have a relation in a knowledge base, these sentences as a whole describe that relation.  This assumption is referred to as distant supervision in the n-ary cross-sentence relation extraction task.  Even though methods based on distant supervision can quickly annotate sentences, they still have two main limitations: 1) they suffer from a noisy labeling problem;  2) the strong distant supervision assumption does not consider the non-consecutive sentences, which reduces the generalizability of the trained model. As the example shown in Table, the sentences at the 18th and 20th positions describe the fact but are not labeled using distant supervision because they are not consecutive. The first sentence is incorrectly labeled and is a noisy labeled data, which describes Alan Turing's work instead of his education.  To address the first limitation, we propose to train a , which is a two-level agent reinforcement learning model. This provides a well-trained model that can select the high-quality labeled sentence groups and alleviate the impact of noisy data. There are previous works on applying reinforcement learning  to remove binary intra-sentence noisy data and achieve state-of-the-art  performance. When applying RL for n-ary cross-sentence relation extraction, a key challenge is that the RL model should not only learn sentence features, but also know the context and relation between each sentence. In this paper, the process of selecting sentences is not only influenced by the feature of the sentence itself, but also by the indicators we defined , which measure the semantic relationship between sentences. Moreover, whether a sentence is selected in a state or not is going to affect the decision of the next state. This state transition property provides the ability to choose the best combination of sentences in each sentence group.  To address the second limitation,  we relax the strong distant supervision assumption that lies at the heart of prior work by replacing it with a weaker distant supervision assumption. The assumption is that the sentence that has at least one main entity or two supplementary entities is annotated with the relation of these entities. We follow the Wikidata Knowledge Base scheme, where the main entity is the ``value'' of each fact and the supplementary entity is the ``qualifer'' of each fact. This assumption introduces some non-consecutive sentences and we propose a novel universal relation extractor to encode both consecutive and non-consecutive sentence groups. This relation extractor has a self-attention and soft attention mechanism layer, which compares the similarity between the word-level features and the relation query vectors. The relation extractor also encodes each sentence via a Piece-wise Convolution Neural Network  layer. The PCNN output is used to learn how the information transforms through sentences via a non-linear transformation layer.  
"," The models of n-ary cross sentence relation extraction based on distant supervision assume that consecutive sentences mentioning $n$ entities describe the relation of these $n$ entities. However, on one hand, this assumption introduces noisy labeled data and harms the models' performance. On the other hand, some non-consecutive sentences also describe one relation and these sentences cannot be labeled under this assumption. In this paper, we relax this strong assumption by a weaker distant supervision assumption to address the second issue and propose a novel sentence distribution estimator model to address the first problem. This estimator selects correctly labeled sentences to alleviate the effect of noisy data is a two-level agent reinforcement learning model. In addition, a novel universal relation extractor with a hybrid approach of attention mechanism and PCNN is proposed such that it can be deployed in any tasks, including consecutive and non-consecutive sentences. Experiments demonstrate that the proposed model can reduce the impact of noisy data and achieve better performance on general n-ary cross sentence relation extraction task compared to baseline models.",21
" Healthcare information systems store huge volumes of electronic health records  that contain detailed visit information about patients over a period of time. The data is structured in three levels from top to bottom: the patient journey, the individual visit and the medical code. Fig. provides a typical example of this structure. An anonymous patient visits his/her doctor, a pathology lab and is admitted to the hospital on different days. The procedures and diagnoses performed at each of these visits are recorded as industry-standard medical codes. Each medical code, i.e. International Classification of Diseases  and Current Procedure Terminology , at the lowest level, records an independent observation while the set of codes at a higher level can depict the medical conditions of a patient at a given time point. At the top level, all occurrences of medical events at different time-stamps are chained together as a patient journey, which offers more informative details. Predicting sequential medical outcomes based on a patient's journey, such as hospital re-admissions and diagnoses, is a core research task that significantly benefits for healthcare management by hospitals and governments. For example, re-admission statistics could be used to measure the quality of care; Diagnoses can be used to understand more fully a patient's problems and relevant medical research. However, researchers have encountered many challenges in their attempts to represent patient journeys and predict medical outcomes from EHR data with the characteristics of temporality, high-dimensionality and irregularity.   Recurrent neural networks  have been widely used to analyze sequential data, unsurprisingly including medical events modelling for clinical prediction. For example, Choi et al. proposed a multi-level representation learning, which integrates visits and medical concepts based on visit sequences and the co-occurrence of medical concepts. They indirectly exploited an RNN to embed the visit sequences into a patient representation for downstream prediction tasks. Some other research works directly employed RNNs to model time-ordered patient visits for predicting diagnoses.  However, when the length of the patient visit sequence grows, such RNN-based models are restricted by the less expressive power of RNNs, such as vanishing gradient and forgetfulness.  However, such RNN-based models are constrained by forgetfulness, i.e., their predictive power drops significantly when the sequence of patient visits grows too long.  To memorize historical records, LSTM and GRU have been developed to utilize memory and gate mechanism for mitigating these issues.  To go further, Song et al. proposed to utilise attention mechanism in a deep framework to model sequential medical events.  It is worth noting that sequences of medical events are often found to be lengthy, especially when a patient suffers from chronic disease. Hence, due to the restricted ability of RNNs for long-term dependency modeling , the traditional RNNs, even with memory cells and gates, usually underperform in the cases of a long sequence of medical events. In light of this, a neural model that can overcome the performance bottleneck of RNN-based models is particularly desirable for medical predictions based on longitudinal EHR data.   %%%%%%%%  WHAT THE RELATION BETWEEN SHEN2018DISAN AND THIS ONE?? in a Directional self-attention networks can alleviate long sequence problems to improve the accuracy of predictions, as these models can be trained on all available input information - past and future.. CAN WE COME TO THE CONCLUSION: ONE OF CONTRIBUTION IS WE HAVE FULLY CONSIDERED ALL MEDICAL EVENTS COMPARING TO OTHER WORKS THAT CAN ONLY PARTIALLY CONSIDER.  % Recently, attention mechanism has been integrated into RNNs to model sequential EHRs data, which achieves good prediction accuracy. Although the attention-based RNNs relatively improves the prediction performance, the limitations of RNNs weaken the advantage of attention mechanism. In natural language processing , a sole attention mechanism has been used to construct a sequence to sequence model that achieves a state-of-the-art quality score on the neural machine translation  task. The attention mechanism has more flexibility in sequence length than RNN, and is more task/data-driven when modeling dependencies. Unlike sequential models, its computation can be easily and significantly accelerated by existing distributed/parallel computing schemes. However, to the best of our knowledge, a neural net entirely based on attention has not been designed for patient journey in EHRs data.  Most recently, attention mechanisms have sprung to the fore as effective integrations with RNNs for modeling sequential EHR data. So far, these approaches have shown satisfactory prediction accuracy, but some argue that the power of attention in an RNN is limited by weaknesses in the RNN itself . In particular, Vaswani et al. used a sole attention mechanism, i.e., multi-head attention and self-attention, to construct a sequence-to-sequence model for neural machine translation tasks and achieved a state-of-the-art quality score. And according to  Shen et al., self-attention mechanism allows for more flexibility in sequence lengths than RNNs and is more task/data-driven when modeling contextual dependencies. Unlike recurrent models, attention procedure is easy to compute and the computation can also be significantly accelerated with distributed/parallel computing schemes.  For example, Song et al. proposed to employ 1D CNN  to model local context and use attention mechanism  to capture long-term dependency for sequential medical events.  However, when applied to EHR data instead of regular sequential data , the current attention models cannot appropriately deal with some aspects of EHR data, such as arbitrary time-stamps and hierarchical data format.  Hence, to the best of our knowledge, a neural network-based entirely on attention has never been designed for analytics with EHR data.   To bridge the gap in this literature and address some of the open issues listed above, we propose a novel attention mechanism called Masked Encoder  for temporal context fusion. It uses self-attention to capture contextual information and temporal dependencies between a patient's visits.  Then, we propose an end-to-end neural network, called Bidirectional temporal encoder Network , to predict medical outcomes by leveraging a learned representation of the patient journey,  where the representation is generated solely by the proposed attention mechanism, MasEnc. BiteNet constructs a multi-level self-attention network to represent visits and patient journeys simultaneously, using attention pooling and stacked MasEnc layers. It is worth noting that, compared to the existed RNN-based methods, BiteNet can yield better prediction performance for long sequences of medical records.   Experiments conducted on two supervised prediction and two unsupervised clustering tasks with real-world EHR datasets demonstrate that the proposed BiteNet model is superior to prior state-of-the-art baseline methods.   To summarize, our main contributions are:  	  % The remainders of this paper are organized as follows. Section reviews related studies. In Section, we briefly discuss some preliminary, and details about our model are presented in Section. In Section, we demonstrate the experimental results conducted on real-world datasets. Lastly, we conclude our study in Section.%and outline our future work  % 
"," Electronic health records  are longitudinal records of a patient's interactions with healthcare systems. A patient's EHR data is organized as a three-level hierarchy from top to bottom: patient journey - all the experiences of diagnoses and treatments over a period of time; individual visit - a set of medical codes in a particular visit; and medical code - a specific record in the form of medical codes. As EHRs begin to amass in millions, the potential benefits, which these data might hold for medical research and medical outcome prediction, are staggering - including, for example, predicting future admissions to hospitals, diagnosing illnesses or determining the efficacy of medical treatments. Each of these analytics tasks requires a domain knowledge extraction method to transform the hierarchical patient journey into a vector representation for further prediction procedure. The representations should embed a sequence of visits and a set of medical codes with a specific timestamp, which are crucial to any downstream prediction tasks. Hence, expressively powerful representations are appealing to boost learning performance. To this end, we propose a novel self-attention mechanism that captures the contextual dependency and temporal relationships within a patient's healthcare journey. An end-to-end bidirectional temporal encoder network  then learns representations of the patient's journeys, based solely on the proposed attention mechanism. We have evaluated the effectiveness of our methods on two supervised prediction and two unsupervised clustering tasks with a real-world EHR dataset. The empirical results demonstrate the proposed BiteNet model produces higher-quality representations than state-of-the-art baseline methods.",22
" The International Classification of Diseases  establishes a standardized fine-grained classification system for a broad range of diseases, disorders, injuries, symptoms, and other related health conditions . It is primarily intended for use by healthcare workers, policymakers, insurers and national health program managers. The United States incurs administrative costs in billions of dollars annually arising from a complex billing infrastructure . Specifically, the ICD code assignment is typically a manual process, consuming on average between 25 to 43 minutes per patient depending on the ICD version . It is also prone to errors resulting from inexperienced coders, variation between coders, incorrect grouping of codes or mistakes in the patient discharge summaries. These errors are very costly with one report estimating that preventable errors in ICD coding have cost Medicare system 31.6 billion in FY2018 .\\\\ Recent work  has tried to automate the task of ICD code assignment using deep learning.  Typically framed as a multilabel classification problem, researchers have trained Convolutional Neural Networks , Recurrent Neural Networks , and Transformer models to predict ICD-9 codes from patient discharge summaries.  These models have outperformed rule-based approaches and those utilizing conventional algorithms such as Logistic Regression, Support Vector Machines, Random Forests etc., achieving competitive micro F1-scores in the range 42\% - 68\%. Amongst these models, those based on CNNs have achieved the best performance.   Neural network models have revolutionized the field of NLP and SOTA models for various NLP tasks involve deep neural network models such as BERT, Bidirectional RNN or CNN-based methods. Recent works  have shown a particular vulnerability of such deep models to adversarial examples that are often produced by adding small and imperceptible perturbations to the input data. The state of the art models of NLP are no exceptions to such perturbations.  provides a review of different adversarial attacks and defense strategies in the NLP literature. Based on granularity of the perturbation, adversarial attack strategies in NLP can be classified into three types - character-level attacks, word-level attacks and sentence-level attacks. In a character-level attack strategy, the model induces noise at the character level. Character-level noise can be induced due to naturally occurring reasons such as typos and misspellings or due to intentional modification by a malicious third-party.  are some of the existing character-level attack strategies in NLP. To accurately model the naturally occurring typos,  restrict the typos distribution based on the character constraints found in a standard English keyboard. We follow this strategy in our work. Furthermore, we assume a white-box setting where the adversary has access to gradients of the loss function wrt to the model inputs. To our knowledge, this is the first work to investigate the effects of adversarial samples in clinical NLP domain.    
","   Manual annotation of ICD-9 codes is a time consuming and error-prone process. Deep learning based systems tackling the problem of automated ICD-9 coding have achieved competitive performance. Given the increased proliferation of electronic medical records, such automated systems are expected to eventually replace human coders. In this work, we investigate how a simple typo-based adversarial attack strategy can impact the performance of state-of-the-art models for the task of predicting the top 50 most frequent ICD-9 codes from discharge summaries. Preliminary results indicate that a malicious adversary, using gradient information, can craft specific perturbations, that appear as regular human typos, for less than $3\%$ of words in the discharge summary to significantly affect the performance of the baseline model.",23
"   Systematic Generalization has been characterized as the capacity to understand and produce a potentially infinite number of novel combinations from known components . For example, in Figure, a model could be exposed to a set of facts , but not to all the possible facts that can be inferred by combination of the known components . More recent work has examined systematic generalization in terms of the ability of ``a model to manipulate concepts in new combinations after being trained on all concepts, but only on a limited set of their combinations'' . This view of systematic generalization shifts emphasis from reasoning to learning. %If a model is able to perfectly accomplish a task by leveraging existing facts to infer new ones, we deem the model is generalizing systematically. Here we examine systematic generalization through measuring the ability of a model to reason about new inference step combinations despite being trained on a limited subset of them. %, and conditioning upon a small subset of active relationships at inference time.   Recent developments in natural language processing  have shown that Transformer  Language Models  are able to capture linguistic knowledge , and yield state-of-the-art performance for many NLP tasks , including but not limited to answering reading comprehension questions  and generating factual knowledge  with little to no task supervision. These models are optimized on large corpora to predict the next words or a set of masked words in a sentence. While yielding impressive results, it is not clear if TLMs rely on many superficial patterns in the data or if they actually learn re-usable skills, enabling them to generalize to new tasks by leveraging the compositionality of those skills . Training on massive data can give certain advantages with respect to understanding the meanings of words, but we conjecture that such data gives models less experience with reasoning over inference chains.  [14]{R}{0.25\textwidth}                   In our work, we study the less understood issues related to how well TLMs are able to perform long chains of reasoning. In particular, we use TLMs for the task of theorem proving, where facts and proofs are specified in natural language. Using theorem proving, we test if TLMs can generate interpretable proofs with logically consistent language modeling as their main objective. % In this setting, language models have various attractive properties: they require no logical rule engineering while still being interpretable, do not need human annotations, and are easy to extend to more data. % Language models have many advantages over theorem provers: they require no rule engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. In particular, we study their behavior as logical reasoners on text by analyzing the generated proofs and the final answer. This setup allows us to evaluate the reasoning and generalization capabilities of TLMs. Recent work such as  suggest that language models can be treated as knowledge bases. This directly motivates us to investigate if language models can also learn certain reasoning strategies. Studying these abilities can give us insights to facilitate the use of such models as dynamic knowledge bases that could infer new knowledge even if it is not seen during pre-training.  For natural language theorem proving, we use the question answering CLUTRR benchmark suite  to perform controlled studies. This dataset is of interest because:  the compositional nature of tasks involved make it well suited for evaluating systematic generalization, and  each question--answer pair is accompanied by a proof that can be used to explain how to arrive at the answer. %Our goal is not to obtain state-of-the-art results on this dataset, rather, We use this dataset as a medium to understand the reasoning capacity of TLMs.  Our experiments reveal the following: [itemsep=0pt,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]    to proofs requiring more proof steps than seen during training time.   %In contrast, they suffer less severely to intrapolate to proofs requiring less proof steps than what it encountered during training.   % However, they do interpolate to unseen proofs requiring less proof steps than the most complex proof seen in training.     To the best of our knowledge, we are the first to use a language modeling objective to do interpretable theorem proving with a Transformer. We hope that this work can shed some light on the reasoning capacity of TLMs and inspire future research to design models with greater reasoning capacity.   
"," We are interested in understanding how well Transformer language models  can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies.",24
" Singing voice synthesis  aims to synthesize high-quality and expressive singing voices based on musical score information, and attracts a lot of attention in both industry and academia ~. Singing voice synthesis shares similar pipeline with text to speech synthesis, and has achieved rapid progress~ with the techniques developed in text to speech synthesis~.   Most previous works on SVS~ adopt the same sampling rate  as used in text to speech, where the frequency bands or sampling data points are not enough to convey expression and emotion as in high-fidelity singing voices. However, simply increasing the sampling rate will cause several challenges in singing modeling. First, the audio with higher sampling rate contains wider and higher frequency bands\footnote{According to Nyquist-Shannon sampling theorem~, a sampling rate  can cover the frequency band up to . Therefore, the frequency band for the audio with 48kHz sampling rate spans from 0$, which also increases the difficulty of vocoder modeling in time domain. As a consequence, even if some previous works~ adopt higher sampling rate , they either leverage coarse-grained MFCC~ as acoustic features in slow autoregressive neural vocoder~, or use non-neural vocoder such as Griffin-Lim~ and WORLD~ to generate waveform, which do not fully exploit the potential of high sampling rate and thus cannot yield good voice quality.  In this paper, we develop HiFiSinger, an SVS system towards high-fidelity singing voices. HiFiSinger adopts FastSpeech~ as the acoustic model and Parallel WaveGAN~ as the vocoder since they are popular in speech synthesis~ to ensure fast training and inference speed and also high quality. %. Instead of using Griffin-Lim, WORLD or autoregressive neural model such as WaveRNN and WaveNet as the vocoder, HiFiSinger leverages  To address the challenges of high sampling rate in singing modeling , we design multi-scale adversarial training on both acoustic model and vocoder, and introduce several additional systematic designs and findings that are crucial to improve singing modeling: [leftmargin=*]   We conduct experiments on our internal singing voice synthesis datasets that contain 11 hours high-fidelity singing recordings with 48kHz sampling rate. Experiment results demonstrate the advantages of our developed HiFiSinger over previous singing voice synthesis system. Further ablation studies verify the effectiveness of each design in HiFiSinger to generate high-fidelity voices.  
"," High-fidelity singing voices usually require higher sampling rate  with large range of frequency to convey expression and emotion. However, higher sampling rate causes the wider frequency band and longer waveform sequences and throws challenges for singing modeling in both frequency and time domains in singing voice synthesis . Conventional SVS systems that adopt moderate sampling rate  cannot well address the above challenges. In this paper, we develop HiFiSinger, an SVS system towards high-fidelity singing voice using 48kHz sampling rate. HiFiSinger consists of a FastSpeech based neural acoustic model and a Parallel WaveGAN based neural vocoder to ensure fast training and inference and also high voice quality. To tackle the difficulty of singing modeling caused by high sampling rate , we introduce multi-scale adversarial training in both the acoustic model and vocoder to improve singing modeling. Specifically, 1) To handle the larger range of frequencies caused by higher sampling rate , we propose a novel sub-frequency GAN  on mel-spectrogram generation, which splits the full 80-dimensional mel-frequency into multiple sub-bands  and models each sub-band with a separate discriminator. 2) To model longer waveform sequences caused by higher sampling rate, we propose a multi-length GAN  for waveform generation to model different lengths of waveform sequences with separate discriminators. 3) We also introduce several additional designs and findings in HiFiSinger that are crucial for high-fidelity voices, such as adding F0  and V/UV  as acoustic features, choosing an appropriate window/hop size for mel-spectrogram, and increasing the receptive field in vocoder for long vowel modeling in singing voices. Experiment results show that HiFiSinger synthesizes high-fidelity singing voices with much higher quality: 0.32/0.44 MOS gain over 48kHz/24kHz baseline and 0.83 MOS gain over previous SVS systems. Audio samples are available at \url{https://speechresearch.github.io/hifisinger/}.",25
" Deep speech representation learning has been the subject of a large number of past works. Many techniques have been developed and employed for extracting representations from speech for related tasks such as speaker recognition  and speech emotion recognition  using deep learning. A significant number of these deep learning models have been based on Convolutional Neural Networks  for SR  and SER . The most common approach to training CNN models for speech-related tasks is to use time-frequency inputs such as spectrograms derived from raw audio signals. Given sufficient data, such deep learning models enable the extraction of better speech representations compared to other methods such as i-Vectors .   Attention mechanisms have been shown to have a positive impact on extracting effective deep representations from input data, for instance speech signals. Considerable improvements in accuracy of emotion recognition models  and speaker recognition models  are some of the examples that demonstrate the potential benefits of using attention mechanisms for representation learning.   Attention models uphold a memory-query paradigm, where the memory is a set of information items such as CNN embeddings of a region of the spectral representation in speech-related tasks , or a part of the utterance embedded by a recurrent cell in a recurrent neural network  . The query is derived from a hidden state of the model from either the same modality or a different one . The majority of attention models used in speech-related tasks, use features extracted from utterances using a deep neural network as the information items or memory, and the last hidden layer of the model as the query . The general purpose of an attention model in generating deep representations of speech signals is to focus on each information item individually.   The information items considered in an attention model define the granularity of what the model can focus on. The spectral representation of an utterance enables deep learning models to consider fine-grained features such as frequency bins in very short time-frames. However, typical attention models used on audio signals utilize an embedding obtained from a CNN model as the memory and the final embedding of the model as query. Using embeddings obtained from CNNs, limits the granularity of the attention models to large regions of the spectral representation. On the other hand, improving the granularity of CNN embeddings of an utterance leads to very large attention models which are harder to train and prone to over-fitting. While there have been a number of studies investigating various attention models using CNN embeddings utterances , very limited number of studies aim to use more fine-grained attention models on spectral representation of the utterance.   In this paper, we address the challenge of improving granularity of attention models by introducing a fine-grained attention mechanism for audio signals. This mechanism enables deep learning models to focus on individual frequency bins of a spectrogram without the drawbacks of having very complex models that typically involve large number of parameters. The aim of this model is to attend to each frequency bin in the spectrogram representation in order to boost the contribution of most salient bins. This mechanism also helps reduce the importance of bins with no useful information leading to more accurate representations, which can also lead to more robustness with respect to existing noise in the input audio. The performance of the proposed attention mechanism has been tested using a select set of most prominent CNN architectures on two tasks of SR and SER. The experimental results show that deploying the fine-grained frequency attention mechanism improves the performance of all the benchmark networks substantially while being less impacted by added noise.   Our contributions in this paper are as follows:        The rest of this paper is organized as follows. First, we discuss the related work in the area of speech representation learning followed by particular approaches that have used attention mechanisms for this purpose. Next, we present the proposed attention mechanism. In the following section, we discuss the experiments along with implementation details. Next, we provide the results of our work. And finally, we summarize and conclude the paper.   
"," Deep learning techniques have considerably improved speech processing in recent years. Speech representations extracted by deep learning models are being used in a wide range of tasks such as speech recognition, speaker recognition, and speech emotion recognition. Attention models play an important role in improving deep learning models. However current attention mechanisms are unable to attend to fine-grained information items. In this paper we propose the novel Fine-grained Early Frequency Attention  for speech signals. This model is capable of focusing on information items as small as frequency bins. We evaluate the proposed model on two popular tasks of speaker recognition and speech emotion recognition. Two widely used public datasets, VoxCeleb and IEMOCAP, are used for our experiments. The model is implemented on top of several prominent deep models as backbone networks to evaluate its impact on performance compared to the original networks and other related work. Our experiments show that by adding FEFA to different CNN architectures, performance is consistently improved by substantial margins, even setting a new state-of-the-art for the speaker recognition task. We also tested our model against different levels of added noise showing improvements in robustness and less sensitivity compared to the backbone networks.",26
" %%%%%% % TH % % First we mention the recent progress of TTS systems due to seq2seq and end-to-end training.  Text-to-speech  systems have made great strides with the introduction of sequence-to-sequence  neural models, combined with end-to-end trainable architectures . Neural models typically take character as input and learn a direct mapping to spectrogram or waveform output, without the need for feature engineering.  % Then we explain that the common paradigm is sentence-based synthesis and start defining the vocabulatory for past/future, left/right/full context.   However, most of these neural TTS systems are designed to work at the sentence level, i.e. the synthetic speech signal is generated after the user has typed a complete sentence. When processing a given word, the system can thus rely on its full linguistic context  to build its internal representation. % Now we explain why these paradim is problematic in several context   Despite its ability to generate high-quality speech, this synthesis paradigm is not ideal for several applications. For example, when used as a substitute voice by people with severe communication disorders or integrated in a dialog system , the system's need to wait until the end  of a sentence introduces a latency which might be disruptive to conversational flow and system interactivity.  % Now we introduce iTTS  Incremental TTS  aims to address these issues by synthesizing speech on-the-fly, that is by outputting audio chunks as soon as a new word  become available. This task is particularly challenging since producing speech without relying on the full linguistic context can result in both segmental  and supra-segmental  errors .   % Now we present the state of the art in iTTS  % first for HMM-based synthesis Early iTTS systems were developed in the context of HMM-based speech synthesis . In this paradigm, models are trained on a set of explicit linguistic features . The authors of  developed coping mechanisms to handle missing features when making predictions for iTTS: unknown future context information is replaced with the most common values for these features at inference time in , whereas uncertainty on those features is explicitly integrated at training time by . In , an adaptive decoding policy based on the online estimation of the stability of the linguistic features is proposed: the synthesis of a given word is delayed if its part-of-speech  is likely to change when additional  words are added.    Several strategies have been proposed to reduce the latency of a sequence-to-sequence model with input text for neural machine translation  or incremental speech translation. However, only a few studies have attempted to adapt these models for iTTS . The authors of  proposed an approach that consists in  marking three subunits within the training sentences using start, middle and end tags,  training a Tacotron 2 TTS model with these tags so it learns intrasentential boundary characteristics, and   synthesizing sentences by inputting chunks of length  words  with the appropriate middle or end tag. An alternate policy  reported in  \laurent{ consists in having access to a future context of  input tokens while generating speech output. They also rely on the soft attention to learn the relationship between the predicted spectrogram and the currently available source text.} %triggering the synthesis when the attention weights of the Tacotron decoder have moved past the increment the model is processing.  These two approaches give promising results but introduce a fixed size  latency.   %%%1) splitting the training corpus into incremental units of fixed size   2) training a standard Tacotron 2 while considering each incremental unit as a training sequence, 3) triggering the synthesis each time a new incremental unit is available.   The goal of the present paper is to pave the way toward an adaptive decoding policy for a neural iTTS. Similarly to the HMM-based iTTS system described in , the envisioned neural iTTS is expected to modulate the lookahead  by the uncertainty on some features due to the lack of future context. However, the gain in naturalness provided by end-to-end models  is also accompanied by reduced interpretability. Because of the black box nature of the models, studying the importance of missing features is a challenging task. To address this, %we finely investigate how a neural TTS such as the Tacotron2 exploits the future context to build its internal representations. We  we analyse the evolution of the encoder representations of a neural TTS  when words are incrementally added . We also investigate which text features are the most influential on this evolution towards the final encoder representation. Finally, we evaluate the effect of the lookahead at the perceptual level using a MUSHRA listening test.    %The rest of this paper is organized as follows: we describe our methodology and experimental material in section .  We follow with results and discussion in section . We finally present our conclusions in section .  %%%%% %Most current text-to-speech  systems rely on full sentence input to produce natural sounding speech. This type of system however is not ideal for applications where the user wants to communicate in real time, like in simultaneous speech interpretation or assistive technologies for the speech impaired. \laurent{In those situations, the system has to produce an  output before it has access to the entire input. To deal with this low latency constraint, several strategies were proposed for encoder-decoder models with input text but only a few works have investigated incremental speech translation or incremental text-to-speech synthesis .  This latter task is particularly challenging since}  %with automatic interpreters or assistive technologies for the speech impaired; the latency that waiting until the end of a sentence implies is very disruptive to conversational flow. Conversely, producing speech before the full context is known can result in segmental errors and unnatural prosody.  %\laurent{In this work we study the behavior of a neural sequence-to-sequence TTS system when used in an incremental mode, i.e. when generating speech output for token , the system has access to  tokens from the text sequence.}  %To find a balance between latency and naturalness, we measure the relative importance of different degrees of future context by studying the encoder representations of the Tacotron 2  system.  We believe that by measuring the changes in representation for different values of lookahead parameter  it might be possible define an adaptive decoding policy for iTTS. We also estimate how much future context  is needed to decode a cohesive audio output.  %The rest of this paper is organized as follows: we present in section  related work pertaining to iTTS. We describe our methodology and experimental material in section .  We follow with results and discussion in section . We finally present our conclusions in section .   %
"," In incremental text to speech synthesis , the synthesizer produces an audio output before it has access to the entire input sentence. In this paper, we study the behavior of a neural sequence-to-sequence TTS system when used in an incremental mode, i.e. when generating speech output for token $n$, the system has access to $n+k$ tokens from the text sequence. We first analyze the impact of this incremental policy on the evolution of the encoder representations of token $n$ for different values of $k$ . The results show that, on average, tokens travel $88\%$ of the way to their full context representation with a one-word lookahead and $94\%$ after 2 words. We then investigate which text features are the most influential on the evolution towards the final representation using a random forest analysis. The results show that the most salient factors are related to token length. We finally evaluate the effects of lookahead $k$ at the decoder level, using a MUSHRA listening test. This test shows results that contrast with the above high figures: speech synthesis quality obtained with 2 word-lookahead is significantly lower than the one obtained with the full sentence.",27
" Text summarization aims to produce condensed summaries covering salient and non-redundant information in the source documents. Recent studies on single-document summarization  benefit from the advances in neural sequence learning  as well as pre-trained language models  and make great progress.  However, in multi-document summarization  tasks, neural models are still facing challenges and often underperform classical statistical methods built upon handcrafted features.    We observe two major challenges when adapting advanced neural SDS methods to MDS:   Large search space.  MDS aims at producing summaries from multiple source documents, which exceeds the capacity of neural SDS models  and sets learning obstacles for adequate representations, especially considering that MDS labeled data is more limited. For example, there are 287K training samples  on the CNN/Daily Mail SDS dataset and only 30 on the DUC 2003 MDS dataset .  High redundancy. In MDS, the same statement or even sentence can spread across different documents. Although SDS models adopt attention mechanisms as implicit measures to reduce redundancy, they fail to handle the much higher redundancy of MDS effectively .       There have been attempts to solve the aforementioned challenges in MDS. Regarding the large search space, prior studies  perform sentence filtering using a sentence ranker and only take top-ranked  sentences. However, such a hard cutoff of the search space makes these approaches insufficient in the exploration of the  labeled data and limited by the ranker since most sentences are discarded,\footnote{ is set to 7 in~ and 15 in~. One document set in DUC 2004, for example, averages 265.4 sentences.} albeit the discarded sentences are important and could have been favored. As a result, although these studies perform better than directly applying their base SDS models  to MDS,  they do not outperform state-of-the-art MDS methods.  Regarding the high redundancy,  various redundancy measures have been proposed, including heuristic post-processing such as counting new bi-grams  and cosine similarity, or dynamic scoring that compares each source sentence with the current summary like Maximal Marginal Relevance .   Nevertheless, these methods still use lexical features without semantic representation learning. One extension of these studies uses capsule networks to improve redundancy measures. However, its capsule networks are pre-trained on SDS and fixed as feature inputs of classical methods  without end-to-end representation learning.  In this paper, we present a deep RL framework, MMR-guided Reinforcement Learning  for MDS, which unifies advances in SDS and one classical MDS approach, MMR through end-to-end learning. \ours addresses the MDS challenges as follows:  \ours overcomes the large search space through soft attention. Compared to hard cutoff, our soft attention favors top-ranked candidates of the sentence ranker . However, it does not discard low-ranked ones, as the ranker is imperfect, and those sentences ranked low may also contribute to a high-quality summary. Soft attention restrains the search space while allowing more exploration of the limited labeled data, leading to better representation learning. Specifically, \ours infuses the entire prediction of MMR into  its neural module by attending  to important sentences and downplaying the rest instead of completely discarding them.  \ours resolves the high redundancy of MDS in a unified way: the explicit redundancy measure in MMR is incorporated into the neural representation of the current state, and the two modules are coordinated by RL reward optimization, which encourages non-redundant summaries.  We conduct extensive experiments and ablation studies to examine the effectiveness of \ours. Experimental results show that \ours achieves state-of-the-art performance on the DUC 2004 and TAC 2011 datasets . A comparison between various combination mechanisms demonstrates the benefits of soft attention in the large search space of MDS . In addition, ablation and manual studies confirm that \ours is superior to applying either RL or MMR to MDS alone, and MMR guidance is effective for redundancy avoidance .    We present an RL-based MDS framework that combines the advances of classical MDS and neural SDS methods via end-to-end learning.   We show that our proposed soft attention is better than the hard cutoff of previous methods for learning adequate neural representations. Also, infusing the neural representation of the current summary with explicit MMR measures significantly reduces summary redundancy.  We demonstrate that \ours achieves new state-of-the-art results on benchmark MDS datasets.    
"," While neural sequence learning methods have made significant progress in single-document summarization , they produce unsatisfactory results on multi-document summarization . We observe two major challenges when adapting SDS advances to MDS:  MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations;  MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present \ours, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS.  \ours casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that \ours achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.\footnote{Code can be found at \url{https://github.com/morningmoni/RL-MMR}.}",28
" Natural language generators  for task-oriented dialogue take meaning representations  as inputs, i.e. a set of dialogue acts with attributes and their values, and output natural language utterances realizing the MR.  Current NLGs are trained end-to-end with a corpus of MR/utterance pairs where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. However, when building an NLG for a new domain ontology, it should be possible to re-use data built on existing domain ontologies.  If this were possible, it would speed up development of new dialogue systems significantly.  [t!bh]  {p{.15in}|p{.49in}|p{2.8in}|p{2.2in}} \toprule recommend[yes]}, inform & \underline {{I suggest you go to [{ and \underline{atmosphere} \underline{are all excellent}}, even if it is {expensive}. Its in [{, area[area],    {near[point-of-interest]})  &  [{]}} in [{]}}. It has a {\underline{high customer rating}}.   \\ , {eatType[restaurant-type]},  {food = excellent}, location[area], {near[point-of-interest]}, {customer-rating[high]}, {d\'ecor = excellent, service=excellent, price=expensive}) &  {[{.  It is a {[{ in [{]} with a {high customer rating}, but it is {expensive}. \\      blue} and NYC is in {red}. Some attributes are shared    between both sources: here the unique dialogue acts and attributes    for each source are underlined in E1 and E2.  E3 illustrates an MR    from the target test set that we dub COM. All the MRs in COM combine dialogue acts    and attributes from E2E and NYC. There is no training data     corresponding to E3.     %: the goal of the task is to re-use    %the existing training data from E2E and NYC %and train an NLG that    %can generalize to unseen combinations such %as shown in E3.      The MRs    illustrate how some attribute values, e.g. {  Here we experiment with one version of this task by building a new domain ontology based on { ontology not seen in the training data, e.g. for MRs that specify values for {, {.  Figure illustrates this task. Example E1 is from a training set referred to as NYC, from previous work on controllable sentence planning in NLG , while E2 is from the E2E NLG shared task . As we describe in detail in Section, E1 and E2 are based on two distinct ontologies.  Example E3  %in Figure  illustrates the task addressed in this paper: we create a test set of novel MRs for the combined ontology, and train a model to generate high quality outputs where individual sentences realize attributes from both ontologies.  To our knowledge, this is a completely novel task.  While it is common practice in NLG to construct test sets of MRs that realize attribute combinations not seen in training, initial experiments showed that this task is surprisingly adversarial.  However, methods for supporting this type of generalization and extension to new cases would be of great benefit to  task-oriented dialogue systems, where it is  common to start with a restricted set of attributes and then enlarge the domain ontology over time. New attributes are constantly being added to databases of restaurants, hotels and other entities to support better recommendations and better search.  Our experiments test whether existing data that only covers a subset of attributes can be used to produce an NLG for the enlarged ontology.   We describe below how we create a test set --- that we call {\sc com} --- of combined MRs to test different methods for creating such an NLG.  A baseline sequence-to-sequence NLG model has a slot error rate  of .45 and only produces semantically perfect outputs 3.5\% of the time. To improve performance, we experiment with three different ways of conditioning the model by incorporating { produce many {    We start in Section by defining the task in more detail, describe our models and metrics in Section, and results in Section.  We discuss related work throughout the paper where it is most relevant and in the conclusion in Section.  
"," Natural language generators  for task-oriented dialogue typically take a meaning representation  as input, and are trained end-to-end with a corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies.   Here we explore, for the first time, whether  it  is possible to train  an NLG for a new { ontology. We create a new, larger { method that identifies  model outputs, automatically  constructs a corrected MR input to form a new  training pair, and then repeatedly adds these new instances back into the training data. %that combine attributes from both sources %and then automatically construct an MR that matches the string that %was actually generated .   %We repeatedly construct and add these %new instances back into training, resulting in a self-trained %model that produces semantically perfect outputs 83\% of the time. %We repeatedly construct and add these %new instances back into training, resulting  We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4\% improvement over the baseline model.  %can produce semantically perfect outputs 83\% of the time. %improves the proportion of semantically perfect outputs for the new combined ontology  from 5.5\% to 83\%.  We also report a human qualitative evaluation of the final  model showing that it achieves high naturalness, semantic coherence and grammaticality.",29
" In recent years, neural LMs  have shown profound abilities to generate texts that could be almost indistinguishable from human writings . Neural LMs could be used to generate concise summaries , coherent stories , and complete documents given prompts . It is natural to question their source and extent of rhetorical knowledge: What makes neural LMs articulate, and how?  While some recent works query the linguistic knowledge , this open question remain unanswered. We hypothesize that contextualized neural LMs encode rhetorical knowledge in their intermediate representations, and would like to quantify the extent they encode rhetorical knowledge.  To verify our hypothesis, we hand-craft a set of 24 rhetorical features including those used to examine rhetorical capacities of students , and evaluate how well neural LMs encode these rhetorical features in the representations while encoding texts.  Recent work has started to evaluate encoded features from hidden representations. Among them, probing  has been a popular choice. Previous work probed morphological , agreement , and syntactic features . Probing involves optimizing a simple projection model from representations to features. The loss of this optimization measures the difficulty to decode features from the representations.   In this work, we use a probe containing self attention mechanism. We first project the variable-length embeddings to a fixed-length latent representation per document. Then, we apply a simple diagnostic classifier to detect rhetorical features from this latent representation. This design of probe reduces the total number of parameters, and enable us to better understand each model's ability to encode rhetorical knowledge. We find that: [noitemsep]       These observations allow us to investigate the mechanisms of neural LMs to better understand the degree to which they encode linguistic knowledge. We demonstrate how discourse-level features can be queried and analyzed from neural LMs. All of our code and parsed tree data will be available at github.   
"," Recently, neural language models  have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, to date, there has been no analysis of the inter-sentential, rhetorical knowledge.  In this paper, we propose a method that quantitatively evaluates the rhetorical  capacities of neural LMs. We examine the capacities of neural LMs understanding the rhetoric of discourse by evaluating their abilities to encode a set of linguistic features derived from Rhetorical Structure Theory . Our experiments show that BERT-based LMs outperform other Transformer LMs, revealing the richer discourse knowledge in their intermediate layer representations. In addition, GPT-2 and XLNet apparently encode less rhetorical knowledge, and we suggest an explanation drawing from linguistic philosophy. Our method presents an avenue towards quantifying the rhetorical capacities of neural LMs.",30
"  Our WeChat AI team participates in the WMT 2020 shared news translation  task on ChineseEnglish. In this year閳ユ獨 translation task, we mainly focus on exploiting several effective model architectures, better data augmentation, training and model ensemble strategies.  For model architectures, we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with larger filter-size and the average attention based transformer. For the RNMT, we use the deep transition based DTMT model. We finally ensemble four kinds of models in our system.  For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method to leverage the target side monolingual data and the knowledge distillation method to leverage source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source side monolingual data and golden parallel data.  Furthermore, data augmentation methods, including noisy fake data and sampling, are used for training more robust NMT models. %We also apply these techniques on the corresponding side of golden parallel data.  For training strategies, we mainly focus on the parallel scheduled sampling, the target denoising and minimum risk training algorithm for in-domain finetuning.  We also exploit a self-bleu  based model ensemble approach to enhance our system. As a result, our constrained ChineseEnglish system achieves the highest case-sensitive BLEU score among all submitted systems.  In the remainder of this paper, we start with an overview of model architectures in Section.  Section describes the details of our systems and training strategies.  Then Section shows the experimental settings and results.  Finally, we conclude our work in Section.  
"," We participate in the WMT 2020 shared news translation task on Chinese$\to$English. Our system is based on the Transformer~ with effective variants and the DTMT~ architecture. In our experiments, we employ data selection, several synthetic data generation approaches ,  advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese$\to$English system achieves 36.9 case-sensitive BLEU score, which is the highest among all submissions.",31
"  Social media has become an essential element of our society by which people communicate and exchange information on a daily basis. The strong influence of social media on internet users has been of great benefit to many individuals, businesses, and organizations. Many companies and organizations nowadays use social media to reach customers, promote products, and ensure customer satisfaction. Despite the benefits associated with the widespread use of social media, they remain vulnerable to ill-intentioned activities, as the openness, anonymity, and informal structure of these platforms have contributed to the spread of harmful and violent content. \par  Although social media service providers have policies to control these ill-intentioned behaviors, these rules are rarely followed by users. Social media providers also allow their users to report any inappropriate content, but unreported content may not be discovered due to the huge volume of data on these platforms. Some countries have restricted the use of social media, and others have taken legal action regarding violent or harmful content that might target particular individuals or communities. However, these violations might end up unpunished due to the anonymous nature of these platforms, allowing ill-intentioned users to fearlessly share harmful content by using nicknames or fake identities. One of the most-shared harmful content on social media is hate content, which might take different forms such as text, photos, and/or video. Hate speech is any expression that encourages, promotes, or justifies violence, hatred, or discrimination against a person or group of individuals based on characteristics such as color, gender, race, sexual orientation, nationality, religion, or other attributes. Online hate speech is rapidly increasing over the entire world, as nearly \% of the world閳ユ獨 population  communicates on social media. Studies have shown that nearly \% of Americans have experienced online hate and harassment. This result is \% higher than the results of a comparable questionnaire conducted in  . For younger people, the results show that \% of teenagers frequently encounter hate speech on social media.  \par   One of the most dangerous and influential forms of online hate speech is led and spread by supporters of extreme ideologies who target other racial groups or minorities. White supremacists are one of the ideological groups who believe that people of the white race are superior and should be dominant over people of other races; this is also referred to as white nationalism in more radical ideologies. White supremacists claim that they are undermined by dark skin people, Jews, and multicultural Muslims, and they want to restore white people閳ユ獨 power, violently if necessary. They have also claimed responsibility for many violent incidents that happened in the s, including bank robberies, bombings, and murders. The white supremacist ideology has been adopted by both right-wing and left-wing extremists who combine white supremacy with political movements. \par   White supremacist hate speech has become a significant threat to the community, either by influencing young people with hateful ideas or by creating movements to implement their goals in the real world. A study has also suggested links between hate speech and hate crimes against others . Several recent brutal attacks have also been committed by supporters of radical white supremacists who were very active members on social media. The mass shootings in New Zealand, Texas, and Norway were committed by white supremacists who had shared their opinions and ideologies on social media. The attacker of two mosques in Christchurch, New Zealand, was a 28 year old man who identified himself as a white nationalist hero, and posted a manifesto that discussed his intent to kill people as a way to reinforce the sovereignty of white extremists. From a psychological point of view, any violent attack must be preceded by warning behaviors, which includes any behavior that shows before a violent attack that is associated with it, and can in certain situations predict it. Warning behaviors can be either real-world markers  or linguistic markers or signs  which can happen in real life and/or online.  \par   Automatic detection of white supremacist content on social media can be used to predict hate crimes and violent events. Perpetrators can be caught before attacks happen by examining online posts that give strong indications of an intent to make an attack. Predicting violent attacks based on monitoring online behavior would be helpful in crime prevention, and detecting hateful speech on social media will also help to reduce hatred and incivility among social media users, especially younger generations. \par  Studies have investigated the detection of different kinds of hate speech such as detecting cyberbullying , offensive language  , or targeted hate speech in general by distinguishing between types of hate speech and neutral expressions. Others have dealt with the problem by detecting a specific types of hate speech, such as anti-religion, jihadist, sexist, and racist. However, less attention has been given to detecting white supremacism in particular, with limited studies.   \par  White supremacist extremists tend to use rhetoric   in their language. They also use specific vocabulary, abbreviations, and coded words to express their beliefs and intent to promote hatred or encourage violence to avoid being detected by traditional detection methods. They mostly use hate speech against other races and religions, or claim that other races are undermining them. Figure shows an example of a white supremacist tweet.  \par    In this paper, we aim to detect white supremacist tweets based on textual features by using deep learning techniques. We collected about  tweets from white supremacist accounts and hashtags to extract word embeddings, and then we labeled about  subsets of the data corpus to build a white supremacist dataset. We applied two approaches: the first uses domain-specific word embedding learned from the corpus and then classifies  tweets using a Bidirectional LSTM-based deep model. This approach is evaluated on multiple dataset and achieved different results depending on the datasets that ranged from a \% to a \% F1-score. The second approach uses a pre-trained language model that is fine-tune on the white supremacist dataset using Neural Network dense layer. The BERT language model F1-scores ranged from \% to \%. Thus, the research contribution can be summarized as follow:    \par  The rest of the paper proceeds with the Background Section , which provides information on the methodology used, related studies in the Literature Review section , a detailed description of methods in the Methodology section , details of the used datasets in the Dataset section , specifications of the methodologies and the results of each approach in the Experiments and Results section , observations and analysis of the performance of each approach in the Discussion section , and finally, the Conclusion and Future Work section .    
","  White supremacists embrace a radical ideology that considers white people superior to people of other races. The critical influence of these groups is no longer limited to social media; they also have a significant effect on society in many ways by promoting racial hatred and violence. White supremacist hate speech is one of the most recently observed harmful content on social media. Traditional channels of reporting hate speech have proved inadequate due to the tremendous explosion of information, and therefore, it is necessary to find an automatic way to detect such speech in a timely manner. This research investigates the viability of automatically detecting white supremacist hate speech on Twitter by using deep learning and natural language processing techniques. Through our experiments, we used two approaches, the first approach is by using domain-specific embeddings which are extracted from white supremacist corpus in order to catch the meaning of this white supremacist slang with bidirectional Long Short-Term Memory  deep learning model, this approach reached a 0.74890 F1-score. The second approach is by using the one of the most recent language model which is BERT, BERT model provides the state of the art of most NLP tasks. It reached to a 0.79605 F1-score. Both approaches are tested on a balanced dataset given that our experiments were based on textual data only. The dataset was combined from dataset created from Twitter and a Stormfront dataset compiled from that white supremacist forum.",32
"   Graph Neural Networks  have in recent years been shown to provide a scalable and highly performant means of incorporating linguistic information and other structural biases into NLP models. They have been applied to various kinds of representations  and shown effective on a range of tasks, including relation extraction~, question answering~, syntactic and semantic parsing tasks~, summarization ~, machine translation~ and abusive language detection in social networks~.     While GNNs often yield strong performance, % such models are % complex, and it can be difficult to understand the `reasoning' behind their predictions. For NLP practitioners, it is highly desirable to know which linguistic information a given model encodes and how that encoding happens~. The difficulty in interpreting GNNs represents a barrier to such analysis. %  Furthermore,  this opaqueness decreases user trust% , impedes the discovery of harmful biases, and complicates error analysis% ~,   an issue for GNNs where seemingly small implementation differences can make or break models~.  In this work, we focus on {[topsep=0pt,itemsep=0pt]      across layers, as paths are one of the most natural ways of presenting GNN reasoning patterns to users;      to be applicable to modern GNN-based NLP models;     ~ as possible, providing insights into how the model truly arrives at the prediction.      A simple way to perform interpretation is to use  erasure search~, an approach wherein attribution happens by searching for a maximal subset of features which can be entirely removed without affecting model predictions. % The removal guarantees that all information about the discarded features is ignored by the model. This  contrasts with approaches which use heuristics to define feature importance, for example attention-based methods~ or back-propagation techniques~. They do not guarantee that the model ignores low-scoring features, attracting criticism in recent years . % The trust in erasure search is reflected in the literature through other methods % motivated as approximations of erasure~, or through new attribution techniques % evaluated using erasure search as ground truth~.  Applied to GNNs, erasure search would involve a search for the largest subgraph which can be completely discarded. Besides faithfulness considerations and conceptual simplicity, discrete attributions would also simplify the comparison of relevance between paths; this is in contrast to continuous attribution to edges, where it is not straightforward to extract and visualize important paths. Furthermore, in contrast to techniques based on artificial gradients~, erasure search would provide implementation invariance~. This is important in NLP, as models commonly use highly parametrized decoders on top of GNNs, e.g.~.   While arguably satisfying criteria  and  in our desiderata, erasure search unfortunately fails on tractability. In practical scenarios, it is infeasible, and even approximations, which remove one feature at a time~ and underestimate their contribution due to saturation~,  remain prohibitively expensive.   Our GraphMask aims at meeting the above desiderata by achieving the same benefits as erasure search in a scalable manner. That is, our method makes easily interpretable hard choices on whether to retain or discard edges such that discarded edges have no relevance to model predictions, while remaining tractable and model-agnostic~. GraphMask  can be understood as a differentiable form of subset erasure, where, instead of finding an optimal subset to erase for every given example, we learn an erasure function which predicts for every edge  at every layer  whether that connection should be retained. Given an example graph , our method returns for each layer  a subgraph  such that we can faithfully claim that no edges outside  influence the predictions of the model. To enable gradient-based optimization for our erasure function, we rely on sparse stochastic gates~.  In erasure search, optimization happens individually for each example. This can result in a form of overfitting where even non-superfluous edges are aggressively pruned, because a similar prediction could be made using an alternative smaller subgraph; we refer to this problem as hindsight bias. % Because our model relies on a parametrized erasure function rather than an individual per-edge choice, we can address this issue by amortizing parameter learning over a training dataset through a process similar to the readout bottleneck introduced in~. As we demonstrate in Section, this strategy avoids hindsight bias.   Our contributions are as follows: [nosep]      to analyse GNN models for two NLP tasks: semantic role labeling~ and multi-hop question answering~.    
"," Graph neural networks  have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs  contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. % Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected  $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.",33
"    [t]     ''         LUKE outputs contextualized representation for each word and entity in the text.         The model is trained to predict randomly masked words  and entities .         Downstream tasks are solved using its output representations with linear classifiers.}        Many natural language tasks involve entities, e.g., relation classification, entity typing, named entity recognition , and question answering . Key to solving such entity-related tasks is a model to learn the effective representations of entities. Conventional entity representations assign each entity a fixed embedding vector that stores information regarding the entity in a knowledge base  . Although these models capture the rich information in the KB, they require entity linking to represent entities in a text, and cannot represent entities that do not exist in the KB.  By contrast, contextualized word representations  based on the transformer , such as BERT , and RoBERTa , provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling. Many recent studies have solved entity-related tasks using the contextualized representations of entities computed based on CWRs . However, the architecture of CWRs is not well suited to representing entities for the following two reasons:  Because CWRs do not output the span-level representations of entities, they typically need to learn how to compute such representations based on a downstream dataset that is typically small.  Many entity-related tasks, e.g., relation classification and QA, involve reasoning about the relationships between entities. Although the transformer can capture the complex relationships between words by relating them to each other multiple times using the self-attention mechanism , it is difficult to perform such reasoning between entities because many entities are split into multiple tokens in the model. Furthermore, the word-based pretraining task of CWRs is not suitable for learning the representations of entities because predicting a masked word given other words in the entity, e.g., predicting ``Rings'' given ``The Lord of the [MASK]'', is clearly easier than predicting the entire entity.  In this paper, we propose new pretrained contextualized representations of words and entities by developing LUKE . LUKE is based on a transformer  trained using a large amount of entity-annotated corpus obtained from Wikipedia. An important difference between LUKE and existing CWRs is that it treats not only words, but also entities as independent tokens, and computes intermediate and output representations for all tokens using the transformer . Since entities are treated as tokens, LUKE can directly model the relationships between entities.  LUKE is trained using a new pretraining task, a straightforward extension of BERT's masked language model  . The task involves randomly masking entities by replacing them with  entities, and trains the model by predicting the originals of these masked entities. We use RoBERTa as base pre-trained model, and conduct pretraining of the model by simultaneously optimizing the objectives of the MLM and our proposed task. When applied to downstream tasks, the resulting model can compute representations of arbitrary entities in the text using  entities as inputs. Furthermore, if entity annotation is available in the task, the model can compute entity representations based on the rich entity-centric information encoded in the corresponding entity embeddings.  Another key contribution of this paper is that it extends the transformer using our entity-aware self-attention mechanism. Unlike existing CWRs, our model needs to deal with two types of tokens, i.e., words and entities. Therefore, we assume that it is beneficial to enable the mechanism to easily determine the types of tokens. To this end, we enhance the self-attention mechanism by adopting different query mechanisms based on the attending token and the token attended to.  We validate the effectiveness of our proposed model by conducting extensive experiments on five standard entity-related tasks: entity typing,  relation classification, NER,  cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset , relation classification on the TACRED dataset , NER on the CoNLL-2003 dataset , cloze-style QA on the ReCoRD dataset , and extractive QA on the SQuAD 1.1 dataset . We publicize our source code and pretrained representations at \url{https://github.com/studio-ousia/luke}.  The main contributions of this paper are summarized as follows: [leftmargin=10pt,topsep=1pt,itemsep=0pt]       
","     Entity representations are useful in natural language tasks involving entities.     In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer .     The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.     Our model is trained using a new pretraining task based on the masked language model of BERT .     The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia.     We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens  when computing attention scores.     The proposed model achieves impressive empirical performance on a wide range of entity-related tasks.     In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity , TACRED , CoNLL-2003 , ReCoRD , and SQuAD 1.1 .     Our source code and pretrained representations are available at \url{https://github.com/studio-ousia/luke}.",34
" Despite the success of self-supervised \pting over \light{large-scale `task-external' data} to learn NLP tasks in a `text-to-text' framework  there are three major, interdependent, open challenges. First, a reliance on large to Web-scale, `end-task external \pting data'  on extensive \pting hardware , create a need for more data efficient models . Second, ``current fine-tuned probes introduce uncontrolled external biases into the evaluation results of text encoders, whereas zero-shot probing avoids these biases'' . Third, concerns about unintended contra-minority biases and resource costs . This created calls for evaluation of   and ``closer to real-world'' long-tail settings, where learnable training signals  that \light{imbalanced, few or zero-shot learning become default settings} . Although improved zero-shot and rare phenomenon  prediction from very limited data is paramount in algorithmic bias and disease detection considerations, current \light{self-supervised} encoder \pting methods require increasingly larger \pting data in NLP  or vision . \light{Unfortunately, since the long-tail grows with data size, simply adding more data and compute only aggravates the bias against long-tail  information and resource cost or access concerns.} %  %  Contributions: To address the above text encoder \pting limitations and evaluation challenges,   and evaluate it under an appropriately small \pting data task  that follows a noisy long-tail class distribution seen in . CLESS proposes self-supervised \pting over pseudo label embeddings. This makes it inherently capable of  zero-shot prediction, unlike methods that rely on supervised \pting for zero-shot prediction  or self-supervision approaches that are incapable of zero-shot prediction  -- details in . By predicting labels as word embeddings and words as pseudo label embeddings, we can . % as text input  and text output  prediction. This trains an NLP task as a `text-to-text' objective like in , but learns to match `text-embedding to label-embeddings', where positive and negative pseudo or real labels are sampled for contrastive training of a single binary match classifier . This classifier is reusable for any unseen label tasks, where new labels are expressed in words and embedded via e.g.\ . % TODO insert mention that we evaluate in a time-split fashion to avoid overly optimistic evaluation as pointed out in https://arxiv.org/pdf/2102.01951.pdf Unlike , CLESS does not require external Web-scale \pting data and can effectively pretrain a text-encoder on 3 orders of magnitude smaller data than for example the size of the English Wikipedia -- details in .  Findings: As a result of contrastive, self-supervised \pting, CLESS boosts minority, long-tail class prediction performance and learning speed considerably -- see , . During few-shot learning, contrastive self-supervised \pting produces large performance improvements and doubled convergence, while also removing learning instability compared to training from scratch, especially in extreme few-shot settings . We also find that  increases zero to few-shot , end-task and long-tail performance over baselines that: are either optimized via generalization methods  or , i.e.\ without self-supervised pseudo label \pting -- details .  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
","  % ALTERNATIVE % core problem % For natural language processing `text-to-text' tasks, the prevailing approaches heavily rely on \pting large self-supervised models on massive external data sources, which led to exceptional \pting data requirements and a diminished ability to effectively pretrain on small data. However, fundamental \pting method capabilities like few to zero-shot learning or preserving minority concept  prediction performance along with accordingly designed evaluation scenarios remain open challenges.  % % core problem For natural language processing `text-to-text' tasks, the prevailing approaches heavily rely on \pting large self-supervised models on massive external data sources, which incurs exceptional \pting data requirements and a diminished ability to pretrain over small datasets. However, fundamental \pting method capabilities like few to zero-shot learning or preserving minority concept  prediction performance along with accordingly designed evaluation scenarios remain open challenges.  % solution We thus propose Contrastive Label-Embedding Self-Supervision  \pting, which enables , while still strongly improving fully supervised, long-tail, few-shot and self-supervised zero-shot learning abilities. % eval Accordingly, we analyse improvements in learning dynamics over baselines on a challenging long-tailed, low-resource, multi-label text classification scenario with noisy, highly sparse labels and many minority concepts.   % result  We find that long-tailed zero and few-shot learning markedly benefit from increasing `dataset-internal' self-supervised \pting signals, to help reduce the reliance on large external sources.",35
"  Modern methods of natural language processing  are based on complex neural network architectures, where language units are represented in a metric space . Such a phenomenon allows us to express linguistic features  mathematically.   The method of obtaining such representation and their interpretations were described in multiple overview works. Almeida and Xex\'eo surveyed different types of static word embeddings , and Liu et al.  focused on contextual representations found in the most recent neural models. Belinkov and Glass  surveyed the strategies of interpreting latent representation. Best to our knowledge, we are the first to focus on the syntactic and morphological abilities of the word representations. We also cover the latest approaches, which go beyond the interpretation of latent vectors and analyze the attentions present in state-of-the-art Transformer models. %analyzed matrix representation of the neural networks. %.    %\tltodo{Maybe use ToC as instroduction to section and remove them from here} %The survey is organized in the following way: %In Section, we introduce several types of NLP models that are going to be analyzed. Section shortly describes the metrics used to evaluate syntactic information captured by the models. The observations and results for static and contextual word embeddings are presented in Section. The observations on attention matrices for different Transformer architectures are described in Section. We summarize our findings in Section. %for attention matrices in Transformer models. %We conclude the survey by mentioning supervised approaches to enhance syntactic signal.   %
","  Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. %The syntax is captured by the natural language processing models even when not provided as a supervision signal. This %This phenomenon  indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. % This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. %This overview paper covers approaches to evaluating of syntactic information in the representation of words in neural networks. We compare the spectrum of model architectures and the training data. We mainly summarize research on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models.   %Particularly we consider corpora in one language, mainly English used for training Language Models, and multilingual data for Machine Translation Systems and Multilingual Language Models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.  % We hope that our comparison will help in finding pretrained model for transfer   % The survey covers the research on producing representation of language and evaluation of captured syntactic information. I focus on the works that do not use syntactic supervision during training of the representation, and are obtained on large mono or multilingual corpora.  % The aim of this work is to examine to what extent syntactic features can be extracted from plain text and how it can be compared to expert annotations.",36
" Texts represent the main source of knowledge for our society. However, they can be written in various manners, thus creating a barrier between the readers and the ideas they intend to convey. Therefore, document comprehension is the main challenge users have to overcome, by understanding the meaning behind troublesome words and becoming familiar with them. Complex Word Identification  is a task that intends to identify hard-to-understand tokens, highlighting them for further clarification and assisting users to grasping the contents of the document.  Motivation. Each culture includes exclusive ideas, available only for the ones who can pass the obstacle of language. However, properly understanding language can prove to be a difficult task. By identifying complex words, users can make consistent steps towards adapting to the culture and accessing the knowledge it has to offer. As an example, entries like ""mayoritariamente""  or ""gobernatura""  in the Spanish environment can create understanding problems for non-native Spanish speakers, thus requiring users to familiarize themselves with these particular terms.  Challenges. The identification task becomes increasingly more difficult, as proper complex word identification is not guaranteed. For example, if we use human identification techniques, language learners may consider a new word to be complex, while others might not share the same opinion by relying on their prior knowledge in that language. Therefore, universal annotation techniques are required, such that a ground truth can be established and the same set of words is considered complex in any context.  Proposed Approach. We consider state-of-the-art solutions, namely multilingual Transformer-based approaches, to address the CWI challenge. First, we apply a zero-shot learning approach. This was performed by training Recurrent Neural Networks  and Transformer-based models on a source language corpus, followed by validating and testing on a corpus from a target language, different from the source language.  A second experiment consists of a one-shot learning approach that considers training on each of the three languages , but only keeping one entry from the target language, and validating and testing on English, German, Spanish, and French, respectively.   In addition, we performed few-shot learning experiments by validating and testing on a language, and training on the others, but with the addition of a small number of training entries from the target language. The model learns sample structures from the language and, in general, performs better when applied on multiple entries. Furthermore, this training process can help the model adapt to situations in which the number of training inputs is scarce. The dataset provided by the CWI Shared Task 2018  was used to perform all experiments.  This paper is structured as follows. The second section describes related work and its impact on the CWI task. The third section describes the corpus and outlines our method based on multilingual embeddings and Transformer-based models, together with the corresponding experimental setup. The fourth section details the results, alongside a discussion and an error analysis. The fifth section concludes the paper and outlines the main ideas, together with potential extensions.  
"," Complex Word Identification  is a task centered on detecting hard-to-understand words, or groups of words, in texts from different areas of expertise. The purpose of CWI is to highlight problematic structures that non-native speakers would usually find difficult to understand. Our approach uses zero-shot, one-shot, and few-shot learning techniques, alongside state-of-the-art solutions for Natural Language Processing  tasks . Our aim is to provide evidence that the proposed models can learn the characteristics of complex words in a multilingual environment by relying on the CWI shared task 2018 dataset available for four different languages . Our approach surpasses state-of-the-art cross-lingual results in terms of macro F1-score on English , German , and Spanish  languages, for the zero-shot learning scenario. At the same time, our model also outperforms the state-of-the-art monolingual result for German .",37
" Aspect based sentiment analysis   is a fine-grained sentiment analysis task. ABSA contains several subtasks, four of which are aspect category detection  detecting aspect categories mentioned in sentences, aspect category sentiment analysis  predicting the sentiments of the detected aspect categories, aspect term extraction  identifying aspect terms presenting in sentences and aspect term sentiment analysis  classifying the sentiments toward the identified aspect terms. While aspect categories mentioned in a sentence are from a few predefined categories and may not occur in the sentence, aspect terms  explicitly appear in sentences. Fig.  shows an example. ACD detects the two aspect categories  and  and ACSA predicts the positive and negative sentiments toward them. ATE identifies the two aspect terms ``taste'' and ``service'' and ATSA classifies the positive and negative sentiments toward them. In this paper, we concentrate on the ACSA task. The ACD task as a auxiliary is used to find aspect category-related nodes from sentence constituency parse trees for the ACSA task.    Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate appropriate sentiment words for given aspect categories. Wang et al.  were the first to explore attention mechanism on the ACSA task and proposed an attention based LSTM . For a given sentence and an aspect category mentioned in the sentence, AT-LSTM first models the sentence via a LSTM model,  then combines the hidden states from the LSTM with the representation of the aspect category to generate aspect category-specific word representations, finally applies an attention mechanism over the word representations to find the aspect category-related sentiment words, that are used to predict the sentiment of the aspect category. The constrained attention networks   handles multiple aspect categories of a sentence simultaneously and introduces orthogonal and sparse regularizations to constrain the attention weight allocation. The aspect-level sentiment capsules model  performs ACD and ACSA simultaneously, which also uses an attention mechanism to find aspect category related sentiment words and achieves state-of-the-art performances on the ACSA task.  However, these models directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. For the example in Fig., ``Great'' and ``bad'' can be used interchangeably. It is hard for attention-based methods to distinguish which word is associated with aspect category  or  among ``good'' and ``bad''. To solve the problem, The HiErarchical ATtention network  first finds the aspect terms indicating the given aspect cagegory, then finds the aspect category-related sentiment words  depending on the position information and semantics of the aspect terms. Although HEAT obtains good results, to train HEAT, we additionally need to annotate the aspect terms indicating the given aspect category, which can be time-consuming and expensive.  To mitigate the mismatch problem, we propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis which does not require any additional annotation. SCAN contains two graph attention networks   and an interactive loss function. Given a sentence, we first use the Berkeley Neural Parser  to generate the constituency parse tree. The two GATs generate representations of the nodes in the sentence constituency parse tree for the ACD task and the ACSA task, respectively. The GAT for ACD mainly attends to the words indicating aspect categories, while the GAT for ACSA mainly attends to sentiment words. For a given aspect category, the interactive loss function helps the ACD task to find the nodes that can predict the aspect category but can閳ユ獩 predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. Fig.  shows the constituency parse tree of the sentence ``Greate taste bad service.''. For the aspect category , SCAN first finds the yellow nodes ``Greate taste'' and ``taste'', then predict the sentiment of  based on the sentiment word ``Great'' in the node ``Great taste''. SCAN excludes the blue node ``Great taste bad service.'' for , because it can predict not only  but also .  The main contributions of our work can be summarized as follows:  	   
"," Aspect category sentiment analysis  aims to predict the sentiment polarities of the aspect categories discussed in sentences. Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate the appropriate sentiment words for the given aspect category and obtain promising results. However, most of these methods directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. To mitigate this problem, we propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis. SCAN contains two graph attention modules and an interactive loss function. The graph attention modules generate representations of the nodes in sentence constituency parse trees for the aspect category detection  task and the ACSA task, respectively. ACD aims to detect aspect categories discussed in sentences and is a auxiliary task. For a given aspect category, the interactive loss function helps the ACD task to find the nodes which can predict the aspect category but can闁炽儲鐛 predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. The experimental results on five public datasets demonstrate the effectiveness of SCAN. \footnote{Data and code can be found at https://github.com/l294265421/SCAN}",38
"  With the rapid development of e-commerce, online reviews written by  users  have become increasingly important for reflecting real customer experiences. To ease the process of review writing, the task of personalized review generation~ has been proposed to automatically produce review text conditioned on necessary context data, , while another user may emphasize the .  To address these issues, we propose to improve the PRG task with external knowledge graph . By associating online items with KG entities, we are able to obtain rich attribute or feature information for items, which is potentially useful for the PRG task. Although the idea is intuitive, it is not easy to fully utilize the knowledge information for generating review text in our task. KG typically organizes facts as triples, describing the relation between two involved entities. It may not be suitable to simply integrate KG information to enhance text representations or capture user preference due to varying intrinsic characteristics of different data signals.  In order to bridge the semantic gap, we augment the original KG with user and word nodes, and construct a heterogeneous knowledge graph  by adding user-item links and entity-word links. User-item links are formed according to user-item interactions, and entity-word links are formed according to their co-occurrence in review sentences. We seek to learn a unified semantic space that is able to encode different kinds of nodes. Figure presents an illustrative example for the HKG. Given such a graph, we focus on two kinds of useful information for the PRG task. First, the associated facts regarding to an item  can be incorporated to enrich the review content. Second, considering users as target nodes, we can utilize this graph to infer users' preference  on some specific relation or aspect . The two kinds of information reflect word- and aspect-level enrichment, respectively. To utilize the semantics at the two levels, we decompose  the review generation process into two stages, namely aspect sequence generation and sentence generation.  We aim to inject multi-granularity KG information in different generation stages for improving the PRG task.     To this end, in this paper, we propose a KG-enhanced personalized review generation model based on capsule graph neural networks~. Compared with most of existing GNN-based methods representing graphs as individual scalar features, Caps-GNN can extract underlying characteristics of graphs as  at the graph level through the dynamic routing mechanism and each capsule reflects the graph properties in different aspects. Based on the constructed HKG, we utilize Caps-GNN to extract graph properties in different aspects as , which may be helpful to infer aspect- and word-level user preference. For aspect sequence generation, we propose a novel adaptive learning algorithm that is able to capture personalized user preference at the aspect level, called , from the graph capsules.  We associate an aspect capsule with a unique aspect from unsupervised topic models.   Furthermore, for the generation of sentences, we utilize the learned aspect capsules to capture personalized user preference at the word level. Specially, we design a graph-based copy mechanism to generate related entities or words by copying them from the HKG, which can enrich the review contents.  In this way, KG information has been effectively utilized  at both aspect and word levels in our model.   %To our knowledge, we are the first to utilize knowledge graph to generate personalized review text, which is able to capture both aspect- and word-level KG semantics for learning user preference.  To our knowledge, we are the first to utilize KG to capture both aspect- and word-level user preference for generating personalized review text. For evaluation, we constructed three review datasets by associating items with KG entities. Extensive experiments  demonstrate the effectiveness of KG information and our model. %%       
"," Personalized review generation  aims to automatically produce review text reflecting user preference, which is a challenging natural language generation task. Most of previous studies do not explicitly model  factual description of products, tending to generate uninformative content. Moreover, they mainly focus on word-level generation, but cannot accurately reflect more abstractive  user preference in multiple aspects.  To address the above issues, we propose a novel knowledge-enhanced PRG model  based on capsule graph neural network~. We first  construct a heterogeneous knowledge graph  for utilizing rich item attributes. We adopt  Caps-GNN to learn graph capsules for encoding underlying characteristics from the HKG. Our generation process contains two major steps, namely aspect sequence generation and sentence generation. First, based on graph capsules, we adaptively learn aspect capsules for inferring the aspect sequence.   Then, conditioned on the inferred aspect label, we design a graph-based copy mechanism to generate sentences by incorporating related entities or words from HKG. To our knowledge, we are the first to utilize knowledge graph for the PRG task. The incorporated KG information is able to enhance user preference at both aspect and word levels. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our model on the PRG task.",39
" % significance of sentence functions for dialog Humans express intentions in conversations through sentence functions, such as interrogation for acquiring further information, declaration for making statements, and imperative for making requests and instructions. For machines to interact with humans, it is therefore essential to enable them to make use of sentence functions for dialogue generation. Sentence function is an important linguistic feature indicating the communicative purpose of a sentence in a conversation. There are four major sentence functions: Declarative, Interrogative, Exclamatory and Imperative . Each major sentence function can be further decomposed into fine-grained ones according to different purposes indicated in conversations. For example, Interrogative is divided into Wh-style Interrogative, Yes-no Interrogative and other types. These fine-grained sentence functions have great influences on the structures of utterances in conversations including word orders, syntactic patterns, and other aspects . Figure  presents how sentence functions influence the responses.  Given the same query expressed in Positive Declarative, the responses expressed in Wh-style Interrogative and in Negative Declarative are completely different.    % challenges, quantitatively list some numbers Although the use of sentence functions improves the overall quality of generated responses , it suffers from the data imbalance issue. For example, in the recently released response generation dataset with manually annotated sentence functions STC-SeFun , more than 40\% of utterances are Positive Declarative while utterances annotated with Declarative with Interrogative words account for less than 1\%  of the entire dataset. Therefore, dialogue generation models suffer from data deficiency for these infrequent sentence functions.  % proposed method Recently, model-agnostic meta-learning ~  has shown promising results on several low-resource natural language generation  tasks, including neural machine translation , personalized response generation  and domain-adaptive dialogue generation . They treat languages of translation, personas of dialog and dialog domains as separate tasks in MAML respectively. In the same spirit of previous works, we first treat dialogue generation conditioned on different sentence functions as separate tasks, and meta-train a dialogue generation model using high-resource sentence functions. Moreover, we observe that sentence functions have hierarchical structures: four major sentence functions can be further divided into twenty fine-grained types. Some fine-grained sentence functions may share some similarities while some others are disparate. For example, utterances belong to Wh-style Interrogative and Yes-no Interrogative may share some transferable word patterns while utterances in Wh-style Interrogative and in Exclamatory with interjections totally differ from each other.  Motivated by this observation, we explore a structured meta-learning  considering inherent structures among fine-grained sentence functions. Inspired from recent advances on learning several initializations with a set of meta-learners , we develop our own approach to utilize the underlying structure of sentence functions. More specifically, our proposed SML explicitly tailors transferable knowledge among different sentence functions. It utilizes the learned representations of fine-grained sentence functions as parameter gates to influence the globally shared parameter initialization. Therefore, conversation models for similar sentence functions can share similar parameter initializations and vice versa. As a result, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions.  % experiments The experimental results on STC-SeFun dataset  show that responses generated from our proposed structured meta-learning algorithm are of better quality over several baselines in both human and automatic evaluations.  Moreover, our proposed model can generate responses consistent with the target sentence functions while baseline models may ignore the target sentence functions or generate some generic responses. We further conduct a detailed analysis on our proposed model and show that it indeed can learn word orders and syntactic patterns for different fine-grained sentence functions.     
"," Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning  approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data.  Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions.  Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions.",40
" As mentioned in Chapter , models trained simply to obtain a high accuracy on held-out sets can often learn to rely on shallow input statistics, resulting in brittle models. % susceptible to adversarial attacks. For example,  present a document classifier that distinguishes between { with a test accuracy of . However, on close inspection, the model spuriously separates classes based on words contained in the headers, such as ``Posting'', ``Host'', and ``Re''.  Spurious correlations in both training and test sets allow for such undesired models to obtain high accuracies. Much more complex hidden correlations may be present in any arbitrarily large and human-annotated dataset . Such correlations may be difficult to spot, and even when one identifies them, it is an open question how to mitigate them .   In this chapter, I investigate a direction that has the potential to both steer neural models away from relying on spurious correlations and provide explanations for the predictions of these models. This direction is that of enhancing neural models with the capability to learn from natural language explanations during training time and to generate such explanations at test time. For humans, it has been shown that explanations play a key role in structuring conceptual representations for categorisation and generalisation . Humans also benefit tremendously from reading explanations before acting in an environment for the first time . Thus, explanations may also be used to set a model in a better initial position to further learn the correct functionality. Meanwhile, at test time, generating correct argumentation in addition to obtaining a high accuracy has the potential to endow a model with a higher level of transparency and trust.     %In this work, we introduce a new dataset and models for exploiting and generating explanations for the task of recognizing textual entailment.  Incorporating external knowledge into a neural model was shown to result in more robust models . % show that models achieving high accuracies on SNLI, such as , show dramatically reduced performance on this simpler dataset, while the model of  is more robust due to incorporating external knowledge.  Free-form natural language explanations are a form of external knowledge that has the following advantages over formal language. First, it is easy for humans to provide free-form language, eliminating the additional effort of learning to produce formal language, thus making it simpler to collect such datasets. Secondly, natural language explanations might potentially be mined from existing large-scale free-form text. Finally, natural language is readily comprehensible to an end-user who needs to assert the reliability of a model.  %Thirdly, the formal languages chosen by researchers may differ from work to work and therefore models constructed over one formal language might not be trivially transferred to another. Meanwhile free-form explanations are generic and applicable to diverse areas of research, such as natural language processing, computer vision, or policy learning.   Despite the potential for natural language explanations to improve both learning and transparency, there is a scarcity of such datasets in the community, as discussed in Section .  To address this deficiency, I collected a large corpus of ${.} to advance research in the direction of training with and generation of free-form natural language explanations.    %To demonstrate the efficacy of the e-SNLI dataset,  %I show that it is much more difficult for neural models to produce correct natural language explanations based on spurious correlations than it is to produce correct labels. Further, I develop models that predict a label and generate an explanation for their prediction. I also investigate how the presence of natural language explanations at training time can guide neural models into learning better universal sentence representations  and into having better capabilities to solve out-of-domain instances.  Secondly, I show that it is much more difficult for a neural model to produce correct natural language explanations based on spurious correlations than it is for it to produce correct labels based on such correlations.   Thirdly, I develop models that predict a label and generate an explanation for their prediction, and I investigate the correctness of the generated explanations.   Finally, I investigate whether training a neural model with natural language explanations can result in better universal sentence representations produced by this model and in better performance on out-of-domain datasets.    In this chapter, I use the concept of correct explanation to refer to the correct argumentation for the ground-truth label on an instance.  This should not be confused with the concept of faithful explanation, which refers to the accuracy with which an explanation describes the decision-making process of a model, as described in Section .  The capability of a neural model to generate correct explanations is an important aspect of the development of such models.  For example, correct argumentation may sometimes be needed in practice, alongside the correct final answer. Hence, in this chapter, I inspect the correctness of the explanations generated by the introduced neural models. In the next chapter, I will take a step towards verifying the faithfulness of these explanations.% is given in Chapter .   
","  Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored.   In this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model , and that provide explanations in terms of input features, such as tokens for text and superpixels for images . The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model. The contributions in these directions are as follows.   % In this thesis, I investigate the topic of explaining deep neural networks. This topic is crucial nowadays as neural model are becoming more and more employed in real-world applications due to their high performance in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes learned by these models are not generally human-interpretable. In various real-world applications, such as healthcare, finance, or criminal justice, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored.   % a series of methods have recently been developed to provide explanations for the predictions of neural models. This thesis brings contributions to two major directions for explaining deep neural networks: feature-based post-hoc explanatory methods and self-explanatory neural models that generate natural language explanations for their predictions. The contributions are as follows.   %However, it is still an open question how to verify whether the explanations provided by these methods are faithfully describing the decision-making processes of the models that they aim to explain. Secondly, it is also an open question whether neural networks can learn from human-provided natural language explanations for the ground-truth labels at training time, as well as support their predictions with natural language explanations at test time, just like humans do.    First, I reveal certain difficulties of explaining even trivial models using only input features. I show that, despite the apparent implicit assumption that explanatory methods should look for one specific ground-truth feature-based explanation, there is often more than one such explanation for a prediction. I also show that two prevalent classes of explanatory methods target different types of ground-truth explanations without explicitly mentioning it. Moreover, I show that, sometimes, neither of these explanations is enough to provide a complete view of a decision-making process on an instance. %These findings can have an important impact on how users choose explanatory methods to best suit their needs.    Second, I introduce a framework for automatically verifying the faithfulness with which feature-based post-hoc explanatory methods describe the decision-making processes of the models that they aim to explain. This framework relies on the use of a particular type of model that is expected to provide insight into its decision-making process. I analyse potential limitations of this approach and introduce ways to alleviate them.  % The introduced verification framework is generic and can be instantiated on different tasks and domains to provide off-the-shelf sanity tests that can be used to test feature-based post-hoc explanatory methods. I instantiate this framework on a task of sentiment analysis and provide sanity tests\footnote{The sanity tests are available at \\ \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} %to test any feature-based post-hoc explanatory method. Furthermore,  on which I present the performances of three popular explanatory methods. %The results show that these methods may provide unfaithful explanations.  %I also discuss ways in which the current limitations of the framework can further be addressed to lead to more robust and flexible verifications.    %In the process of developing this framework, I uncover several ways in which a particular type of model that is expected to provide insight into its decision-making process can provide misleading such insight. I also introduce checks that can be done to account for this misleading insight in order to use this type of model in the proposed framework.  % %%%%%%%% BEFORE %%%%%%%%%%The framework is generic and can be instantiated on different tasks and domains. I instantiate it on a task of sentiment analysis and provide sanity tests that can be used off-the-shelf\footnote{The tests are available at \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} to test any feature-based post-hoc explanatory method. Furthermore, I present preliminary results of three explanatory methods on these tests, which raise awareness of the unfaithful explanations that these methods may provide. %I discuss ways in which the limitations of this verification framework can further be addressed and open the path towards more robust and flexible verification frameworks that can be adapted to users' needs.  %%%% this framework relies on the use of a particular type of model that is expected to provide insight into its decision-making process. I analyse the potential limitations of this approach and introduce ways to overcome them. By constructions   %In addition, as a step towards addressing the question of verifying if explanatory methods faithfully describe the decision-making processes learned by the models they aim to explain, I investigate a particular type of self-explanatory neural model and I show three ways in which this type of model can provide misleading explanations. % on its decision-making process.    %Secondly, I present a novel verification framework that can generate a multitude of sanity tests for explanatory methods. I instantiate this framework on the task of sentiment analysis and provide three sanity tests, which can be used off-the-shelf.\footnote{The tests are available at \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} I present the results of three explanatory methods on these tests. I discuss ways in which the limitations of this verification framework can further be addressed and open the path towards more robust and flexible verification frameworks that can be adapted to users' needs.  % improve their behaviour and performance %exhibit improved behaviour  % if they are additionally given natural language explanations for the ground-truth label at training time  Third, to explore the direction of self-explanatory neural models that generate natural language explanations for their predictions, I collected a large dataset of $.} %, which I release publicly\footnote{The dataset is available at \url{https://github.com/OanaMariaCamburu/e-SNLI}.} %to advance research in the direction of training with and generation of natural language explanations.  % Further, I provide empirical evidence that models generating correct explanations are more reliable than models that just predict the correct labels.  % I also train different neural models that generate natural language explanations at test time, and I measure the success of these models to generate correct explanations. I also investigate whether the presence of natural language explanations at training time can lead a model to produce better universal sentence representations and to perform better on out-of-domain datasets. I do a series of experiments that investigate both the capabilities of neural models to generate correct natural language explanations at test time, and the benefits of providing natural language explanations at training time.  Fourth, I show that current self-explanatory models that generate natural language explanations for their own predictions may generate inconsistent explanations, such as ``There is a dog in the image.'' and ``There is no dog in the [same] image.''. Inconsistent explanations reveal either that the explanations are not faithfully describing the decision-making process of the model or that the model learned a flawed decision-making process.  I introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, I address the problem of adversarial attacks with exact target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks, and which can be useful for other tasks in natural language processing. I apply the framework on a state of the art neural model on e-SNLI and show that this model can generate a significant number of inconsistencies.  This work paves the way for obtaining more robust neural models accompanied by faithful explanations for their predictions.  %My hope is that in the future feature-based post-hoc explanatory methods will be superseded  by robust and accurate neural models that faithfully explain themselves to their human users in natural language.",41
"  We use a sequence of vectors to represent a sentence, where each vector consists of  a semantic-role  tag, a part-of-speech  tag, and other syntactic and semantic tags,  and we refer to such a sequence as a \textsl{meta sequence}.  We present an application using meta-sequence learning to generate, on a given article,  adequate QAPs to form multiple-choice questions.  In particular, we develop a scheme called MetaQA to learn meta sequences  of declarative sentences and the corresponding interrogative sentences from a training dataset. % consisting of such sentences. Combining and removing redundant meta sequences yields a set called MSDIP  , with each element being a pair of an MD and corresponding MI, where MD and MI stand for, respectively, a meta sequence for a declarative sentence and for an interrogative sentence. A trained MetaQA model generates QAPs for a given declarative sentence  as follows: Generate a meta sequence for , find a best-matched MD from MSDIP, generates meta sequences for interrogative sentences according to the corresponding MIs and the meta sequence of , identifies the meta-sequence answer to each MI, and coverts them back to text to form a QAP.   In an effort to build an online learning tool for helping students improve reading comprehension, it calls for a system to automatically generate adequate  multiple-choice questions  to assess student's understanding of a given article's main points. % from an arbitrary document.  An article's main points include \textsl{direct} and \textsl{derived} points.  A direct point is expressed in a declarative sentence. A derived point  is inferred from multiple direct points, %inferred that  which could be a causal relation between them, an aggregation over them, or a conclusion drawn from them.  We study automatic generation of question-answer pairs  with an emphasis on the grammatical correctness of the questions  and the suitability of the answers. By grammatical correctness we mean that the questions being generated are syntactically and semantically correct and  conform to what a native speaker would say. We refer to such QAPs as \textsl{adequate} QAPs. Other tasks of generating MCQs not addressed in this paper are how to provide adequate distractors for an answer.  Existing methods on QAP generation are based on handcrafted features or neural networks. While they have met with certain success from different perspectives, the grand challenge of generating adequate QAPs  still remains.   We present a new approach to tackling this challenge.  In particular, we use a sequence of vectors to represent a sentence, where each vector consists of  a semantic-role  tag, a part-of-speech  tag, and other syntactic and semantic tags,  and we refer to such a sequence as a \textsl{meta sequence}. We then present a scheme called MetaQA to learn meta sequences  of declarative sentences and the corresponding interrogative sentences from a training dataset. % consisting of such sentences. Combining and removing redundant meta sequences yields a set called MSDIP  , with each element being a pair of an MD and corresponding MI, where MD and MI stand for, respectively, a meta sequence for a declarative sentence and for an interrogative sentence. A trained MetaQA model generates QAPs for a given declarative sentence  as follows: Generate a meta sequence for , find a best-matched MD from MSDIP, generates meta sequences for interrogative sentences according to the corresponding MIs and the meta sequence of , identifies the meta-sequence answer to each MI, and coverts them back to text to form a QAP.   We implement MetaQA for the English language using  SR, POS, and NE  tags. %, and %allow fuzzy representation when an existing tool fails to produce a tag for %a given word.  We then train MetaQA using a moderate initial dataset and show that MetaQA generates efficiently a large number of  adequate QAPs with an accuracy of 97\% on the official SAT practice reading tests. These tests contain a large number of declarative sentences in different patterns, and  there is no match in the initial MSDIP for some of these sentences. After learning interrogative for some of these sentences, MetaQA successfully generate many more adequate QAPs.  The rest of the paper is organized as follows: We describe in Section  related work, in Section  the details of meta sequence learning. We then present in Section  the answer generation. We report evaluation results in Section . Finally, we conclude the paper in Section .     
"," %Creating multiple-choice questions to assess reading comprehension of a given article %involves generating question-answer pairs  on the main points of the document. We present a meta-sequence representation of sentences and demonstrate how to use meta-sequence learning to generate adequate question-answer pairs  over a given article. %learning scheme to generate adequate QAPs  %via meta-sequence representations of sentences.   %without handcrafted features.  A meta sequence is a sequence of vectors of semantic and syntactic tags. %In particular, %we devise a scheme called MetaQA to %learn meta sequences from training data to form  %pairs of a meta sequence for a declarative sentence   %and a corresponding  interrogative sentences . % indexed for fast retrieval,  On a given declarative sentence, a trained model  converts it to a meta sequence,  finds a matched meta sequence in its learned database,  and   uses the corresponding meta sequence for interrogative sentence to generate QAPs. %We implement MetaQA for the English language using  %semantic-role labeling,  %part-of-speech tagging, and  named-entity recognition, We show that, trained on a small dataset,  our method generates efficiently, on the official SAT practice reading tests, a large number of syntactically and semantically correct QAPs with high accuracy.",42
"  The desire for human-like interfaces to technical systems, as evidenced by growing use of intelligent assistants, belies the need for conversational AI systems that can accomplish a wide range of tasks, such as booking restaurants, trains, and flights, IT help desk and accessing financial accounts and transaction records. The wide range of tasks have necessitated the need for a flexible and scalable dialogue system that can support a variety of use cases with minimal development and maintenance effort. Existing dialogue systems are broken into two major categories,  open-domain dialogue systems, which focus on non-task related conversations, and task-oriented dialogue systems, which focus on user task completion. A typical open-domain system uses an end-to-end neural architecture often trained with input and output utterances from human-to-human conversations . While open-domain systems are optimized for engaging in human-like conversation, they lack any inherent ability to interface with any other systems on behalf of their conversation partner. Whereas, a typical task-oriented dialogue system seeks to understand human intents and execute them. This is done by adopting a modularized pipeline architecture with three modules that are sequentially connected as shown in Fig. . A natural language understanding  module that recognizes user intents and extract useful entity information . The dialogue management  module contains two submodules, the dialogue state tracker  and the dialogue action policy  modules. The DST module tracks the mapping of entities to slots that are relevant or required for completing user tasks . The POL module decides which actions to execute via the API. Finally, the natural language generation  module generates the user response based on the user aspects of the system actions . In some cases, multiple modules are combined together, e.g. systems with a composite NLU and DST module , and systems with a composite POL and NLG module that maps previous utterances and dialogue states to the system response .  Despite research advances in modular neural approaches, they are hardly used in practice. Industrial dialogue systems, though modularized, still use expensive expert driven rule-based heuristics implemented with several lines of codes and hand-crafted templates, and therefore difficult to scale as the number of use cases grows. More recently, there has been a renewed effort to apply a single end-to-end neural architecture  to model task-oriented dialogue with the use of autoregressive transformer architecture . This has led to the reformulation of dialogue system design as a text generation or sequence modeling task. While some of these efforts have obtained state-of-the-art performance on publicly available task-oriented dialogue datasets, there is still room for improvement, especially in the areas of generality and practicality. First, their problem formulation fails to reconcile open-domain and task-oriented dialogue in the same model architecture. Also, in many cases, they do not address the complexity of the action policy especially towards the back-end API system. Finally, they don't fully incorporate the control, verification and explanation capabilities that make modularized approaches attractive.  To resolve these shortcomings, we propose DLGNet-Task, an end-to-end neural network that simultaneously handles both open-domain and task-oriented dialogue, in such a way that the model outputs are controllable, verifiable, and explainable at the module level. This system is compatible with both data driven and expert driven rule-based approaches.   That is, our approach is simultaneously modular and end-to-end, and can be a drop-in replacement for traditional modular task-oriented dialogue  systems. To the best of our knowledge, this is the most expressive approach to date in achieving this objective. In summary, we are able to model the individual behavior of NLU, DM and NLG components with a single neural network model trained end-to-end. Still, the model is flexible enough to allow individual modules to be separately trained and validated in line with the traditional TOD system.  % Validation at module level can provide information about where additional training is needed. It could also help in balancing the contribution of each module if the model is finetuned with module-level objectives.  % The DLGNet-Task model is based on the autoregressive transformer architecture similar to DLGNet  and GPT-2/3  models. To evaluate the performance of DLGNet-Task, we trained the model with just the system-level training objective on a modified MultiWoz2.1 dataset. The dataset modification is done mainly to support DLGNet-Task design framework . Based on the widely used TOD metrics, such as inform rate, success rate, and BLEU score , our experiments show that DLGNet-Task produces a comparable performance to the state-of-the-art approaches on the MultiWoz2.1 dataset.  % in addition to the controllable, verifiable, and explainable model's intermediate outputs.    
"," Task oriented dialogue  requires the complex interleaving of a number of individually controllable components with strong guarantees for explainability and verifiability. This has made it difficult to adopt the multi-turn multi-domain dialogue generation capabilities of streamlined end-to-end open-domain dialogue systems. In this paper, we present a new framework, DLGNet-Task, a unified task-oriented dialogue system which employs autoregressive transformer networks such as DLGNet and GPT-2/3 to complete user tasks in multi-turn multi-domain conversations. Our framework enjoys the controllable, verifiable, and explainable outputs of modular approaches, and the low development, deployment and maintenance cost of end-to-end systems. Treating open-domain system components as additional TOD system modules allows DLGNet-Task to learn the joint distribution of the inputs and outputs of all the functional blocks of existing modular approaches such as, natural language understanding , state tracking, action policy, as well as natural language generation . Rather than training the modules individually, as is common in real-world systems, we trained them jointly  with appropriate module separations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows comparable performance to the existing state-of-the-art approaches. Furthermore, using DLGNet-Task in conversational AI systems reduces the level of effort required for developing, deploying, and maintaining intelligent assistants at scale.  % significant improvement over existing approaches, and achieves state-of-the-art performance at both the module and system levels.",43
"   Knowledge graphs  represent knowledge of the world as relationships between entities, i.e., triples with the form  . Such knowledge resource provides clean and structured evidence for many downstream applications such as question answering. KGs are usually constructed by human experts, which is time-consuming and leads to highly incomplete graphs . Therefore automatic KG completion  is proposed to infer a missing link of relationship  between a head entity  and a tail entity .    Existing KG completion work mainly makes use of two types of information: 1) co-occurrence of entities and relations and 2) deducible reasoning paths of tuples. KG embeddings encode entities and relations, the first type of information, together into continuous vector space with low-rank tensor approximations~.  Ours approach utilizes the second type of information, reasoning path of tuples that can be deduced to the target tuple~. Here a reasoning path starts with the head entity  and ends with the tail entity : {\rightarrow} e_1  \overset{r_k}{\rightarrow} e_k \overset{r_N}{\rightarrow} t}, where  forms a relation chain that infers the existence of . Therefore these methods are also referred as multi-hop reasoning over KGs, which learns a multi-hop chain as a rule to deduce the target . An example of such a chain is given in Figurea to infer whether an athlete plays in an location. Multi-hop reasoning approaches can usually utilize richer evidence and self-justifiable in terms of  reasoning path rules used in the predictions, making the prediction of missing relations more interpretable.   Despite  advantages and  success of the multi-hop reasoning approach , a target relationship may not be perfectly inferred from a single relation chain. There could exist multiple weak relation chains that correlate with the target relation. Figure gives examples of such cases.  These multiple chains could be leveraged in following ways:  the reasoning process naturally relies on the logic conjunction of multiple chains ;  more commonly, there are instances for which none of the chains is accurate, but aggregating multiple pieces of evidence improves the confidence , as also observed in the case-based study works. Inspired by these observations, we propose the concept of  multi-chain multi-hop rule set.  Here, instead of treating each single multi-hop chain as a rule, we learn rules consisting of a small set of multi-hop chains. Therefore the inference of target relationships becomes a joint scoring of such  a set of chains. {We  treat each set of chains as one rule and, since different query pairs can follow different rules, together we have  a set of rules to reason each relation.}  Learning the generalized multi-hop rule set is a combinatorial search problem.  We address this challenge with a game-theoretic approach inspired by. Our approach consists of two steps:  selecting a generalized multi-hop rule set by employing a Multi-Layer Perceptron  over the candidate chains;   reasoning with the generalized rule set, which uses another MLP to model the conditional probability of the target relationship given the selected relation chains. The nonlinearity of MLP as reasoner provides the potential to model the logic conjunction among the selected chains in the rule set.  We demonstrate the advantage of our method on KG completion tasks in FB15K-237 and NELL-995. Our method outperforms existing single-chain approaches, showing that our defined generalized rules are necessary for many reasoning tasks.  
"," Multi-hop reasoning approaches over knowledge graphs infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where each rule is a set of relation chains.  To learn such generalized rules efficiently, we propose a two-step approach that first selects a small set of relation chains as a rule and then evaluates the confidence of the target relationship by jointly scoring the selected chains. A game-theoretical framework is proposed to this end to simultaneously optimize the rule selection and prediction steps. Empirical results show that our multi-chain multi-hop  rules result in superior results compared to the standard single-chain approaches, justifying both our formulation  of  generalized rules  and the effectiveness of the proposed learning framework.",44
" Recently, there has been great success in automatic text summarization and generation. To better compare and improve the performance of models, evaluation for such systems has been a problem of interest. The selection of evaluation metrics will greatly affect the assessed quality of a generated summary and thus affect the evaluation of summarization models.   The most ideal metric is definitely human judgement, which is often treated as the gold standard. But human evaluation is time-consuming and labor-intensive, an automatic evaluation metric that cannot only save human resources but also simulate the ability of human judgement is of crucial importance.   Most of the existing automatic evaluation methods assess a summary by comparing it with reference texts written by humans. Some of them are model-free and simply use hand-crafted matching functions to calculate the similarity between the candidate summary and the reference  . These methods consider both the reference and the candidate as a sequence of tokens or n-gram blocks. For instance, as the de facto standard evaluation metric, ROUGE  calculates the n-gram overlap between the machine-generated summaries and reference summaries. Although these methods have the advantage of interpretability and efficiency, they are found to correlate poorly with human evaluation.   To reduce the requirement of exact word matching, some recent work tried to match the reference and the candidate summary in the embedding space of words or sentences . For instance, BERTScore  uses contextual word embeddings generated by BERT and performs a greedy matching to obtain the maximum cosine similarity between two texts. %  designed a metric that combines sentence-level embeddings with the word mover閳ユ獨 distance   to calculate the distance of moving the candidate sequence into the reference and transforms the distance into a similarity score, while MoverScore  combines n-gram embeddings with WMD.   These methods are proved to correlate better with human judgement than ROUGE on many datasets, which demonstrates the effectiveness of using contextual embeddings.  [ht] { {lccc}  \toprule        & Semantic & Linguistic  & Else \\              \makecell[l]{~DUC-05, DUC- 06 and DUC-07\\ }  & \makecell[c]{focus, \   & \makecell[c]{coherence,\\fluency}  &-   \\         \makecell[l]{~NYT and CNN/Daily Mail \\} & informativeness   & \makecell[c]{grammaticality,\\ coherence}  &-  \\   }  , all the three dimensions focus on evaluating the linguistic quality of summaries.}    However, the aforementioned methods all have some intrinsic drawbacks: these methods always need at least one human-generated reference to assess a candidate summary. References written by humans are costly to obtain. In addition, most of them only consider the semantic similarities with references, i.e. semantic qualities of the summaries, which ignores the linguistic qualities and other important aspects. In this paper, we propose a new unsupervised contrastive learning framework for automatically evaluating the summary qualities without comparing with reference summaries or training with human ratings. Specifically, we design an evaluator to consider both linguistic and semantic aspects of a summary. Then for each of the aspect we create a set of negative samples by perturbing the training samples. We compare the scores of original training samples and the negative samples to obtain the contrastive loss function and learn the evaluator. The experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method has much higher correlation with human judgement.  We summarize our contributions as follows:         
"," Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.",45
" Part-of-speech  tags and dependency parsing have formed a long-standing union in NLP. But equally long-standing has been the question of its efficacy. % of this union. %POS tags as features for parsers.   %Certainly in the nigh-on forgotten pre-deep learning era of NLP, it seemed as if they were useful for syntactic disambiguation in certain contexts  . However, for neural network implementations, especially those which utilise character embeddings, POS tags have been shown to be much less useful .   Others have found that POS tags can still have a positive impact when using character representations given that the accuracy of the predicted POS tags used is sufficiently high .  undertook a systematic study of the impact of features for Universal Dependency  parsing and found that using universal POS  tags does still offer a marginal improvement for their transition-based neural parser. The use of fine-grained POS tags still seems to garner noticeable improvements %even for challenging multi-lingual settings  .   %By far and away the most common use of  Latterly, POS tags have been commonly utilised implicitly for neural network parsers in multi-learning frameworks where they can be leveraged without the cost of error-propagation . Beyond multi-learning systems,  introduced dependency parsing as sequence labelling by encoding dependencies using relative positions of UPOS tags, thus explicitly requiring them at runtime. %So even if coarse POS tags, universal or otherwise, prove to be superfluous for graph- or transition-based neural parsers as direct features, there are still many uses for them.% in dependency parsing.   We follow the work of  and evaluate the interplay of word embeddings, character embeddings, and POS tags as features for two modern parsers, one a graph-based parser, Biaffine, and the other a transition-based parser, UUParser . Similar to , we focus on the contribution of POS tags but evaluate UPOS tags.   We analyse the effect UPOS accuracy has on two dependency parser systems for a number of UD treebanks. Our results suggest that in order to leverage UPOS tags as explicit features for these neural parsers, a prohibitively high tagging accuracy is needed, and that gold tag annotation seems to possess some exceptionality. We also investigate what aspects of predicted UPOS tags have the most impact on parsing accuracy.  
"," We present an analysis %contributing to the discussion  on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as features for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.",46
"  Conversational Machine Reading  is challenging because the rule text may not contain the literal answer, but provide a procedure to derive it through interactions . In this case, the machine needs to read the rule text, interpret the user scenario, clarify the unknown user's background by asking questions, and derive the final answer. Taking Figure  as an example, to answer the user whether he is suitable for the loan program, the machine needs to interpret the rule text to know what are the requirements, understand he meets ``American small business'' from the user scenario, ask follow-up clarification questions about ``for-profit business'' and ``not get financing from other resources'', and finally it concludes the answer ``Yes'' to the user's initial question.    Existing approaches  decompose this problem into two sub-tasks.  Given the rule text, user question, user scenario, and dialog history , the first sub-task is to make a decision among ``Yes'', ``No'', ``Inquire'' and ``Irrelevant''. The ``Yes/No'' directly answers the user question and ``Irrelevant'' means the user question is unanswerable by the rule text. If the user-provided information  are not enough to determine his fulfillment or eligibility, an ``Inquire'' decision is made and the second sub-task is activated. The second sub-task is to capture the underspecified condition from the rule text and generate a follow-up question to clarify it.  adopt BERT  to reason out the decision, and propose an entailment-driven extracting and editing framework to extract a span from the rule text and edit it into the follow-up question.  The current , Contradiction or Neutral by reading the user scenario description and existing dialog. Then we map the scores to an entailment vector for each condition, and reason out the decision based on the entailment vectors and the logical structure of rules. Compared to previous methods that do little entailment reasoning  or use it as multi-task learning , \modelnameshort is the first method to explicitly build the dependency between entailment states and decisions at each dialog turn.   \modelnameshort achieves new \sota results on the blind, held out test set of ShARC. In particular, \modelnameshort outperforms the previous best model EMT  by 3.8\% in micro-averaged decision accuracy and 3.5\% in macro-averaged decision accuracy. Specifically, \modelnameshort performs well on simple in-line conditions and conjunctions of rules while still needing improvements on understanding disjunctions. Finally, we conduct comprehensive analyses to unveil the limitation of \modelnameshort and current challenges for the ShARC benchmark. We find one of the biggest bottlenecks is the user scenario interpretation, in which various types of reasoning are required. % Code and models will be released to facilitate research along this line.   
","  Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose \modelnameshortnsp, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding for both document and dialog. Specifically, we split the document into clause-like elementary discourse units  using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision ``yes/no/irrelevant"" of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark  show that \modelnameshort achieves .",47
"   .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Neural Language Models  have become a central component in NLP systems over the last few years, showing outstanding performance and improving the state-of-the-art on many tasks . However, the introduction of such systems has come at the cost of interpretability %and explainability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. % and, specifically, of understanding how linguistic predictors - that were common as features in earlier systems - are encoded in such models.  Recent work has begun to study these models in order to understand whether they encode %are able to learn  linguistic phenomena even without being explicitly designed %forse meglio trained?  to learn such properties . Much of this work focused on the analysis and interpretation of attention mechanisms  and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations.   Probing models trained  on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena  and even to organize this information in a hierarchical manner . However, the way in which this knowledge affects the decisions they make when solving specific downstream tasks has been less studied.  In this paper, we extended prior work by studying the linguistic properties encoded by one of the most prominent NLM, BERT , and how these properties affect its predictions when solving a specific downstream task. %,  using a suite of more than 80 probing tasks.  % qui vedere se tenere 'several' perch鑼 abbiamo 10 task di classificazione o dire che 鐚 uno solo diviso in 10 ""sotto-task"". We defined three research questions aimed at understanding:  what kind of linguistic properties are already encoded in a pre-trained version of BERT and where across its 12 layers;  how the knowledge of these properties is modified after a fine-tuning process;  whether this implicit knowledge %of these properties  affects the ability of the model to solve a specific downstream task, i.e. Native Language Identification . %With this aim, we firstly perform a very large suite of probing tasks using %on %DOMI: SPOSTIAMO QUESTA PARTE %To answer the first two questions, we firstly perform a very large suite of probing tasks using %on %the sentence representations extracted from the internal layers of BERT. Each of these tasks makes explicit a particular property of the sentence, from very shallow features  to more complex aspects of morpho--syntactic and syntactic structure , thus making them as particularly suitable to assess the implicit linguistic knowledge encoded in a NLM at a deep level of granularity. %with respect to a wide spectrum of phenomena overing lexical, morpho-syntactic and syntactic structure.  To tackle the first two questions, we adopted an approach inspired to the `linguistic profiling' methodology put forth by , which assumes that wide counts of linguistic features automatically extracted from parsed corpora allow modeling a specific language variety and detecting how it changes with respect to other varieties, e.g. complex vs simple language, female vs male--authored texts, texts written in the same L2 language by authors with different L1 languages.  Particularly relevant for our study, is that multi-level linguistic features have been shown to have a highly predictive role in tracking the evolution of learners' linguistic competence across time and developmental levels, both in first and second language acquisition scenarios .  %when leveraged by traditional learning models on a variety of text classification problems, all of which can be successfully tackled using formal, rather than content based aspects of a text: from the assessment of sentence complexity and text readability , to the identification of personal and sociodemographics traits of an author, such as his/her native language, gender, age etc.  and to the prediction of the evolution of learners' linguistic competence across time . %From this perspective, our approach can be considered as a particular implementation of the `linguistic profiling' methodology put forth by , which assumes that wide counts of linguistic features automatically extracted from parsed corpora allow modeling a specific language variety and detecting in what way it changes with respect to other varieties, e.g. complex vs simple language, female vs male--authored texts, texts written in the same L2 language by authors with different L1 languages. Given the strong informative power of these features to encode a variety of language phenomena across stages of acquisition, we assume that they can be also helpful to dig into the issues of interpretability of NLMs. In particular, we would like to investigate whether features successfully exploited to model the evolution of language competence can be similarly helpful in profiling how the implicit linguistic knowledge of a NLM changes across layers and before and after tuning on a specific downstream task. We chose the NLI task, i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language .  %Secondly, we investigate the type and degree of variations of linguistic information before and after fine-tuning the pre-trained model on 10 distinct  datasets used to solve Native Language Identification , i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language .   As shown by , linguistic features play a very important role when NLI is tackled as a sentence--classification task rather than as a traditional document--classification task.  %NLI can be addressed by exploiting only linguistic features extracted at sentence--level reaching comparable performance to those obtained by state--of--the--art models based on word embeddings .  This is the reason why we considered the sentence-level NLI classification as a task particularly suitable for probing the NLM linguistic knowledge. %perch鑼 鐚 un task che per essere risolto 鐚 necessario che il modello codifichi un'ampia gamma di informazioni linguistiche e anche perch鑼 鐚 un task basato sull'info estratta dalla sentence -come dimostrato da Cimino et al  nonostante lo stato dell'arte 鐚 stato definito soltanto usando word embeddings  %vecchia versione: a fine-tuning process based on a Native Language Identification  downstream task.  %vecchia versione: -base and 10 fine-tuned models obtained training BERT on as many Native Language Identification  tasks.  Finally, we investigated whether and which linguistic information encoded by BERT is involved in discriminating the sentences correctly or incorrectly classified by the fine-tuned models. To this end, we tried to understand if the linguistic knowledge that the model has of a sentence affects the ability to solve a specific downstream task involving that sentence.   %vecchia versione: Adopting a suite of more than 80 probing tasks, we firstly perform % We perform our experiments using a suite of more than 80 probing tasks, each of which corresponds to a specific/distinct sentence-level feature. We find that / We show that  %The remainder of the paper is organized as follows. We start by presenting some related works which are more closely related to our study  and in Section  we highlight the main novelties of our approach. We then describe in more details the data , the probing tasks  and the models  we used. Experiments and results are described in Section ,  and . To conclude, in Section  we summarize the main findings of the study.   In this paper:  we carried out an in-depth linguistic profiling of BERT's internal representations %deep analysis of the implicit linguistic knowledge stored in BERT's internal representations and how it changes across layers using a wide suite of sentence-level probing tasks, corresponding to a wide spectrum of linguistic phenomena at different level of complexity; % we verify the implicit linguistic knowledge stored in BERT's internal representations using a suite of more than 80 probing tasks corresponding to a wide range of linguistic phenomena at different level of complexity;   we showed that contextualized representations tend to lose their precision in encoding a wide range of linguistic properties %general-purpose linguistic properties  after a fine-tuning process; % RIVEDERE 'GENERAL-PURPOSE' COME TERMINE PER DESCRIVERE LE NOSTRE FEATURES  we showed that the linguistic knowledge stored in the contextualized representations of BERT positively affects its ability to solve NLI downstream tasks: the more BERT stores information about these features% in its embeddings/internal representations , the higher will be its capacity of predicting the correct label.   
"," In this paper we investigate the linguistic knowledge learned by a Neural Language Model  before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT's capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence.",48
"   Recent emergent-communication studies, renewed by the astonishing success of neural networks, are often motivated by a desire to develop neural network agents eventually able to verbally interact with humans . To facilitate such interaction, neural networks' emergent language should possess many natural-language-like properties. However, it has been shown that, even if these emergent languages lead to successful communication, they often do not bear core properties of natural language .  In this work, we focus on one basic property of natural language that resides on the tendency to use messages that are close to the informational optimum. This is illustrated in the Zipf's law of Abbreviation , an empirical law that states that in natural language, the more frequent a word is, the shorter it tends to be . Crucially, ZLA is considered to be an  property of our language .  Besides the obvious fact that an efficient code would be easier to process for us, it is also argued to be a core property of natural language, likely to be correlated with other fundamental aspects of human communication, such as regularity and compositionality . Encouraging it might hence lead to emergent languages that are also more likely to develop these other desirable properties.   Despite the importance of such property,   showed that standard neural network agents, when trained to play a simple signaling game , develop an inefficient code, which even displays an ZLA pattern. That is, counterintuitively, more frequent inputs are coded with longer messages than less frequent ones. This inefficiency was related to  neural networks' ``innate preference'' for long messages. In this work, we aim at understanding which constraints need to be introduced on neural network agents in order to overcome  their innate preferences and communicate efficiently, showing a proper ZLA pattern.  To this end, we %follow  and use a reconstruction game where we have two neural network agents: speaker and listener. For each input, the speaker outputs a sequence of symbols  sent to the listener. The latter needs then to predict the speaker's input based on the given message. Also, similarly to the previous work, inputs are drawn from a power-law distribution.   We first describe the experimental and optimization framework . In particular, we introduce a new communication system called `LazImpa', comprising two different constraints  iness on the speaker side and  tience on the listener side. The former constraint is inspired by the least-effort principle which is attested to be a ubiquitous pressure in human communication .   However, if such a constraint is applied too early, the system does not learn an efficient system. We show that incrementally penalizing long messages in the cost function enables an early exploration of the message space  and prevents converging to an inefficient local minimum.   The other constraint, on the listener side, relies on the prediction mechanism, argued to be important in language comprehension , and is achieved by allowing the listener to reconstruct the intended input as soon as possible. We also provide a two-level analytical method: first, metrics quantifying the efficiency of a code; second, a new protocol to measure its informativeness . Applying these metrics, we demonstrate that, contrary to the standard speaker/listener agents, our new communication system `LazImpa' leads to the emergence of an efficient code. The latter follows a  distribution, close to natural languages . Besides the plausibility of the introduced constraints, our new communication system is, first, task- and architecture-agnostic , and second allows stable optimization of the speaker/listener. We also show how both listener and speaker constraints are fundamental to the emergence of a ZLA-like distribution, as efficient as natural language .  
"," Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes.  This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation  observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, ``LazImpa'', where the speaker is made increasingly y, i.e.,~avoids long messages, and the listener tient, i.e.,~seeks to guess the intended content as soon as possible.",49
"  Relation extraction  aims at extracting relational facts between entities from text, e.g., extracting the fact  from the sentence in Figure.  Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs , and eventually support downstream applications like question answering~, dialog systems~ and search engines~. With the recent advance of deep learning, neural relation extraction  models~ have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks.  % During the development of relation extraction systems, there have been pattern-based methods , feature-based methods , kernel-based methods , graphical models , etc. With the recent advance of deep learning and pre-trained language models , relation extraction systems with neural networks  have achieved new state-of-the-arts.    % Relation extraction  aims at extracting relational facts between entities from textual corpora. For example, for the entity pair  and the given sentence `` was founded in 2002 by \underline{Elon Musk}}'', we can extract the relationship   to form the fact triple  with the entity pair. From unstructured data , RE extracts structural formats of knowledge , construct  knowledge graphs, and eventually support downstream applications like question answering, recommender systems and search engines.   % During the development of relation extraction systems, there have been pattern-based methods , feature-based methods , kernel-based methods , graphical models , etc. With the recent advance of deep learning and pre-trained language models , relation extraction systems with neural networks  have achieved new state-of-the-arts.     The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions .   %So which kind of information do existing models rely on more? % So what kind of information do these two sources provide?  From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure, ``'' is a pattern for the relation . The early RE systems formalize patterns into string templates and determine relations by matching these templates. The later neural models prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better.  Besides, entity mentions also provide much information for relation classification. As shown in Figure , we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE~. Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training~.  %From human intuition, both the two sources contribute to relation extraction, and textual context should take a larger proportion. Entity types can help models filter out impossible relations . Other knowledge linked to the entities may help to infer the relation. However, for human, the evidence of final classification mainly comes from the context, because even for unknown entity names or blanked entity, in most cases we can correctly determine the relations.   In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that:   Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities.   We also get the following observations:    In certain circumstances, entity type information is necessary for correct classification.   Entity mentions also provide other information besides type that helps to understand text.   Blocking entity mentions will mislead models to incorrect predictions even on those sentences with strong relational patterns.  % Even with strong relational patterns, RE models rely on the entity mentions instead of contextual text for correct classification sometimes, suggesting the lack of understanding of context.  %In fact, most existing models rely heavily on entity names so that blocking them causes significant drop on performance. Further more, we notice that   Existing RE benchmarks may leak shallow cues via entity mentions, which contribute to the high performance of existing models. Our experiments show that models still can achieve high performance only given entity mentions as input, suggesting that there exist biased statistical cues from entity mentions in these datasets.   %suggesting that they can exploit superficial statistical cues in the mentions. % suggesting that the distribution of test data may be too easy.  %These two factors suggest that though existing models have achieved high performance on RE datasets, more efforts are needed towards robust and unbiased RE systems.  The above observations demonstrate how existing models work on RE datasets, and suggest a way to further improve RE models: we should enhance them via better understanding context and utilizing entity types, while preventing them from simply memorizing entities or exploiting biased cues in mentions.  From these points, we investigate an entity-masked contrastive pre-training framework for RE. % to better understand relational semantics in contexts while avoiding being biased by memorization or shallow patterns of the entity mentions.  We use Wikidata to gather sentences that may express the same relations, and let the model learn which sentences are close and which are not in relational semantics by a contrastive objective. % in relational semantics. In this process, we randomly mask entity mentions to avoid being biased by them.  We show its effectiveness across several settings and benchmarks, and suggest that better pre-training technique is a reliable direction towards better RE.   %The rest of the paper is organized as follows: xxx.   
","  Neural models have achieved remarkable success on relation extraction  benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions . We find that  while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and  existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at \url{https://github.com/thunlp/RE-Context-or-Names}.",50
" % 1 - What problem are you solving? Entity typing classifies textual mentions of entities, according to their semantic class, within a set of labels  organized in an inventory. %Multi-label text classification is the task of assigning to a sample all the relevant labels from a label  inventory . The task has progressed from recognizing a few coarse classes , to extremely large inventories, with hundreds  or thousands of labels . Therefore, exploiting inter-label correlations has become critical to improve performance.   % 2 - Why is it an interesting/important problem? % es interesante porque son buenos para modelar redes y estructuras jer璋﹔quicas. % Problema: su adopcion en nlp ha sido baja dado que no hay una forma muy intuitiva de modelar texto en ellos. Distintos papers muestran como agregar un peque甯給 cambio pero no una aplicacion real y completa Large inventories tend to exhibit a hierarchical structure, either by an explicit tree-like arrangement of the labels , or implicitly through the label distribution in the dataset . %A natural solution for dealing with large inventories is to organize them in hierarchy ranging from general, coarse labels near the top, to more specific, fine classes at the bottom. Prior work has integrated only explicit hierarchical information by formulating a hierarchy-aware loss  or by representing instances and labels in a joint Euclidean embedding space .  However, the resulting space is hard to interpret, and these methods fail to capture implicit relations in the label inventory. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures . Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance from the root  . %Its tree-like properties make it efficient to learn hierarchical representations with low distortion .     % Embeddings  that  are  close  to  the  origin  of  the  disk  will have a relatively small distance to all other points, rep-resenting the root of the hierarchy.  On the other hand,embeddings that are close to the boundary of the disk will have a relatively large distance to all other points and are well suited to represent leaf nodes   % 3 - How are you going to solve it? In this work, we propose a fully hyperbolic neural model for fine-grained entity typing. Noticing a perfect match between hierarchical label inventories in the linguistic task and the benefits of hyperbolic spaces, we endow a classification model with a suitable geometry to capture this fundamental property of the data distribution. By virtue of the hyperbolic representations, the proposed approach automatically infers the latent hierarchy arising from the class distribution and achieves a meaningful and interpretable organization of the label space. This arrangement captures implicit hyponymic relations  in the inventory and enables the model to excel at fine-grained classification. To the best of our knowledge, this work is the first to apply hyperbolic geometry from beginning to end to perform multi-label classification on real NLP datasets.  %NICE PHRASE FROM GULCEHRE: The focus of this work is to endow neural network representations with suitable geometry to capture fundamental properties of data... given the perfect fit between the label distribution in the linguistic task of entity typing and the mathematical properties of hyperbolic spaces.   % esto deberia ser ""hay componentes ya hechos"". Y lo conecto al toque con el parrafo sig.  Recent work has proposed hyperbolic neural components, such as word embeddings , recurrent neural networks  and attention layers . %Advantages of hyperbolic representations are well-established for discrete data such as networks  and graphs . In the realm of Natural Language Processing  components that exploit hyperbolic geometry have been developed as well, such as word embeddings , recurrent neural networks  and attention layers . %or classifiers  Me encanta este paper pero no hace NLP :. We address these issues. Our model encodes textual inputs, applies a novel attention mechanism, and performs multi-class multi-label classification, executing all operations in the Poincar\'e model of hyperbolic space . %By employing the leveraging the geometric properties of hyperbolic space through    %The lack of systems that utilize hyperbolic space from beginning to end is due to three main difficulties: %First, there are different analytic models of hyperbolic space, and not all previous work operates in the same one, which hinders their combination.  %Second, it is not clear how to integrate these components into conventional Euclidean neural models since a mapping of the data from one space onto the other is required. Third, optimization of hyperbolic models is non-trivial.   %We bridge the gaps among previous work by developing the missing connections and adapting different components to employ the Poincar\'e model of hyperbolic space in all layers of the network.  % We bridge the gaps among previous work by developing the missing connections and adapting different components, in order to accomplish a full hyperbolic neural network. This is, a network that extracts features from text, applies attention layers and performs \todo{I am the only one doing this}{multi-class classification}, executing all operations in hyperbolic geometry.   % able to perform multi-label multi-class classification with text as input    %The model is proposed in a generic manner such that it can be applied to classify sequential data . Since hyperbolic geometry is naturally equipped to model hierarchical structures, we hypothesize that the model will excel at tasks that profit from the incorporation of hierarchical information. % \todo{awful}{systems} that operate under this metric space result in superior performance when incorporating hierarchical information.   %We evaluate our model on the task of fine-grained entity type classification , which we consider a suitable testbed due to its connection with textual inputs and hierarchical type inventories.  % Introduce main results % HNN's phrase: ""On a series of experiments and datasets we showcase the effectiveness of our hyperbolic neural network layers compared to their ""classic"" Euclidean variants on"" % \todo[inline]{Forwarding a bit of the results is a good idea . %\todo[inline]{Cambiar esta frase a la idea de que ""imponer the right metric es como imponer the right bias""}  %We impose an inductive bias on the model by means of the geometry of its internal representation. This allows us to operate on very low-dimensional spaces thus substantially reducing the parameter cost. Instead of relying on large pre-trained models, we impose a suitable inductive bias by choosing an adequate metric space to embed the data, which does not introduce extra burden on the parameter footprint. %Phrase from xiong2019inductiveBias: ""Instead of using an explicit graphical model, we enforce a relational bias on model parameters, which does not introduce extra burden on label decoding."" % Misma idea pero yo meto el bias en la representacion, lo cual no introduce un costo adicional y permite operar con MUCHOS menos par璋﹎etros.   %Our components are developed in a modular way which allows them to be seamlessly integrated into NLP architectures.    %\todo{Remove!}{While there now exist several hyperbolic components, a practitioner faced with these options has a simple question: How to integrate them with conventional layers? In this work, we answer this question.}  By means of the exponential and logarithmic maps  we are able to mix hyperbolic and Euclidean components into one model, aiming to exploit their strengths at different levels of the representation. We perform a thorough ablation that allows us to understand the impact of each hyperbolic component in the final performance of the system , and showcases its ease of integration with Euclidean layers.  %In summary, we make the following contributions: %%%%% % %      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions.  Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers. \footnote{Code available at:\\ \url{https://github.com/nlpAThits/hyfi}}",51
"  Entity Recognition  involves detection  and classification of entities mentioned in unstructured text into pre-defined categories. It is one of the foundational sub-task of several Information Extraction   and Natural Language Processing  pipelines. Hence, errors introduced during the extraction of entities can propagate further and degrade the performance of the complete IE or NLP pipeline. In the domains of experimental biology, the growing complexity of experiments has resulted in a need to automate wet laboratory procedures. Such an automation will be useful in avoiding human errors introduced in the wet lab protocols and thereby will enhance the reproducibility of experimental biological research.   To achieve this reproducibility, some of the previous research works have focussed on defining machine-readable formats for writing wet lab protocols . However, the vast majority of today閳ユ獨 protocols are written in natural language with jargon and colloquial language constructs that emerge as a byproduct of ad-hoc protocol documentation. This motivates the need for machine reading systems that can interpret the meaning of these natural language instructions, to enhance reproducibility via semantic protocols  and enable robotic automation  by mapping natural language instructions to executable actions. In order to enable research on interpreting natural language instructions, with practical applications in biology and life sciences, an annotated database  of wet lab protocols was introduced.   The first step in interpreting natural language lab protocols is to extract entities, followed by identification of relations between them. To address the research focussing on entity recognition over Wet Lab Protocols a shared task  was introduced at EMNLP WNUT-2020 Workshop. The task was based on the annotated database  of wet lab protocols. We tackle this task in two phases. In the first phase, we experiment with various contextualised word embeddings  and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and SLE.  The rest of the paper is structured as follows: Section 2 states the task definition. Section 3 describes the specifics of our methodology. Section 4 explains the experimental setup and the results, and Section 5 concludes the paper.  
"," In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with various contextualised word embeddings  and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and Structured Learning Ensembling . Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for the partial and exact match of the entity spans, respectively. We were ranked first and second, in terms of partial and exact match, respectively.",52
"  %  %       Reinforcement learning has shown great success in environments with large state spaces. Using neural networks to capture state representations has allowed end-to-end training of agents on domains like Atari  and Go . It is natural to emulate this success in text domains, especially given that the state space in language-based tasks is combinatorially large. A sentence of length  with allowed vocabulary  has  possible states, and tabular methods like learning  will fail unless coupled with powerful function approximators like neural networks.\\  While the current state of RL has multiple challenges, sparse rewards are one that leads to slow, and sometimes no convergence. Consider an agent learning in an environment with a large state space, with only a few states leading to a reward . An agent starting on the far left must take a large number of actions before encountering a reward. In turn, this sparse feedback results in a very noisy gradient for training the neural network. In an extreme scenario, as in Figure , an agent might have to take an exponential number of actions to reach a single leaf that has a reward.      Some early work, such as reward shaping , attempted to solve the sparse reward problem by introducing dense rewards based on heuristics, e.g., how close the agent is to the goal. However, these require complex design choices that might result in unexpected behavior from the agents.\\  Sparse rewards are common because they are the most straightforward way to specify how a task needs to be solved. If a robot is expected to pour water from a jug into a glass, the simplest way is to give a reward of  if it fills the glass, and  otherwise. This type of reward design is common in text-based games, in which the agent is rewarded upon reaching the goal state, and task-oriented dialogue, in which the agent is rewarded based on the successful completion of the task.\\  For this study, we examine text-based games and find that providing dense rewards with the help of sentiment analysis improves performance under some conditions.  
"," While reinforcement learning  has been successful in natural language processing  domains such as dialogue generation and text-based games, it typically faces the problem of sparse rewards that leads to slow or no convergence. Traditional methods that use text descriptions to extract only a state representation ignore the feedback inherently present in them. In text-based games, for example, descriptions like ``Good Job! You ate the food'' indicate progress, and descriptions like ``You entered a new room'' indicate exploration. Positive and negative cues like these can be converted to rewards through sentiment analysis. This technique converts the sparse reward problem into a dense one, which is easier to solve. Furthermore, this can enable reinforcement learning without rewards, in which the agent learns entirely from these intrinsic sentiment rewards. This framework is similar to intrinsic motivation, where the environment does not necessarily provide the rewards, but the agent analyzes and realizes them by itself. We find that providing dense rewards in text-based games using sentiment analysis improves performance under some conditions.",53
"  Natural language data is , but most of the structure is not visible at the surface.  Machine learning models tackling high-level language tasks would benefit from uncovering underlying structures such as trees, sequence tags, or segmentations.  Traditionally, practitioners turn to  approaches where an external, pretrained model is used to predict, , combining the transparency of the pipeline approach with the end-to-end unsupervised representation learning that makes deep models appealing. Moreover, large-capacity model tend to rediscover structure from scratch , so structured latent variables may reduce the required capacity.  Learning with discrete, combinatorial latent variables is, however, challenging, due to the intersection of  and  issues. For example, when learning a latent dependency tree, the latent parser must choose among an exponentially large set of possible trees; what's more, the parser may only learn from gradient information from the downstream task. If the highest-scoring tree is selected using an  operation, the gradients will be zero, preventing learning.  One strategy for dealing with the null gradient issue is to use a surrogate gradient, explicitly overriding the zero gradient from the chain rule, as if a different computation had been performed. The most commonly known example is the  , which pretends that the  node was instead an  operator. Such methods lead to a fundamental mismatch between the objective and the learning algorithm. The effect of this mismatch  is still insufficiently understood, and the design of successful new variants is therefore challenging. For example, the recently-proposed SPIGOT method  found it beneficial to use a projection as part of the surrogate gradient.  In this paper, we study surrogate gradient methods for deterministic learning with discrete structured latent variables. Our contributions are:   , thereby inducing pseudo-supervision on the latent variable. This leads to new insight into both STE and SPIGOT.   While the discrete methods do not outperform the relaxed alternatives using the same building \linebreak blocks, we hope that our interpretation and insights would trigger future latent structure research.  The code for the paper is available on \url{https://github.com/deep-spin/understanding-spigot}.      
"," Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on , a popular strategy to deal with this problem. We explore latent structure learning through the angle of  the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator  as well as the recently-proposed SPIGOT---a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.",54
"     than apricot---the student needs less data to train and can generalize better.  We show how this principle can apply equally well to improve unsupervised topic modeling, which to our knowledge has not previously been attempted.  While distillation usually involves two models of the same type, it  also apply to models of differing architectures. Our method is conceptually quite straightforward: we fine-tune a pretrained transformer  on a document reconstruction objective, where it acts in the capacity of an autoencoder. When a document is passed through this BERT autoencoder, it generates a distribution over words that includes unobserved but related terms. We then incorporate this distilled document representation into the loss function for topic model estimation.    To connect this method to the more standard supervised knowledge distillation, observe that the unsupervised ``task'' for both an autoencoder and a topic model is the reconstruction of the original document, i.e. prediction of a distribution over the vocabulary. The BERT autoencoder, as ``teacher'', provides a dense prediction that is richly informed by training on a large corpus. The topic model, as ``student'', is generating its own prediction of that distribution. We use the former to guide the latter, essentially as if predicting word distributions were a multi-class labeling problem. [1]{}  {BAT }   \left[ #2 \right] } {B} \DeclareMathOperator*{\argmin}{arg\,min}  \TeX}  \title{Improving Neural Topic Models using Knowledge Distillation}  \author{Alexander Hoyle\thanks{\, Equal contribution.} \\   Computer Science \\   University of Maryland \\   College Park, MD \\    \\\And   Pranav Goel\footnotemark[1] \\   Computer Science \\   University of Maryland \\   College Park, MD \\    \\\And   Philip Resnik \\   Linguistics / UMIACS \\   University of Maryland \\   College Park, MD \\    \\}  \date{}                              \clearpage \appendix 
","     Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.",55
"      Interactive systems capable of understanding natural language and responding in the form of natural language text have high potentials in various applications.  In pursuit of building and evaluating such systems, we study learning agents for Interactive Fiction  games. IF games are world-simulating software in which players use text commands to control the protagonist and influence the world, as illustrated in Figure. IF gameplay agents need to simultaneously understand the game's information from a text display  and generate natural language command  via a text input interface.  Without providing an explicit game strategy, the agents need to identify behaviors that maximize objective-encoded cumulative rewards.    IF games composed of human-written texts  create superb new opportunities for studying and evaluating natural language understanding  techniques due to their unique characteristics.   Game designers elaborately craft on the literariness of the narrative texts to attract players when creating IF games. The resulted texts in IF games are more linguistically diverse and sophisticated than the template-generated ones in synthetic text games.  The language contexts of IF games are more versatile because various designers contribute to enormous domains and genres, such as adventure, fantasy, horror, and sci-fi.  The text commands to control characters are less restricted, having sizes over six orders of magnitude larger than previous text games.  The recently introduced Jericho benchmark provides a collection of such IF games.   The complexity of IF games demands more sophisticated NLU techniques than those used in synthetic text games. Moreover, the task of designing IF game-play agents, intersecting NLU and reinforcement learning , poses several unique challenges on the NLU techniques. The first challenge is the difficulty of exploration in }. To make RL agents learn efficiently %via trial-and-error  without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others.  To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently; or embed each valid action as another vector and predict action value based on the vector-space similarities. These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient.   The second challenge is }.  At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world.  But the latest observation is often not a sufficient summary of the interaction history and may not provide enough information to determine the long-term effects of actions.  Previous approaches address this problem by building a representation over past observations . These methods treat the historical observations equally and summarize the information into a single vector without focusing on important contexts related to the action prediction for the current observation. Therefore, their usages of history also bring noise, and the improvement is not always significant.  We propose a novel formulation of IF game playing as Multi-Passage Reading Comprehension  and harness MPRC techniques to solve the  and  challenges. The graphical illustration is shown in Figure.  First, the action value prediction  is essentially . We base on the fact that each action is an instantiation of a template, i.e., a verb phrase with a few placeholders of object arguments it takes~. Then the action generation process can be viewed as extracting objects for a template's placeholders from the textual observation, based on the interaction between the template verb phrase and the relevant context of the objects in the observation. Our approach addresses the structured prediction and interaction problems with the idea of context-question attention mechanism in RC models.  Specifically, we treat the observation as a passage and each template verb phrase as a question.  The filling of object placeholders in the template thus becomes an extractive QA problem that selects objects from the observation given the template. Simultaneously each action  gets its evaluation value predicted by the RC model. Our formulation and approach better capture the fine-grained interactions between observation texts and structural actions, in contrast to previous approaches that represent the observation as a single vector and ignore the fine-grained dependency among action elements.  Second, alleviating partial observability is essentially  and . Our approach retrieves potentially relevant historical observations with an object-centric approach  , so that the retrieved ones are more likely to be connected to the current observation as they describe at least one shared interactable object. Our attention mechanisms are then applied across the retrieved multiple observation texts to focus on informative contexts for action value prediction.   We evaluated our approach on the suite of Jericho IF games, compared to all previous approaches. Our approaches achieved or outperformed the state-of-the-art performance on 25 out of 33 games, trained with less than one-tenth of game interaction data used by prior art.  We also provided ablation studies on our models and retrieval strategies.     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," Interactive Fiction  games with real human-written natural language texts provide a new natural evaluation for language understanding techniques.  In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension  tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations.  Extensive experiments on the recent IF benchmark  demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.\footnote{Source code is available at: \url{https://github.com/XiaoxiaoGuo/rcdqn}. }",56
"   Recent advances in self-supervised pre-training have resulted in impressive downstream performance on several NLP tasks. However, this has led to the development of enormous models, which often require days of training on non-commodity hardware . Furthermore, studies have shown that it is quite challenging to successfully train these large Transformer models, requiring complicated learning schemes and extensive hyperparameter tuning.  Despite these expensive training regimes, recent studies have found that once trained, these bi-directional language models exhibit simple patterns of self-attention without much linguistic backing. For example, 40\% of heads in a pre-trained BERT model simply pay attention to delimiters added by the tokenizer . Since these attention patterns are independent of linguistic phenomena, a natural question arises: can Transformer models be guided towards such attention patterns without requiring extensive training?    In this paper, we propose an attention guidance  mechanism for self-attention modules in Transformer architectures to enable faster, more efficient, and robust self-supervised learning. Our approach is simple and agnostic to the training objective. Specifically, we introduce an auxiliary loss function to guide the self-attention heads in each layer towards a set of pre-determined patterns . These patterns encourage the formation of both  global  and local  structures in the model.   Through several experiments, we show that our approach enables training large Transformer models considerably faster 閳 for example, we can train a 16-layer RoBERTa model with SOTA performance on a low-resource domain in just two days using four GPUs, while excluding our loss leads to slow or no convergence. Our method also achieves competitive performance with BERT on three English natural language understanding tasks, and outperforms the baseline masked language modeling  models on eleven out of twelve settings considered.  Further, we also show that our initialization is agnostic to the training objective by demonstrating gains on the replaced token detection objective proposed by ELECTRA and on machine translation with Transformers. Finally, we provide an analysis of the attention heads learned using our method. Surprisingly, contrary to recent studies, we find that it is possible to train models that perform well on language modeling without learning a single attention head that models coreferences. % . For example, our model fails the co-reference test in  while still performing well on language modeling and downstream tasks.  To summarize, our main contributions are:  	
"," % Despite being successful in downstream language understanding tasks, modern language models~ contain millions of parameters and require multiple days of training on specialized hardware such as TPUs. Training such models on commodity hardware  often means slow convergence, making it practically intractable for many researchers.  In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.\footnote{Code: {https://github.com/ameet-1997/AttentionGuidance}}",57
" %  % Transformer models  have outperformed previously used RNN based models and traditional statistical MT techniques.  This improvement, though, comes at the cost of higher computation complexity. The decoder computation is sequential and becomes the bottleneck due to the autoregressive nature, large depth and self-attention structure.   % Another recent trend has been making the models larger and ensembling multiple models to achieve the best possible translation quality . Leading solutions on common benchmark  usually use an ensemble of Transformer big models, which combined can have more than 1 billion parameters.   % In this paper, we focus on developing architectures which are faster during inference and have less number of parameters, without sacrificing translation quality.  % Recent work  proposed methods to replace self-attention in the decoder with simpler simple recurrent units  and used knowledge distillation to simplify training for the final architecture.  also proposed to make the decoder lightweight by training a deep-encoder, shallow decoder architecture. Another line of effort to make NMT architectures more efficient is pruning different components of the model.  and  show that most of the attention heads in the network learn redundant information and can be pruned away.  % All of the above works use the vanilla Transformer architecture as their baseline, so it is not clear if these approaches can give complimentary results when combined together. In this work, we explore and benchmark combining all of the above techniques, with the goal of maximizing inference speed without hurting in translation quality. % %We adapt the same approach and  extend it with the following ideas. First, we optimized the SSRU to make it more efficient. Second, we removed the feed-forward network in the decoder completely. Then, we kept only 1 layer in the decoder and used very deep encoder. Last we pruned all the redundant heads in the deep encoder.  % After carefully stacking the approaches, our proposed architecture is able to achieve a significant speed improvement of 84\% on GPU and 102\% on CPU architectures without any degradation of translation quality in terms of BLEU.  % %%%%%%%% original Related Work %%%%%%%%% % 
"," Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to $109$\% and $84$\% speedup on CPU and GPU respectively and reduce the number of parameters by $25$\% while maintaining the same translation quality in terms of BLEU. %State-of-the-art neural machine translation has become compute and parameter intensive in the last several years, which puts significant pressure on the latency and hardware resources during inference. In this paper, we change the standard Transformer architecture to reduce the number of parameters and increase inference speed without sacrificing translation quality. We demonstrate that combination of replacing decoder self-attention with the simpler simple recurrent units, adopting a deep encoder and shallow decoder architecture, and multi-head attention pruning, we can achieve up to 102\% speedup and reduce the number of parameters by 13\% while maintaining the same translation quality in terms of BLEU.",58
" Intent Detection  is a crucial task in natural language understanding, whose objective is to extract underlying intents behind the given utterances. The extracted intents could provide further contexts for further downstream Natural Language Processing tasks such as dialogue state tracking or question answering. Unlike traditional text classification, ID is challenging for two main reasons  Utterances are usually short and diversely expressed,  Emerging intents occur continuously, especially across different domains .  Despite recent advances, state-of-the-art ID methods  require a large amount of annotated data to achieve competitive performance. This requirement inhibits models' capability in generalizing to newly emerging intents with no or limited annotations during inference. Re-training or fine-tuning large models on few samples of emerging classes could easily lead to overfitting problems.      Motivated by human capability in correctly categorizing new classes with only a few examples , few-shot learning  paradigms are adopted to tackle the scarcity problems of emerging classes. FSL methods take advantage of a small set of labeled examples  to learn how to discriminate unlabeled samples  between classes, even those not seen during training.  Recent works in FSL  focus on learning the matching information between the labeled samples  and the unlabeled samples  to provide additional contextual information for instance-level representations, leading to effective prototype representation. However, these methods only extract similarity based on fine-grained word semantics, failing to capture the diverse expressions of users' utterances. This problem could further lead to overfitting either to seen intents or novel intents, especially in the challenging Generalized Few-shot Intent Detection  setting  where both seen and novel intents are existent in a joint label space during inference. Instead, matching support and query samples on coarser-grained semantic components could provide additional informative contexts beyond word levels. For instance, two utterances ""i need to get a table at a pub with southeastern cuisine"" and ``book a spot for six friends"" share a similar intent label ``Book Restaurant"". While word-level semantics might find similar action words as ``get"" and ``book"", these words do not necessarily contribute to the correct intent findings. Instead, coarser-grained semantics such as ``get a table"" and ``book a spot"" could provide further hints to identify ``Book Restaurant"" intent.      As semantic components  could be effectively extracted from multi-head self-attention, matching these SC between support and query can enhance both query and support representations, leading to improvements in generalization from seen training classes to unseen testing classes. To further enhance the dynamics of extracted SC across various domains and diversely expressed utterances, we introduce additional head regularizations. In addition, to overcome the insufficiency of a single similarity measure for matching sentences with diverse semantics, a more comprehensive matching method is further explored.      Our main contribution is summarized as follows:        
"," Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available \footnote{\url{https://github.com/nhhoang96/Semantic\_Matching}} .",59
" Multilingual Neural Machine Translation , which leverages a single NMT model to handle the translation of multiple languages, has drawn research attention in recent years. MNMT is appealing since it greatly reduces the cost of training and serving separate models for different language pairs. It has shown great potential in knowledge transfer among languages, improving the translation quality for low-resource and zero-shot language pairs.  Previous works on MNMT has mostly focused on model architecture design with different strategies of parameter sharing or representation sharing. Existing MNMT systems mainly rely on bitext training data, which is limited and costly to collect. Therefore, effective utilization of monolingual data for different languages is an important research question yet is less studied for MNMT.  Utilizing monolingual data  has been widely explored in various NMT and natural language processing  applications. Back translation , which leverages a target-to-source model to translate the target-side monolingual data into source language and generate pseudo bitext, has been one of the most effective approaches in NMT. However, well trained NMT models are required to generate back translations for each language pair, it is computationally expensive to scale in the multilingual setup. Moreover, it is less applicable to low-resource language pairs without adequate bitext data. Self-supervised pre-training approaches, which train the model with denoising learning objectives on the large-scale monolingual data, have achieved remarkable performances in many NLP applications. However, catastrophic forgetting effect, where finetuning on a task leads to degradation on the main task, limits the success of continuing training NMT on models pre-trained with monolingual data. Furthermore, the separated pre-training and finetuning stages make the framework less flexible to introducing additional monolingual data or new languages into the MNMT system.  In this paper, we propose a multi-task learning  framework to effectively utilize monolingual data for MNMT. Specifically, the model is jointly trained with translation task on multilingual parallel data and two auxiliary tasks: masked language modeling  and denoising auto-encoding  on the source-side and target-side monolingual data respectively. We further present two simple yet effective scheduling strategies for the multilingual and multi-task framework. In particular, we introduce a dynamic temperature-based sampling strategy for the multilingual data. To encourage the model to keep learning from the large-scale monolingual data, we adopt dynamic noising ratio for the denoising objectives to gradually increase the difficulty level of the tasks.   We evaluate the proposed approach on a large-scale multilingual setup with  language pairs from the WMT datasets. We study three English-centric multilingual systems, including many-to-English, English-to-many, and many-to-many. We show that the proposed MTL approach significantly boosts the translation quality for both high-resource and low-resource languages. Furthermore, we demonstrate that MTL can effectively improve the translation quality on zero-shot language pairs with no bitext training data. In particular, MTL achieves even better performance than the pivoting approach for multiple low-resource language pairs. We further show that MTL outperforms pre-training approaches on both NMT tasks as well as cross-lingual transfer learning for NLU tasks, despite being trained on very small amount of data in comparison to pre-training approaches.  The contributions of this paper are as follows. First, we propose a new MTL approach to effectively utilize monolingual data for MNMT. Second, we introduce two simple yet effective scheduling strategies, namely the dynamic temperature-based sampling and dynamic noising ratio strategy. Third, we present detailed ablation studies to analyze various aspects of the proposed approach. Finally, we demonstrate for the first time that MNMT with MTL models can be effectively used for cross-lingual transfer learning for NLU tasks with similar or better performance than the state-of-the-art massive scale pre-trained models using single task.   
"," While monolingual data has been shown to be useful in improving bilingual neural machine translation , effectively and efficiently leveraging monolingual data for Multilingual NMT  systems is a less explored area. In this work, we propose a multi-task learning  framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with $10$ language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",60
"  Neural machine translation  is a data-hungry approach, which requires a large amount of data to train a well-performing NMT model. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult.  To relieve this problem, several approaches have been proposed to better exploit the training data, such as curriculum learning, data diversification, and data denoising.  In this paper, we explore an interesting alternative which is to reactivate the { to rejuvenate the inactive examples to improve the performance of NMT models.  Specifically, we train an NMT model on the active examples as the rejuvenation model to re-label the inactive examples, resulting in the rejuvenated examples~. The final NMT model is trained on the combination of the active examples and rejuvenated examples. Experimental results show that the data rejuvenation approach consistently and significantly improves performance on SOTA NMT models  on the benchmark WMT14 English-German and English-French datasets~. Encouragingly, our approach is also complementary to existing data manipulation methods , and combining them can further improve performance.   [t]     , then rejuvenated by the {   Finally, we conduct extensive analyses to better understand the inactive examples and the proposed data rejuvenation approach. Quantitative analyses reveal that the inactive examples are more difficult to learn than active ones, and rejuvenation can reduce the learning difficulty~. The rejuvenated examples stabilize and accelerate the training process of NMT models~, resulting in final models with better generalization capability~.  Our contributions of this work are as follows:       
"," Large-scale training datasets lie at the core of the recent success of neural machine translation  models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce  to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases.  First, we train an { on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed  consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.}  %In this work, we propose to improve the training of NMT models on large-scale datasets by exploiting inactive training examples, which contribute less to the model performance. Specifically, the proposed framework consists of three phases. First, we identify the inactive examples with their sentence-level prediction confidence assigned by an { on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed  consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.",61
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.   
"," This document contains the instructions for preparing a manuscript for the proceedings of EMNLP 2020. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.",62
" Modern neural machine translation~ models employ sufficient capacity to fit the massive data well by utilizing a large number of parameters, and suffer from the widely recognized issue, namely, over-parameterization. For example,  showed that over 40\% of the parameters in an RNN-based NMT model can be pruned with negligible performance loss. However, the low utilization efficiency of parameters results in a waste of computational resources , as well as renders the model stuck in a local optimum.   In response to the over-parameterization issue, network pruning has been widely investigated for both computer vision   and natural language processing  tasks . Recent work has proven that such spare parameters can be reused to maximize the utilization of models in CV tasks such as image classification. The leverage of parameter rejuvenation in sequence-to-sequence learning, however, has received relatively little attention from the research community. In this paper, we empirically study the efficiency issue for NMT models.  Specifically, we first investigate the effects of weight pruning on advanced Transformer models, showing that 20\% parameters can be directly pruned, and by continuously training the sparse networks, we can prune 50\% with no performance loss. Starting from this observation, we then exploit whether these redundant parameters are able to be re-utilized for improving the performance of NMT models. Experiments are systematically conducted on different datasets  and NMT architectures . Results demonstrate that the rejuvenation approach can significantly and consistently improve the translation quality by up to +0.8 BLEU points. Further analyses reveal that the rejuvenated parameters are reallocated to enhance the ability to model the source-side low-level information, lacking of which leads to a number of problems in NMT models.   Our key contributions are:  \setlength   
"," Modern neural machine translation  models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.",63
"  Sentiment analysis  has attracted increasing attention recently. Aspect-based sentiment analysis   is a fine-grained sentiment analysis task and includes many subtasks, two of which are aspect category detection  that detects the aspect categories mentioned in a sentence and aspect-category sentiment analysis  that predicts the sentiment polarities with respect to the detected aspect categories. Figure shows an example. ACD detects the two aspect categories,  and , and ACSA predicts the negative and positive sentiment toward them respectively. In this work, we focus on ACSA, while ACD as an auxiliary task is used to find the words indicating the aspect categories in sentences for ACSA.    Since a sentence usually contains one or more aspect categories, previous studies have developed various methods for generating aspect category-specific sentence representations to detect the sentiment toward a particular aspect category in a sentence. To name a few, attention-based models  allocate the appropriate sentiment words for the given aspect category.  proposed to generate aspect category-specific representations based on convolutional neural networks and gating mechanisms. Since aspect-related information may already be discarded and aspect-irrelevant information may be retained in an aspect independent encoder, some existing methods  utilized the given aspect to guide the sentence encoding from scratch. Recently, BERT based models  have obtained promising performance on the ACSA task. However, these models ignored that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category. It leads to suboptimal performance of these models. For the example in Figure, both ``drinks'' and ``food'' indicate the aspect category . The sentiment about  is a combination of the sentiments of ``drinks'' and ``food''. Note that, words indicating aspect categories not only contain aspect terms explicitly indicating an aspect category but also contain other words implicitly indicating an aspect category . In Figure, while ``drinks'' and ``food'' are aspect terms explicitly indicating the aspect category , ``large'' and ``noisy'' are not aspect terms implicitly indicating the aspect category .  In this paper, we propose a Multi-Instance Multi-label Learning Network for Aspect-Category sentiment analysis . AC-MIMLLN explicitly models the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category. Specifically, AC-MIMLLN treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances  of the aspect category. Given a bag and the aspect categories mentioned in the bag, AC-MIMLLN first predicts the instance sentiments, then finds the key instances for the aspect categories, finally aggregates the sentiments of the key instances to get the bag-level sentiments of the aspect categories.  Our main contributions can be summarized as follows:  	  
"," 	Aspect-category sentiment analysis  aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis , which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN \footnote{Data and code are available at https://github.com/l294265421/AC-MIMLLN}.",64
" The recent success of the  approaches~, which train language models on diverse text corpora with self-supervised or multi-task learning, have brought up huge performance improvements on several natural language understanding  tasks~. The key to this success is their ability to learn generalizable text embeddings that achieve near optimal performance on diverse tasks with only a few additional steps of fine-tuning on each downstream task.    Most of the existing works on language model aim to obtain a universal language model that can address nearly the entire set of available natural language tasks on heterogeneous domains. Although this train-once and use-anywhere approach has been shown to be helpful for various natural language tasks~, there have been considerable needs on adapting the learned language models to domain-specific corpora . Such domains may contain new entities that are not included in the common text corpora, and may contain only a small amount of labeled data as obtaining annotation on them may require expert knowledge.  Some recent works~ suggest to further pre-train the language model with self-supervised tasks on the domain-specific text corpus for adaptation, and show that it yields improved performance on tasks from the target domain.  Masked Language Models  objective in BERT~ has shown to be effective for the language model to learn the knowledge of the language in a bi-directional manner~. In general, masks in MLMs are sampled at random~, which seems reasonable for learning a generic language model pre-trained from scratch, since it needs to learn about as many words in the vocabulary as possible in diverse contexts.  However, in the case of further pre-training of the already pre-trained language model, such a conventional selection method may lead a domain adaptation in an inefficient way, since not all words will be equally important for the target task. Repeatedly learning for uninformative instances thus will be wasteful. Instead, as done with instance selection, it will be more effective if the masks focus on the most important words for the target domain, and for the specific NLU task at hands. How can we then  such a masking strategy to train the MLMs?   Several works~ propose rule-based masking strategies which work better than random masking ~ when applied to language model pre-training from scratch. Based on those works, we assume that adaptation of the pre-trained language model can be improved via a  masking policy which selects the words to mask. Yet, existing models are inevitably suboptimal since they do not consider the target domain and the task. To overcome this limitation, in this work, we propose to adaptively generate mask by learning the optimal masking policy for the given task, for the task-adaptive pre-training~ of the language model.  As described in Figure , we want to further pre-train the language model on a specific task with a task-dependent masking policy, such that it directs the solution to the set of parameters that can better adapt to the target domain, while task-agnostic random policy leads the model to an arbitrary solution.  To tackle this problem, we pose the given learning problem as a meta-learning problem where we learn the task-adaptive mask-generating policy, such that the model learned with the masking strategy obtains high accuracy on the target task.  We refer to this meta-learner as the Neural Mask Generator . Specifically, we formulate mask learning as a bi-level problem where we pre-train and fine-tune a target language model in the inner loop, and learn the NMG at the outer loop, and solve it using renforcement learning. We validate our method on diverse NLU tasks, including question answering and text classification. The results show that the models trained using our NMG outperforms the models pre-trained using rule-based masking strategies, as well as finds a proper adaptive masking strategy for each domain and task.  Our contribution is threefold:  	  
"," We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task . Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our  on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings. \footnote{Code is available at \url{github.com/Nardien/NMG}.}",65
"   Sentiment analysis has become an increasingly popular natural language  processing  task in academia and industry.  It provides real-time  feedback on consumer experience and their needs, which helps  producers to offer better services.  To deal with the presence of  multiple categories in one document,  ACSA tasks, including aspect-category  sentiment analysis  and targeted aspect-category sentiment analysis , were introduced.   The main purpose for ACSA task  is to identify sentiment polarity  of an input sentence upon specific predefined categories . For  example, as shown in Table , giving an input sentence ``Food is  always fresh and hot-ready to eat, but it is too expensive."" and predefined categories \{food, service, price,  ambience and anecdotes/miscellaneous\},  the sentiment of category food is positive, the polarity  regarding to category price is negative, while is none for others.  In this task, the models should  capture both explicit expressions and implicit expressions. For example, the phrase ``too expensive"" indicates the  negative polarity  in the price category, without a direct indication of ``price"".    In order to  deal with ACSA with both multiple categories and multiple targets, TACSA task was introduced  to analyze sentiment polarity on a set of predefined target-category pairs. An example is shown in Table , given targets ``restaurant-1"" and ``restaurant-2"", in the case ``I like  restaurant-1 because it's cheap, but restaurant-2 is too  expansive"", the category price for target ``restaurant-1"" is positive, but is  negative for target ``restaurant-2"", while is none for other target-category pairs. A mathematical definition for ACSA is given  as follows: giving a  sentence  as input, a predefined set of targets  and a predefined set of  aspect categories , a model predicts the sentiment polarity  for  each target-category pair $\{ : t [!t] 	 	{ 		{|c|c|c|}%{l*{3}{c}} 			%\toprule[1pt] 			 	   Multi-task learning, with shared encoders but individual decoders for each category, is an approach to analyze all the categories in one sample simultaneously for ACSA . Compared with single-task ways , multi-task approaches utilize category-specific knowledge in training signals from each task and get better performance. However, current multi-task models still suffer from a lack of  features such as category name . Models with category name features encoded in the model may further improve the performance.  On the other hand, the predefined categories in ACSA task make the application  in new categories inflexible, as for ACSA applications, the number of categories maybe  varied over time.  For example, fuel consumption, price level, engine power, space and so  on are source categories to be analyzed in the gasoline automotive domain. For  electromotive domain, source categories in the automotive domain will still be used, while new target category such as battery duration should also be analyzed.  Incremental learning is a way to solve this problem. Therefore, it is necessary to propose an  incremental learning task and an incremental learning model concerned with new  category for ACSA tasks.  Unfortunately, in the current multi-task learning ACSA models, the encoder is shared but the decoders for each category are individual. This parameter sharing mechanism results in only the shared encoder  and target-category-related decoders are finetuned during the finetuning process, while the decoder of source categories remains unchanged. The finetuned encoder and original decoder of source categories may cause catastrophic forgetting problem in the origin  categories. For real applications, high accuracy is excepted in source  categories and target  categories.  Based on the previous researches that decoders between different tasks are usually modeled by mean regularization   , an idea comes up to further make the decoders the same by sharing the decoders in all categories to decrease the catastrophic forgetting problem. But here raises another question, how to identify each category in the encoder and decoder shared network? In our approach, we  solve the category discrimination problem by the input category name feature.   In this paper,  we proposed a multi-task category name embedding network  .  The multi-task learning  framework makes full use of training signals from all categories. To make it feasible for incremental learning, both encoder and decoders for each category are shared. The category names were applied as another input feature for task discrimination. We also present a new task for ACSA incremental learning. In particular,  our contribution is three-folded:    We proposed a multi-task CNE-net framework with both encoder and decoder shared to weaken catastrophic forgetting problem in multi-task learning ACSA model.     We achieved  state-of-the-art on the two ACSA datasets, SemEval14-Task4  and Sentihood.   We proposed a new task for incremental learning in ACSA. By sharing both encoder layers and decoder layers of all the tasks, we   achieved better results compared with other baselines both in source  categories and in the target category.   
"," ACSA tasks, including aspect-category sentiment analysis  and  targeted  aspect-category sentiment analysis , aims at identifying sentiment  polarity on predefined categories. Incremental learning on new categories is necessary for ACSA real applications. Though current multi-task learning models achieve good performance in ACSA tasks, they suffer from catastrophic forgetting problems in ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name  Embedding network  . We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination.  Our model achieved state-of-the-art  on two ACSA benchmark datasets. Furthermore, we proposed  a dataset for ACSA incremental learning and achieved the best performance compared with other strong baselines.",66
"   Conditional random fields  have been shown to perform well in various sequence labeling tasks. Recent work uses rich neural network architectures to define the ``unary'' potentials, i.e., terms that only consider a single position's label at a time~. However, ``binary'' potentials, which consider pairs of adjacent labels, are usually quite simple and may consist solely of a parameter or parameter vector for each unique label transition. Models with unary and binary potentials are generally referred to as ``first order'' models.   A major challenge with CRFs is the complexity of training and inference, which are quadratic in the number of output labels for first order models and grow exponentially when higher order dependencies are considered. This explains why the most common type of CRF used in practice is a first order model, also referred to as a ``linear chain'' CRF.   One promising alternative to CRFs is structured prediction energy networks , which use deep neural networks to parameterize arbitrary potential functions for structured prediction. While SPENs also pose challenges for learning and inference,  proposed a way to train SPENs jointly with ``inference networks'', neural networks trained to approximate structured  inference.   In this paper, we leverage the frameworks of SPENs and inference networks to explore high-order energy functions for sequence labeling. Naively instantiating high-order energy terms can lead to a very large number of parameters to learn, so we instead develop concise neural parameterizations for high-order terms. In particular, we draw from vectorized Kronecker products, convolutional networks, recurrent networks, and self-attention.  We also consider ``skip-chain'' connections~ with various skip distances and ways of reducing their total parameter count for increased learnability.   Our experimental results on four sequence labeling tasks show that a range of high-order energy functions can yield performance improvements. While the optimal energy function varies by task, we find strong performance from skip-chain terms with short skip distances, convolutional networks with filters that consider label trigrams, and recurrent networks and self-attention networks that consider large subsequences of labels.     We also demonstrate that modeling high-order dependencies can lead to significant performance improvements in the setting of noisy training and test sets.  Visualizations of the high-order energies show various methods capture intuitive structured dependencies among output labels.   Throughout, we use inference networks that share the same architecture as unstructured classifiers for sequence labeling, so test time inference speeds are unchanged between local models and our method.  Enlarging the inference network architecture by adding one layer leads consistently to better results, rivaling or improving over a BiLSTM-CRF baseline,  suggesting that training efficient inference networks with high-order energy terms can make up for errors arising from approximate inference. While we focus on sequence labeling in this paper, our results show the potential of developing high-order structured models for other NLP tasks in the future.   
"," Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks~ for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers.  We also find high-order energies to help in noisy data conditions.\footnote{Code  is available at \url{https://github.com/tyliupku/Arbitrary-Order-Infnet}}",67
" Long document coreference resolution poses runtime and memory challenges. Current best models % for coreference resolution have large memory requirements and quadratic runtime in the document length~, making them impractical for long documents. %  Recent work revisiting the entity-mention paradigm~, which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-the-art models~. In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters~ , the entity-mention paradigm stores representations only of the entity clusters, which are updated incrementally as coreference predictions are made. While such an approach requires less memory than those that additionally store mention representations, the number of entities can be impractically large when processing long documents, making the storing of all entity representations problematic.  Is it necessary to maintain an unbounded number of mentions or entities?  Psycholinguistic evidence suggests it is not, as human language processing is incremental  and has limited working memory~. In practice, we find that most entities have a small spread , and thus do not need to be kept persistently in memory. This observation suggests that tracking a limited, small number of entities at any time can resolve the computational %  issues, albeit at a potential accuracy tradeoff.  Previous work on bounded memory models for coreference resolution has shown potential, but has been tested only on short documents  % . % Moreover, this previous work makes token-level predictions while standard coreference datasets have span-level annotations.  % We propose a bounded memory model that performs quasi-online coreference resolution,    
"," Long document coreference resolution remains a challenging task	 due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. % We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that  the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and  the model learns an efficient memory management strategy easily outperforming a rule-based strategy.",68
" Since the early days of NLP, conversational agents have been designed to interact with humans through language to solve diverse tasks, e.g., remote instructions or booking assistants . In this goal-oriented dialogue setting, the conversational agents are often designed to compose with predefined language utterances. Even if such approaches are efficient, they also tend to narrow down the agent's language diversity.  To remove this restriction, recent work has been exploring interactive word-based training. In this setting, the agents are generally trained through a two-stage process: Firstly, the agent is pretrained on a human-labeled corpus through supervised learning to generate grammatically reasonable sentences. Secondly, the agent is finetuned to maximize the task-completion score by interacting with a user. Due to sample-complexity and reproducibility issues, the user is generally replaced by a game simulator that may evolve with the conversational agent. Unfortunately, this pairing may lead to the  phenomenon, where the conversational agents gradually co-adapt, and drift away from the pretrained natural language. The model thus becomes unfit to interact with humans.  While domain-specific methods exist to counter language drift, a simple task-agnostic method consists of combining interactive and supervised training losses on a pretraining corpus, which was later formalized as Supervised SelfPlay  .   Inspired by language evolution and cultural transmission, recent work proposes Seeded Iterated Learning  as another task-agnostic method to counter language drift. SIL modifies the training dynamics by iteratively refining a pretrained student agent by imitating interactive agents, as illustrated in Figure. At each iteration, a teacher agent is created by duplicating the student agent, which is then finetuned towards task completion. A new dataset is then generated by greedily sampling the teacher, and those samples are used to refine the student through supervised learning. The authors empirically show that this iterated learning procedure induces an inductive learning bias that successfully maintains the language grounding while improving task-completion.  [ht]               \vskip -0.6em                    \vskip -1em   As a first contribution, we further examine the performance of these two methods in the setting of a translation game.  We show that S2P is unable to maintain a high grounding score and experiences a late-stage collapse, while SIL has a higher negative likelihood when evaluated on human corpus.  We propose to combine SIL with S2P by applying an S2P loss in the interactive stage of SIL. We show that the resulting   algorithm manages to get the best of both algorithms in the translation game. Finally, we observe that the late-stage collapse of S2P is correlated with conflicting gradients before showing that \algo empirically reduces this gradient discrepancy.    
"," Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay  and Seeded Iterated Learning .  While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce \longalgo~ to combine both methods to minimize their respective weaknesses.  We then show the effectiveness of \algo in the language-drift translation game.",69
"  Advances in pretraining language models   as general-purpose representations have pushed  the state of the art on a variety of natural language tasks. However, not all languages enjoy large public datasets for pretraining and/or downstream tasks. Multilingual language models such as mBERT  and XLM   have been proven effective for cross-lingual transfer learning  by pretraining a single shared Transformer model   jointly on multiple languages. The goals of multilingual modeling are not limited  to improving language modeling in low-resource languages ,  but also include zero-shot cross-lingual transfer on downstream tasks---it  has been shown that multilingual models can generalize to target languages  even when labeled training data is only available in the source language   on a wide range of tasks .   However, multilingual models are not equally beneficial for all languages.  demonstrated that including more languages in a single model  can improve performance for low-resource languages but hurt performance for high-resource languages. Similarly, recent work   in multilingual neural machine translation  also observed  performance degradation on high-resource language pairs. In multi-task learning , this phenomenon is known as  or  ,  where training multiple tasks jointly hinders the performance on individual tasks. % In multilingual language modeling, each language is a single task and negative interference during pretraining can hurt the model's generalization on individual languages.  Despite these empirical observations, little prior work analyzed or showed  how to mitigate negative interference in multilingual language models. Particularly, it is natural to ask:  Can negative interference occur for low-resource languages also?  What factors play an important role in causing it?  Can we mitigate negative interference to improve the model's cross-lingual transferability?   In this paper, we take a step towards addressing these questions. We pretrain a set of monolingual and bilingual models and evaluate them  on a range of downstream tasks to analyze negative interference. We seek to individually characterize the underlying factors of negative interference  through a set of ablation studies and glean insights on its causes. Specifically, we examine if training corpus size and language similarity affect negative interference,  and also measure gradient and parameter similarities between languages.  Our results show that negative interference can occur in both high-resource and low-resource languages. In particular, we observe that neither subsampling the training corpus  nor adding typologically similar languages substantially impacts negative interference. On the other hand, we show that gradient conflicts  and language-specific parameters do exist in multilingual models,  suggesting that languages are fighting for model capacity, which potentially causes negative interference. We further test whether explicitly assigning language-specific modules  to each language can alleviate negative interference, and find that the resulting model performs better  within each individual language but worse on zero-shot cross-lingual tasks.  Motivated by these observations, we further propose  to meta-learn these language-specific parameters  to explicitly improve generalization of shared parameters on all languages. Empirically, our method improves not only within-language performance on monolingual tasks  but also cross-lingual transferability on zero-shot transfer benchmarks. To the best of our knowledge, this is the first work to systematically study  and remedy negative interference in multilingual language models.  % Advances in pretraining language models   % as general-purpose representations have pushed the state-of-the-art on a variety of natural language tasks. % However, not all languages have large amounts of training data for pretraining and/or downstream tasks. % Multilingual language models such as mBERT  and XLM  have been proven effective for cross-lingual transfer learning by pretraining a single shared Transformer model  jointly on multiple languages. % The goal is to not only improve language modeling in low-resource languages , but also enable zero-shot cross-lingual transfer on downstream tasks -- it has been shown that multilingual models can generalize to target languages when labeled training data is only available in the source language  on a wide range of tasks .   % However, multilingual models are not equally beneficial for all languages. %  demonstrated that including more languages in a single model can improve performance for low-resource languages but hurt performance for high-resource languages. % Similarly, recent work  in multilingual neural machine translation  also observed performance degradation on high-resource language pairs. % In multi-task learning , this phenomenon is known as  or  ,  % where training multiple tasks jointly hinders the performance on individual tasks. % % In multilingual language modeling, each language is a single task and negative interference during pretraining can hurt the model's generalization on individual languages.  % Despite these empirical observations, little prior work analyzed or showed how to mitigate negative interference in multilingual language models. % Particularly, it is natural to ask: %  Can negative interference occur for low-resource languages also? %  What factors play an important role in causing it? %  Can we mitigate negative interference to improve the model's cross-lingual transferability?   % In this paper, we take a step towards addressing these questions. % We pretrain a set of monolingual and bilingual models, and evaluate them on a range of downstream tasks to analyze negative interference. % We seek to individually characterize the underlying factors of negative interference through a set of ablation studies and glean insights on its causes. % Specifically, we examine if training corpus size and language similarity affect negative interference, and also measure gradient and parameter similarities between languages.  % Our results show that negative interference can occur in both high-resource and low-resource languages. % In particular, we observe that subsampling the training corpus or adding typologically similar languages has little impact on negative interference. % On the other hand, we show that gradient conflicts and language-specific parameters do exist in multilingual models, suggesting that languages are fighting for model capacity which potentially causes negative interference. % Thus, we further test whether explicitly assigning language-specific modules to each language can alleviate negative interference.  % To our surprise, the model performs better within each individual language but worse on zero-shot cross-lingual tasks.  % Motivated by these observations, we further propose to meta-learn these language-specific parameters to explicitly improve generalization of shared parameters on all languages. % Empirically, our method improves not only within-language performance on monolingual tasks but also cross-lingual transferability on zero-shot transfer benchmarks. % To the best of our knowledge, this is the first work to systematically study and treat negative interference in multilingual language models.   
"," Modern multilingual models are trained on concatenated text  from multiple languages in hopes of conferring benefits to each , with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade  performance on high-resource languages,  a phenomenon known as . In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief,  negative interference  also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures,  we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations,  we also present a meta-learning algorithm that obtains  better cross-lingual transferability  and alleviates negative interference,  by adding language-specific layers as meta-parameters  and training them in a manner that explicitly improves  shared layers' generalization on all languages. Overall, our results show that negative interference  is more common than previously known,  suggesting new directions for improving multilingual representations.\footnote{Source code is available at \url{https://github.com/iedwardwangi/MetaAdapter}.} %  % State-of-the-art multilingual models are trained on concatenated text from multiple languages to enable positive cross-lingual transfer, especially from high-resource languages to low-resource languages. % However, recent work found that such a training paradigm can degrade the model's performance on high-resource languages too, a phenomenon known as . % In this paper, we present the first systematic study of negative interference. % We show that, contrary to what was previously hypothesized, negative interference is not exclusive to high-resource but can also occur in low-resource settings. % In addition, despite that parameters are shared with the goal to learn language-universal structures, we demonstrate that language-specific parameters in multilingual models are a potential cause of negative interference. % Motivated by these observations, we show that we can obtain better cross-lingual transferability and alleviate negative interference through a meta-learning algorithm, which considers language-specific layers as meta parameters and trains them in the manner that explicitly improves the generalization of shared parameters across all languages. % Overall, our results show that negative interference occurs more commonly than previously believed and suggest a new direction towards improving multilingual representations by resolving language conflicts.\footnote{Code will be released upon publication.}",70
"  Event argument extraction  aims to identify the entities that serve as arguments of an event and to classify the specific roles they play. As in Fig., ``two soldiers'' and ``yesterday'' are arguments, where the event triggers are ``attacked''   and ``injured'' . For the trigger ``attacked'', ``two soldiers'' plays the argument role Target while ``yesterday'' plays the argument role Attack\_Time. For the event trigger ``injured'', ``two soldiers'' and ``yesterday'' play the role Victim and INJURY\_Time, respectively. There has been significant work on event extraction  , but the EAE task remains a challenge and has become the bottleneck for improving the overall performance of EE.\footnote{EAE has similarities with semantic role labeling. Event triggers are comparable to predicates in SRL and the roles in most SRL datasets have a standard convention of interpreting who did what to whom. EAE has a custom taxonomy of roles by domain. We also use inspiration from the SRL body of work .}     Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that,  We use  BERT as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike%previous studies ~ who added a final/prediction layer to BERT for argument extraction, we use BERT as token embedder and build a sequence of EAE task-specific components .  We use  in-domain data to adapt the BERT model parameters in a subsequent pretraining step as in . This makes the encoder domain-aware.  We perform self-training to construct auto-labeled data .  A crucial aspect for EAE is to integrate event trigger information into the learned representations. This is important because arguments are dependent on triggers, i.e., the same argument span plays completely different roles toward different triggers. An example is shown in Fig., where ``two soldiers'' plays the role Target for the event ATTACK and the role Victim for INJURY. Different from existing work that relies on regular sequence encoders, we design a novel trigger-aware encoder which simultaneously learns four different types of trigger-informed sequence representations. %for candidate arguments.   Capturing the long-range dependency is another important factor, e.g., the connection between an event trigger and a distant argument. Syntactic information could be useful in this case, as it could help bridge the gap from a word to another distant but highly related word. We modify a Transformer  by explicitly incorporating syntax via an attention layer driven by the dependency parse of the sequence. % .  %Since arguments of an event are entities, entity mentions are very effective hints.  We design our role-specific argument decoder to seamlessly accommodate both settings . We also tackle the role overlap problem  using a set of classifiers or taggers in our decoder.   Our model achieves the new state-of-the-art on ACE2005 Events data.% for EAE.  % % Motivation 1: data scarcity. Proposed and used solutions:  pretrained model BERT  External embedding   Self-training   BERT MLM  MLM encoder and decoder joint pre-training.  Teacher-Student    %
"," Event argument extraction  aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges:  Data scarcity.  Capturing the long-range dependency, specifically, the connection between an event trigger and a distant event argument.  Integrating event trigger information into candidate argument representation. For , we explore using unlabeled data in different ways. For , we propose to use a syntax-attending Transformer that can utilize dependency parses to guide the attention mechanism. For , we propose a trigger-aware sequence encoder with several types of trigger-dependent sequence representations. We also support argument extraction either from text annotated with gold entities or from plain text. Experiments on the English ACE2005 benchmark show that our approach achieves a new state-of-the-art.",71
"  Topic segmentation is a fundamental NLP task that has received considerable attention in recent years .  It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units ,  and . The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization , question answering , machine reading  and dialogue modeling .   {  {|m{23em}|}     \underline{Preface:} \\  \rowcolor{Gray}  Marcus is a city in Cherokee County, Iowa, United States. \\  }} \\  \underline{S1}: The first building in Marcus was erected in 1871.\\  \underline{S2}: Marcus was incorporated on May 15, 1882. \\  }} \\  \underline{S3}: Marcus is located at .\\  \underline{S4}: According to the United States Census Bureau, the city has a total area of 1.54 square miles, all land. \\  }} \\  \underline{S5}: As of the census of 2010, there were 1,117 people, 494 households, and 310 families residing in the city. \\  ... ...\\   covering three topics: ,  and }   A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps , Bayesian contexts  or   %the  semantic relatedness graphs  to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them. More recently, several works have framed topic segmentation as neural supervised learning, because of the remarkable success achieved by such models in most NLP tasks .  %While one line of research forms topic segmentation as a sequence labeling problem and builds neural models to predict segment boundaries directly ;  %another line of works first trains neural models for other tasks , and then uses these models' outputs to predict boundaries .  Despite %the  minor architectural differences, most of these neural solutions adopt Recurrent Neural Network  and its variants  as their main framework.  On the one hand, RNNs are appropriate because topic segmentation can be modelled as a sequence labeling task where each sentence is either the end of a segment or not. On the other hand, this choice makes these neural models limited in how to model the context. Because some sophisticated RNNs  are able to preserve long-distance information , which can largely help language models. But for topic segmentation, it is critical to supervise the model to focus more on the local context.    %In fact, RNNs are superior on many NLP tasks due to their capability of preserving long-distance information . %However, for topic segmentation, it is also critical to supervise the model to learn the right information from the local context.   As illustrated in Table, the prediction of the segment boundary between  and  hardly depends on the content in . Bringing in excessive long-distance signals may cause unnecessary noise and %further  hurt %model's  performance. Moreover, text coherence has strong relation with topic segmentation .  For instance, in Table, sentence pairs from the same segment  %should be  are more coherent %to put together than sentence pairs across segments .  Arguably, with a proper way of modeling the coherence between adjacent sentences, a topic segmenter can be further enhanced.   %\textcolor{red}{We hypothesize that topic segment prediction should rely on local contextual information in a way that cannot be effectively captured by RNNs.} %\textcolor{red}{In essence, RNNs are able to model long and short-distance dependencies only implicitly.}  %However, with restricted self-attention, our model can pay attention to the local context from the neighboring sentences in a more explicitly constrained way . %In essence, local contextual information is critical in predicting topical boundaries, but simple Recurrent Neural Network  and its variants are arguably not sufficiently powerful to represent the necessary information.  %However, both approaches still face the challenge of insufficient context modeling. Topic segment boundary prediction usually heavily relies on local contextual information. Hence, how to effectively select local contexts and model the relations between contexts becomes important. Neural models like RNN and its variants can represent the state of each timestep by memorizing or forgetting the information from its previous and later contexts. But how these learned contextual information contribute to model's decision is not straightforward and sufficiently transparent.  In this paper, we propose to enhance a state-of-the-art  topic segmenter  based on hierarchical attention BiLSTM network to better model the local context of a sentence in two complementary ways. First, we add a coherence-related auxiliary task to make our model learn more informative hidden states for all the sentences in a document.  %More specifically, we refine the objective of our model to encourage that the coherence of the sentences from different segments is smaller than the coherence of the sentences from the same segment.  More specifically, we refine the objective of our model to encourage smaller coherence for the sentences from different segments and larger coherence for the sentences from the same segment.  Secondly, we enhance context modeling by utilizing restricted self-attention , which enables our model to pay attention to the local context and make better use of the information from the closer neighbors of each sentence .  Our empirical results show  that our proposed context modeling strategy significantly improves the performance of the SOTA neural segmenter on three datasets,  that the enhanced segmenter is more robust in domain transfer setting when applied to four challenging real-world test sets, sampled differently from the training data,  that our context modeling strategy is also effective for the segmenters trained on other challenging languages , rather than just English.   
","      Topic segmentation is critical %, the process of splitting a document into topic-coherent pieces,      %plays a vital role      in key NLP tasks and recent works favor highly effective neural supervised  approaches.     %Due to the high effectiveness of neural models, more recent works have favored framing topic segmentation as a neural-based supervised learning problem.     However, current neural solutions are arguably limited in how they model context.     %topic segmenters proposed so far are still limited by the insufficient context modeling.      In this paper, we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context, by adding a coherence-related auxiliary task and restricted self-attention. Our optimized segmenter\footnote{Our code will be publicly available at \url{www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/}} outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed model in domain transfer setting by training a model on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed strategy to two other languages , and show its effectiveness in multilingual scenarios.",72
"  Deep learning techniques, including contextualized word embeddings based on transformers and pretrained on language modelling, have resulted in considerable improvements for many NLP tasks. However, they often require large amounts of labeled training data, and there is also growing evidence that transferring approaches from high to low-resource settings is not straightforward. In , rule-based or linguistically motivated CRFs still outperform RNN-based methods on several tasks for South African languages. For pretraining approaches where labeled data exists in a high-resource language, and the information is transferred to a low-resource language,  find a significant gap between performance on English and the cross-lingually transferred models. In a recent study,  find that the transfer for multilingual transformer models is less effective for resource-lean settings and distant languages. A popular technique to obtain labeled data quickly and cheaply is distant and weak supervision.  recently inspected POS classifiers trained on weak supervision. They found that in contrast to scenarios with simulated low-resource settings of high-resource languages, in truly low-resource settings this is still a difficult problem. These findings also highlight the importance of aiming for realistic experiments when studying low-resource scenarios.   In this work, we analyse multilingual transformer models, namely mBERT  and XLM-RoBERTa . We evaluate both sequence and token classification tasks in the form of news title topic classification and named entity recognition . A variety of approaches have been proposed to improve performance in low-resource settings. In this work, we study  transfer learning from a high-resource language and  distant supervision. We selected these as they are two of the most popular techniques in the recent literature and are rather independent of a specific model architecture. Both need auxiliary data. For transfer learning, this is labeled data in a high-resource language, and for distant supervision, this is expert insight and a mechanism to automatically generate labels. We see them, therefore, as orthogonal and depending on the scenario and the data availability, either one or the other approach might be applicable.  Our study is performed on three, linguistically different African languages: Hausa, isiXhosa and \yoruba. These represent languages with millions of users and active use of digital infrastructure, but with only very limited support for NLP technologies. For this aim, we also collected three new datasets that are made publicly available alongside the code and additional material.  We show both challenges and opportunities when working with multilingual transformer models evaluating trends for different levels of resource scarcity. The paper is structured into the following questions we are interested in:  [leftmargin=*]        
"," Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and \yoruba on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.",73
"  In computer vision, { modality, here we focus on the {   To get an intuition about the task setup and our proposed solution, consider the following situation. Imagine you have never seen a zebra but have seen a horse. What if you were given a text describing a zebra: . This description would probably be very close to a description of a horse having  and you would probably be looking for an image that reminds you of a horse but has . So, even without ever seeing a zebra, using text-descriptions of the zebra and knowledge already acquired about horses, one can correctly classify unknown classes like a zebra.   Our proposed solution has two-phases. First, based on the intuition that similar objects  tend to have similar texts, we encode a similarity feature that enhances text descriptions' separability.  In addition, we leverage the intuition that the differences between text descriptions of species would be their most salient visual features, and extract visually relevant descriptions from the text.   Our experiments empirically demonstrate both the { capacity of our proposed solution.  On two large ZSL datasets, in both the { scenarios, the similarity method obtains a ratio improvement of up to 18.3\%. With the addition of extracting visually relevant descriptions, we obtain a ratio improvement of up to 48.16\% over the state-of-the-art. We further show that our visual-summarization method generalizes from the CUB dataset  to the NAB dataset , and we demonstrate its contribution to additional models by a ratio improvement of up to 59.62\%.    The contributions of this paper are threefold. First, to the best of our knowledge, we are the first to showcase the critical importance of the text representation in zero-shot image-recognition scenarios, and we present two concrete text-based processing methods that vastly improve the results. Second, we demonstrate the efficacy and generalizability of our proposed methods by applying them to both the { tasks, outperforming all previously reported results on the CUB and NAB Benchmarks.   Finally, we show that visual aspects learned from one dataset can be transferred effectively to another dataset without the need to obtain dataset-specific captions.  The efficacy of our proposed solution on these benchmarks illustrates that purposefully exposing the visual features in texts is indispensable for tasks that learn to align the vision-and-language modalities.   
","    We study the problem of recognizing visual entities from the textual descriptions of their classes. Specifically, given birds' images with free-text descriptions of their species, we learn to classify images of previously-unseen species based on specie descriptions. This setup has been studied in the vision community under the name { between species, reflected in the similarity between text descriptions of the species.  we derive { features that tend to be reflected in images. We propose a simple attention-based model augmented with the similarity and visual summaries components. Our empirical results consistently and significantly outperform the state-of-the-art on the largest benchmarks for text-based zero-shot learning, illustrating the critical importance of texts for zero-shot image-recognition.",74
"  Taking advantage of monolingual training data via Back-Translation~, Iterative Back-Translation~ or Dual Learning~  has become a de facto requirement for building high quality Neural Machine Translation  systems~.  However, these methods rely on unrelated heuristic optimization objectives, and it is not clear what their respective strengths and weaknesses are, nor how they relate to the ideal but intractable objective of maximizing the marginal likelihood of the monolingual data  =  p_\theta q coincides with the target sentence distribution~. We also show that Iterative Back-Translation  and Dual Learning can be viewed as different ways to approximate its optimization.  \looseness=-1 Theory suggests that IBT approximates the dual reconstruction objective more closely than the more complex Dual Learning approach, and in particular that Dual Learning's additional language model loss is redundant. We investigate whether these differences matter in practice by conducting the first controlled empirical comparison of Back-Translation, IBT, and Dual Learning in high-resource , low-resource , and cross-domain settings . Results support our theory that the additional language model loss and policy gradient estimation in Dual Learning is redundant and show that IBT outperforms the more complex Dual Learning algorithm in terms of translation quality. Furthermore, we also compare different optimization strategies used in IBT to better balance translation quality against the computational cost.      
"," While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.",75
" Due to the growing number of Internet users, cyber-violence emerged with offensive language pervasive across social media. With anonymity as a 閳ユ笡rivilege閳, netizens hide behind the screens, behaving in a manner most of them would not otherwise in reality. Thus, government organizations, online communities, and technology companies are all striving for ways to detect aggressive language in social media and help build a more friendly online environment.  Manual filtering is very time consuming and it can cause post-traumatic stress disorder-like symptoms to human annotators. One of the most common strategies  to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside human moderation.  SemEval 2020 Task-12  is the second edition of OffensEval . In this competition, organizers offers 5 languages datasets including Arabic , Danish , English , Turkish  and Greek . In Sub-task A, the participants need to predict whether a post uses offensives language. Besides, the organizers provide other two sub-tasks which mainly focus on English, to predict the type and target of offensive language.  Participating in all 3 Sub-tasks, we proposed several methods based on pre-training language models including ERNIE and XLM-R. In Sub-task A, we scored 0.9199, 0.851, 0.8258, 0.802, 0.8989 in English, Greek, Turkish, Danish and Arabic respectively. We ranked first in average F1 scores, and ranked in top three across all languages. In Sub-task B and Sub-task C, we also took the first place with 0.7462 and 0.7145. In the following sections, we will elaborate the methods, dataset and experiments of our system.  
","   This paper describes Galileo闁炽儲鐛 performance in SemEval-2020 Task 12 on detecting and categorizing offensive language in social media. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A - Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages.  We also took the first place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence Target Identification.",76
" Understanding and reasoning over natural language plays a significant role in artificial intelligence tasks such as Machine Reading Comprehension  and Question Answering . Several QA tasks have been proposed in recent years to evaluate the language understanding capabilities of machines . These tasks are single-hop QA tasks and consider answering a question given only one single paragraph. % The drawback of single-hop QA tasks is the lack of evaluating deep reasoning capability.  % We observe that many existing neural models achieve promising performance without reasoning.  Many existing neural models rely on learning context and type-matching heuristics. Those rarely build reasoning modules but achieve promising performance on single-hop QA tasks. The main reason is that these single-hop QA tasks are lacking a realistic evaluation of reasoning capabilities because they do not require complex reasoning.   Recently multi-hop QA tasks, such as HotpotQA  and WikiHop, have been proposed to assess multi-hop reasoning ability. HotpotQA task provides annotations to evaluate document level question answering and finding supporting facts. Providing supervision for supporting facts improves explainabilty of the predicted answer because they clarify the cross paragraph reasoning path.   Due to the requirement of multi-hop reasoning over multiple documents with strong distraction, multi-hop QA tasks are challenging.  Figure shows an example of HotpotQA. Given a question and 10 paragraphs, only paragraph  and paragraph  are relevant. The second sentence in paragraph  and the first sentence in paragraph  are the supporting facts. The answer is ``Geelong Football Club''.   Primary studies in HotpotQA task prefer to use a reading comprehension neural model. First, they use a neural retriever model to find the relevant paragraphs to the question. After that, a neural reader model is applied to the selected paragraphs for answer prediction. Although these approaches obtain promising results, the performance of evaluating multi-hop reasoning capability is unsatisfactory.   To solve the multi-hop reasoning problem, some models tried to construct an entity graph using Spacy or Stanford CoreNLP and then applied a graph model to infer the entity path from question to the answer. However, these models ignore the importance of the semantic structure of the sentences and the edge information and entity types in the entity graph. To take the in-depth semantic roles and semantic edges between words into account here we use semantic role labeling  graph as the backbone of a graph convolutional network. Semantic role labeling provides the semantic structure of the sentence in terms of argument-predicate relationships.  % such as ``who did what to whom.'' The argument-predicate relationship graph can significantly improve the multi-hop reasoning results. Our experiments show that SRL is effective in finding the cross paragraph reasoning path and answering the question.  Our proposed semantic role labeling graph reasoning network  jointly learns to find cross paragraph reasoning paths and answers questions on multi-hop QA. In SRLGRN model, firstly, we train a paragraph selection module to retrieve gold documents and minimize distractor. Second, we build a heterogeneous document-level graph that contains sentences as nodes ,  % and the sentence nodes include  and SRL sub-graphs including semantic role labeling arguments as nodes and predicates as edges. Third, we train a graph encoder to obtain the graph node representations that incorporate the argument types and the semantics of the predicate edges in the learned representations. Finally, we jointly train a multi-hop supporting fact prediction module that finds the cross paragraph reasoning path, and answer prediction module that obtains the final answer. Notice that both supporting fact prediction and answer prediction are based on contextual semantics graph representations as well as token-level BERT pre-trained representations. The contributions of this work are as follows:    Our proposed model obtains competitive results on both HotpotQA  and the SQuAD benchmarks.  
"," This work deals with the challenge of learning and reasoning over multi-hop question answering . We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence , and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.",77
"   The organizers of the 2020 VarDial Evaluation Campaign  proposed a shared task targeted towards the geolocation of short texts, e.g.~tweets, namely the Social Media Variety Geolocation  task. Typically formulated as a double regression problem, the task is about predicting the location, expressed in latitude and longitude, from where the text received as input was posted on a certain social media platform. Twitter and Jodel are the platforms used for data collection, divided by the language area in three subtasks, namely:        In this paper, we focus only on the second subtask, SMG-CH, proposing a variety of handcrafted and deep learning models, as well as an ensemble model that combines all our previous models through meta-learning. Our first model is a Support Vector Regression  classifier  based on string kernels, which are known to perform well in other dialect identification tasks . Our second model is a character-level convolutional neural network  , which is also known to provide good results in dialect identification . Due to the high popularity and the outstanding results of Bidirectional Encoder Representations from Transformers   in solving mainstream NLP tasks, we decided to try out a Long Short-Term Memory  network  based on German BERT embeddings as our third model. Lastly, we combine our three models into an ensemble that employs Extreme Gradient Boosting   as meta-learner. We conducted experiments on the development set provided by the organizers, in order to decide which models to choose for our three submissions for the SMG-CH subtask. Our results indicate that the ensemble model attains the best results. Perhaps surprisingly, our shallow approach based on string kernels outperforms both deep learning models. Our observations are consistent across the development and the test sets provided by the organizers.  % We experimented with a few Machine Learning algorithms for the second subtask, namely CH,  % Geolocation can be framed as a double regression task, but more sophisticated model architectures have been proposed .  % Jodel is a mobile chat application that lets people anonymously talk to other users within a 10km-radius around them.   % All three subtasks will use the same data format and evaluation methodology, and participants are encouraged to submit their systems for all subtasks.  The rest of this paper is organized as follows. We present related work on dialect identification and geolocation of short texts in Section. Our approaches are described in more detail in Section. We present the experiments and empirical results in Section. Finally, our conclusions are drawn in Section.  
"," In this work, we introduce the methods proposed by the UnibucKernel team in solving the Social Media Variety Geolocation task featured in the 2020 VarDial Evaluation Campaign. We address only the second subtask, which targets a data set composed of nearly 30 thousand Swiss German Jodels. The dialect identification task is about accurately predicting the latitude and longitude of test samples. We frame the task as a double regression problem, employing a variety of machine learning approaches to predict both latitude and longitude. From simple models for regression, such as Support Vector Regression, to deep neural networks, such as Long Short-Term Memory networks and character-level convolutional neural networks, and, finally, to ensemble models based on meta-learners, such as XGBoost, our interest is focused on approaching the problem from a few different perspectives, in an attempt to minimize the prediction error. With the same goal in mind, we also considered many types of features, from high-level features, such as BERT embeddings, to low-level features, such as characters n-grams, which are known to provide good results in dialect identification. Our empirical results indicate that the handcrafted model based on string kernels outperforms the deep learning approaches. Nevertheless, our best performance is given by the ensemble model that combines both handcrafted and deep learning models.",78
"  Comparing and contrasting the meaning of text conveyed in different languages is a fundamental nlp task. It can be used to curate clean parallel corpora for downstream tasks such as machine translation~, cross-lingual transfer learning, or semantic modeling~, and it is also useful to directly analyze multilingual corpora. For instance, detecting the commonalities and divergences between sentences drawn from English and French Wikipedia articles about the same topic would help analyze language bias~, or mitigate differences in coverage and usage across languages~. This requires not only detecting coarse content mismatches, but also fine-grained differences in sentences that overlap in content.  Consider the following English and French sentences, sampled from the WikiMatrix parallel corpus. While they share important content, highlighted words convey meaning missing from the other language:  {p{0.9  as an unofficial Canadian national anthem.\\ fr  pro canadien anglais.\\ \textcolor{darkgray}{gloss    We show that explicitly considering diverse types of semantic divergences in bilingual text benefits both the annotation and prediction of cross-lingual semantic divergences. We create and release the Rationalized English-French Semantic Divergences corpus , based on a novel divergence annotation protocol that exploits rationales to improve annotator agreement. We introduce \modelname, a  bert-based model that detects fine-grained semantic divergences without supervision by learning to rank synthetic divergences of varying granularity. Experiments on \dataset show that our model distinguishes semantically equivalent from divergent examples much better than a strong sentence similarity baseline and that unsupervised token-level divergence tagging offers promise to refine distinctions among divergent instances. We make our code and data publicly available.\footnote{Implementations of \modelname can be found at: \url{https://github.com/Elbria/xling-SemDiv}; the \dataset dataset is hosted at:   \url{https://github.com/Elbria/xling-SemDiv/tree/master/REFreSD}.}       
","  Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual nlp and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale.~This work improves the prediction and annotation of fine-grained semantic divergences.~We introduce a training strategy for multilingual bert models by learning to rank synthetic divergent examples of varying granularity.~We evaluate our models on the~Rationalized~English-French~Semantic~Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.~Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.",79
"  There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports to congressional bills and meeting conversations. The lack of annotated resources suggests that end-to-end systems may not be a ``one-size-fits-all'' solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators to realize the full potential of neural abstractive summarization.   We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate  results of such a module, rather than associating it with text generation.  Existing neural abstractive systems can perform content selection implicitly using end-to-end models, or more explicitly, with an external module to select important sentences or words to aid generation. However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary.           In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the source document, then highlights their summary-worthy segments and uses those as a basis for composing a summary sentence. When a pair of sentences are selected, it is important to ensure that they are ---there exists cohesive devices that tie the two sentences together into a coherent text---to avoid generating nonsensical outputs.  Highlighting sentence segments allows us to perform fine-grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence.  The contributions of this work are summarized as follows. [topsep=5pt,itemsep=0pt]  }}     
","  We present an empirical study in favor of a cascade architecture to neural text summarization. Summarization practices vary widely but few other than news summarization can provide a sufficient amount of training data enough to meet the requirement of end-to-end neural abstractive systems which perform content selection and surface realization jointly to generate abstracts.  Such systems also pose a challenge to summarization evaluation, as they force content selection to be evaluated along with text generation, yet evaluation of the latter remains an unsolved problem. In this paper, we present empirical results showing that the performance of a cascaded pipeline that separately identifies important content pieces and stitches them together into a coherent text is comparable to or outranks that of end-to-end systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research.",80
"   The recent advances in neural machine translation   have provided the research community and the commercial landscape with effective translation models that can at times achieve near-human performance. However, this usually holds at phrase or sentence level. When using these models in larger units of text, such as paragraphs or documents, the quality of the translation may drop considerably in terms of discourse attributes such as lexical and stylistic consistency.  In fact, document-level translation is still a very open and challenging problem. The sentences that make up a document are not unrelated pieces of text that can be predicted independently; rather, a set of sequences linked together by complex underlying linguistics aspects, also known as the discourse . The discourse of a document includes several properties such as grammatical cohesion , lexical cohesion , document coherence  and the use of discourse connectives . Ensuring that the translation retain such linguistic properties is expected to significantly improve its overall readability and flow.  However, due to the limitations of current decoder technology, NMT models are still bound to translate at sentence level. In order to capture the discourse properties of the source document in the translation, researchers have attempted to incorporate more contextual information from surrounding sentences. Most document-level NMT approaches augment the model with multiple encoders, extra attention layers and memory caches to encode the surrounding sentences, and leave the model to implicitly learn the discourse attributes by simply minimizing a conventional NLL objective. The hope is that the model will spontaneously identify and retain the discourse patterns within the source document. Conversely, very little work has attempted to model the discourse attributes explicitly. Even the evaluation metrics typically used in translation such as BLEU  are not designed to assess the discourse quality of the translated documents.  For these reasons, in this paper we propose training an NMT model by directly targeting two specific discourse metrics: lexical cohesion  and coherence . LC is a measure of the frequency of semantically-similar words co-occurring in a document  . For example, car, vehicle, engine or wheels are all semantically-related terms. There is significant empirical evidence that ensuring lexical cohesion in a text eases its understanding . At its turn, COH measures how well adjacent sentences in a text are linked to each other. In the following example from Hobbs :  	 ``John took a train from Paris to Istanbul. He likes spinach.''	      approach from reinforcement learning  which allows using any evaluation metric as a reward without having to differentiate it. By combining different types of rewards, the model can be trained to simultaneously achieve more lexically-cohesive and more coherent document translations, while at the same time retaining faithfulness to the reference translation. %the information contained in the source document.  %The rest of the paper is organized as follows. Section  discusses related work. Section  describes the baseline NMT architectures used for the experiments. Section  presents the proposed training approach and the discourse rewards used with it. Section  presents the experiments and, finally, Section  concludes the paper.   
","   Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion  and coherence , by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of $2.46$ percentage points  in LC and $1.17$ pp in COH over the runner-up, while at the same time improving $0.63$ pp in BLEU score and $0.47$ pp in $\mathrm{F}_{\mathrm{BERT}}$.      %In fact, in some cases our training approach has even improved translation accuracy metrics such as BLEU and the recently proposed $F_{\text{BERT}}$.",81
" In recent years, neural models have led to state-of-the-art results in machine translation  . Many of these systems can broadly be characterized as following a multi-layer encoder-decoder neural network design: both the encoder and decoder learn representations of word sequences by a stack of layers , building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model , deep systems have shown promising BLEU improvements by either easing the information flow through the network  or constraining the gradient norm across layers . An improved system can even learn a 35-layer encoder, which is  deeper than that of vanilla Transformer .  Although these methods have enabled training deep neural MT  models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner . It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely deep model is expensive although a narrow-and-deep network can speed up training . For example, it takes us  longer time to train the model when we deepen the network from 6 layers to 48 layers. This might prevent us from exploiting deeper models in large-scale systems.  In this paper, we explore why deep architectures work to render learning NMT models more effectively. By investigating the change of the hidden states in different layers, we find that new representations are learned by continually stacking layers on top of the base model. More stacked layers lead to a stronger model of representing the sentence. This particularly makes sense in the deep NMT scenario because it has been proven that deep models can benefit from an enriched representation .  In addition, the finding here inspires us to develop a simple yet efficient method to train a deep NMT encoder: we train model parameters from shallow to deep, rather than training the entire model from scratch. To stabilize training, we design a sparse linear combination method of connecting lower-level layers to the top. It makes efficient pass of information through the deep network but does not require large memory footprint as in dense networks. We experiment with the method in a state-of-the-art deep Transformer system. Our encoder consists of 48-54 layers, which is almost the deepest Transformer model used in NMT. On WMT En-De and En-Fr tasks, it yields a  speedup of training, matching the state-of-the-art on the WMT'16 En-De task.  
","    Deep encoders have been proven to be effective in improving neural machine translation  systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is $1.4$ $\times$ faster than training from scratch, and achieves a BLEU score of $30.33$ and $43.29$ on two tasks. The code is publicly available at {https://github.com/libeineu/SDT-Training}.",82
"  Annual Reports may extend up to 250 pages long as stated above, which contains different sections General Corporate Information, financial and operating cost, CEOs message, Narrative texts, accounting policies, Financial statement including balance sheet and summary of financial data documents. In the Financial narrative summarisation task, only the narrative section is summarised, which is not explicitly marked in the dataset, making it challenging and interesting.  In recent years, previous manual small-scale research in the Accounting and Finance literature has been scaled up with the aid of NLP and ML methods, for example, to examine approaches to retrieving structured content from financial reports, and to study the causes and consequences of corporate disclosure and financial reporting outcomes . \par Companies produce glossy brochures of annual reports with a much looser structure, and this makes automatic summarisation of narratives in UK annual reports a challenging task . Hence we summarize the narrative section of annual reports, particular narrative sentences that are spread loosely across the document need to be first identified and summarise those sentences. The summarisation limit is set to 1000 words, where the actual length of the report may go up to 250 pages long. Hence to summarize these long annual reports using a combination of extractive and abstractive summarisation.\par The text summary method can be classified into two paradigms: extractive and abstractive. The extractive summarisation method extracts the meaningful sentences or a section of text from the original text and combines them  to form a summary . Whereas abstractive summarisation generates words and sentences that are similar in meaning to the given text to form a summary that may not be in actual text . When summarizing long documents such as in our case up to 250 pages long, extractive summarisation may not produce a coherent and readable summary, and abstractive summarisation cannot cover complete information using encoder-decoder architecture. One problem is that typical seq2seq frameworks often generate unnatural summaries consisting of repeated words or phrases . Hence, we come up with a combination of extractive and abstractive summarisation to first select important narrative sentences and concisely convey them. \par Pointer Networks  is used in various combinatorial optimization problems, such as Travelling Salesman Problem , Convex hull optimization. We used pointer networks in our task of financial narrative summarization to extract relevant narrative sentences in a particular order to have a logical flow in summary. These extracted sentences are paraphrased to summarise these sentences in an abstractive way using the T-5 sequence-to-sequence model. We train the complete model by optimizing the ROUGE-LCS evaluation metric through a reinforcement learning objective.   % % The following footnote without marker is nebe fireded for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version             % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } 
","   Companies provide annual reports to their shareholders at the end of the financial year that describes their operations and financial conditions. The average length of these reports is 80, and it may extend up to 250 pages long. In this paper, we propose our methodology PoinT-5  algorithms) that we used in the Financial Narrative Summarisation  2020 task. The proposed method uses Pointer networks to extract important narrative sentences from the report, and then T-5 is used to paraphrase extracted sentences into a concise yet informative sentence. We evaluate our method using $\operatorname{ROUGE}$-N , L,and SU4. The proposed method achieves the highest precision scores in all the metrics and highest F1 scores in $\operatorname{ROUGE}$ 1,and LCS and only solution to cross MUSE solution baseline in $\operatorname{ROUGE}$-LCS metrics.",83
"   Neural Architecture Search  methods aim to automatically discover neural architectures that perform well on a given task and dataset. These methods search over a space of possible model architectures, looking for ones that perform well on the task and will generalize to unseen data. There has been substantial prior work on how to define the architecture search space, search over that space, and estimate model performance .    Recent works, however, cast doubt on the quality and performance of NAS-optimized architectures , showing that current methods fail to find the best performing architectures for a given task and perform similarly to random architecture search.  In this work, we explore applications of a SOTA NAS algorithm, ENAS , to two sentence-pair tasks, paraphrase detection  and semantic textual similarity . We conduct a large set of experiments testing the effectiveness of ENAS-optimized RNN architectures across multiple models , embeddings  and datasets . We are the first, to our knowledge, to apply ENAS to PD and STS, to explore applications across multiple embeddings and traditionally LSTM-based NLP models, and to conduct extensive SOTA HPT across multiple ENAS-RNN architecture candidates.   Our experiments suggest that baseline LSTM models, with appropriate hyperparameter tuning , can sometimes match or exceed the performance of models with ENAS-RNNs. We also observe that random architectures sampled from the ENAS search space offer a strong baseline, and can sometimes outperform ENAS-RNNs. Given these observations, we recommend that researchers  conduct extensive HPT  across various candidate architectures for the fairest comparisons;  compare the performances of ENAS-RNNs against both standard architectures like LSTMs and RNN cells randomly sampled from the ENAS search space;  examine the computational  requirements of ENAS methods alongside the gains observed.   
","  Neural Architecture Search  methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art  performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search   to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets , with two different models , and two sets of embeddings . In contrast to prior work applying ENAS to NLP tasks, our results are mixed -- we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search.",84
" Constituency parsing is a well-studied problem in natural language processing, but most state-of-the-art parsers have only been tested on written text, e.g.\ the standard Penn Treebank Wall Street Journal  dataset .  These recent neural parsers are commonly formulated as encoder-decoder systems, where the encoder learns the input sentence representation and the decoder learns to predict a parse tree. While input is often represented by word-level features, representation for the output trees varies:  as a sequence of parse symbols , a set of spans ,  syntactic distances , or per-word structure-rich labels . A key characteristic in many of these neural parsers is the recurrent network structure, particularly Long Short-Term Memory networks ; however, Kitaev and Klein  have shown that a non-recurrent encoder such as the Transformer network introduced in  is also capable of encoding timing information through self-attention mechanisms, achieving state-of-the-art parse results on the Treebank WSJ dataset.  Further, these parsers  %seem to mainly  benefit from contextualized information learned from larger external text data, such as ELMo  and BERT .  It is not clear that these advances will transfer to speech data, particularly for the different styles of speech. Even when perfect transcripts are available, speech poses many challenges to parsers learned from written text due to the lack of punctuation and case, and the presence of disfluencies.  On the other hand, speech signals carry rich information beyond words via variations in timing, intonation, and loudness, i.e. in . Linguistic studies have shown that prosodic cues align with constituent structure , signal disfluencies by marking the interruption point , and help listeners resolve syntactic ambiguities . Empirical evidence, however, has been mixed regarding the utility of prosody for constituency parsing. Most gains have been observed when sentence boundaries are unknown , or with annotated prosodic labels . Most related to our current work, Tran et al.\  recently showed the benefit of using prosody in parsing within a sequence-to-sequence framework, proposing a convolutional neural network  as a mechanism to combine discrete word-level features with frame-level acoustic-prosodic features.  In this study, we extend the work in  and  to explore the utility of recent neural advances on spontaneous speech data, and compare the utility of prosody in read vs.\ spontaneous speech. Specifically, the goal of the current study is to answer the following questions:  [topsep=1pt,itemsep=0ex,partopsep=0ex,parsep=0.2ex]   % TT: may cut this if space is lacking. But I didn't want to end the intro with questions without saying anything further %The rest of this paper is organized as follows: Section  describes the models used in this work; Section  reviews the datasets and metrics in constituency parsing; Section  presents our experiments, results, and analyses; and Section  summarizes the findings.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Moved data table here since it was oddly arranged %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% [th]    %  \usepackage{INTERSPEECH2019} \usepackage{url} \usepackage{multirow} \usepackage{xcolor} \usepackage{subcaption,enumitem} \usepackage{booktabs} \usepackage{comment}  [1]{\textcolor{red}{[1]{\textcolor{blue}{[1]{\textcolor{cyan}{[1]{\textcolor{green}{  %The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate. \address{   Electrical \& Computer Engineering, University of Washington\\   LAIX Inc.} @uw.edu, \{jiahong.yuan,yang.liu\}@liulishuo.com}     %   The differences in written text and conversational speech are substantial; previous parsers trained on treebanked text have given very poor results on spontaneous speech. For spoken language, the mismatch in style also extends to prosodic cues, though it is less well understood.  This paper re-examines the use of written text in parsing speech in the context of recent advances in neural language processing. We show that neural approaches facilitate using written text to improve  parsing of spontaneous speech, and that prosody further improves over this state-of-the-art result. Further, we find an asymmetric degradation from read vs.\ spontaneous mismatch, with spontaneous speech more generally useful for training parsers.  %  Prosodic information in the speech signal has been shown to correlate with syntactic structure of a sentence; however, the impact of prosody on parsing has been mixed. Recent results show a benefit for conversational speech, particularly in utterances with disfluencies, but there is little recent work on other speaking styles. In this work, we extend recent advances in constituency parsing of spontaneous speech, integrating acoustic-prosodic cues and achieving SOTA results on the Switchboard dataset. We then explore the performance of the parser on mismatched training/testing scenarios. Specifically, we show that training on spontaneous speech results in a small degradation when testing on read speech, while fine-tuning with WSJ read speech substantially degrades the performance on spontaneous speech.   : constituency parsing, prosody, spontaneous speech, contextualized embeddings %: constituency parsing, prosody, spontaneous speech, read speech, switchboard, ELMo, BERT, contextualized embeddings %\ttcomment{take out some of these?}                  
"," The differences in written text and conversational speech are substantial; previous parsers trained on treebanked text have given very poor results on spontaneous speech. For spoken language, the mismatch in style also extends to prosodic cues, though it is less well understood.  This paper re-examines the use of written text in parsing speech in the context of recent advances in neural language processing. We show that neural approaches facilitate using written text to improve  parsing of spontaneous speech, and that prosody further improves over this state-of-the-art result. Further, we find an asymmetric degradation from read vs.\ spontaneous mismatch, with spontaneous speech more generally useful for training parsers.  %  Prosodic information in the speech signal has been shown to correlate with syntactic structure of a sentence; however, the impact of prosody on parsing has been mixed. Recent results show a benefit for conversational speech, particularly in utterances with disfluencies, but there is little recent work on other speaking styles. In this work, we extend recent advances in constituency parsing of spontaneous speech, integrating acoustic-prosodic cues and achieving SOTA results on the Switchboard dataset. We then explore the performance of the parser on mismatched training/testing scenarios. Specifically, we show that training on spontaneous speech results in a small degradation when testing on read speech, while fine-tuning with WSJ read speech substantially degrades the performance on spontaneous speech.",85
"  There is a growing interest in using formal languages to study fundamental properties of neural architectures, which has led to the extraction of interpretable models .  Recent work has explored the generalized Dyck-n  languages, a subset of context-free languages.  consists of ``well-balanced'' strings of parentheses with  different types of bracket pairs, and it is the canonical formal language to study nested structures.   show that LSTMs  are a variant of the -counter machine and can recognize  languages. The dynamic counting mechanisms, however, are not sufficient for  as it requires emulating a pushdown automata.   shows that for a sufficiently large length, Transformers  will fail to transduce the  language.     We empirically show that with the addition of a starting symbol to the vocabulary,  a two-layer multi-headed SA network  is able to learn  languages, and generalize to longer sequences, although not perfectly. As shown in Figure , the network is able to identify the corresponding closing bracket for an opening bracket, in what resembles a stack-based automaton. For example, the symbol ``]'' in the string ``'', will first pop ``['' from the stack, then it attends to `` enables the model to learn the occurrence of the end of a clause or the end of the sequence, which can be regarded as a mechanism to represent an empty stack.   Our work is the first to perform an empirical exploration of SA on formal languages. We present detailed comparison between an SA which incorporates a starting symbol , and one that does not , and demonstrate significant differences in their generalization across the length of sequences and the depth of dependencies.   Recent work has suggested that the ability of self-attention mechanisms to model hierarchical structures is limited.  show that the performance of Transformers on tasks such as logical inference and ListOps is either poor or worse than LSTMs.  have also reported similar results on SA, concluding that recurrence is necessary to model hierarchical structures. In comparison, our results show that SA outperforms LSTM on  languages except for  on longer sequences.   posit that the ability of neural models to learn hierarchical structures can be attributed to a ``looking back'' capability, rather than directly encoding hierarchies. Our analysis sheds light on the ability of SA to learn hierarchical structures by elegantly attending to the correct preceding symbol.  
","   We focus on the recognition of Dyck-n  languages with self-attention  networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol  and one without . Our results show that SA$^+$ is able to generalize to longer sequences and deeper dependencies. For $\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences whereas the accuracy of SA$^+$ is 58.82$\%$. We find attention maps learned by SA$^+$ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn hierarchies without recursion.",86
"  \vsec Automatic text summarization\footnote{We refer to abstractive summarization in this paper.} is an attractive technique for helping humans to grasp the content of documents effortlessly. While supervised neural methods have shown good performances, the unsupervised approach is starting to attract interest due to its advantage of not requiring costly parallel corpora. However, the empirical performance of unsupervised methods is currently behind that of state-of-the-art supervised models. Unsupervised text summarization is still developing and is now at the stage where various solutions should be actively explored.     One previous unsupervised approach extends neural encoder-decoder modeling to the zero paired data scenario, where a model is trained with a paradigm called compression-reconstruction  learning. The mechanism is similar to that of the back-translation: the model consists of a compressor  and a reconstructor, and they are co-trained so that the reconstructor can recover the original sentence from the summary generated by the compressor~. Experimental results showed that such an unsupervised encoder-decoder-based summarizer is able to learn the mapping from a sentence to a summary without paired data. % Also,  proposes a more straightforward method that mimics the reconstruction part by means of contextual similarity between an original input sentence and a top of a generating summary. % However, the performance of any unsupervised methods is still deficient compared to the latest supervised models.   Reinforcement learning  is also a potential solution for the no paired data situation. In related fields, for example, there are unsupervised methods for text simplification and text compression with policy-gradient learning. Recent RL techniques take a value-based approach  such as DQN or the combination of policy and value-based approaches such as Asynchronous Advantage Actor-Critic. A critical requirement to leverage a value-based method is a value function that represents the goodness of an action on a given state. We can naturally define the value function by utilizing the CR-learning paradigm, and it makes the latest value-based approaches available for unsupervised text summarization. % , and they require to define value-function. % We can leverage the values-based approach  % A crucial requirement for RL is a value function that represents a goodness of action on a given state. % We can satisfy the requirement by leveraging the definition in CR learning paradigm. % One concern is, however, that RL with large action space   generally has difficulty in the training. % In addition, the latest techniques to improve RL are from a value-based approach  such as DQN or the combination of policy-based and value-based approaches such as Asynchronous Advantage Actor-Critic.   In this paper, we propose a new method based on Q-learning and an edit-based summarization~. The edit-based summarization generates a summary by operating an edit action  for each word in the input sentence. Our method implements the editing process with two modules: 1) an {gent that predicts edit actions, and 2) a {model  converter that deterministically decodes a sentence on the basis of action signals, which we call  % As illustrated in the right-hand side of Figure , the agent determines { the fixed-LM so that we obtain desired sentences as the results of compression and reconstruction.  % The primary contribution of this paper is to provide a new option leveraging Q-learning with a language model to the growing field of unsupervised text summarization. % Introducing Q-learning, we open the problem to sophisticated techniques on value-based Reinforcement Learning  algorithms , which is not covered only with policy-based RL algorithms employed so far.\footnote{RL algorithms are classified into value-based  and policy-based . To the best of our knowledge, most of the text summarization methods with RL, both in supervised and unsupervised settings, leverages policy-based RL algorithms . Combining such a previous policy-based and our value-based methods for sentence compression will lead to the applicability of more advanced RL algorithms such as Actor-Critic  and Asynchronous Advantage Actor-Critic .} % Also, proposing an approach to fixedly utilize the pre-trained language model, we benefit from its powerful performance capturing sentence semantics along with mitigating issues generative models inherently hold such as complexity in co-training of multiple generators or repetition in decoding. % Experimentally, our approach shows promising results; it achieves competitive performance in standard datasets and outperforms the previous generator models in out-of-domain circumstances. % This paper brings novel insights for unsupervised text summarization and contributes to be flourishing in the future.  % This paper is organized as follows: Section defines the problem statement of unsupervised text summarization with the \algoname\ paradigm. % After reviewing the previous methods in Section, we introduce our approach in Section . % Then, we report experimental results in Section . % Discussing insights from the experiment in Section , we conclude the contribution of this paper for future unsupervised text summarization in Section .  \vsecu 
"," % Unsupervised methods for abstractive text summarization are attractive because they do not require parallel corpora. % However, their performance is still somehow lacking, therefore research on promising solutions is ongoing. % In this paper, we propose a new approach based on Q-learning with an edit-based summarization. % Our method combines two key modules to form an {gent and {odel converter~. % The agent predicts edit actions, and then the LM converter deterministically generates a summary on the basis of the action signals. % Q-learning is leveraged to train the agent to output proper edit actions. % Experimental results show that } Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required.  However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.   In this paper, we propose a new approach based on Q-learning with an edit-based summarization.  The method combines two key modules to form an Editorial Agent and Language Model converter .  The agent predicts edit actions , and then the LM converter deterministically generates a summary on the basis of the action signals.  Q-learning is leveraged to train the agent to produce proper edit actions.  Experimental results show that }",87
" Neural machine translation  systems are data driven models, which highly depend on the training corpus.  NMT models have a tendency towards over-fitting to frequent observations  while neglecting those low-frequency observations.  Unfortunately, there exists a token imbalance phenomenon in natural languages as different tokens appear with different frequencies, which roughly obey the Zipf's Law.  Table shows that there is a serious imbalance between high-frequency tokens and low-frequency tokens.  NMT models rarely have the opportunity to learn and generate those ground-truth low-frequency tokens in the training process. %It is harder for the NMT model to generate ground-truth low-frequency tokens even in the training process.  %Compared to the reference, the NMT model tends to generate more high-frequency tokens and less low-frequency tokens, which hurts the translation quality.  Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary or adding extra components, which bring in extra training complexity and computing expense.  Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model, BPE-based model and word-piece-based model. %For example, the sub-word model adapted byte pair encoding  technique to the task of word segmentation.  These effective work alleviate the token imbalance phenomenon to a certain extent and become the {' is split into two tokens as '{', there still exist obvious token-level imbalance between '{            \fi    [t]     <{<{<{<{*{Vanilla}   & Sen. & Please & be & slower& ~  \\      ~ & Freq. & 3,368 & 56,953 & 133 & ~ \\     ~ & Weight & 1.0 & 1.0 & 1.0 & ~\\     *{BPE}   & Sen. & Please & be & slow& er  \\      ~ & Freq. & 3,368 & 56,953 & 285 & 38,397 \\     ~ & Weight & 1.0 & 1.0 & 1.0 & 1.0\\     *{Ours}   & Sen. & Please & be & slow& er  \\      ~ & Freq. & 3,368 & 56,953 & 285 & 38,397 \\     ~ & Weight & 1.8 & 1.0 & 2.5 & 1.1\\     \hline                 \fi Furthermore, current NMT models generally assign equal training weights to target tokens without considering their frequencies.  It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the training sets. The parameters related to them can not be adequately trained, which will, in turn, make NMT models tend to prioritize output fluency over translation adequacy, and ignore the generation of low-frequency tokens during decoding, which is illustrated in Table. It shows that the vanilla NMT model tends to generate more high-frequency tokens and less low-frequency tokens. %This will, in turn, make the model %tend to generate too many high-frequency tokens and too less low-frequency tokens during decoding. However, low-frequency tokens may carry critical semantic information which may affect translation quality once they are neglected.   %It is very likely for NMT models to ignore the loss produced by rare words so that the patterns learned by the encoder, decoder, or attention modules from them can't be adequately updated. What's more, NMT models tend to prioritize output fluency over translation adequacy and ignore the translation of rare words during generation.  %In our experiments, we observed that vanilla NMT models usually produce more frequent words and less rare words than real references. Therefore, some techniques should be adopted to improve the translation of rare words. %distribution.   %It is obvious that there are always rare tokens no matter what the number of merge operations of BPE is and the problem of token distribution imbalance still exists.  %One of the advantages of this technique is that it reduces the number of rare words by splitting them into more frequent subword tokens , which in fact  %relieve the imbalance of word   %The strength is that NMT models can make use of large amounts of parallel training sentences and learn the knowledge and features embodied in the training data. However, one of the weaknesses is that NMT models have a tendency towards over-fitting to frequent observations , but neglecting those rare cases which are not frequently observed. Unfortunately, there is a natural word distribution imbalance in the corpus. According to the Zipf's Law, the frequency of any word is inversely proportional to its ranking in the frequency table, which indicates that the occurrences of some words are far more than others naturally.     %For word-level NMT models, NMT has its limitation in handling a larger vocabulary because of the training complexity and computing expense.   % %In their work, they first represent each word as a sequence of characters and then iteratively combine the most frequent pair as a new symbol. %which achieved better accuracy for the translation of rare words %, we seek to further alleviate the token imbalance problem based on the above analysis. For this purpose,  To address the above issue, we proposed token-level adaptive training objectives based on target token frequencies.  We aimed that those meaningful but relatively low-frequency tokens could be assigned with larger loss weights during training so that the model will learn more about them. %In our objectives, those relatively low-frequency but valuable tokens will be assigned with larger loss weights during training to encourage the model to learn more about them. To explore suitable adaptive objectives for NMT, we first applied existing adaptive objectives from other tasks to NMT and analyzed their performance. We found that though they could bring modest improvement on the translation of low-frequency tokens, they did much damage to the translation of high-frequency tokens, which led to an obvious degradation on the overall performance. This implies that the objective should ensure the training of high-frequency tokens first. %training of high-frequency tokens should be ensured first. %We should ensure the training of high-frequency tokens and enlarge the weights of low-frequency tokens at the same time. %We firstly tried the focal loss, which was proposed for solving the token imbalance problem in the CV task, and analyzed the performance.  Then, based on our observations, we proposed two heuristic criteria for designing the token-level adaptive objectives based on the target token frequencies. Last, we presented two specific forms for different application scenarios according to the criteria. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation. %We carried out experiments on ZHEN, ENRO, and ENDE translation tasks to validate our methods. The experimental results show that our methods achieve significant improvement in translation quality, especially in sentences that contain more low-frequency tokens.  %Besides, the token distribution of our translations becomes closer to references for test sets.  %Besides, our method also improves the diversity of the translations.   Our contributions can be summarized as follows:      %  %More specifically, NMT models are first trained with equal weights and then fine-tuned with well-defined weights introduced by the scoring functions. In this way, it won't hurt the translation of frequent tokens, but also can improve the translation of rare tokens to a certain degree. To the best of our knowledge, this is the first work trying to concern about the training weights at the token level to solve the distribution imbalance problem in NMT. The experiments on multiple translation tasks show that our method can improve the overall translation performance without almost any additional computing or storage expense. And the analysis experiments indicate that our method can improve the rare tokens translation significantly and the tokens distribution of our translation are much closer to the references than the baseline translations.  
"," There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation .  The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. %%% However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected.   In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training.  We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. %More specifically, those relatively low-frequency but valuable target tokens will be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. %%% %We conducted experiments  Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation. %Experiments on multiple translation tasks show that our methods can achieve significant improvement in translation quality, especially on sentences that contain more low-frequency tokens.  %Besides, our method also improves translation diversity. %Besides, the token distribution of our translations becomes closer to the reference of test sets.  %.  %Rare words translation has always been one of the key challenges to Neural Machine Translation .",88
"   Graph structures play a pivotal role in NLP because they are able to capture particularly rich structural information. For example, Figure shows a directed, labeled Abstract Meaning Representation  graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts  neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks  have been explored to better encode structural information for this task .   % \tzy{papers before 2018??? Gated Graph Neural networks??? Do not miss an important paper.}     One type of such GNNs is Graph Convolutional Networks .  GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate  neighbors.  Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions .  However, prior efforts  have shown that the locality property of existing GCNs precludes efficient non-local information propagation.  further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks  have been explored as an alternative to capture global dependencies. As shown in Figure , SANs associate each node with other nodes such that we model interactions between any two nodes in the graph. Still, this approach ignores the structure of the original graph.  and  propose structured SANs that incorporate additional neural components to encode the structural information of the input graph.   Convolutional operations, however, are more computationally efficient than self-attention operations because the computation of attention weights scales quadratically while convolutions scale linearly with respect to the input length . Therefore, it is worthwhile to explore the possibility of models based on graph convolutions. One potential approach that has been considered is to incorporate information from higher order neighbors, which helps to facilitate non-local information aggregation for node classification . However, simple concatenation of different order representations may not be able to model complex interactions in semantics for text generation .    We propose to better integrate high-order information, by introducing a novel dynamic fusion mechanism and propose the Lightweight, Dynamic Graph Convolutional Networks . As shown in Figure  , nodes in the LDGCN model are able to integrate information from first to third-order neighbors. With the help of the dynamic mechanism, LDGCNs can effectively synthesize information from different orders to model complex interactions in the AMR graph for text generation. Also, LDGCNs require no additional computational overhead, in contrast to vanilla GCN models. We further develop two novel weight sharing strategies based on the group graph convolutions and weight tied convolutions. These strategies allow the LDGCN model to reduce memory usage and model complexity.  Experiments on AMR-to-text generation show that LDGCNs outperform best reported GCNs and SANs trained on LDC2015E86 and LDC2017T10 with significantly fewer parameters. On the large-scale semi-supervised setting, our model is also consistently better than others, showing the effectiveness of the model on a large training set. We release our code and pretrained models at \url{https://github.com/yanzhang92/LDGCNs}.\footnote{Our implementation is based on  MXNET  and the Sockeye toolkit .}   
"," 	 	% Camera-Ready 	AMR-to-text generation is used to transduce Abstract Meaning Representation structures  into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks  were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local  information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks  that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.",89
"  [h!]       marginalize over all possible factorizations of the joint distribution within and across all channels .      \modelabbv{} is trained to predict the tokens to be inserted , given partially observed inputs.     At inference, \modelabbv{} can take full, partial, or empty sequence from each channel and generate the full sequence for each channel.}             A natural way to consider two parallel sentences in different languages is that each language expresses the same underlying meaning from a different viewpoint.  Each language can be thought of as a transformation that maps an underlying concept into a view that we collectively agree is determined as `English' or `French'.  Similarly, an image of a cat and the word `cat' are expressing two views of the same underlying concept.  In this case, the image corresponds to a high bandwidth channel and the word `cat' to a low bandwidth channel.  This way of conceptualizing parallel viewpoints naturally leads to the formulation of a fully generative model over each instance, where the transformation corresponds to a particular generation of the underlying view.  We define each of these views as a channel. As a concrete example, given a parallel corpus of English and French sentences, English and French become two channels, and the corresponding generative model becomes .  One key advantage of this formulation is that a single model can be trained to capture the full expressivity of the underlying concept, allowing us to compute conditionals and marginals along with the joint.  In parallel sentences, the conditionals correspond to translations from one channel to another while the marginals correspond to standard monolingual language models.  In this work, we present a general framework for modeling the joint distribution  over  channels by marginalizing over all possible factorizations across the channels and within each channel.  This formulation allows our framework to perform: 1) unconditional generation, 2) fully conditional generation , and 3) partial conditional generation .  The key contributions in this work are:      , a multichannel generative modeling framework. \modelabbv{} models the joint distribution  over  channels by marginalizing over all possible factorization across and within sequences.      is trained over all possible factorizations, \modelabbv{} can perform both conditional generation , and partially observed conditional generation across different channels .      and prior work.   We highlight that while we focus on languages as a specific instantiation of a channel, our framework can generalize to any arbitrary specification, such as other types of tasks  or other modalities .   %%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model . MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels.  MGLM endows flexible inference, including unconditional generation, conditional generation , and partially observed generation .  We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.",90
" Neural machine translation  has achieved promising results with the use of various optimization tricks.  In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive.  As an alternative mitigation, curriculum learning~ has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training.  CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of ``difficulty'' and the strategy of curricula design. Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length  and word rarity , and manually tune the learning schedule.  However, neither there exists a clear distinction between easy and hard examples, nor these human intuitions exactly conform to effective model training.  Instead, we resolve this problem by introducing self-paced learning, where the emphasis of learning can be dynamically determined by model itself rather than human intuitions. Specifically, our model measures the level of confidence on each training example, where an easy sample is actually the one of high confidence by the current trained model. Then, the confidence score is served as a factor to weight the loss of its corresponding example. In this way, the training process can be dynamically guided by model itself, refraining from human predefined patterns.   We evaluate our proposed method on IWSLT15 EnVi, WMT14 EnDe, as well as WMT17 ZhEn translation tasks. Experimental results reveal that our approach consistently yields better translation quality and faster convergence speed than Transformer baseline and recent models that exploit CL. Quantitative analyses further confirm that the intuitive curriculum schedule for a human does not fully cope with that for model learning.  
"," Recent studies have proven that the training of neural machine translation  can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the hand-crafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step.  Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.\footnote{Our codes:  {https://github.com/NLP2CT/SPL\_for\_NMT}.}",91
" In recent years, cyberbullying has become one of the most pressing online risks among youth and raised serious concerns in society. Cyberbullying is commonly defined as the electronic transmission of insulting or embarrassing comments, photos or videos, as illustrated in Figure~ . Harmful bullying behavior can include posting rumors, threats, pejorative labels, and sexual remarks. Research from the American Psychological Association and the White House has revealed more than  of young people in the US indicate that they have been bullied on social media platforms~. Such a growing prevalence of cyberbullying on social media has detrimental societal effects, such as victims may experience lower self-esteem, increased suicidal ideation, and a variety of negative emotional responses~. Therefore, it has become critically important to be able to detect and prevent cyberbullying on social media. Research in computer science aimed at identifying, predicting, and ultimately preventing cyberbullying through better understanding the nature and key characteristics of online cyberbullying.     In the literature, existing efforts toward automatically detecting cyberbullying have primarily focused on textual analysis of user comments, including keywords~ and sentiments analysis ~. These studies attempt to build a generic binary classifier by taking high-dimensional text features as the input and make predictions accordingly. Despite their satisfactory detection performance in practice, these models largely overlooked temporal information of cyberbullying behaviors. They also ignore user interactions in social networks. Furthermore, the majority of these methods focus on detecting cyberbullying sessions effectively but cannot explain ``why'' a media session was detected as cyberbullying. Given a sequence of comments with user attributes, we think sequential learning can allow us to better exploit and model the evolution and correlations among individual comments. Besides, graph-based learning can enable us to represent and learn how users interact with each other in a session.   This work aims to detect cyberbullying by jointly exploring explainable information from user comments on social media. To this end, we build an explainable cyberbullying detection framework, \underline{HE}terogeneous \underline{N}eural \underline{I}nteraction \underline{N}etworks , through a coherent process. HENIN consists of three main components that learn various interactions among heterogeneous information displayed in social media sessions. A comment encoder is created to learn the representations of user comments through a hierarchical self-attention neural network so that the semantic and syntactic cues on cyberbullying can be captured. We create a post-comment co-attention mechanism to learn the interactions between a posted text and its comments. Moreover, two graph convolutional networks are leveraged to learn the latent representations depicting how sessions interact with one another in terms of users, and how posts are correlated with each other in terms of words.  Specifically, we address several challenges in this work:  how to perform explainable cyberbullying detection that can boost detection performance,  how to highlight explainable comments without the ground truth,  how to model the correlation between posted text and user comments, and  how to model the interactions between sessions in terms of users, and the interactions between textual posts in terms of words. Our solutions to these challenges result in a novel framework HENIN.   Our contributions are summarized as follows. %   We study a novel problem of explainable cyberbullying detection on social media.  We provide a novel model, HENIN~, which jointly exploits posted text, user comments, and the interactions between sessions and between posts to learn the latent representations for cyberbullying detection.  Experiments conducted on Instagram and Vine datasets exhibit the promising performance of HENIN, and the evidential comments and words highlighted by HENIN, for detecting cyberbullying media sessions with explanations. %   
"," In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks , for explainable cyberbullying detection. HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.",92
"  % Recent work in NLP has seen a flurry of interest in the question: are the representations learned by neural networks compositional? That is, are representations of longer phrases built recursively from representations of shorter phrases, as they are in many linguistic theories? If so, how and when do they learn to do this?  For years the LSTM dominated language architectures. It remains a popular architecture in NLP, and unlike Transformer-based models, it can be trained on small corpora~.\footnote{As evidence of the ongoing popularity of LSTMs in NLP, a Google Scholar search restricted to  since 2019 finds 191 citations to the original LSTM paper  and 242 citations to the original Transformer paper .}  even found that the recurrent inductive biases behind the LSTM's success are so essential that distilling from them can improve the performance of fully attentional models. However, the reasons behind the LSTM's effectiveness in language domains remain poorly understood.   A Transformer can encode syntax using attention , and some LSTM variants explicitly encode syntax . So, the success of these models is partly explained by their ability to model syntactic relationships when predicting a word. By contrast, an LSTM simply scans a sentence from left to right, accumulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed -gram data, implying that they exploit linguistic structure in long-distance dependencies . Their internal representations appear to encode constituency  and syntactic agreement . In this paper, we consider how such representations are learned, and what kind of inductive bias supports them.   To understand how LSTMs exploit syntax, we use contextual decomposition , a method that computes how much the hidden representation of an LSTM depends on particular past span of words. We then extend CD to Decompositional Interdependence , a measure of interaction between spans of words to produce the representation at a particular timestep. For example, in the sentence ``Socrates asked the student trick questions閳ユ瑢, we might expect the hidden representation of the LSTM at the word ``questions閳ユ瑢 to interact primarily with its syntactic head ``asked閳ユ瑢, and less with the direct object ``the student''. If so, then an LSTM could be seen as implementing compositional  : if a hidden representation encodes meaning, then this meaning is composed from local syntactic relationships. Our experiments on syntactically-parsed corpora  illustrate this property --- interdependence decreases with syntactic distance, stratified by surface distance.  We then turn to a hypothesis about how such representations are learned. Using a simple synthetic corpus , we allow LSTMs to learn to represent short sequences before they learn longer sequences that are dependent on them. Our goal is to then illustrate how they use representations of short sequences in order to learn longer dependencies---if these smaller constituents are unfamiliar, LSTMs learn more slowly. Further experiments  isolate hierarchical behavior from other factors causing local relations to be learned first, indicating that the model tends to build a subtree from its smaller constituents. We conclude that LSTMs compose hierachically because they learn bottom-up.   
"," Recent work in NLP shows that LSTM language models capture hierarchical structure in language data. In contrast to existing work, we consider the learning process that leads to their compositional behavior. For a closer look at how an LSTM's sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence  between word meanings in an LSTM, based on their gate interactions. We connect this measure to syntax with experiments on English language data, where DI is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a specific hypothesis about how hierarchical structures are discovered over the course of training: that LSTM constituent representations are learned bottom-up, relying on effective representations of their shorter children, rather than learning the longer-range relations independently from children.",93
"  Systematic reviews are part of the field of evidence-based analysis, and are a methodology for conducting literature surveys, where the focus is on comprehensively summarising and synthesising existing research for the purpose of answering research questions . The aim of this process is to be very broad coverage to avoid unknown bias creeping into results via the alternative of cherry-picking scientific results . %As many relevant documents as possible should be included, and the process should also be thoroughly documented to aid replicability.  Conducting systematic reviews requires trained researchers with domain knowledge. The stages of the process are time-consuming, but vary in how much physical and mental labour they require . As a result, systematic reviews suffer from three primary challenges :    Which techniques are best for identifying and extracting the desired information?  How much labelled training data is needed? Can existing resources be leveraged?  How generalisable is a pipeline to new diseases and countries?  What is the trade-off between pipeline accuracy and human time savings?  How important is model architecture as applied to extraction tasks? How important is embedding pre-training, and how important is pre-training on scientific literature vs. general content ?\\  We find that surprisingly little training data  are necessary to get an accurate document classifier, and that it generalises well to unseen African countries , which enables systematic reviews to be expanded to new areas with essentially constant time. In our text extraction experiments, we find that both sentence and phrase level extraction models can each play a role in such a pipeline,  %given their complementary strengths and weaknesses on this kind of data,  but that phrase extraction, which has not previously been done for this task, performed better than expected both with baseline CNN models  and with BERT-based Transformers , with Transformers based on scientific pre-training  performing best. We demonstrate how the creation of labelled training data can be sped up through annotation tools, and that consideration should be given to the balance of training examples present within this data, since doing so may require less data overall while still maintaining good performance. Furthermore, besides automatic information extraction, much labour in constructing systematic reviews can be saved through simply automating the process of searching and downloading documents.   We empirically demonstrate that most of the three month pipeline of a systematic review can be automated to require very little human intervention, with acceptable accuracy of results. We release our code, annotation schema, and labelled data to assist in the expansion of systematic reviews via automation.  While we demonstrate this system on one domain, the framework is domain independent and could be applied to other kinds of systematic reviews. New training data and annotation schemes would be necessary to switch to medical or other domains, but our findings on time saving processes for annotation  would apply, and confidence thresholds that we implement are adjustable to customise to different levels of accuracy to human time trade-offs that are appropriate to different fields. Our exploration into necessary amounts of training data for accuracy and generalisability are broadly applicable.  
"," Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15\% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.\footnote{Code and links to models available at \url{https://github.com/seraphinatarrant/systematic_reviews}}",94
"   Although recent neural models of language have made advances in learning syntactic behavior, research continues to suggest that inductive bias plays a key role in data efficiency and human-like syntactic generalization . Based on the long-held observation that language exhibits hierarchical structure, previous work has proposed coupling recurrent neural networks  with differentiable stack data structures  to give them some of the computational power of pushdown automata , the class of automata that recognize context-free languages . However, previously proposed differentiable stack data structures only model deterministic stacks, which store only one version of the stack contents at a time, theoretically limiting the power of these stack RNNs to the deterministic~CFLs.  A sentence's syntactic structure often cannot be fully resolved until its conclusion , requiring a human listener to track multiple possibilities while hearing the sentence. Past work in psycholinguistics has suggested that models that keep multiple candidate parses in memory at once can explain human reading times better than models which assume harsher computational constraints. This ability also plays an important role in calculating expectations that facilitate more efficient language processing . Current neural language models do not track multiple parses, if they learn syntax generalizations at all .  We propose a new differentiable stack data structure that explicitly models a nondeterministic PDA, adapting an algorithm by  and reformulating it in terms of tensor operations. The algorithm is able to represent an exponential number of stack configurations at once using cubic time and quadratic space complexity. As with existing stack RNN architectures, we combine this data structure with an RNN controller, and we call the resulting model a \ourmodel{} .  We predict that nondeterminism can help language processing in two ways. First, it will improve trainability, since all possible sequences of stack operations contribute to the objective function, not just the sequence used by the current model. Second, it will improve expressivity, as it is able to model concurrent parses in ways that a deterministic stack cannot. We demonstrate these claims by comparing the \om{} to deterministic stack RNNs on formal language modeling tasks of varying complexity. To show that nondeterminism aids training, we show that the \om{} achieves lower cross-entropy, in fewer parameter updates, on some deterministic CFLs. To show that nondeterminism improves expressivity, we show that the \om{} achieves lower cross-entropy on nondeterministic CFLs, including the ``hardest context-free language"" , a language which is at least as difficult to parse as any other CFL and inherently requires nondeterminism. Our code is available at \url{https://github.com/bdusell/nondeterministic-stack-rnn}.  
"," We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang闁炽儲鐛 algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network  controller a \ourmodel. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks.",95
"   Cryptography has been used since antiquity to encode important secrets.  There are many unsolved ciphers of historical interest, residing in national libraries, private archives, and recent corpora collection projects .  Solving classical ciphers with automatic methods is a needed step in analyzing these materials.  In this work, we are concerned with automatic algorithms for solving a historically-common type of book code, in which word tokens are systematically replaced with numerical codes. Encoding and decoding are done with reference to a dictionary possessed by both sender and recipient.  While this type of code is common, automatic decipherment algorithms do not yet exist.  The contributions of our work are:     
"," We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.  We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress.  We are able to decipher 75.1\% of the cipher-word tokens correctly.",96
"   Neural network language models , pretrained on vast amounts of raw text, have become  the dominant input to downstream tasks . Commonly, these tasks involve aspects of language  comprehension . One explicit example is coreference resolution, wherein anaphora  are linked to antecedents  requiring knowledge of syntax, semantics,  and world-knowledge to match human-like comprehension.   Recent work has suggested that LMs acquire abstract, often human-like, knowledge of syntax  . Additionally, knowledge of grammatical and referential aspects linking a pronoun to its antecedent noun   have been demonstrated for both  transformer and long short-term memory architectures . Humans are able  to modulate both referential and syntactic comprehension  given abstract linguistic knowledge . Contrary to humans, we find that discourse structure  only influences LM behavior  for reference, not syntax, despite model representations that encode the necessary discourse information.  The particular discourse structure we examined is governed by implicit causality  verbs . Such verbs influence pronoun comprehension:   and Mary, so  both are possible antecedents. However, English speakers overwhelmingly  interpret she as referring to Sally in  and Mary  in , despite the semantic overlap between the verbs. Verbs that  have a subject preference  are called subject-biased IC verbs, and verbs with a object preference  are called object-biased IC verbs.   In addition to pronoun resolution, IC verbs also interact with relative clause  attachment:    in  and  and  continuations modifying the children in  and . We might expect  human continuation preferences to be the same in  and . However, the use  of an object-biased IC verb  in  increases the proportion of continuations given by human participants  that refer to the children . Without  an object-biased IC verb the majority of continuations refer to the more recent noun  .  Effects  of IC have received renewed interest in the field of psycholinguistics in recent years . Current accounts of IC claim that the phenomenon is inherently a linguistic process, which  does not rely on additional pragmatic inferences by comprehenders . Thus, IC is argued to be contained within the linguistic signal, analogous to  evidence of syntactic agreement and verb argument structure within corpora. We  hypothesize that if these claims are correct, then current LMs will be able to  condition reference and syntactic attachment by  IC verbs with just language data .   We tested this hypothesis using unidirectional transformer and long short-term memory network  language models. We find that LSTM  LMs fail to acquire a subject/object-biased IC distinction that influences reference or RC attachment.   In contrast, transformers learned a representational  distinction between subject-biased and object-biased IC verbs that interacts  with both reference and RC attachment,  but the distinction only influenced model output for reference. The apparent failure of model  syntactic behavior to exhibit an IC  contrast that is present in model representations raises questions  about the broader capacity of LMs to display  human-like linguistic knowledge.  
","  Language models  trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference  and syntactic processing on the same discourse structure . We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.",97
" Word ordering often determines the meaning of a sentence; therefore how to utilize the position information of a word sequence has been an important topic in NLP and widely investigated recently. A common approach for modeling word ordering is to use recurrent neural networks , such as long short-term memory   or gated  recurrent unit  , which use a hidden state to represent the information of an ordered sequence and update model weights by backpropagation through time  ; thus the ordering information can be modeled by this structure.  However, RNN and BPTT are very inefficient in modern GPU computation due to the difficulty of parallelization with the time dependency. To solve this problem, recent work, such as convolutional seq2seq  and Transformers  which apply convolutional neural network   and self-attention respectively, succeed to eliminate the time dependency to take the computational advantage of GPU.  Instead of storing the information of ordered sequences, these models utilize the position information by using a feature-level positional encoding. For example, convolutional seq2seq proposed learnable position embeddings to represent the positions in a sequence.  Recently, various pre-trained Transformer language models keep breaking state-of-the-art results in numerous NLP tasks.  There are many different ways to pre-train a Transformer language model. For example, using an encoder, decoder, or the whole part of the Transformer, adapting the self-attention masks, or training with different objectives .  However, in terms of positional encoding, most work only used a learned position embedding which is originally proposed in convolutional seq2seq  without any analysis, even different objectives may learn completely different position information.  Motivated by the above observations, our goal is to investigate what position information the pre-trained Transformers could learn under different settings. We conduct a deep analysis of the learned position embeddings among three iconic pre-trained Transformer language models: BERT , RoBERTa  and GPT-2 . To examine the performance of different NLP types, we conduct the experiments on text classification, language modeling, and machine translation, and empirically analyze and explain the meaning and influence of position embeddings from different aspects.  The contributions of this paper are 3-fold:    
"," In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks.  Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention.  Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will.  Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks?  This paper focuses on providing a new insight of pre-trained position embeddings through feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future work to choose the suitable positional encoding function for specific tasks given the application property.\footnote{The source code is available at: \url{https://github.com/MiuLab/PE-Study}} %to make our study more convincing.",98
"  is the task of learning the grammar of a target corpus without exposure to the parsing ground truth or any expert-labeled tree structures . Recently emerging  models provide a new approach to this problem . They learn syntactic parsing under only indirect supervision from their main training tasks such as language modelling and natural language inference.  In this study, we analyze ON-LSTM , a new latent tree learning model that set the state of the art on unsupervised constituency parsing on WSJ test  when it was published at ICLR 2019. The model is trained on language modelling and can generate binary constituency parsing trees of input sentences like the one in Figure .     As far as we know, though there is an excellent theoretical analysis paper  of the ON-LSTM model that focuses on the model's architecture and its parsing algorithm, there is no systematic analysis of the parses the model generates. There are no in-depth investigations of  whether the model's parsing behavior is consistent among different restarts or  how the parses it produces are different from PTB gold standards. Answering these questions is crucial for a better understanding of the capability of the model and may bring insights into how to build more advanced latent tree learning models in the future.  Therefore, we replicate the model with 5 random restarts and look into the parses it generates. We find that  ON-LSTM has fairly consistent parsing behaviors across different restarts, achieving a self F1 of 65.7 on WSJ test.  The model struggles to correctly parse the internal structures of complex noun phrases.  The model has a consistent tendency to overestimate the height of the split points right before verbs or auxiliary verbs, leading to a major difference between its parses and the Penn Treebank gold-standard parses. We speculate that both problems can be explained by the training task, unidirectional language modelling, and thus we hypothesize that training a bidirectional model on a more syntax-related task like acceptability judgement might be a good choice for future latent tree learning models.   
"," Recent  models can learn constituency parsing without any exposure to human-annotated tree structures. One such model is ON-LSTM , which is trained on language modelling and has near-state-of-the-art performance on unsupervised parsing. In order to better understand the  performance and consistency of the model as well as how the parses it generates are different from gold-standard PTB parses, we replicate the model with different restarts and examine their parses. We find that  the model has reasonably consistent parsing behaviors across different restarts,  the model struggles with the internal structures of complex noun phrases,  the model has a tendency to overestimate the height of the split points right before verbs. We speculate that both problems could potentially be solved by adopting a different training task other than unidirectional language modelling.",99
"  Commonsense reasoning is an important yet challenging task in artificial intelligence and natural language processing. Take commonsense question answering as an example, given a question and multiple choices, some commonsense knowledge is usually required to make the correct answer from the provided choices. Table show some typical commonsense question answering examples extracted from the dataset of commonsenseQA.        {l p{5cm}} \toprule Question: & Where is a good idea but not required to have a fire extinguisher?\\ Choices: & ~school bus ~boat ~house ~hospital ~school\\ % Answer: & ~house\\ \midrule Question: & Where can you put a picture frame when it's not hung vertically?\\ Choices: & ~art show ~wall ~newspaper ~car ~table\\ % Answer: & ~wall\\    \vskip -0.25in   Existing commonsense reasoning methods mainly utilize raw texts to conduct the data representation and answer prediction process. However, the background knowledge required in the commonsense reasoning task, such as spatial relations, causes and effects, scientific facts and social conventions, are usually not explicitly provided by the text. Therefore, it is difficult to capture such knowledge solely from the raw texts. Some other works propose to leverage knowledge bases to extract related commonsense knowledge. However, the construction of a knowledge base is expensive, and the contained knowledge is too limited to fulfill the requirement. Furthermore, most commonsense question answering datasets, such as CommonsenseQA, are constructed from an existing knowledge base, e.g., ConceptNet . So it is unfair to use the knowledge base in these tasks. To sum up, how to automatically learn commonsense remains a challenging problem in NLP.  Motivated by the fact that images usually contain richer scene information, which can be viewed as an important supplementary resource to perceive for commonsense knowledge, this paper proposes to learn commonsense from images and incorporate such knowledge into the commonsense reasoning process. Take the question `' shown in Table as an example. Solving this problem requires a strong background knowledge that fire extinguishers are usually equipped in public places, such as hospitals, schools, and school buses. We can see that such background knowledge is not explicitly provided by the raw texts, and meanwhile, too abstract and complex to be extracted by the current language model techniques. In this case, images will help. For example, we could find many images where fire extinguishers appear in these scenes of public places. Therefore, this commonsense knowledge could be learned by perceiving the scene information of these images, and the corresponding question will be well answered. These analyses are in accordance with Minsky's statement in , `perhaps a good architecture theory based on multiple representations and multi-modal reasoning would help us to design better systems that allow us to study and understand commonsense reasoning.'   Our approach, named Loire , consists of two stages, i.e.~visual commonsense learning and knowledge-augmented reasoning. In the first stage, a scene layout generation task is conducted on a bi-modal data such as the representative benchmark COCO. Firstly, a text encoder Visual BERT  is employed to obtain the representation of a caption. ViBERT is then incorporated into the recurrent encoder-decoder structure for the labeled bounding box generation. This module is trained separately by a supervised learning approach, based on the ground-truth bounding boxes of images. In this way, the required visual commonsense knowledge will be encoded in ViBERT. In the following commonsense reasoning stage, the concerned text representations  will be obtained by concatenating ViBERT and a traditional pre-trained language model, e.g. ~BERT. Then the language model is fine-tuned on the commonsense reasoning data, with ViBERT fixed as some prior knowledge. Experimental results on two commonsense reasoning tasks, i.e.~CommonsenseQA and WinoGrande , demonstrate that the learnt commonsense from images brings improvements to traditional models, such as BERT fine-tune  and RoBERTa fine-tune . We also give some case studies to show how the learned visual commonsense knowledge helps the reasoning process.   To the best of our knowledge, we are the first to propose learning commonsense knowledge from images to facilitate the commonsense reasoning in NLP. The proposed model of using scene layout generation as the supervision demonstrates a preliminary exploration in this direction. Other methods like learning commonsense from retrieved relevant images could also be investigated. We believe this novel approach may provide a new perspective for commonsense reasoning in NLP.  
"," This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thousand words, where richer scene information could be leveraged to help distill the commonsense knowledge, which is often hidden in languages. Our approach, namely Loire, consists of two stages. In the first stage, a bi-modal sequence-to-sequence approach is utilized to conduct the scene layout generation task, based on a text representation model ViBERT. In this way, the required visual scene knowledge, such as spatial relations, will be encoded in ViBERT by the supervised learning process with some bi-modal data like COCO. Then ViBERT is concatenated with a pre-trained language model to perform the downstream commonsense reasoning tasks. Experimental results on two commonsense reasoning problems, i.e.~commonsense question answering and pronoun resolution, demonstrate that Loire outperforms traditional language-based methods. We also give some case studies to show what knowledge is learned from images and explain how the generated scene layout helps the commonsense reasoning process. \let\thefootnote\relax\footnotetext{*Corresponding Author}",100
"  Our SJTU-NICT team participated in the WMT20 shared task, including supervised track, unsupervised, and low-resource track. During the participation, we placed our attention on Polish   English  and English   Chinese  on the supervised track, while on the unsupervised and low-resource track, the German   Upper Sorbian  both directions are focused.  Our  baseline system in supervised track is based on the Transformer big architecture proposed by , in which its open-source implementation version Fairseq  is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework , and used the two-stage training mode of masked language modeling  pre-training + back-translation  finetune to obtain a very strong baseline performance. Marian  toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets.  In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team , we divided the three language pairs we participated in into three categories:   %  toolkit to performs reranking to get the final system output.  
","  In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation  techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT,  data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.",101
"     Neural summarizers have achieved impressive performance when evaluated by ROUGE ~ on in-domain setting, and the recent success of pre-trained models drives the state-of-the-art results on benchmarks to a new level ~. However, the superior performance is not a guarantee of a perfect system since exsiting models tend to show defects when evaluated from other aspects. For example,  observes that many abstractive systems tend to be near-extractive in practice.  reveal that most generated summaries are factually incorrect. These non-mainstream evaluation methods make it easier to identify the model's weaknesses.  Orthogonal to above two evaluation aspects, we aim to diagnose the limitation of existing systems under , in which a summarization system trained on  one corpus would be evaluated on a range of out-of-dataset corpora. Instead of evaluating the quality of summarizers solely based on one dataset or multiple datasets individually, cross-dataset evaluation enables us to evaluate model performance from a  different angle. For example, Fig. shows the ranking of  summarization systems studied in this paper under different  evaluation metrics, in which the ranking list `` in-dataset R2'' is obtained by traditional ranking criteria while other two are based on our designed cross-dataset measures. Intuitively, we observe that 1) there are different definitions of a ``good'' system in various evaluation aspects; 2) abstractive and extractive systems exhibit diverse behaviors when evaluated under the cross-dataset setting.    The above example recaps the general motivation of this work, encouraging us to rethink the generalization ability of current top-scoring summarization systems from the perspective of cross-dataset evaluation. Specifically, we ask two questions as follows:   Q1: {How do different neural architectures of summarizers influence the cross-dataset generalization performances?} When designing summarization systems, a plethora of neural components can be adopted ~. For example, will   and    mechanisms improve the cross-dataset generalization ability of summarizers? Is there a risk that  summarizers will perform worse when adapted to new areas compared with the ones without BERT? So far, the generalization ability of current summarization systems when transferring to new datasets still remains unclear, which poses a significant challenge to design a reliable system in realistic scenarios. Thus, in this work, we take a closer look at the effect of model architectures on cross-dataset generalization setting.    Q2: {Do different generation ways  of summarizers influence the cross-dataset generalization ability?} Extractive and abstractive models, as two typical ways to summarize texts, usually follow diverse learning frameworks and favor different datasets.  It would be absorbing to know their discrepancy from the perspective of cross-dataset generalization.      To answer the questions above, we have conducted a comprehensive experimental analysis, which involves eleven summarization systems , five benchmark datasets from different domains, and two evaluation aspects. Tab. illustrates the overall analysis framework. We explore the effect of different architectures and generation ways on model generalization ability in order to answer  and . Semantic equivalency  and factuality are adopted to characterize the different aspects of cross-dataset generalization ability. Additionally, we strengthen our analysis by presenting two views of evaluation:  and  views .   [t]   {2}   \resizebox{0.47\textwidth}{12mm}{     {lcc}     \toprule     Framework & \makecell{Semantic equivalency \\ } & \makecell{Factuality \\ } \\     \midrule     \makecell[l]{Q1: Architecture \\ } & Sec.  & Sec.\\     \makecell[l]{Q2: Generation way \\ } & Sec.  & Sec.\\     }%        % %  Our contributions can be summarized as: 1) Cross-dataset evaluation is orthogonal to other evaluation aspects , which can be used to re-evaluate current summarization systems, accelerating the creation of more robust summarization systems. 2) We have design two measures Stiffness and Stableness, which could help us to characterize generalization ability in different views, encouraging us to diagnose the weaknesses of state-of-the-art systems.  3) We conduct dataset bias-aided analysis  and suggest that a better understanding of datasets will be helpful for us to interpret systems'  behaviours.     
"," Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an  setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways  on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in \url{https://github.com/zide05/CDEvalSumm}.",102
"    As robots are deployed in collaborative applications like healthcare and household assistance , there is a growing need for reliable human-robot communication. One such communication modality that is both user-friendly and versatile is natural language; to this end, we focus on robust natural language interfaces  that can map utterances to executable behavior .  Most existing work on NLIs  falls into a static train-then-deploy paradigm: models are first trained on large datasets of  pairs and then deployed, with the hope they will reliably generalize to new utterances. Yet, what happens when such models make mistakes or are faced with types of utterances unseen at training --- for example, providing a household robot with a novel utterance like ``wash the coffee mug?'' Such static systems will fail with no way to recover, burdening the user to find alternate utterances to accomplish the task . Instead, we argue that NLIs need to be dynamic and adaptive, learning interactively from user feedback to index and perform more complicated behaviors.   In this work, we explore building NLIs for simulated robotics that learn from real humans. Inspired by , we leverage the idea of learning from decomposition to learn new abstractions. Just like how a human interactively teaches a new task to a friend by breaking it down, users interactively teach our system by simplifying utterances that the system cannot understand  into lower-level utterances that it can .  To map language to executable behavior,  and  built adaptive NLIs that leverage grammar-based parsers that allow reliable one-shot generalization but lack lexical flexibility. For example, a grammar-based system that understands how to ``wash the coffee mug'' may not generalize to ``clean the mug.'' Meanwhile, recent semantic parsers are based primarily on neural sequence-to-sequence models . While these models excel from a lexical flexibility perspective, they lack the ability to perform reliable one-shot generalization: it is difficult to train them to generalize from individual examples .    In this paper we propose a new interactive NLI that is lexically flexible and can reliably and efficiently perform one-shot generalization. We introduce a novel exemplar-based neural network semantic parser that first abstracts away entities , allowing for generalization to previously taught utterances with novel object combinations. Our parser then retrieves the corresponding ``lifted'' utterance and respective program  from the training examples based on a learned metric , giving us the lexical flexibility of sequence-to-sequence models.  We demonstrate the efficacy of our learning from decomposition framework through a set of human-in-the-loop experiments where crowdworkers use our NLI to solve a suite of simulated robotics tasks in household environments. Crucially, after completing a task, we update the semantic parser so that users can immediately reuse what they taught. We show that over time, users are able to complete complex tasks  more efficiently with our exemplar-based method compared to a neural sequence-to-sequence baseline. However, for more straightforward tasks that can be completed in fewer steps, we see similar performance to the baseline. We end with an error analysis and discussion of user trust and incentives in the context of building interactive semantic parsing systems, paving the way for future work that better realizes the potential of the interactive paradigm.  
","  Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm.",103
"  As neural machine translation  significantly improved sentence-level translation qualities, recent studies have been focused on document-level translation.  In particular, discourse in document-level translation is one of the central research interests, such as addressing coreference and anaphora resolution; and preserving cohesion and coherence in translation; {{ipxm}閺呭倽鈻''  into ``clock'' and ``watch,'' while the cohesive translations consistently translate the word into ``watch.''   Previous studies approached discourse phenomena in NMT using a context-aware NMT model, which inputs previous source sentences and their translations as contexts.  However,~ showed that lexical cohesion is hard to solve with only context-aware models.  We conjecture this is because context-aware models handle previous translations as a whole and are not sensitive enough to word usage consistency.   In this study, we employ a copy mechanism  on the context-aware NMT model for document-level translation to explicitly address the lexical cohesion problem.  Our model computes a probability of copying a target word from previous translation outputs and boosts its output probability in the translation of a current sentence.    We conduct experiments on Japanese to English document translation. % using the evaluation dataset designed for discourse phenomena.  The results indicate that our model achieves significantly better lexical cohesion, comparing to previous context-aware NMT models.  [t]  \def\arraystretch{1.2}%  1 is the default, change whatever you need     {l|l}     {*}{Source} & {UTF8}{ipxm}閻㈤鑵戦妵鏇樺遍妴浣靛涢妵鍒搖nderline{閺呭倽鈻搣閵堟帇浜伴幐浣典画閵囇佷粴閵囶厹淇揺nd{CJK}\\     & {UTF8}{ipxm}閵囧倶鍊為妵灞讳缓閵囧棎浣典壕閵囩敍underline{閺呭倽鈻搣閵囶垳顨涢悥韬蹭紕瑜般垼顩伴妵顏傚遍妵褋浠氶妴淇揺nd{CJK} \\     [2]{*}{Incohesive translations}& You have a good \underline{clock}, Mr. Tanaka.\\     &Thank you, this \underline{watch} is a memento of my grandfather.\\      [2]{*}{Cohesive translations} & You have a good \underline{watch}, Mr. Tanaka. is a memento of my grandfather.                   
"," Lexically cohesive translations preserve consistency in word choices in document-level translation.  We employ a copy mechanism into a context-aware neural machine translation model to allow copying words from previous translation outputs.  Different from previous context-aware neural machine translation models that handle all the discourse phenomena implicitly, our model explicitly addresses the lexical cohesion problem by boosting the probabilities to output words consistently.   We conduct experiments on Japanese to English translation using an evaluation dataset for discourse translation.  The results showed that the proposed model significantly improved lexical cohesion compared to previous context-aware models.",104
"  % ============== version 5.0 ================= Intent detection, a fundamental component of task-oriented dialogue system , is increasingly raising attention as a Multi-Label Classification  problem , since a single utterance often carries multiple user intents .  In real-world scenarios, intent detection often suffers from lack of training data, because dialogue tasks/domains change rapidly and new domains usually contain only a few data examples.  Recent success of Few-Shot Learning  presents a promising solution for such data scarcity challenges.  It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience. % from old domains.   %For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy to convert the multi-class classification into binary-class classifications .  State-of-the-art works for multi-label intent detection focus on threshold-based strategy, where a common practice is estimating label-instance relevance scores and picking the intent labels with score higher than a threshold value .  Usually, the coordination and respective quality of the two modules, i.e. thresholding and relevance scoring, are crucial to the performance of MLC models.  However, in few-shot scenarios, such multi-label setting poses unique challenges for both threshold estimation and label-instance relevance scoring.  For thresholding, previous works explore to tune a fixed threshold  or to learn thresholds from data . But, these thresholds work well only when learning examples are sufficient.  In few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %In few-shot scenarios, it is pretty hard to determine appropriate thresholds  %with only a few examples. %without overfitting to the limited examples. % to the limited examples. %For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. Besides, it is also difficult to directly transfer the pre-learned thresholds due to the domain differences, such as differences in label number per instance, score density and scale.    Estimation of the label-instance relevance scores is also challenging. %It is also challenging to compute the label-instance relevance scores.  Few-shot learning has achieved impressive progress with similarity-based methods  , where the relevance scores can be modeled as label-instance similarities.  And the label representations can be obtained from corresponding support examples.  Unfortunately, despite huge success in previous single-label tasks, these similarity-based methods become impractical for multi-label problems.  When instances have multiple labels, representations of different labels may be obtained from the same support examples and become confused with each other. For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation,  %Such confused label representations  which makes it impossible to predict correct labels with similarity scores.  %In such situations, vanilla similarities will assign query x equal score to query\_time and query\_loc  In this paper, we study the few-shot learning problem of multi-label intent detection and propose a novel framework to tackle the challenges from both thresholding and label-instance relevance scoring.  To solve the thresholding difficulties of prior-knowledge transferring and domain adaption with limited examples, we propose a Meta Calibrated Threshold  mechanism that first learns universal thresholding experience on data-rich domains, then adapts the thresholds to certain few-shot domains with a Kernel Regression based calibration.  Such combination of universal training and domain-specific calibration allows to estimate threshold using both prior domain experience and new domain knowledge.  %Here, as a non-parametric learning method, Kernel Regression allows to alleviate overfitting by calibrating the thresholds without finetuning.  To tackle the challenge of confused label representation in relevance scoring, we propose the Anchored Label Representation  to obtain well-separated label representations. Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represents each label with both support examples and corresponding anchors.  Different from the previous single-label intent detection that uses label embedding as additional features , our label embeddings here have unique effects of separating different labels in metric space.  Finally, to encourage better coordination between thresholding and label-instance relevance scoring, we introduce the Logit-adapting mechanism to MCT that automatically adapts thresholds to different score densities.   Experiments on two datasets show that our methods significantly outperform strong baselines.  Our contributions are summarized as follows:   We explore the few-shot multi-label problem in intent detection of task-oriented dialogue, which is also an early attempt for the few-shot multi-label classification.   We propose a Meta Calibrated Threshold mechanism with Kernel Regression and Logits Adapting that estimates threshold using both prior domain experience and new domain knowledge.  We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance scoring.    %% ============== version 4.0 ================= %Intent detection, a fundamental component of task-oriented dialogue system , is increasingly raising attention as a Multi-Label Classification  problem , since a single utterance often carries multiple user intents .  %In real-world scenarios, intent detection often suffers from lack of training data, because dialogue tasks/domains change rapidly and new domains usually contain only a few data examples.  %Recent success of Few-Shot Learning  presents a promising solution for such data scarcity challenges.  %It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience. %% from old domains.  % %%For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy to convert the multi-class classification into binary-class classifications .  %State-of-the-art works for multi-label intent detection focus on threshold-based strategy, where a common practice is estimating label-instance relevance scores and picking the intent labels with score higher than a threshold value .  %Usually, the coordination and respective quality of the two modules, i.e. thresholding and relevance scoring, are crucial to the performance of MLC models.  %However, in few-shot scenarios, such multi-label setting poses unique challenges for both threshold estimation and label-instance relevance scoring. % %For thresholding, previous works explore to tune a fixed threshold  or to learn thresholds from data . %But, these thresholds work well only when learning examples are sufficient.  %In few-shot scenarios, it is pretty hard to determine appropriate thresholds without overfitting. %% to the limited examples. %%For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %Besides, it is also difficult to directly transfer the pre-learned thresholds due to the domain differences, such as differences in label number per instance, score density and scale. % % % %It is also challenging to compute the label-instance relevance scores.  %Few-shot learning has achieved impressive progress with similarity-based methods  , where the relevance scores can be modeled as label-instance similarities.  %And the label representations can be obtained from corresponding support examples.  %Unfortunately, despite huge success in previous single-label tasks, these similarity-based methods become impractical for multi-label problems.  %When instances have multiple labels, representations of different labels may be obtained from the same support examples and become confused with each other. %For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation,  %%Such confused label representations  %which makes it impossible to predict correct labels with similarity scores.  %%In such situations, vanilla similarities will assign query x equal score to query\_time and query\_loc % %In this paper, we study the few-shot learning problem of multi-label intent detection and propose a novel framework to tackle the challenges from both thresholding and label-instance relevance scoring. % %To solve the thresholding difficulties of prior-knowledge transferring and overfitting, we propose a Meta Calibrated Threshold  mechanism that first learns universal thresholding experience on data-rich domains, then adapts the thresholds to certain few-shot domains with a Kernel Regression based calibration.  %Here, as a non-parametric learning method, Kernel Regression allows to avoid overfitting by calibrating the thresholds without finetuning. % %To tackle the challenge of confused label representation in relevance scoring, we propose the Anchored Label Representation  to obtain well-separated label representations. %Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represents each label with both support examples and corresponding anchors.  %Different from the previous single-label intent detection that uses label embedding as additional features , our label embeddings here have unique effects of separating different labels in metric space. % %Finally, to encourage better coordination between thresholding and label-instance relevance scoring, we introduce the logit-adapting mechanism to MCT that automatically adapts thresholds to different score densities.  % %Experiments on two datasets show that our methods significantly outperform strong baselines.  %Our contributions are summarized as follows:  % We explore the few-shot multi-label problem in intent detection of task-oriented dialogue, which is also an early attempt for the few-shot multi-label classification.  % We propose a Meta Calibrated Threshold mechanism with Kernel Regression and Logits Adapting that estimates threshold using both prior domain experience and new domain knowledge. % We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance scoring.    %% ============== version 3.0 EMNLP version ================= % %Intent detection  is a fundamental component for task-oriented dialogue system . %In real-word scenarios, intent detection often suffers from rapid changing of domains, because the new domains are usually lacking in data and may contain only a few data examples.  %Few-Shot Learning  is a promising solution to this problem.  %It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience from old domains.  % %In addition to data scarcity problem, intent detection also faces the problem of multi-label prediction. %As shown in Fig , a single utterance may carry multiple user intents.  %For this consideration, intent detection needs to be formulated as a Multi-Label Classification  problem , where a common practice is estimating label-instance relevance scores and picking the labels with score higher than a threshold value . % %Usually, the threshold is crucial to the performance of MLC models. %For multi-label intent detection, previous works explore to tune a fixed threshold  or to learn thresholds from data .  %However, these thresholds work well only when learning examples are sufficient.  %For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %Also, it is difficult to directly transfer the threshold learned in data-rich domains due to the domain differences, such as differences in label number per instance, score density and scale. % % % %It is also challenging to compute the label-instance relevance scores for few-shot MLC.  %Previous few-shot research mainly focuses on single label classification and has achieved impressive progress with similarity-based methods  .  %Generally, these methods first obtain per class representations from a few examples , and then classify an  instance according to its similarity with the representation of each class. %However, such similarity scores rely on well-separated class  representations, which poses unique challenges in multi-label settings. %When instances have multiple labels, representations of different labels may be obtained from same support examples and become confused with each other. %For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation.  % %In this paper, we study the few-shot learning problem of multi-label intent detection . %As mentioned above, it is difficult to estimate and transfer thresholds for few-shot MLC. %To solve this, we first learn universal thresholding experience on data-rich domains, and exploit the experience to estimate appropriate thresholds for unseen few-shot domains. %Specifically, we propose Meta Calibrated Threshold , which first learns a domain-general meta threshold, and then learns to calibrate it to fit specific domains with Kernel-Regression.  %To further encourage threshold generalization, we introduce the logit-adapting mechanism that automatically adapts meta thresholds to different score densities.  % %For computing label-instance score of few-shot MLC, we propose the Anchored Label Representation  to obtain well-separated label representations. %Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represent each label with both support examples and corresponding anchors.  % %Experiments on two datasets show that our methods significantly outperform strong baselines.  %Our contributions are summarized as follows:  % We explore the few-shot multi-label problem in intent detection of task-oriented dialogue,  %which is also an early attempt for few-shot multi-label classification.  % We propose a Meta Calibrated Threshold mechanism that estimate threshold using both prior domain experience and new domain knowledge. % We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance score calculation.    
"," % ========== Version 6.0 ============= In this paper, we study the few-shot multi-label classification for user intent detection.  For multi-label intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a calibration based on non-parametric learning. %on metric learning. %, that does not require fine tuning to avoid overfitting. %Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refines representations of different classes to be well-separated from each other. Experiments on two datasets show that the proposed model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Data and code are available at \url{https://github.com/AtmaHou/FewShotMultiLabel}}   %% ========== Version 5.0 ============= %In this paper, we study the few-shot multi-label classification for user intent detection.  %For multi-label intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a calibration based on Kernel Regression, that does not require fine tuning to avoid overfitting. %%Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refines representations of different classes to be well-separated from each other. %Experiments on two datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Data and code are available at \url{https://anonymous.com}}  %% ========== Version 5.0 ============= %In this paper, we study the few-shot multi-label classification for user intent detection.  %For multi-intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a Kernel Regression based calibration.  %Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refine representations of different classes to be well-separated from each other. %Experiments on two datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Code is available at \url{https://anonymous.com}}  %% ========= version 4.0 EMNLP version ========= %In this paper, we study the few-shot multi-label classification for user intent detection.  %Multi-label classification usually estimates label-instance relevance scores and uses a threshold to select multiple associated labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then calibrate the learned universal thresholds to fit certain few-shot domains. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refine representations of different classes to be well-separated from each other. %Experiments on both open and in-house datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Code is available at: \url{https://anonymous.com}}",105
"  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .          % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Translation into languages with grammatical gender involves correctly inferring the grammatical gender of all entities in a sentence. In some languages this grammatical gender is dependent on the social gender of human referents. For example, in the Spanish translation of the sentence `This is the doctor',  `the doctor' would be either  `el m鑼卍ico', masculine, or `la m鑼卍ica', feminine. Since the noun refers to a person the grammatical gender inflection should be correct for a given referent.   In practice many NMT models struggle at generating such inflections correctly , often instead defaulting to gender-based social stereotypes  or masculine language . For example, an NMT model might always translate `This is the doctor' into a sentence with a masculine inflected noun: `Este es el m鑼卍ico'.     Such behaviour can be viewed as translations  exhibiting gender bias. By `bias' we follow the definition from  of behaviour which `systematically and unfairly  discriminate[s]  against certain individuals or groups of individuals in favor of others.' Specifically, translation performance favors referents fitting into groups corresponding to social stereotypes, such as male doctors.   Such systems propagate the representational harm of erasure to referents -- for example, a non-male doctor would be incorrectly gendered by the above example translation. Systems may also cause allocational harms if the incorrect translations are used as inputs to other systems . System users also experience representational harms via the reinforcement of stereotypes associating occupations with a particular gender . Even if they are not the referent, the user may not wish for their words to be translated in such a way that they  appear to endorse social stereotypes. Users will also experience a lower quality of service in receiving grammatically incorrect translations.   A common approach to this broad problem in NMT is the use of gender features, implicit or explicit. The gender of one or more words in a test sentence  is determined from external context  or by reliance on `gender signals' from words in the source sentence such as gendered pronouns. That information can then be used when translating. Such approaches combine two distinct tasks: identifying the gender inflection feature, and then applying it to translate words in the source sentence. These feature-based approaches make the unstated assumption that if we  correctly identify that, e.g., the doctor in the above example should be female, we could inflect entities in the sentence correctly, reducing the effect of gender bias.   Our contribution is an exploration of this assumption. We propose a scheme for incorporating an explicit gender inflection tag into NMT, particularly for translating coreference sentences . Experimenting with translation from English to Spanish and English to German, we find that simple existing approaches overgeneralize from a gender signal, incorrectly using the same inflection for every entity in the sentence. We show that a tagged-coreference adaptation approach is effective for combatting this behaviour.  Although we only work with English source sentences to extend prior work, we note that our approach can be extended to source languages without inherent gender signals like gendered pronouns, unlike approaches that rely on those signals.  Intuitively, if gender tagging does not perform well when it can use the label determined by human coreference resolution, it will be even less useful when a gender label must be automatically inferred.  Conversely, gender tagging that is effective in this scenario may be beneficial when the user can specify the gendered language to use for the referent, such as Google Translate's translation inflection selection , or for translations where the grammatical gender to use for  all human referents is known.  We also find that our approach works well  with RoBERTa-based gender tagging for English test sentences.    Existing work in NMT gender bias has focused on the translation of sentences based on binary gender signals, such as exclusively male or female personal pronouns. This excludes and erases those who do not use binary gendered language, including but not limited to non-binary individuals . As part of this work we therefore explore applying tagging to indicate gender-neutral referents, and produce a WinoMT set to assess translation of coreference sentences with gender-neutral entities.        Variations on a gender tag or signal for machine translation have been proposed in several forms.  incorporate a `speaker gender' tag into training data, allowing gender to be conveyed at the sentence level. However, this does not allow more fine-grained control, for example if there is more than one referent in a sentence. Similar approaches from   and   infer and use gender information from discourse context.  also incorporate a single explicit gender feature for each sentence at inference.    integrate coreference links into machine translation reranking to improve pronoun translation with cross-sentence context.  propose NMT gender bias reduction by `mixing signals' with the addition of pro-stereotypical adjectives. Also related to our work is the very recent approach of , who train their NMT models from scratch with all source language words annotated with target language grammatical gender.  In  we treat gender bias as a domain adaptation problem by adapting to a small set of synthetic sentences with equal numbers of entities using masculine and feminine inflections. We also interpret this as a gender `tagging' approach, since the gendered terms in the synthetic dataset give a strong signal to the model. In this work we extend the synthetic datasets from this work to explore this effect further.  Other approaches to reducing gender bias effects involve adjusting the word embeddings either directly  or by training with counterfactual data augmentation  . We view these approaches as orthogonal to our proposed scheme: they have similar goals but do not directly control inference-time gender inflection at the word or sentence level.   
"," Neural Machine Translation  has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level.    In this paper we propose schemes for incorporating explicit word-level gender inflection tags into NMT. We explore the potential of this gender-inflection controlled translation when the gender feature can be determined from a human reference, or when a test sentence can be automatically gender-tagged, assessing on English-to-Spanish and English-to-German translation.  We find that simple existing approaches can over-generalize a gender-feature to multiple entities in a sentence, and suggest effective alternatives in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender-neutral entities from English given a corresponding linguistic convention, such as a non-binary inflection, in the target language.",106
"    Self-supervised pretraining through language modeling on massive datasets has revolutionized NLP. One reason this method works is that pretraining shapes a model's hypothesis space, giving it inductive biases that help it learn linguistic tasks . Numerous probing studies have provided support for this idea by showing that language models learn representations that encode linguistic features .   However, feature learning is just the first step to acquiring helpful inductive biases. Models must also be able to learn which features matter. The NLU datasets these models are often fine-tuned on are ambiguous and contain artifacts, and often support multiple possible generalizations. Neural networks are not mind readers: Models that have been shown to represent linguistic features sometimes fail to use them during fine-tuning on NLU tasks, instead adopting shallow surface generalizations . To this end, recent work in probing pretrained models advocates for shifting the focus of study away from whether they represent linguistic features and in favor of whether they learn  representations of those features .  [ht!]     {!}{%     {lllll}     \toprule         & {*}{\rotatebox[origin=c]{90}{{*}{\rotatebox[origin=c]{90}{% }             We investigate how RoBERTa  acquires language-specific inductive biases during self-supervised pretraining. We track separately how RoBERTa's representation of linguistic features and its preferences for linguistic generalizations over surface generalizations change as the amount of pretraining data increases. We pretrain RoBERTa from scratch on datasets ranging from 1M to 1B words and evaluate these models alongside RoBERTa has the strongest linguistic bias, and requires little to no inoculating data to reliably make the linguistic generalization. In general, models with more pretraining data can generally be induced to adopt linguistic generalizations with less inoculating data. We also find a large gap between the amount of pretraining data that RoBERTa needs to learn the linguistic features necessary to generalize out-of-domain and the amount it needs to learns that it should prefer those features when generalizing. The control experiments on unambiguous data reveal that models with little pretraining do actually represent the linguistic features, but nonetheless show a strong surface bias. In other words, the main contribution of pretraining to linguistic bias learning is devoted not to extracting features, but to learning which features matter.   We conclude that helpful inductive biases can be learned through pretraining, but current models require abundant data to do so. The implications of this conclusion point in two directions: First, we can probably continue to pretrain on increasingly massive training sets to improve on the generalization and few-shot learning abilities of models like T5  and GPT-3 . Second, since models learn useful features early, there is hope that future advances could accelerate by reducing the amount of data needed to learn which features matter. To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: {\url{https://github.com/nyu-mll/msgs}}.  
","   One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to  those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS , which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning. We pretrain RoBERTa models from scratch on quantities of data ranging from 1M to 1B words and compare their performance on \dataset\ to the publicly available RoBERTa$$. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa$$ does demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",107
" %缁楊兛绔村▓纰夌窗娣団剝浼呮径姘帗閸 Existing experiments  have proven that multimodal news can significantly improve users閳 sense of satisfaction for informativeness. As one of these multimedia data forms, introducing news events with video and textual descriptions is becoming increasingly popular, and has been employed as the main form of news reporting by news media including BBC, Weibo, CNN, and Daily Mail. An illustration is shown in Figure, where the news contains a video with a cover picture and a full news article with a short textual summary. In such a case, automatically generating multimodal summaries, , which learns to summarize article and video simultaneously by conducting a dual interaction strategy in the process. Specifically, we first employ Recurrent Neural Networks  to encode text and video. Note that by the encoding RNN, the spatial and temporal dependencies between images in the video are captured.  % The features of segments and text can be constraint in the same space through L2 normalization which modifies the vector in a way that each row the sum of the squares will always be up to 1. Next, we design a dual interaction module to let the video and text fully interact with each other.  Specifically, we propose a conditional self-attention mechanism which learns local video representation under the guidance of article, and a global-attention mechanism to learn high-level representation of video-aware article and article-aware video. Last, the multimodal generator generates the textual summary and extracts the cover image based on the fusion representation from the last step. To evaluate the performance of our model, we collect the first large-scale news article-summary dataset associated with video-cover from social media websites. Extensive experiments on this dataset show that DIMS significantly outperforms the state-of-the-art baseline methods in commonly-used metrics by a large margin.  %缁楊剙娲撳▓纰夌窗閹崵绮╟ontribution To summarize, our contributions are threefold:   $      encodes the input article and video separately;  Dual Interaction Module learns fused representation of video and article from different level;  Multi-Generator generates the textual summary and chooses the video cover simultaneously.     }          
"," % Multimodal summarization has drawn much attention due to the rapid growth of multimedia data.  A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively. Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output  to tackle such a problem. The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article. To this end, we propose a Dual-Interaction-based Multimodal Summarizer , consisting of a dual interaction module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset\footnote{https://github.com/yingtaomj/VMSMO} show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",108
"  Natural language processing for deception detection focus on preprocessing text into computational data with required features for the propose. As deception detection is about understanding the meaning of the text or how the text is viewed by people, the sequence of the text is always considered as one of primary source of context. For example, N-gram, the representative method of natural language processing, contains the data of a word and its subsequent word and its statistical probabilities. The attribute subsequent contains the continuous context of the text, or the linguist  will describes as linearity. In contrast, feature extractions without considering this language's linearity seems to be nonsense. However, if the data that been processed out of those non-linear feature extractions shows notable accuracy of detecting deceptions, it is possible to suggest that some of those preprocessing methods could be used as one of possible natural language processing for certain situations. \  In this paper, we discuss the effectiveness of APV, a simple natural language processing method using alphabet frequency, in the context of application on fake news detection. By using deep learning algorithm and fake news dataset in Kaggle, our findings suggest that simple deep learning algorithms using APV as pre-processing method could show prominent accuracy on predicting deception of the text. \  In section 2, we investigate conventional natural language processing that is used for machine learning and deep learning algorithms. In section 3, we define APV and its mathematical structure. We will also discuss the hypothesis that might improve feature extraction of APV.  In section 4, basic experiment protocol will be set including the structure of deep learning algorithms and performance metrics that will be used in the experiment. In section 5, we present the result of the algorithms performance. Finally, in section 6, we conclude the study.  
","     Feature extraction is an important process of machine learning and deep learning, as the process make algorithms function more efficiently, and also accurate. In natural language processing used in deception detection such as fake news detection, several ways of feature extraction in statistical aspect had been introduced . In this research, it will be shown that by using  deep learning algorithms and alphabet frequencies of the original text of a news without any information about the sequence of the alphabet can actually be used to classify fake news and trustworthy ones in high accuracy . As this pre-processing method makes the data notably compact but also include the feature that is needed for the classifier, it seems that alphabet frequencies contains some useful features for understanding complex context or meaning of the original text.\\\\  keywords: {[FEATURE EXTRACTION], [DEEP LEARNING]}  % Received, Accepted 闂嗩喚濞嬮～锟犳禃 闂夋稑瀚靛 闂夋稑鎳為悧鎾荤垷濮楀喚娼 濮ｉ潧鐗楅崝顖炵亙.",109
"  Sentence matching is a fundamental technology in natural language processing. Over the past few years, deep learning as a data-driven technique has yielded state-of-the-art results on sentence matching . However, this data-driven technique typically requires large amounts of manual annotation and brings much cost. If large labeled data can't be obtained, the advantages of deep learning will significantly diminish.  To alleviate this problem, active learning is proposed to achieve better performance with fewer labeled training instances . Instead of randomly selecting instances, active learning can measure the whole candidate instances according to some criteria, and then select more efficient instances for annotation . However, previous active learning approaches in natural language processing mainly depend on the entropy-based uncertainty criterion , and ignore the characteristics of natural language. To be more specific, if we ignore the linguistic similarity, we may select redundant instances and waste many annotation resources. Thus, how to devise linguistic criteria to measure candidate instances is an important challenge.  Recently, pre-trained language models  have been shown to be powerful for learning language representation. Accordingly, pre-trained language models may provide a reliable way to help capture language characteristics. In this paper, we devise linguistic criteria from a pre-trained language model to capture language characteristics, and then utilize these extra linguistic criteria  to enhance active learning. It is shown in Figure . Experiments on both English and Chinese sentence matching datasets demonstrate the pre-trained language model can enhance active learning.   
"," Active learning is able to significantly reduce the annotation cost for data-driven techniques. However, previous active learning approaches for natural language processing mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria to measure instances and help select more efficient instances for annotation. Experiments demonstrate our approach can achieve greater accuracy with fewer labeled training instances.",110
"  FILM  shortcoming: 1.the whole paper does not show the advantage clearly 濞岋紕澹掗崚顐ヮ嚛濞撳懏顨 閻滄澘婀悧鐟板焼閻ㄥ嫪绱崝   1.1 瀵缚鐨熻箛鐛竌st閿涙岸娓剁憰浣烘倞鐠侀缚鐦夐弰搴☆槻閺夊倸瀹崇圭偤鐛欑佃鐦拠瀛樻韫囶偓绱濋獮鍫曟付鐟曚礁鎷板ǎ鍗炲鐎涳缚绡勯弬瑙勭《鐎佃鐦敍鍧卍am閿  1.2 閻х晫娲呴敍姘舵付鐟曚礁濮為悧鐟板焼閻ㄥ嫯顕╅弰搴㈡瀮鐎涙绱濆Ο鈥崇烽弰顖滃殠閹呮畱閿涘本澧嶆禒銉ф閻  2.鐎圭偤鐛欐稉宥咁檮 experiment is not enough for text matching  2.1 add dataset閿涙瓛ahoo!   2.2閸旂姴顕В鏃撶窗other deep models, one experiment aims at proving our model is fastest, the other experiment aims at showing metric leanring閼宠棄鐫嶉悳鏉垮毉閺夘櫨閸掓繂顫愰惃鍒ature閺勭姴鐨犻崚鎵畱閺勵垯绮堟稊鍫濇勾閺傜櫢绱   閸滃本澹樼悰銊с仛閻ㄥ嫭鏌熷▔鏇烆嚠濮ｆ棑绱皌ext matching閸旂姵鏆熼幑顕娉﹂妴   瀵板牆顦縠nd2end閻ㄥ嫭鏌熷▔鏇熷鐞涖劎銇氶敍宀骞囬崷銊︽ЦKNN閸嬫矮鎹㈤崝  濮ｅ繋绔存稉鐚夐敍瀛婚惃鍕冪粈鐑樺閸掗绨  鎼存棁顕氶弰顖涙箒娑擃亝婵囧厒閿涙艾鎻╅妴浣稿讲鐟欙綁鍣撮敍  鐟欙絽鍠呴弬瑙勭《閿涙碍鍨滄禒顒鍟块惈matching娴犺濮熼弶銉嚛閿涘瞼娴夋导鍏佳勬Ц娴犳稊鍫礉閺勭柨閻ㄥ嫪缍嗙紒纾嬨冪粈鐚寸礉閸欘垯浜掗弰顖炴姜鐢摜鐣濋崡鏇礉metric learning閿 娴犮儱绶氶張澶婁粵閻ㄥ嫸绱濈粻妤绶辫箛..閿涘牐顩︾捄鐕傜礆    閸︺劌鎮楅棃銏ゆ珟娴滃棗鐤勬宀勫劥閸掑棗鐣崰鍕剁窗1閵嗕椒琚辨稉顏呮殶閹诡噣娉﹂妴浣疯⒈娑擃亝鏆熼幑顕娉︾圭偤鐛欑紒鎾寸亯閺勵垰鐣弫瀵告畱  2閵嗕礁宸辩拫鍐ㄦ彥閻ㄥ嫬鐤勬  3閵嗕礁褰茬憴锝夊櫞閻ㄥ嫬鐤勬宀嬬窗metric leanring閼宠棄鐫嶉悳鏉垮毉閺夈儲妲х亸鍕煂閻ㄥ嫭妲告禒娑斿牆婀撮弬鐧哥礉x閸掓繂顫愰惃鍒ature  缁炬寧褏娈慹mbed閸戠儤娼甸惃鍒岀拫浣告嫲鐠嬩焦娲块惄绋垮彠閺勵垰褰叉禒銉ョ潔閻滄壆娈戦敍灞芥躬娑撴禍娑楃伐鐎涙劒绗傞弰顖氬讲娴犮儳鎮婄憴锝囨畱  Y缁炬寧褏绮嶉崥鍫濐劅閸戠儤娼甸惃鍕Ц閸濐亙绨虹拠宥囩矋閹存劗娈慟A閻ㄥ嫯銆冪粈鐚寸礉娑撹桨绮堟稊鍧tch閸︺劋绔寸挧閿嬫Ц閸ョ姳璐熼崫顏冪昂鐠囧稄绱濋敍鍫濈潔閻滄澘鍤璍閿涘鐦俊鍌炵彯娴滎喚娈戦弬鐟扮础閺勵垰褰叉禒銉ф倞鐟欙絿娈戦敍宀冨奔绗夐弰顖滄暏deep learn瀹歌尙绮￠惇瀣╃瑝閸戠儤娼  缁犻崡鏇炲讲鐟欙綁鍣撮幀顪欶IDF  缂傝櫣鍋ｉ敍 1.motivation娑撳秵妲戠涵顕嗙礉contribution閸︺劌鎽 2.娑撳秴鐣弫  閸撳秹娼伴弰搴ｂ橀妴浣告倵闂堛垹鐣弫娣胶鐣诲▔鏇氱瑝閺勵垱褰侀惃    %text matching閺堝绶㈡径姝瀍ep learning閺傝纭堕敍灞炬櫏閺嬫粈绗夐柨娆欑礉娴ｅ棙妲搁崣顖澬掗柌濠傛▕閿涘奔绗栭柅鐔峰娑撳秹鐝  The neural networks represent two sentences individually to a dense vector in the same embedding space, and then define different functions to calculate the matching degree of the two-sentence vectors. However, they are getting extremely time-consuming as the networks are becoming more sophisticated and introducing more parameters. Even worse, it is still a black box for researchers and practitioners, and in urgent need of interpretability. We can't figure out what's the specific meaning of the representation obtained from neural networks, which is unaccountable or challenging to comprehend and will lead to an untrusty and irresponsible result.   %閹存垳婊戠亸杈ㄥ厒閹靛彞绔存稉顏勫嫉韫囶偄寮垫總鍊熜掗柌濠忕礉娴犲簼浜掗崜宥囨畱deep learning鐠囦焦妲戞禍鍡楊劅閺傚洦婀伴惃鍕秵缂佺銆冪粈鐑樻Ц闂堢姾姘ㄩ惃鍕剁礉閹垫禒顧砮tric learn閸掓艾銈界亸杈ㄦЦ鏉╂瑦鐗遍敍灞界穿閸忣櫝etric learning閿涘奔绗栭敍鍫滀簰瀵伴弰顖涘簼绠為悽鈺〆tric learning閿  To tackle these, we aim to find a fast and interpretable approach for sentence matching. There are several studies focused on learning low-dimensional representations of the data, which called metric learning and even some of them combine it with some similarity metrics for ranking tasks .  Moreover, some researchers apply metric learning principles to design the loss function in information retrieval and question-answering tasks. But for the deep metric learning that they utilized, the neural network part still demands a lot of time. It hardly runs on a memory-limited device, together with high energy consumption.  %閹存垳婊戝銉ょ稊閺勵垰婀猼ext matching娑撳﹥褰佹稉娑擃亜鎻╅柅鐔烘畱閺傝纭堕妴鍌樺倶鍌涘娴狀櫑pply閵嗗倶鍌  It is considering the unexplainable implications brought from neural networks, such as fairness or transparency, and the challenge of time-consuming. In this paper, we apply metric learning approaches to address the problems mentioned above. Because metric learning has an advantage in time and memory usage on large-scale and high-dimensional datasets compared with methods above. Here, metric learning finds a representation of the data that preserves these constraints that are placed by human-provided labels. Building on its success in learning ``label constraint preserving'' representations, or , we explore two Fast, Interpretable, and Low-rank Metric learning approaches, what we called FILM.   %閻鍩岄弫鍫熺亯metric learning閼宠棄鐤勯悳鎵娴艰偐娈戠紒鎾寸亯閿涘奔绗栬箛顐︾喍绗栭崣顖澬掗柌濠冄嶇窗缁炬寧褏娈   Notably, we explore FILM methods on text matching tasks, which is also known as the semantic equivalence problem in the IR community~. To be more specific, one based on an interpretable low-rank manifold optimization method. To solve this optimization problem,  we apply the Cayley transformation method with the Barzilai-Borwein step size. After being trained for this task, both are added to the kNN index for prediction for efficient retrieval. The input question is encoded and used as a query to the index, returning the top k most similar questions. We test our approaches on data from the Quora Challenge and SemEval-2017 Semantic Textual Similarity  Task, which provide pairwise sentence similarity labels.   %\footnote{}   %Our motivation is to investigate whether FILM approaches can perform as well as, if not better than, some ``black box'' approaches that are so popular these days.    Our contributions are as follows:  ---that relies on an interpretable linear model. Due to space constraints, we will focus on our main approach.   The rest of this paper is organized as follows. In Section , we provide a quick overview of metric learning. In Section  we present the interpretable FILM method. In Section , we summarize the Quora dataset and task, explain how FILM is applied to the task, and summarize our deep neural network approach. In Section  we report some results.    
"," Detection of semantic similarity plays a vital role in sentence matching. It requires to learn discriminative representations of natural language. Recently, owing to more and more sophisticated model architecture, impressive progress has been made, along with a time-consuming training process and not-interpretable inference. % In sentence matching and semantic analysis, detecting semantic similarity is a challenge that requires learning discriminative representations of natural language. Recent advances in the deep neural network enable us to learn semantic representation, but are getting time-consuming and fail in interpretation. To alleviate this problem, we explore a metric learning approach, named FILM  to efficiently find a high discriminative projection of the high-dimensional data. We construct this metric learning problem as a manifold optimization problem, and solve it with the Cayley transformation method with Barzilai-Borwein step size. % To alleviate this problem, in this paper we construct sentence matching as a manifold optimization problem that learns a distance function between sentences. % % and obtain the semantic representation by learning a similarity or distance function. % We explore a metric learning approach, named FILM  to efficiently find a high discriminative projection of the high-dimensional data. % that still preserves high discriminative power. % To this end, our manifold optimization method is solved by the Cayley transformation method with Barzilai-Borwein step size.  In experiments, we apply FILM with triplet loss minimization objective to the Quora Challenge and Semantic Textual Similarity  Task. The results demonstrate that the FILM method achieves a superior performance as well as the fastest computation speed, which is consistent with our theoretical analysis of time complexity.",111
" %A common situation for language learners is to encounter unrecognized words. %In this case, looking up the dictionary may be the preferred solution for many people. %However, the capacity of dictionaries is limited, and they may not contain new words or new meanings of words. %What's more, not all language pairs have dictionaries, especially those with low resources. %Therefore, it may be a good idea to directly generate definitions for words.  The definition modeling task proposed by  is to generate a dictionary definition of a specific word. This task can prove useful for language learners, such as provide reading help by giving definitions for words in the text. However, definition modeling can only work for a specific language, which puts high demands on users because it requires them to read definitions written in this language. Besides, many low-resource languages lack large-scale dictionary data, making it difficult to train definition generation models for these languages. %This task can prove useful for language learners, such as provide reading help by giving definitions for words in the text. %However, definition modeling can only work for a specific language, which puts high demands on users because it requires them to read definitions written in this language.  Therefore, we emphasize the necessity of generating definitions cross-lingually, which can generate definitions for various language inputs, as illustrated in figure . Since English is widely used around the world, and English dictionary resources are relatively easy to obtain, we choose to generate definitions in English. In this way, a cross-lingual model trained on English can be directly applied to other languages.  The challenging issue is how to effectively transfer the knowledge of definition generation learned in English to other languages. To solve this problem, we propose to employ cross-lingual pretrained language models  as encoders. These models have shown to be able to encode sequences of various languages, which enables the ability of cross-lingual transfer . %In this work, we emphasize the necessity of generating definitions cross-lingually, which requires the model to generate definitions with just one language for words in various languages as illustrated in figure . %Considering English is widely used around the world, and English dictionary resources are relatively easy to obtain, we choose to use English to generate definitions for other languages in this work.  %Recently, cross-lingual pretrained language models  have shown to be capable of encoding sequences of different languages into the same vector space, which enables the ability of cross-lingual transfer. %Therefore, we propose to employ them as encoders for cross-lingual definition generation. %After training and fine-tuning the model on English dataset, we directly apply the obtained model to generate definitions for other languages.    To verify our proposed method, we build an English dataset for model training and a Chinese dataset for zero-shot cross-lingual evaluation. %We collected English words, example sentences and definitions in the OALD as the English dataset, and collected Chinese words, example sentences and English definitions in the Chinese WordNet   as the Chinese dataset. Experiments and manual analyses on the constructed datasets show that our proposed models have good cross-lingual transfer ability. Compared with the reference definitions in the CWN dataset, although the generated definitions are still insufficient on the accuracy, their fluency is already good enough.  Furthermore, considering the generated definitions are provided for language learners, and many of them are non-English native speakers, we argue that the difficulty of definitions should be under control. We control the lexical complexity of generated definitions by limiting definitions in the training set to the Oxford 3000 vocabulary, which is a list of important and useful words that are carefully selected by language experts and experienced teachers . %These words have been used to write definitions in the Oxford Advanced Learner's Dictionary  , in order to make them easy to understand. %We compute the Type/Token Ratio  as a measure of lexical complexity. %The TTR of generated definitions  is much lower than that of reference definitions , which indicates a lower lexical complexity. We compute four different metrics to measure the lexical complexity. Definitions generated by our models outperform the reference definitions on all four metrics by a large margin. The result shows that our method can generate simpler definitions, which is suitable for language learners.  
"," Generating dictionary definitions automatically can prove useful for language learners. However, it's still a challenging task of cross-lingual definition generation. In this work, we propose to generate definitions in English for words in various languages. To achieve this, we present a simple yet effective approach based on publicly available pretrained language models. In this approach, models can be directly applied to other languages after trained on the English dataset. We demonstrate the effectiveness of this approach on zero-shot definition generation. Experiments and manual analyses on newly constructed datasets show that our models have a strong cross-lingual transfer ability and can generate fluent English definitions for Chinese words. We further measure the lexical complexity of generated and reference definitions. The results show that the generated definitions are much simpler, which is more suitable for language learners. %We further conduct a manual analysis of the generated Chinese definitions and find that although these definitions are insufficient on the accuracy, they are already good enough on fluency and lexical complexity.",112
"  The CoNLL 2020 MRP Shared Task  combines five frameworks for graph-based meaning representation: EDS, PTG, UCCA, AMR and DRG. It further includes evaluations in English, Czech, German and Chinese. While EDS, UCCA and AMR participated in the 2019 MRP shared task , which focused only on English, PTG and DRG are newly-added frameworks to the MRP uniform format.  For this shared task, we extended TUPA , which was adapted as the baseline system in the 2019 MRP shared task , to support the two new frameworks and the different languages. In order to add this support, only minimal changes were needed, demonstrating TUPA's strength in parsing a wide array of representations.  TUPA is a general transition-based parser for directed acyclic graphs , originally designed for parsing UCCA . It was previously used as the baseline system in SemEval 2019 Task 1 , and generalized to support other frameworks .  We also experimented with the HIT-SCIR parser . This was the parser with the highest average score across frameworks in the 2019 MRP shared task, and has also since been applied to other frameworks  .  [ht] 	{width=.99\textwidth,margin=1pt,frame} 	{llll|l|lllll} 		\multicolumn{4}{c|}{{*{{c{ & 	extbf{ootnotesize Buffer &  		ootnotesize N. & ootnotesize Edges & &  		ootnotesize Stack & ootnotesize Buffer &  		ootnotesize Nodes & ootnotesize Edges &  		ootnotesize Extra Effect \\  &  &  &  &  & \\ 		 &  &  &  & Reduce &  &  &  &  & \\ 		 &  &  &  & Node &  &  &  &  & S\;|\;xy\;|\;BVE\;|\; &  &  &  &  & S\;|\;xBVEp\leftarrow pS\;|\;y,xBVE_XS\;|\;y,xBVE\;|\; &  &  &  &  & SBVE\;|\;a\leftarrow aS\;|\;x,yBVES\;|\;yx\;|\;BVE[\mathrm{root}] & xx requires that yyx\mathrm{i}<\mathrm{i}\mathrm{i}  [ht]      [ht]    edge, and virtual terminal nodes corresponding to text tokens,   attached according to the anchoring   with Anchor edges.   Same as for all frameworks with node labels and properties ,   labels and properties are replaced with placeholders corresponding to anchored tokens,   where possible.   The placeholder $\langle   
","   This paper describes the HUJI-KU system submission to the shared task   on Cross-Framework Meaning Representation Parsing  at the 2020   Conference for Computational Language Learning ,   employing TUPA and the HIT-SCIR parser, which were, respectively,   the baseline system and winning system in the 2019 MRP shared task.   Both are transition-based parsers using BERT contextualized embeddings.   We generalized TUPA to support the newly-added MRP frameworks and languages,   and experimented with multitask learning with the HIT-SCIR parser.   We reached 4th place in both the cross-framework and cross-lingual tracks.",113
"  Despite   popularity and  effectiveness, little is known about its inner workings. Several attempts have been made to demystify certain aspects of  argue that attention measures the importance of a particular word when computing the next level representation for this word. However,  showed that most attention heads contain trivial linguistic information and follow a vertical pattern (attention to           
","     Although \bert is widely used by the \nlp community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of \bert, often with contradicting conclusions. A much raised concern focuses on \bert's over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune \bert in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification  where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific \bert layers to predict labels from specific hierarchy levels. Experimenting with two \lmtc datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization.",114
" Training open-domain dialog models is inherently difficult, since for each utterance there are many acceptable responses, yet no perfect response. While supervised learning from conversational corpora allows models to learn grammatical structure and even topic coherence, these models do not generalize, since the training objectives mostly lead the models to memorize responses within the corpus.  Humans are the ultimate authority in evaluating what makes one conversational reply better than another. To learn from real conversations with humans, we created an interactive, online platform which hosted a diverse set of neural network dialog models that users could chat with in real time. However, when learning from human interactions in the wild it is crucial to be able to learn offline and test the policy before deploying it, lest it learn inappropriate behaviors . Thus, we need to train and test models offline, to ensure safe model outputs. In order to safely learn to optimize human feedback we pursued an offline reinforcement learning approach to training dialog models .   Offline RL is challenging; most deep RL algorithms fail to learn from data that is not heavily correlated with the current policy . Even models based on off-policy algorithms like -learning fail to learn in the offline RL setting, as the model is not able to explore. If the offline dataset is not sufficient to cover the input-response space, offline RL models suffer from extrapolation error, learning arbitrarily bad estimates of the value of responses not contained in the data.   We solve these problems by developing a new method for offline RL.  The method starts by leveraging a pre-trained language model to constrain offline RL updates. While training with RL, we penalize divergence from this prior model using forms of KL-control. This combats extrapolation error, and ensures that the RL model learns a policy that stays close to the distribution of realistic language, while learning to maximize positive human responses using the offline data. Further, we use dropout to obtain uncertainty estimates of the target -values, and to obtain a lower bound to alleviate over-optimistic bias in estimating future reward. We show that this new method is able to learn successfully from many different reward functions, even in a very large space with 20,000 tokens.  Both linguistic theory  and empirical experiments correlating human judgement with language features suggest that there are many criteria that could be used to evaluate a conversational agent  . We develop a set of reward functions for our dialog agents to optimize, which are designed to approximate implicit human preferences expressed during conversational responses. We show that the new method is better able to optimize these rewards using the offline data, and when tested with a new set of 80 human conversation partners, leads to more positive responses and higher quality ratings than a state-of-the-art offline deep RL method.  Novel contributions of this paper are:           
"," How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning . We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions.  A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions.  We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.  We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.",115
"  Deep neural network-based  models have demonstrated remarkable performance on a multitude of text-to-text  as well as data-to-text generation tasks .  % To reach high performance, DNN models require a large training corpus which is normally not readily available. Indeed, it is rare to have a sufficiently large human-curated corpus of parallel data , and researchers have come up with heuristic rules to mine input-output pairs on a large scale .  No matter how powerful, DNN models are known to be sensitive to data artifacts  and pick on the noise in the training data.    While hallucinations have not been defined formally, the term is standardly used to refer to the generated content which is either unfaithful to the input, or nonsensical . In our work we are concerned with the former hallucination kind which is primarily caused by imperfect quality of the training data. %  If the data are noisy, how can one reduce the chances of hallucinating? % One may try to improve the quality of a dataset and clean it from phrases for which a clear support in the input is missing, or augment the input with information found only in the output. The former path is risky as it easily results in ungrammatical targets. The latter approach of enforcing a stronger alignment between inputs and outputs has been tried previously but it assumes a moderate amount of noise in the data .  % Alternatively, one can leave the data as is and try to put more pressure on the decoder to pay attention to the input at every generation step . This requires significant modifications to the model and may make it harder for the decoder to generate fluent and diverse text as found in the targets.   In contrast to the described approaches, our proposal is to train the model on the data as is without modifying the decoding  architecture but instead introduce a handle on the input side to control the degree of hallucination . With this ""hallucination knob"" one can minimize  the amount of unsupported information in the output during generation . The hallucination or noise degree of every training instance is estimated separately and converted into a categorical value which becomes part of the input, like in a controlled generation setting . We introduce a simple technique to measure the amount of noise in every training example which is based on the intuition that whenever a language model  has a smaller loss than a conditional generator during forced-path decoding, it is a good signal that the next token cannot be explained by the input. % .  We consider a particularly noisy dataset, WikiBio , which has been found to have extra information in 62\% of the references  and where 1:1 correspondence between the input and the output never holds . Our models demonstrate superior performance to the model of  which reports SoTA BLEU results on WikiBio.  % In sum, our contributions are  a novel idea of controlling hallucinations which requires no modification to the model,  a data- and task-independent technique of implementing this idea and  three-way evaluation with human raters which confirms that faithfulness does not need to be traded for coverage.     
"," Neural text generation  demonstrates remarkable performance when training data is abundant which for many applications is not the case.  To collect a large corpus of parallel data, heuristic rules are often used but they inevitably let noise into the data, such as phrases in the output which cannot be explained by the input.  Consequently, models pick up on the noise and may hallucinate--generate fluent but unsupported text.  Our contribution is a simple but powerful technique to treat such hallucinations as a controllable aspect of the generated text, without dismissing any input and without modifying the model architecture. On the WikiBio corpus , a particularly noisy dataset, we demonstrate the efficacy of the technique both in an automatic and in a human evaluation.",116
"   %  %    %Added value of \atomicTT{}: 1) diversity in terms of vocab, style, concepts, 2) higher quality   %\ronan{Cite publications that used ATOMIC in a downstream application}  Commonsense understanding % knowledge modeling and reasoning remain long-standing challenges in general artificial intelligence.  % However, in the subfield of natural language processing, the last few years have brought tremendous progress in AI applications.  However, large-scale language models have brought tremendous progress in the sub-field of natural language processing.  Such large-scale language models   trained on extreme-scale data have been shown to effectively adapt to diverse downstream tasks, achieving significant performance gains across natural language benchmarks .  %%%%%%%OLD %%%%%% Despite these successes, these models have been shown to learn brittle representations, often from only simple surface word associations , which routinely lead them to make nonsensical predictions detached from common sense . Interestingly, as these models have grown larger , their benchmark performance has continued to improve  despite limited conceptual improvements,  %leading many researchers to conjecture as to  leaving open questions regarding  the source of these remarkable generalization properties.   Recent work has hypothesized that many of these performance gains could be a result of language models being able to memorize facts in their parameters during training  that can be leveraged at evaluation time. As a result, a new paradigm of language models as knowledge bases has emerged . In this setting, language models are prompted with natural language prefixes or questions, and they express knowledge through language generation. The initial success of this paradigm for representing commonsense knowledge  %, combined with limited examples of LMs being successfully integrated with structured commonsense knowledge resources for downstream application,  has led to the optimistic claim that language models comprehensively encode commonsense knowledge, and remove the need for structured knowledge resources. %\antoine{run-on sentence, need to shorten}  We take a more skeptical view of this capacity of language models -- Does scaling up language models actually endow them with commonsense knowledge? While language models can successfully express certain types of knowledge, their best results are observed in narrowly specific conditions -- we show  that they perform better when evaluated on knowledge bases that prioritize ontological relations and whose examples resemble language-like assertions .\footnote{An observation supported by 's \gpttt{} model, whose best few-shot performance on commonsense knowledge benchmarks comes on the PhysicalIQA  and HellaSwag  datasets.} Consequently, the types of knowledge that can be directly accessed through the language model's interface remains limited.  %Consequently, while these methods are encouraging, they also demonstrate that the limited interface of language models precludes them from expressing the diversity of commonsense knowledge that must be accessible for robust commonsense reasoning.  %  However, prior work has also shown that training language models on knowledge graph tuples leads them to learn to express their implicit knowledge directly , allowing them to provide commonsense knowledge on-demand. These adapted knowledge models have exhibited promising results on commonsense benchmarks compared with methods that require linking entities to knowledge graphs . Inspired by these successes, we propose a dual use for commonsense knowledge bases going forward: as static graphs that can be linked to for discrete knowledge access, and as resources for adapting language models to hypothesize commonsense knowledge about un-annotated entities and events.   %%%%%%% OLD %%%%%%%% As a result, recent work has investigated augmenting language models with retrieval mechanisms that query commonsense knowledge graphs  for related facts to the entities mentioned in text. The idea behind these approaches is that access to these facts and the potential to compose them with learned reasoning functions would allow models to more robustly leverage commonsense knowledge to make predictions. Despite the premise of these approaches, they are unfortunately limited by the coverage of the resources used to provide commonsense knowledge facts , motivating the need for new, high coverage resources in the short-term.   % Option 1 % With this second purpose in mind, we shift the design goals of commonsense knowledge resources toward prioritizing pieces of knowledge that are not readily accessible in pretrained language models.  % Option 2 With this second purpose in mind, we propose evaluating commonsense knowledge resources based on the complementary information they can bring to pretrained language models. We construct \atomicTT{}, a new, high-quality knowledge graph with M commonsense knowledge tuples across  commonsense relations. We compare \atomicTT{} with respect to its coverage and accuracy in competition with other highly used CSKGs, such as  is able to cover more correct facts about more diverse types of commonsense knowledge than any existing, publicly-available commonsense knowledge resource. However, our results also indicate that there remains a large amount of exclusivity between these KGs, highlighting the challenge of creating resources that cover the scale and diversity of general commonsense knowledge.   %%%%%%% OLD %%%%%%Meanwhile, a new paradigm has emerged that proposes that large-scale language models implicitly learn to represent large amounts of factual and commonsense knowledge . While these methods are promising, they also show that the limited interface of language models precludes them from producing commonsense knowledge robustly. However, using knowledge graph tuples as additional training signal allows these model to be better adapted to representing knowledge . Furthermore, the use of these knowledge models to provide commonsense knowledge on-demand has shown promising results over static knowledge graphs . Consequently, in this work, we propose evaluating commonsense knowledge resources on a new, second purpose: whether they can be used to repurpose language models for commonsense modeling.   Furthermore, we formalize the  across different seed language models and training knowledge graphs, and evaluate the commonsense knowledge hypothesized by these adapted knowledge models. %Our results indicate that this purpose is a promising evaluation for commonsense resources, as  as a transfer resource leads to  as a transfer resource allows language models to learn richer commonsense knowledge representation than training with other resources.   %  %    Key Contributions:  In summary, we make three key contributions in this paper. We present \atomicTT{}---a new commonsense knowledge graph covering social, physical, and eventive aspects of everyday inferential knowledge . Next, we compare \atomicTT{} with other prominent CSKBs head-to-head and show that our new symbolic knowledge graph is more accurate than any current CSKB  . Finally, we show that our new neural knowledge model -\atomicTT{} successfully transfers \atomicTT{}'s declarative knowledge to beat \gpttt{}, the largest pre-trained language model, in spite of using ~400x fewer parameters  . This demonstrates the utility and importance of high-quality symbolic knowledge provided by \atomicTT{} to generalize on commonsense information that LMs cannot expressively capture on their own .  % * Our new symbolic knowledge graph ATOMICTT is superior in accuracy and coverage to the currently existing large-scale knowledge graphs .  % * our neural knowledge model COMET-ATOMICTT successfully transfers the ATOMICTT's declarative knowledge to beat even the most impressively large pretrained model, GPT-3 . This demonstrates LMs, no matter its size, can benefit from the symbolic knowledge provided by high quality KB like ATOMICTT.   
"," % Check out this new knowledge graph! % Storyline: %  %   Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs  has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge.  In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them.   With this new goal, we propose \atomicTT{}, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that \atomicTT{} is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 , while impressive, remains $ despite using  over 430x fewer parameters.  % useful they are for training knowledge models that can generate relevant representative knowledge for new, unseen entities.  % In this work, we propose \atomicTT{}, a new knowledge graph of general-purpose commonsense knowledge facts. To evaluate its utility in comparison to existing resources, we perform the first large-scale pairwise study of commonsense knowledge graphs on coverage and precision. Finally, we posit that a new use for commonsense knowledge graphs is their ability to allow large-scale language models to learn to represent knowledge implicitly. We propose a new evaluation for testing knowledge graphs on how useful they are for training knowledge models that can generate relevant representative knowledge for new, unseen entities.",117
"   Despite its successes, neural machine translation  still has unresolved problems. Among them is the problem of rare words, which are paradoxically very common because of Zipf's Law. In part, this is a problem intrinsic to data-driven machine translation because the system will inevitably encounter words not seen in the training data. In part, however, NMT systems seem particularly challenged by rare words, compared with older statistical models.   One reason is that NMT systems have a fixed-size vocabulary, typically 10k--100k words; words outside this vocabulary are represented using a special symbol like \unk{}. Byte pair encoding  breaks rare words into smaller, more frequent subwords, at least allowing NMT to see them instead of \unk{} . But this by no means solves the problem; even with subwords, NMT seems to have difficulty learning translations of very rare words, possibly an instance of catastrophic forgetting .  Humans deal with rare words by looking them up in a dictionary, and the idea of using dictionaries to assist machine translation is extremely old. From a statistical perspective, dictionaries are a useful complement to running text because the uniform distribution of dictionary headwords can smooth out the long-tailed distribution of running text. In pre-neural statistical machine translation systems, the typical way to incorporate bilingual dictionaries is simply to include them as parallel sentences in the training data. But , this does not work well for NMT systems.  We are aware of only a few previous attempts to find better ways to incorporate bilingual dictionaries in NMT. Some methods use dictionaries to synthesize new training examples .  extend the model to encourage it to generate translations from the  dictionary.  constrain the decoder to generate translations from the dictionary. What these approaches have in common is that they all treat dictionary definitions as target-language text, when, in fact, they often have properties very different from ordinary text. For example, CEDICT defines \zh{濮濄倛鍤  as ``'' which cannot be used as a translation. In the case of a monolingual source-language dictionary, the definitions are, of course, not written in the target language at all.  In this paper, we present an extension of the Transformer  that ``attaches'' the dictionary definitions of rare words to their occurrences in source sentences. We introduce new position encodings to represent the nonlinear structure of a source sentence with its attachments. Then the unmodified translation model can learn how to make use of this attached information. We show that this additional information yields improvements in translation accuracy of up to 3.1 BLEU. Because our method does not force dictionary definitions to be treated as target-language text, it is generalizable to other kinds of information, such as monolingual source-language dictionaries, which yield smaller improvements, but still as much as 0.7 BLEU.        [3]{\mbox{{@{}c@{}}\zh{#1} \\       {%     [x=2.2cm]     \tikzset{every node/.append style={anchor=north}}     ;     \textrm{PE}[1] \\ + \\ \textrm{WE}\left[\text{\zhen{婢堆冾啀}{d鑴縥i鑶﹠{everyone}}\right]};     \textrm{PE}[3] \\ + \\ \textrm{WE}\left[\text{\zhen{閻儵浜緘{zh澧╠鑴縪}{knows}}\right]};     \textrm{PE}[5] \\ + \\ \textrm{WE}\left[\text{\zhen{濮濓絽婀獇{zh鐚玭gz鑴縤}{is}}\right]};     \textrm{PE}[4] \\ + \\ \textrm{WE}[\unk] \\ + \\ \textrm{DPE}[1] \\ + \\  \textrm{WE}\left[\text{the}\right]};     \textrm{PE}[4] \\ + \\ \textrm{WE}[\unk] \\ + \\ \textrm{DPE}[3] \\ + \\ \textrm{WE}\left[\text{Sea}\right]\textrm{WE}[f]f\textrm{PE}[p]p\textrm{DPE}[q]q$ within a dictionary definition. The rare word \zh{濮濈粯鎹  is replaced with \unk{} and defined as . The words of the definition are encoded with both the position of the defined word  and their positions within the definition.}        
"," Despite advances in neural machine translation  quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for ``attaching'' dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.",118
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. % %  % . %     %  %     % % final paper: en-us version  %     % %     %   % space normally used by the marker %     % This work is licensed under a Creative Commons  %     % Attribution 4.0 International License. %     % License details: %     % \url{http://creativecommons.org/licenses/by/4.0/}. % }   % 1. 鐟欙綁鍣 CCG閿涘奔浜掗崣 CCG 閻ㄥ嫰鍣哥憰浣 % 2. CCG parsing 閻ㄥ嫰鍣搁悙鐟版躬娴 supertagging閵嗗倿娓剁憰浣割嚠 contextual information 閺堝鐦潏鍐ㄣ偨閻 encode 閻ㄥ嫭鏌熷▔鏇樺倸澧犳禍铏规畱閺傝纭堕敍灞间簰閸欏﹤鐪梽鎰剁礄閸欘亪鍣伴悽 powerful encoder閿涘本鐥呴張澶嬪赴濮瑰倿顤傛径 contextual feature 閻ㄥ嫪缍旈悽銊ф畱閻梻鈹掗敍 % 3. n-gram 閺勵垯绔存稉顏呮箒閺佸牏娈 contextual feature閿涘苯褰查懗钘夘嚠 supertagging 閺堝鏁ら敍鍫熷絹娓氭稑褰查懗鐣屾畱鐠囧秳绗岀拠宥勭闂 combination 閻ㄥ嫭娈粈鐚寸礆 % 4. 閹存垳婊戦惃 model   % 鐟欙綁鍣 CCG閿涘矁鐦濆Ч鍥瘱閻ｈ揪绱檚upertag閿涘婀伴煬顐㈠瘶閸氼偂绨℃稉鏉跨槣閻ㄥ嫬褰炲▔鏇炴嫲鐠囶厺绠熼惃鍕繆閹 Combinatory categorial grammar  is a lexicalized grammatical formalism, where the lexical categories  of the words in a sentence provide informative syntactic and semantic knowledge for text understanding. % 閹垫禒 ccg閿涘瞼澹掗崚顐ｆЦ supertagging 瀵板牊婀侀悽 Therefore, CCG parse often provides useful information for many downstream natural language processing  tasks such as logical reasoning  and semantic parsing . To perform CCG parsing in different languages, % 閸 ccg parsing 閸掑棔琚卞銉ｄ靠upertagging 鏉╂瑤绔村銉︽付闁插秷顩 most studies conducted a supertagging-parsing pipline , in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. % which is known as ``almost parsing''   % with essential CCG information for a sentence and one can generate its parse directly from supertags with a few rules. % supertagging 闂囩憰 contextual information %  Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. %  Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models , with limited attention paid to modeling extra contextual features such as word pairs with strong relations. % Graph convolutional networks  is demonstrated to be an effective approach to model such contextual information between words in many NLP tasks ; thus we want to determine whether this approach can  also help CCG supertagging.  However, we cannot directly apply conventional GCN models to CCG supertagging because in most of the previous studies the GCN models are built over the edges in the dependency tree of an input sentence. As high-quality dependency parsers are not always available, we do not want our CCG supertaggers to rely on the existence of dependency parsers.  %  Thus, we need another way to extract useful word pairs to build GCN models. For that, we propose to obtain word pairs from frequent chunks  in the corpus, because those chunks are easy to identify with co-occurrence counts. %  %  %  % Such features, which may come from n-grams or dependency parsing results, are demonstrated to be helpful for many NLP tasks , and they are expected to enhance CCG supertagging as well. % Among all such features, the ones from n-grams are more attractive since n-grams are easy to obtain and also provide word relation cues, while dependency parsing results are exactly the goal of CCG and thus conflicts with the problem setting. % % As for the model to encode such features, graph convolutional networks  is one of the promising choices although it is often built over the dependency or semantic parse of the input text. %However, GCN suffers from the limitation of obtaining such parsing results, which is exactly the goal of CCG and thus conflicts with the problem setting. % %So that one and they are expected to enhance CCG supertagging. %especially the n-gram ones because they are easy to obtain and can provide cues for word-word combination if they are appropriately modeled. % %\textcolor{blue}{ %To leverage such contextual features, graph convolutional networks  is one of the privileging approaches to do so, where the graph is often built over the dependency or semantic parsing results of the input text. %However, GCN suffers from the limitation of obtaining such parsing results, which is exactly the goal of CCG and thus conflicts with the problem setting. %} % \textcolor{red}{ % Consider that graph convolutional networks , which is an effective solution to learn contextual information and is demonstrated to be useful in many other NLP tasks , can be potentially useful for CCG supertagging.} % , such as semantic role labeling , sentiment classification , and question answering . %input words based on the results of dependency or semantic parsing of the input texts, which may not be an appropriate way to construct graph for CCG, %since the task itself is about parsing. % \textcolor{blue}{ % Therefore, an appropriate way to construct the graph is required for CCG and n-grams could potentially be helpful since they carry contextual information and provide a group of words in which  % its containing words  % they may have strong relationship with respect to word-word combination if the n-grams are appropriately selected. % % } % Previous studies using GCN often build the graph over the dependency or semantic parsing results of the input text, suffering from the limitation of obtaining such parsing results, which is exactly the goal of CCG thus conflicts with the problem setting. % To appropriately learn from n-grams, one requires the GCN to be able to distinguish different word pairs because such information in n-grams are not explicitly structured as that in dependency parses. %In addition, Because existing GCN models are limited in treating all word pairs equally, %while identifying and learning from essential units are important for syntactic tasks, we propose an adaptation of conventional GCN for CCG supertagging. %especially when the graph are not constructed on dependencies. %  % Inspired by that n-grams can carry contextual information and provide a span in which its containing words may have strong relationships if the n-grams are appropriately selected, we build the graph upon well selected n-grams. % , especially the ones containing words with strong relationships between each other,  % % n-gram 閺勵垯绔存稉顏堝櫢鐟曚胶娈 contexutal feature % Consider that n-grams are conventionally used as a simple yet effective method to represent contextual features in many NLP tasks %in which powerful encoders are used  % , % 閸ョ姵顒濋敍瀹-gram 鐎 supertagging 娑旂喐婀侀悽顭掔礉鐏忋倕鍙鹃弰顖炲亝娴滄稖鍏樻径鐔虹矋閹存劗鐓拠顓犳畱 n-gram閿涘矁鍏樻径鐔稿絹娓氭稑鍙ф禍搴ょ槤娑撳氦鐦濇稊瀣？缂佸嫬鎮庨崗宕囬兇閻ㄥ嫪淇婇幁顖ょ礉閺堝濮禍 supertagging % they are also expected to serve as effective contextual features for CCG supertagging, where they, \textcolor{blue}{especially the ones containing words with strong relationships between each other,} % that are valid phrases,  % should provide plausible cues on potential combinations among words. % 閻掓儼宀嬬礉婵″倷缍嶉張澶嬫櫏閸︽澘鍩勯悽銊ㄧ箹娴 n-gram 娓氭繃妫弰顖欑娑擃亝瀵幋姗堢礉閸ョ姳璐熼柇锝勭昂娑撳秹鍣哥憰浣烘畱 n-gram 閸欘垯浜掔拠顖氼嚤 supertagger %\textcolor{blue}{ % However, it is not trivial to appropriately learn from n-grams for syntactic tasks, % where one needs to identify informative n-grams out of all possible combinations of words for the task. %since the unimportant ones carrying misleading cues for the combination may hurt the performance of a supertagger. %}   % [t] %      %      %     \vskip -1.2em %       % 閹垫禒銉ь儑娑撳顔岄柌宀勬桨鐏忚精顩﹂崨鐓庣安鏉╂瑩鍣烽惃鍕敶鐎圭櫢绱濈悰銊с仛閸戠儤娼甸幋鎴滄粦閺冦垼鍏橀悽鈺猤ram閿涘苯寮甸懗鐣屾暏GCN缂佹獢gram瀵ょ儤膩 % 閹存垳婊戦幓鎰毉 channeled attention 閺 model 鏉╂瑤绨 n-gram %To address these problems, In this paper, we propose attentive GCN  for CCG supertagging, where its input graph is built based on chunks  extracted with unsupervised methods. % In this paper, we propose attentive GCN  for CCG supertagging, where its input graph is built upon word groups suggested by high confident n-grams extracted from unsupervised methods. % , where the graph is constructed on word groups. %which follows the sequence labeling paradigm. % 鐠囷妇绮忔禒瀣矝婵″倷缍嶅銉ょ稊閿涘矂顩婚崗鍫濐嚠 n-gram 閸掑棛绮 % Inspired by that n-grams can carry contextual information and provide a span in which its containing words may have strong relationships if the n-grams are appropriately selected, we build the graph upon the n-grams in the sentence, where an edge will be added to a pair of words if they are in the same n-gram. In detail, two types of edges in the graph are introduced to model word relations within and across chunks %for the word groups to model the word-word relation within and cross the groups. % we build the graph over the words upon the n-grams in the input sentence, where an edge will be added to a pair of words if they are in a span suggested by the same n-gram. % % For edges within a group, feed-forward attention is applied  and an attention mechanism is applied to GCN to weight those edges. %and discriminately learn from them through the edges. %In addition, for each word, a attention mechanism is used % to weight the contextual information carried by all its associated words  according to their contribution to the tagging process. %  In doing so, different contextual information are discriminatively learned to facilitate CCG supertagging without requiring any external resources. % , with the \textcolor{blue}{within and cross chunk relations} % local and global word relations  % weighted on our in-chunk and cross-chunk edges, respectively. %Moreover, the way of building the graph requires no external resources  %suggested by high confident n-grams is learned by A-GCN through the in-group edges; and long distance relations among groups are also leveraged by cross-group edges. %Therefore, a hierarchical structure of word relations are built  %Besides, our approach proposes a novel self-supervised method to build the graph for GCN, where no extra parsing results  are required as extra input. % , but also our attentive GCN is able to discriminately learn from the contextual information carried by different words.} % In the proposed attention, n-grams associated to each word in the input texts are firstly categorized into different groups according to their length,  % 閻掕泛鎮楀В蹇庨嚋 n-gram 閺夈儱濮為弶 % and then fed into a specific channel of attentions according to their groups, so that the n-grams are weighted separately in each group according to their contributions to the supertagging process. % 婵傝棄顦╅敍宀顑囨稉閺勵垰灏崚顐＄啊闁插秷顩﹂惃鍕嫲娑撳秹鍣哥憰浣烘畱 n-gram閿涙稓顑囨禍灞炬Ц閼宠棄顧勬禒搴ㄥ亝娴滄盯鍣哥憰浣烘畱闂 n-gram 娑擃厼顒熼崚鐗堟纯鏉╂粏绐涚粋鑽ゆ畱 context information % In doing so, not only important n-grams are distinguished, but also can our approach discriminatively learn from n-grams in different length, where the infrequent and long n-grams carrying important long range contextual information are appropriately modeled without being influenced by the frequent short ones. %  % 鐎圭偤鐛欑拠浣规閺堝鏅 The validity of our approach is demonstrated by experimental results on the CCGbank , where state-of-the-art performance is obtained for both tagging and parsing.    
","  % supertagging 閻庣敻娑氳壘 CCG parsing 闂傚牏鍋涢悥鍫曟煂瀹ュ牜娲 Supertagging is conventionally regarded as an important task for combinatory categorial grammar  parsing, where effective modeling of contextual information is highly important to this task. % 闂傚嫨鍊撶花鈩冩媴鐠恒劍鏆忛柡鍥ㄦ綑瀹搁亶鎯 encoder闁挎稑鏈惁顔戒繆 biLSTM闁挎稑鑻晶鐘崇閸濆嫷鍤犲ù supertagging 閺夆晜鐟ら柌 task 闁告垹濮崇粻顔尖柦閿涘嫭鏆忛柛鎺楊暒缁牊绋婇崼婵嗙劶闁 context feature闁挎稑鑻畵 n-gram However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders . % 闁哄牜鍓氶弸鍐晬鐏炴儳鐏夊ù鐙鍓氳ぐ渚宕 channeled n-gram attention 闁哄鍎遍ˇ鈺呮偠閸℃氨绠瑰☉鎿冧邯濡埖锛 In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. %  Specifically, we build the graph from chunks  extracted from a lexicon and apply attention over the graph, so that different  % word relations  word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. % 閻庡湱鍋ら悰娆戠磼閹惧浜悶娑栧妽濡叉垿鏁嶇仦鎯х亯濞寸媭鍓涘▓鎴﹀棘鐟欏嫮銆婇柡鍕靛灡濠渚寮崼銏＄暠 The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies % , as well as strong baselines from existing toolkits,  in terms of both supertagging and parsing. %  Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.\footnote{Our code and models for CCG supertagging are released at \url{https://github.com/cuhksz-nlp/NeST-CCG}.}",119
" Pre-trained Transformers  have lead to state-of-the-art results on a wide range of NLP tasks, for example, named entity recognition, relation extraction and question answering, often approaching human inter-rater agreement .  These models have also been demonstrated to learn effective cross-lingual representations, even without access to parallel text or bilingual lexicons .  Multilingual pre-trained Transformers, such as mBERT and XLM-RoBERTa , support surprisingly effective zero-shot cross-lingual transfer, where training and development data are only assumed in a high resource source language , and performance is evaluated on another target language. 	 Because no target language annotations are assumed in this setting, source language data is typically used to select among models that are fine-tuned with different hyperparameters and random seeds.  However, recent work has shown that English dev accuracy does not always correlate well with target language performance .  In this paper, we propose an alternative strategy for model selection in a zero-shot setting.  Our approach, dubbed Learned Model Selection , learns a function that scores the compatibility between a fine-tuned multilingual transformer, and a target language. The compatibility score is calculated based on features of the multilingual model's learned representations and the target language.  A model's features are based on its own internal representations; this is done by aggregating representations over an unlabeled target language text corpus.  These model-specific features capture information about how the cross-lingual representations transfer to the target language after fine-tuning on source language data.  In addition to model-specific representations, we also make use of learned language embeddings from the lang2vec package , which have been shown to encode typological information, for example, whether a language has prepositions or postpositions.  To measure compatibility between a multilingual model's fine-tuned representations and a target language, the model- and language- specific representations are combined in a bilinear layer.  Parameters of the scoring function are optimized to minimize a pairwise ranking loss on a set of held-out models, where the gold ranking is calculated using standard performance metrics, such as accuracy or F, on a set of pivot languages .  LMS does not rely on any annotated data in the target language for meta-learning or hyperparameter tuning, yet it is effective in learning to predict whether a multilingual model's representations are a good match for a specific target language.    In experiments on five well-studied NLP tasks , we find LMS consistently selects models with better target-language performance than those chosen using English dev data.  Appendix  demonstrates that our framework supports multi-task learning, which can be helpful in settings where some target-language annotations are available, but not for the desired task.  Finally, we show that LMS generalizes to both mBERT and XLM-RoBERTa in Appendix .  
"," Transformers that are pre-trained on multilingual text corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer learning results.  In the zero-shot cross-lingual transfer setting, only English training data is assumed, and the fine-tuned model is evaluated on another target language.  No target-language validation data is assumed in this setting, however substantial variance has been observed in target language performance between different fine-tuning runs.  Prior work has relied on English validation/development data to select among models that are fine-tuned with different learning rates, number of steps and other hyperparameters, often resulting in suboptimal choices.  To address this challenge, we propose a meta-learning approach to model selection that uses the fine-tuned model's own internal representations to predict its cross-lingual capabilities.  In extensive experiments we find that our approach consistently selects better models than English validation data across five languages and five well-studied NLP tasks, achieving results that are comparable to small amounts of target language development data.\footnote{We will make our code and data available on publication.}  %We further demonstrate that our method can benefit from pooling data across tasks when auxiliary annotations are available in the target language.",120
"  . }  Definition Extraction refers to the task in Natural Language Processing  of detecting and extracting a term and its definition in different types of text. A common use of automatic definition extraction is to help building dictionaries , but it can be employed for many other applications. For example, ontology building can benefit from methods that extract definitions , whilst the fields of definition extraction and information extraction can employ similar methodologies. It is therefore normal that there is growing interest in the task of definition extraction.  This paper describes our system that participated in two of the three subtasks of Task 6 at SemEval 2020 , a shared task focused on definition extraction from a specialised corpus. Our method employs state-of-the-art neural architectures in combination with automatic methods which extend and clean the provided dataset.  %Task 6 at SemEval 2020  is a shared task for definition extraction from a specialised corpus, tailoured specifically to the needs of definition extraction. This paper describes the RGCL team system that works on all three subtasks of the shared task. We employ state-of-the-art neural architectures and combine them with simple automatic methods to extend and clean the provided dataset where appropriate.  The remaining parts of this paper are structured as follows. First, we present related work in the area of definition extraction and the related field of relation extraction . The three subtasks and the dataset provided by the task organisers are described in Section . Next, we describe our system , followed by the results of the evaluation  and a final conclusion .   
","   This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection.",121
" Event extraction is a process to extract the named entities, event triggers and their relationships from real-world corpora. The named entities refer to those texts about predefined classes  and event triggers are words that express the types of events in texts . In literature, named entities and triggers are connected and named entities with corresponding roles are called arguments for a given trigger of a specific event.  %Named entities refer to the text mentions with predefined classes such as person names, company names and locations, etc. An event trigger is a word that mostly expresses the event types  in text. Named entities link to triggers by different roles, and named entities with corresponding roles are called arguments for a given trigger  of a specific event.  Currently, most existing works divide the event extraction into two independent sub-tasks: named entity recognition and trigger labeling. These two sub-tasks are always formulated as multi-class classification problems, and many works apply the sequence-to-sequence based labeling method which aims to translate a sentence into sequential tags. From our investigation, one problem of these sequence-to-sequence methods is that they ignore the orders of output tags, and therefore, it is difficult to precisely annotate different parts of an entity. To address this issue, some methods propose to incorporate the conditional random field  module to be aware of order-constraints for the annotated tags.  Since entities and triggers are naturally connected around events, recent works try to extract them jointly from corpora. Early methods apply pipeline frameworks with predefined lexical features which lack generality to different applications. Recent works leverage the structural dependency between entities and triggers to further improve the performances of both the entity and trigger identification sub-tasks.  %The prevalent methods can be divided into two categories: a) a parallel framework to obtain entities and triggers simultaneously and b) a pipeline framework to get triggers at first and then perform sub-tasks to extract entities. Takanobu et al.  propose a hierarchical reinforcement learning model to extract triggers first and then evoke a sub-process to get the related entities by referring to the obtained triggers in the same sentences. Nguyen et al.  design an attention mechanism to augment the accuracy for trigger extraction in multilingual environments. Fu el al.  employ graph convolutional network  to capture the local contextual information in sentences and use a two-stage method to extract entities and triggers from text together.   % The main challenges to improve the performance of jointly extract entities and triggers are two-fold: Although existing works have achieved comparable performance on jointly extracting entities and triggers, these approaches still suffer the major limitation of losing co-occurrence relationships between entities and triggers. Many existing methods determine the trigger and entities separately and then match the entities with triggers. % In this way, the co-occurrence relationships between entities and triggers are ignored, therefore, those methods might require more pre-trained features or prior data in order to achieve better performance. In this way, the co-occurrence relationships between entities and triggers are ignored, although pre-trained features or prior data are introduced to achieve better performance. It is also challenging to capture effective co-occurrence relationships between the entities and their triggers. We observed from the experiments that most of the entities and triggers are co-occurred sparsely  throughout a corpus. This issue exacerbates the problem of losing co-occurrence relationships mentioned before.   %However, most existing methods suffer performance degradation when extracting entities and triggers jointly. The reason is that most of the entities and triggers are sparsely  co-occurred throughout a corpus and the previous approaches do not well handle this sparse co-occurred relationship. %In addition, it is challenging to establish an effective interaction mechanism between the sub-tasks for joint-event-extraction, because traditional joint learning may lead to an error-propagation issue that lowers the accuracy of joint tasks.  [htb] 	 		 	 	 %% label for entire figure %  {1.8in} 		 	 	 %% label for entire figure %  {2.4in} 		 	 	 %% label for entire figure %    %% label for entire figure   To address the aforementioned challenge, the core insight of this paper is that in the joint-event-extraction task, the ground-truth annotations for triggers could be leveraged to supervise the extraction of the entities, and vice versa. Based on this insight, this paper proposes a novel method to extract structural information from corpora by utilizing the co-occurrence relationships between triggers and entities. Furthermore, in order to fully address the aforementioned sparsely co-occurrence relationships, we model the entity-trigger co-occurrence pairs as a heterogeneous information network  and supervise the trigger extraction by inferring the entity distribution with given triggers based on the indirect co-occurrence relationships collected along the meta-paths from a heterogeneous information network .  Figure illustrates the process of our proposed method to collect indirect co-occurrence relationships between entities and triggers. Figure is a sub-graph of the ``entity-trigger'' HIN for the ACE 2005 corpus. Figure compares the entity distributions inferred from given triggers based on the direct adjacency matrix and that inferred from the meta-path adjacency matrix. From this figure, we observe that a trigger does not necessarily connect to all entities directly and the direct-adjacency-based distribution is more concentrated on a few entities, while the meta-path-based distribution is spread over a larger number of entities. This shows that a model could collect indirect co-occurrence patterns between entities and triggers based on the meta-path adjacency matrix of an ``entity-trigger'' HIN. Moreover, the obtained indirect patterns could be applied to improve the performance to extract both entities and triggers.  Based on the aforementioned example and analysis, we propose a neural network to extract event entities and triggers. Our model is built on the top of sequence-to-sequence labeling framework and its inner parameters are supervised by both the ground-truth annotations of sentences and ``entity-trigger'' co-occurrence relationships. Furthermore, to fully address the indirect ``entity-trigger'' co-occurrence relationships, we propose the \underline{C}ross-\underline{S}upervised \underline{M}echanism  based on the HIN. The CSM alternatively supervises the entity and trigger extraction with the indirect co-occurrence patterns mined from a corpus. CSM builds a bridge for triggers or entities by collecting their latent co-occurrence patterns along meta-paths of the corresponding heterogeneous information network for a corpus. Then the obtained patterns are applied to boost the performances of entity and triggers extractions alternatively. We define this process as a ``cross-supervise'' mechanism. The experimental results show that our method achieves higher precisions and recalls than several state-of-the-art methods.  In summary, the main contributions of this paper are as follows:        The remainder of this paper is organized as follows. In Section, we first introduce some preliminary knowledge about event extraction and HIN, and also formulate the problem. Section presents our proposed model in detail. Section verifies the effectiveness of our model and compares it with state-of-the-art methods on real-world datasets. Finally, we conclude this paper in Section.  
"," Joint-event-extraction, which extracts structural information  from unstructured real-world corpora, has attracted more and more research attention in natural language processing. Most existing works do not fully address the sparse co-occurrence relationships between entities and triggers, which loses this important information and thus deteriorates the extraction performance. To mitigate this issue, we first define the joint-event-extraction as a sequence-to-sequence labeling task with a tag set composed of tags of triggers and entities. Then, to incorporate the missing information in the aforementioned co-occurrence relationships, we propose a \underline{C}ross-\underline{S}upervised \underline{M}echanism  to alternately supervise the extraction of either triggers or entities based on the type distribution of each other. Moreover, since the connected entities and triggers naturally form a heterogeneous information network , we leverage the latent pattern along meta-paths for a given corpus to further improve the performance of our proposed method. To verify the effectiveness of our proposed method, we conduct extensive experiments on four real-world datasets as well as compare our method with state-of-the-art methods. Empirical results and analysis show that our approach outperforms the state-of-the-art methods in both entity and trigger extraction.",122
"  Recently, pre-trained self-supervised models such as BERT have attracted an increasing amount of attention in natural language processing and vision-language processing.  Benefiting from common knowledge contained in massive unlabeled data, the pretraining-finetuning framework has become a representative paradigm for advancing various language-related downstream tasks.   Most endeavors on pre-trained representation models rely on elaborately designed self-supervised tasks, which typically corrupt the given sequence with certain types of noise , and then train the model to recover the original sequence.  As a consequence, the learned representations tend to be covariant with the input noise of pre-training in this paradigm.  However, when transferred to downstream tasks, the pre-trained model is responsible for encoding the original sequence without noise, and is expected to obtain noise invariant representations.  Such pretrain-finetune discrepancy not only impedes fast fine-tuning, but also may result in suboptimal sequence representations, thus affecting the performance in downstream tasks.   %%%%%%%%%%%% % [t] %    % 	 % 	\vskip 0.15in % 	 % 		\toprule % 		Models  & Noise type       \\ \midrule % 		BERT           & Mask tokens       \\ % 		SpanBERT      & Mask spans      \\ % % 		RoBERTa         & Mask token       \\ % % 		XLNet          & Shuffle token     \\ % 		ELECTRA       & Replace tokens       \\ % 		StructBERT     & Mask + Shuffle tokens   \\  % % 		BART & Mask + Shuffle + Replace. \\ % 		\midrule % 		UNITER       & Mask tokens + regions    \\ % 		LXMERT       & Mask tokens + regions     \\  % 		 % 	\vskip -0.1in %  %%%%%%%%%%%%  %%%%%%%%%%%% %  %%%%%%%%%%%%  To remedy this, we present ContrAstive Pre-Training  to learn noise invariant  sequence representations. %, inspired by the Noise Contrastive Estimation. The core idea of CAPT is to enhance the consistency between semantic representations of the original sequence and that of corresponding corrupted version  via unsupervised instance-wise training signals. %can be fully utilized via elaborately designed semantic contrastive loss. %As shown in Figure, our approach  In more detail, it strives to pull the representation of the corrupted sequence towards that of the original instance in the semantic space, while pushing it away from representations of other instances. % Such training objectives are formulated as a multi-class classification task, which aims at classifying the original sequence to the class of its corrupted version and vice versa, while classifying different instances into different classes. % For implementation feasibility, two effective model extension are proposed to further enhance the capability of the model to extract noise-concentrated and instance-diffused features. Moreover, in order to enable the model to learn from more ``difficult'' and ``diverse'' instances, two effective methods are proposed to further enhance the capability of the model to extract noise-concentrated and instance-diffused features. With such training objective, the pre-trained model is encouraged to learn noise invariant representations, thereby alleviating the pretrain-finetune discrepancy to some extent.  As an additional benefit, CAPT also assists the pre-trained model to more effectively capture the global semantics of the input.  Most prior work only focuses on token-level pre-training tasks , which lacks the modeling of global semantics of the input.  Some other efforts alleviate this problem by introducing sentence-level pre-training tasks  that rely on the relative position of segments in the document. However, the semantic connection between these segments tends to be excessively loose, which may result in confusing gradient signals.  By contrast, our CAPT offers incentives for representations of inputs sharing the same semantics  to be similar, while the representations of inputs expressing different semantics  are penalized to be distinguished from each other. Such more reasonable sentence-level supervision enables our approach to look beyond the local structures of input sequences and become more aware of the global semantics. %With such more reasonable sentence-level supervision, our approach achieves better modeling of global semantics of the input.   We perform the evaluation on a comprehensive suite of benchmark, covering 8 natural language understanding and 3 cross-modal tasks.  Extensive empirical evidence demonstrates that our approach can achieve consistent improvements over the baselines in both language and vision-language domains. To be more specific, our CAPT raises the performance of RoBERTa from 88.9\% to 89.5\% on the GLUE dev set, and also surpasses LXMERT by 0.5\%, 0.6\% and 0.8\% on VQA, GQA and , respectively.    
"," Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training  to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6\% absolute gain on GLUE benchmarks and 0.8\% absolute increment on $\text{NLVR}^2$.",123
"  Ang Natural Language Processing  ay isang subfield ng linguistics, computer science, at artificial intelligence na nauukol sa pag proseso at pag-unawa ng natural na wika . Ang ilan sa mga aplikasyon ng NLP ay ang email spam filters , pag-unawa ng nais sabihin tulad ng mga smart assistants , pagsasalin ng isang wika sa iba pang wika , mag predict ng susunod na salita base sa mga naunang salita , at marami pang iba. Dahil sa kaunlaran sa kasaganahan sa datos at pagiging accessible ng malakas na compute power, nabuhay muli ang machine learning approach. Sa maikling salita, ang machine learning approach ay gumagamit ng malaking datos na ginagamit ng isang computer algorithm upang matutunan ang mga patterns ng datos na ito. Dahil dito, naging epektibo siyang approach sa mga komplikadong problema dahil hindi na kailangan direktang i-program ang mga rules para malutas ang isang problema.   Notorious ang machine learning approach sa pangangailangan nito ng sobrang laking datos para mapakinabangan. Ang Transfer Learning  ay isang area ng research na concerned sa problemang ito . Sa maikling salita, ang TL ay ang pag retain o pagpapanatili ng mga natutunan ng isang model sa isang gawain at paggamit o ""transfer"" ng mga natutunan nito sa iba pero may kaugnayan na gawain. Halimbawa, ang mga natutunan ng isang model sa pag detect ng muka ng tao ay maaring gamitin bilang tuntungan para sa pag-aaral ng model na matutunan kung ang muka ng tao ay galit, masaya, at iba pang facial expressions .   
"," Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na datos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer Learning  techniques ay malaking tulong para sa low-resource setting o mga pagkakataong gipit sa datos. Sa mga nagdaang taon, nanaig ang mga transformer-based TL techniques pagdating sa low-resource tasks ngunit ito ay mataas na compute and memory requirements kaya nangangailangan ng mas mura pero epektibong alternatibo. Ang papel na ito ay may tatlong kontribusyon. Una, maglabas ng pre-trained AWD-LSTM language model sa wikang Filipino upang maging tuntungan sa pagbuo ng mga NLP applications sa wikang Filipino. Pangalawa, mag benchmark ng AWD-LSTM sa Hate Speech classification task at ipakita na kayang nitong makipagsabayan sa mga transformer-based models. Pangatlo, suriin ang performance ng AWD-LSTM sa low-resource setting gamit ang degradation test at ikumpara ito sa mga transformer-based models.",124
"   Given the presumed importance of reasoning across modalities in multimodal machine learning tasks, we should evaluate a model's ability to leverage  cross-modal interactions.   But such evaluation is not straightforward;  for example, an early Visual Question-Answering  challenge was later ``broken''  by a high-performing method that ignored the image entirely .  One response is to create multimodal-reasoning datasets that are specifically  and cleverly balanced  to resist language-only or visual-only models; examples are VQA 2.0 , NLVR2 , and GQA . However,  a balancing approach not always desirable.  For example, if image+text data is collected from an online social network , post-hoc rebalancing may obscure trends in the original data-generating processs. So, what alternative diagnostic tools are available for better understanding what models learn?    The main tool utilized by prior work is  In addition to comparing against text-only and image-only baselines, often, two multimodal models with differing representational capacity  are trained and their performance compared. The argument commonly made is that if model A, with greater expressive capacity, outperforms model B, then the performance differences can be at least partially attributed to that increased expressivity.   {But is that a reliable argument?} Model performance comparisons are an opaque tool for analysis, especially for deep neural networks: performance differences versus baselines, frequently small in magnitude, can often be attributed to hyperparameter search schemes, random seeds, the number of models compared, etc. . Thus, while model comparisons are an acceptable starting point for demonstrating whether or not a model is learning an interesting set of  cross-modal factors, they provide rather indirect evidence.  We propose mpirical   \underline{M}ultimodally-\underline{A}dditive       \makeatletter \renewcommand
","  Modeling expressive cross-modal interactions  seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data.  We propose a new diagnostic tool,  , for isolating whether or not cross-modal {interactions} improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure.  For seven \mbox{image+}text classification tasks , we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning  report the performance not only of unimodal baselines, but also the \emap of their best-performing model.",125
"    \fi  % typically, a single predicate mention  does not constitute what we typically think about as events; we typically think of an event as something that consists of multiple such primitive structures %{\fontsize{10.5}{11} {13pt}}) involves more fine-grained event mentions about people killed , flights canceled  and passengers affected . Some of those mentions also follow strict temporal order . Our goal is to induce such an  that recognizes %organizes  the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering , narrative prediction , timeline construction  and summarization . %\dr{The choice of references is good but revealing; I suggest to replace the summarization with a ``classical"" summarization paper , and supports tasks such as question answering , narrative prediction , timeline construction  and summarization . Typically, events are not just standalone predicate mentions, but rather as structures over multiple such predicates. Consider the example in Figure.  The description to the impact of the storm  also involves mentions about killed people , canceled flights  and affected passengers . Some of mentions thereof also follow temporal order. To support the comprehension of complex events, it is important to recognize the multifaceted relations for the predicate mentions in the text. \fi        % second paragraph  studied event temporal relation  extraction with a statistical common sense resource  and  adopted data-driven methods for TempRel extraction; parent-child relations among events are studied in  and . Though some of the previous work has ensured consistency via adding constraints in the inference phase, essentially they are not improving local predictions and the inconsistent results from the models might not be corrected in the inference stage. Besides, most of the approaches suffered from limited learning resources and the tasks are studied separately. \fi  Recently, significant %much research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation  extraction  and subevent relation extraction . Addressing such challenging tasks requires a model to recognize the inherent connection between event  %\dr{should it be predicate mentions, to ease the ambiguity?}  mentions as well as their contexts in the documents. Accordingly, a few previous methods apply statistical learning methods to characterize the grounded events in the documents . Such methods often require designing various features to characterize the structural, discourse and narrative aspects of the events, which are costly to produce and are often specific to a certain task or dataset. More recent works attempted to use data-driven methods based on neural relation extraction models  which refrain from feature engineering and offer competent performances.      While data-driven methods provide a general and tractable way to capture specific event-event relations, it still remains challenging for those methods to precisely infer the correct relations. One challenge is that almost every task for event-event relation extraction comes with limited available annotated resources. Specifically, most tasks annotate no more than a hundred articles . Even the largest one in the literature, i.e., MATRES  for TempRel extraction, contains annotation for merely 275 articles. The lack of supervision hinders feature learning of events as well as inference of the relations, %Therefore, effectively tackling these tasks inevitably calls  therefore calling upon plausible auxiliary supervision from resources that are external to each of the tasks.    On the other hand, the event-event relations are often constrained by  %\drc{logical \dr{}change everywhere} %logic %\muhao{done.} properties, such as transitivity of TempRels Before and After , as well as that of %the relation between parent and child events subevent relations . In favor of such constraints, literature has employed global inference in the inference phase to comply with the logical properties particularly for TempRels . However, there lacks an effective way to ensure the global logical consistency in the training phase, which is key to making a data-driven machine learning model consistent on the beliefs of training data for various relation types . Moreover, the logical constraints may apply to different categories of %event-event  relations, and form complex conjunctive rules.  Consider the example in Figure : given that e2:died is Before e3:canceled and e3:canceled is a parent event of e4:affecting, the learning process should enforce e2:died Before e4:affecting. %\todo{Add an example of a conjunctive rule containing temporal and subevent relations.} Accordingly, ensuring the logical constraints  task-specific relations is another challenge being overlooked by the literature, the resolve of which provides a natural way to bridge the learning processes on multiple tasks. %\magenta{HW:TCR?} \fi  While data-driven methods provide a general and tractable way for event-event relation extraction, their performance is restricted by the limited annotated resources available. For example, the largest temporal relation extraction dataset MATRES only has 275 articles, which is far from enough for training a well-performing supervised model. The observation that relations and, in particular, event-event relations should be constrained by their logical properties , led to employing global inference to comply with transitivity and symmetry consistency, specifically on TempRel . However, in an event complex, the logical constraints may globally apply to different task-specific relations, and form more complex conjunctive constraints.  Consider the example in Figure : given that e2:died is Before e3:canceled and e3:canceled is a Parent event of e4:affecting, the learning process should enforce e2:died Before e4:affecting by considering the conjunctive constraints on both TempRel and subevent relations. While previous works focus on preserving logical consistency through  inference or structured learning , there was no %lacks an  effective way to endow neural models with the sense of global logical consistency during training.  %\dr{Notice that the previous statement was not correct; I change to limit it to neural models, since structure learning did it} %ensure the global logical consistency in the training phase.  This is key to bridging %bridge  the learning processes of %on both TempRel and subevent relations, which is a research focus of this paper.  %Event-relation extraction is a non-trivial task because of the following challenges: %1) Almost every event relation extraction task comes with limited learning resources with annotations. %2) Event relations are often volatile given different scenarios, and the determination of parent-child relation is especially difficult since there are less explicit lexical expressions compared with the cases for time and causation. %3) Event relations are often endowed with logical properties: % some temporal relations and parent-child relations comply with transitivity; % logical consistency should also be ensured across different categories of event relations.  The  contribution of this work is proposing %to propose  a joint constrained learning model for multifaceted event-event relation extraction.  The joint constrained learning framework seeks to regularize the model towards consistency with the logical constraints across both temporal and subevent relations, for which three types of consistency requirements are considered: ,  and . Such consistency requirements comprehensively define the interdependencies among those relations, essentially unifying the ordered nature of time and the topological nature of multi-granular subevents based on a set of declarative logic rules. Motivated by the logic-driven framework proposed by , the declarative logical constraints are converted into differentiable functions that can be incorporated into the learning objective for relation extraction tasks.  Enforcing logical constraints across temporal and subevent relations is also a natural way to combine %two event-event relation extraction tasks with a shared learning objective. the supervision signals coming from two different datasets, one for each of the  relation extraction tasks with a shared learning objective. %\dr{You said what is the first contribution, but not the second; do you want now to claim this as the second contribution? Note that I modified to emphasize the two datasets} %Besides, the consistency of the final prediction is further enforced by global inference via an ILP solver.  Despite the scarce annotation for both tasks, the proposed method surpasses the SOTA TempRel extraction method on MATRES by relatively 3.27\% in ; %\dr{I don't understand -- is it relative or F1? Also, Tab. 2 shows 2.5\%}  it also offers promising performance on the HiEve dataset for subevent relation extraction, relatively surpassing previous methods by at least 3.12\% in .  %\dr{which table is this from?} %by 3.12\% and 21.4\%. %We further provide ablation studies to show the importance of each component of our framework. %This fact is further illustrated by ablation studies.   From the NLU perspective, %the acquired knowledge of our method is able to simultaneously models the internal membership structure of a complex event, as well as the temporal relations among both simple and complex events. the  contribution of this work lies in providing a general method for inducing an event complex that comprehensively represents the relational structure of several related event %\drc{predicate} % mentions. %in two directions.  This is supported by the memberships vertically identified between multi-granular events, as well as the horizontal temporal reasoning within the event complex. As far as we know, this is %essentially different from all %many  previous works that only formulated relations along a single axis. Our model further demonstrates the potent capability of inducing event complexes  %with promising performance  when evaluated  %based  on the RED dataset .     
","     %\dr{I think that the current version  is too detailed and does not position the work at all, it just says what is being done. Here is a suggestion:}    Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other.     In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them.    Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations.    Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations     %of events     by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process.    We also present a promising case study showing the effectiveness of our approach in inducing event complexes on an external corpus.\footnote{Our code is publicly available at \url{https://cogcomp.seas.upenn.edu/page/publication_view/914}.} %\dr{Doesn't this contradict the statement above regarding the lack of joint data? Do we need to address it somehow}    %\dr{do we need the next clause? really, you show that you don't need it, but it reads like you just don't use it. If you really want to keep it, maybe better to say ""replacing a commonly used, more expensive, global inference process""} even without global inference that is widely used in previous methods.          \fi",126
"  Word embeddings which can capture semantic similarities have been extensively explored in a wide spectrum of Natural Language Processing  applications in recent years.  Word2Vec , FastText , and Glove  are some examples. Even though distributional word embeddings produce high quality representations, representing longer pieces of text such as sentences and paragraphs is still an open research problem. A sentence embedding is a contextual representation of a sentence which is often created by transformation of word embeddings through a composition function. There has been a large body of work in the literature which propose different approaches to represent sentences from word embeddings. SkipThought , InferSent , and Universal Sentence Encoder  are well-known examples.  % Other proposed methods for learning sentence representations include, but are not limited to .  There has been a growing interest in understanding what linguistic knowledge is encoded in deep contextual representation of language. For this purpose, several probing tasks are proposed to understand what these representations are capturing . One of the interesting findings is that despite the existence of explicit syntactic annotations, these learned deep representations encode syntax to some extent . Hewitt et. al. provide an evidence that the entire syntax tree is embedded implicitly in deep model's vector geometry. Kuncoro et. al.  show that LSTMs trained on language modeling objectives capture syntax-sensitive dependencies. Even though deep contextual language models implicitly capture syntactic information of sentences, explicit modeling of syntactic structure of sentences has been shown to further improve the results in different NLP tasks including neural language modeling , machine comprehension , summarization , text generation , machine translation , authorship attribution , etc. Furthermore, Kuncoro et. al. provide evidence that models which have explicit syntactic information result in better performance . Of particular interest, one of the areas where syntactic structure of sentences plays an important role is style-based text classification tasks, including authorship attribution. The syntactic structure of sentences captures the syntactic patterns of sentences adopted by a specific author and reveal how the author structures the sentences in a document.   Inspired by the above observations, our initial work demonstrates that explicit syntactic information of sentences improves the performance of a recurrent neural network classifier in the domain of authorship attribution . We continue this work in this paper by investigating if structural representation of sentences can be learned explicitly. In other words, similar to pre-trained word embeddings which mainly capture semantics, can we have pre-trained embeddings which mainly capture syntactic information of words. Such pre-trained word embeddings can be used in conjunction with semantics embeddings in different domains including authorship attribution. For this purpose, we propose a self-supervised framework using a Siamese  network  to explicitly learn the structural representation of sentences. The Siamese network is comprised of two identical components; a lexical sub-network and a syntactic sub-network; which take the sequence of words in the sentence and its corresponding linearized syntax parse tree as the inputs, respectively. This model is trained based on a contrastive loss objective where each pair of vectors  is close to each other in the embedding space if they belong to an identical sentence , and are far from each other if they belong to two different sentences .    As a result, each word in the sentence is embedded into a vector representation which mainly carries structural information. Due to the -to- mapping of word types to structural labels, the word representation is deduced into structural representations. In other words, semantically different words  are mapped to similar structural labels ; hence, semantically different words may have similar structural representations. These pre-trained structural word representations can be used as complimentary information to their pre-trained semantic embeddings . We use probing tasks proposed by Conneau et al.  to investigate the linguistic features learned by such a training.  The results indicate that structural embeddings show competitive results compared to the semantic embeddings, and concatenation of structural embeddings with semantic embeddings achieves further improvement.  Finally, we investigate the efficiency of the learned structural embeddings of words for the domain of authorship attribution across four datasets. Our experimental results demonstrate classification improvements when structural embeddings are concatenated with the pre-trained word embeddings.  The remainder of this paper is organized as follows: we elaborate our proposed self-supervised framework in Section .  The details of the datasets and experimental configuration are provided and the experimental results reported in Section ; We review the related work in Section . Finally, we conclude this paper in Section .     
","   Syntactic structure of sentences in a document substantially informs about its authorial writing style. Sentence representation learning has been widely explored in recent years and it has been shown that it improves the generalization of different downstream tasks across many domains. Even though utilizing probing methods in several studies suggests that these learned contextual representations implicitly encode some amount of syntax, explicit syntactic information further improves the performance of deep neural models in the domain of authorship attribution. These observations have motivated us to investigate the explicit representation learning of syntactic structure of sentences.  In this paper, we propose a self-supervised framework for learning structural representations of sentences. The self-supervised network contains two components; a lexical sub-network and a syntactic sub-network which take the sequence of words and their corresponding structural labels as the input, respectively. Due to the $n$-to-$1$ mapping of words to their structural labels, each word will be embedded into a vector representation which mainly carries structural information. We evaluate the learned structural representations of sentences using different probing tasks, and subsequently utilize them in the authorship attribution task. Our experimental results indicate that the structural embeddings significantly improve the classification tasks when concatenated with the existing pre-trained word embeddings.",127
"    Since the end of the twentieth century and the spread of mobile communication technologies in the Arab world, youth, in particular, have developed a new chat alphabet to communicate more efficiently in informal Arabic. Because most media and applications initially did not enable chatting in Arabic, these Arab speakers resorted to what is now commonly known as ""Arabizi"". In, Arabizi was defined as the newly-emerged Arabic variant written using the Arabic numeral system and Roman script characters. With the widespread use of social media worldwide in more recent years, Arabizi emerged as an established Arabic writing system for mobile communication and social media in the Arab world.   Compared to the increasing studies of sentiment analysis in Indo-European languages, similar research for Arabic dialects is still very limited.\ This is mainly attributed to the lack of the needed good quality Modern Standard Arabic  publicly-available sentiment analysis resources in general, and more specifically dialectical Arabic publicly-available resources.\ Building such resources involves several difficulties in terms of data collection and annotation, especially for underrepresented Arabic dialects such as the Tunisian dialect. Nevertheless, existing Tunisian annotated datasets focused on code-switching datasets written using the Arabic or the Romanized Alphabet. The studies on these datasets applied off-the-shelf models that have been built for MSA on a dataset of Tunisian Arabic. An intuitive solution is to translate Tunisian Romanized Alphabet into Arabic Script. This approach suffers from the need for a parallel Tunisian-Arabic text corpus, the low average precision performances achieved and the irregularity of the words written.  Using a model trained on Modern Standard Arabic sentiment analysis data and then applying the same model on dialectal sentiment analysis data, does not produce good performances as shown in. This suggests that MSA models cannot be effective when applied to dialectical Arabic. There is, thus, a growing need for the creation of computational resources, not only for MSA but also for dialectical Arabic. The same situation holds when one tries to use computational resources used for a specific dialect of Arabic with another one.  To the best of our knowledge, this is the first study on sentiment analysis TUNIZI Romanized Alphabet. \ This could be deduced in the next sections where we will present TUNIZI and the state-of-the-art of Tunisian sentiment analysis followed by our proposed approach, results and discussion before conclusion and future work.   
"," Tunisians on social media tend to express themselves in their local dialect using Latin script . This raises an additional challenge to the process of exploring and recognizing online opinions. To date, very little work has addressed TUNIZI sentiment analysis due to scarce resources for training an automated system. In this paper, we focus on the Tunisian dialect sentiment analysis used on social media. Most of the previous work used machine learning techniques combined with handcrafted features. More recently, Deep Neural Networks were widely used for this task, especially for the English language. In this paper, we explore the importance of various unsupervised word representations  and we investigate the use of Convolutional Neural Networks and Bidirectional Long Short-Term Memory. Without using any kind of handcrafted features, our experimental results on two publicly available datasets~~ showed  comparable performances to other languages.",128
"   In recent years, neural networks have shown impressive performance gains on long-standing AI problems, such as natural language understanding, speech recognition, and computer vision.  Based on these successes, researchers have considered the application of neural nets to data management problems, including learning indices, query optimization and entity matching.  In applying neural nets to data management, research has so far assumed that the data was modeled by a database schema.    The success of neural networks in processing unstructured data such as natural language and images   raises the question of whether their use can be extended to a point where we can relax the fundamental assumption of database management, which is that the data we process is represented as fields of a pre-defined schema.  What if, instead, data and queries can be represented as short natural language sentences, and queries can be answered from these sentences?  This paper presents a first step in answering that question.  We describe   Realizing the vision of \systemname\ will offer several benefits that database systems have struggled to support for decades.  The first, and most important benefit is that a \ndb, by definition, has no pre-defined schema. Therefore, the scope of the database does not need to be defined in advance and any data that becomes relevant as the application is used can be stored and queried. The second benefit is that updates and queries can be posed in a variety of natural language forms, as is convenient to any user.  In contrast, a traditional database query needs to be based on the database schema.  A third benefit comes from the fact that the \ndb\  is based on a pre-trained language model that already contains a lot of knowledge.   For example, the fact that London is in the UK is already encoded in the language model. Hence, a query asking who lives in the UK can retrieve people who are known to live in London without having to explicitly specify an additional join. Furthermore, using the same paradigm, we can endow the \ndb\  with more domain knowledge by extending the pre-training corpus to that domain.   By nature, a \ndb\ is not meant to provide the same correctness guarantees of a traditional database system, i.e., that the answers returned for a query satisfy the precise binary semantics of the query language.  Hence, \ndb s should not be considered as an alternative to traditional databases in applications where such guarantees are required.    Given its benefits, \neuraldatabases\ are well suited for emerging applications where the schema of the data cannot be determined in advance and data can be stated in a wide range of linguistic patterns.  A family of such applications arise in the area of storing knowledge for personal assistants that currently available for home use and in the future will accompany Augmented Reality glasses. In these applications, users store data about their habits and experiences, their friends and their preferences, and designing a schema for such an application is impractical.  Another class of applications is the modeling and querying of political claims .  Here too, claims can be about a huge variety of topics and expressed in many ways.   Our first contribution is to show that state of the art transformer models can be adapted to answer simple natural language queries. Specifically, the models can process facts that are relevant to a query independent of their specific linguistic form, and combine multiple facts to yield correct answers, effectively performing a join. However, we identify two major limitations of these models:  they do not perform well on aggregation queries , and  since the input size to the transformer is bounded and the complexity of the transformer is quadratic in the size of its input, they only work on a relatively small collection of facts.  Our second contribution is to  propose an architecture for neural databases that uses the power of transformers at its core, but puts in place several other components in order to address the scalability and aggregation issues. Our architecture runs multiple instances of a Neural SPJ operator in parallel. The results of the operator are either the answer to the query or the input to an aggregation operator, which is done in a traditional fashion. Underlying this architecture is a novel algorithm for generating the small sets of database sentences that are fed to each Neural SPJ operator.  Finally, we describe an experimental study that validates the different components of \systemname s, namely the ability of the Neural SPJ to answer queries or create results for a subsequent aggregation operator even with minimal supervision, and our ability to produce support sets that are fed into each of the Neural SPJ operators. Putting all the components together, our   final result shows that we can accurately answer queries over thousands of sentences with very high accuracy. To run the experiments we had to create an experimental dataset with training data for \ndb s, which we make available for future research.    % and capable of generating intermediate results and  accurately predicting the aggregation operation to execute over these intermediate results.   
"," \jt{TODO Before final submission remove page numbers} In recent years, neural networks have shown impressive performance gains on long-standing AI problems, and in particular, answering queries from natural language text. These advances raise the question of whether they can be extended to a point where we can relax the fundamental assumption of database management, namely, that our data is represented as fields of a pre-defined schema.   This paper presents a first step in answering that question.  We describe \ndb, a database system with no pre-defined schema, in which updates and queries are given in natural language. We develop query processing techniques that build on the  primitives offered by the state of the art Natural Language Processing methods.   We begin by demonstrating that at the core, recent NLP transformers, powered by pre-trained language models, can answer select-project-join queries if they are given the exact set of relevant facts. However, they cannot scale to non-trivial databases and cannot perform aggregation queries. Based on these findings, we describe a \ndb\ architecture that runs multiple Neural SPJ operators in parallel, each with a set of database sentences that can produce one of the answers to the query. The result of these operators is fed to an aggregation operator if needed. We describe an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators. Importantly, this algorithm can be trained by the Neural SPJ operator itself. We experimentally validate the accuracy of \systemname\ and its components, showing that we can answer queries over thousands of sentences with very high accuracy.",129
"    Automatic medical code assignment is a routine healthcare task for medical information management and clinical decision support. The International Classification of Diseases  coding system, maintained by the World Health Organization , is widely used among various coding systems.  Thus, the medical code assignment task is also called ICD coding. It uses clinical notes of discharge summaries to predict medical codes in a supervised manner with human-annotated codes, which is formulated as a multi-class multi-label text classification problem in the medical domain.    While there are increasing works in the community in automatic medical code assignment~, this task remains challenging from the perspectives of note representation and code prediction. First, medical note representation, a critical step in understanding medical notes, is formidably challenging due to the lengthy and complex semantic information in the discharge documents. There are typically thousands of tokens in a medical note due to the various diagnoses and procedures experienced by a patient. Furthermore, clinical notes also contain a vocabulary with many professional words and phrases, making it hard for a neural network model to encode and understand critical information. Second, the medical coding system has a very high and sparse dimensional label space, render the code prediction task incredibly difficult. For example, ICD9 and ICD10 coding systems have many labels, i.e., more than 14,000 and 68,000 codes. However, a patient typically is diagnosed with only a couple of codes over the whole coding space.     Early works for medical code assignment typically follow statistical approaches. They either employ rule-based methods  or apply classification methods such as SVM and Bayesian ridge regression  to assign the codes. These methods are shallow and do not exploit the complex semantic information in medical notes, leading to unsatisfactory performance. Recently, Natural language processing  techniques based on deep learning have been developed , which learn the note representation via convolutional neural networks. Specifically, CAML, MultiResCNN and DCAN treat ICD coding as a general text classification problem and develop complex neural encoders to learn the note representation. HyperCore proposes the hyperbolic embedding to capture code hierarchy and co-occurrence. However, these approaches are still ineffective, as they do not explicitly capture the fine-grained interactions between textual elements and medical codes. These interactions naturally represent the interdependencies between the complex medical words and associated codes, and thus should be well exploited.  This paper put forward a novel neural architecture,  Gated Convolutional Neural Network with Note-Code Interaction , for effective medical code assignment. Our goal is to learn rich representation from clinical notes and exploit the interactions between medical texts and clinical codes. To capture the long sequential history of clinical documents, we design a novel dilation information propagation component with a forgetting mechanism to selectively utilize the useful information for note representation learning. To tackle the large labeling space, we formulate textual notes and medical codes as a complete bipartite graph and develop a graph message passing approach to capture the explicit interaction between nodes and codes. The ICD code descriptions are used as an external medical knowledge source to learn more accurate code representations that preserve the semantic relations of the codes. Considering the practical application in real-world medical institutes, especially those with limited computing resources, our architecture also prioritizes computational efficiency when designing the sub-modules.  Our contributions are itemized as follows.     
"," Medical code assignment from clinical text is a fundamental task in clinical information system management. As medical notes are typically lengthy and the medical coding system's code space is large, this task is a long-standing challenge.  Recent work applies deep neural network models to encode the medical notes and assign medical codes to clinical documents. However, these methods are still ineffective as they do not fully encode and capture the lengthy and rich semantic information of medical notes nor explicitly exploit the interactions between the notes and codes. We propose a novel method, gated convolutional neural networks, and a note-code interaction , for automatic medical code assignment to overcome these challenges. Our methods capture the rich semantic information of the lengthy clinical text for better representation by utilizing embedding injection and gated information propagation in the medical note encoding module. With a novel note-code interaction design and a graph message passing mechanism, we explicitly capture the underlying dependency between notes and codes, enabling effective code prediction. A weight sharing scheme is further designed to decrease the number of trainable parameters. Empirical experiments on real-world clinical datasets show that our proposed model outperforms state-of-the-art models in most cases, and our model size is on par with light-weighted baselines.",130
" %  Enabling chatbots to indulge in engaging conversations requires massive datasets of human-human conversations . Training such dialog agents requires substantial time and effort expended in the collection of adequate number of high quality conversation samples.   alleviate this problem by introducing a self-feeding chatbot which can directly learn from user interactions. This chatbot requests users to provide natural language feedback when the users are dissatisfied with its response.   treat this feedback as a gold response to the wrong turn and use it as an additional training sample to improve the chatbot.    Although natural language feedback is cheap to collect from a chatbot's end-users, most often, feedback cannot be used directly as a training sample since feedback is usually not the answer itself, but simply contains hints to the answer. \Cref{tab:response_samples} shows some feedback text samples. Naive modification of feedback using heuristics like regular expressions would lead to generic responses that are ineffective in improving the dialog ability of chatbots . Additionally, writing an exhaustive set of regular expression rules is time consuming and requires extensive analysis of the data.  Annotating data to convert feedback text to natural response is also expensive and defeats the purpose of learning from feedback text.   [t]   gives a bird's-eye view of our problem. We frame this problem as a variant of text style transfer where the generator is tasked with making the feedback resemble the optimal response to the user's previous utterance and the discriminator is a classifier that distinguishes whether a given response is feedback or natural.   Our main contributions are the following: %      ).     %      ).     Our results also reveal that training naively on feedback doesn't help when the original chatbot is already a strong model, whereas \feedresp also helps strong models.   
","  The ubiquitous nature of chatbots and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator's goal is to convert the feedback into a response that answers the user's previous utterance and to fool the discriminator which distinguishes feedback from  natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94\% to 75.96\% in ranking correct responses on the \personachat dataset, a large improvement given that the original model is already trained on 131k samples.\footnote{Our code is released at \url{https://github.com/ekunnii/adversarial-feedback-chatbot/}}",131
"  Text Generation is the task of producing written or spoken narrative from structured or unstructured data. The overarching goal is the seamless human-machine communication by presenting a wealth of data in a way we can comprehend. With respect to the modeling approaches, there are three main paradigms in generating text based on the schema of input and output:  Text-to-Text  Data-to-Text  None-to-Text. Table  presents the categorization of different tasks based on this paradigm. These several tasks deserve undivided attention and accordingly they have been heavily dissected, studied and surveyed in the recent past. For instance, independent and exclusive surveys are periodically conducted on summarization , knowledge to text generation {DBLP:conf/inlg/GardentSNP17, DBLP:conf/naacl/Koncel-Kedziorski19}, machine translation , dialog response generation , storytelling, narrative generation , image captioning  etc., to dig deeper into task specific approaches that are foundational as well as in the bleeding edge of research. While these are extremely necessary, often the focus on techniques that are beneficial to other tightly coupled tasks are overlooked. The goal of this survey is to focus on these key components that are task agnostic to improve the ensemble of tasks in neural text generation. %The rest of the survey is organized as follows: Section  describes the modeling approaches in text generation including the learning paradigms, pre-training and decoding strategies. This is followed by Section  describing the key challenges and solutions to the text generation such as fluency, length, content selection, speed etc.,. Section  describes evaluation and finally Section  presents the conclusions and the prospective future directions.  [t] {% {l|l|l|l} \hline \hline  }     [t!]       %https://www.sciencedirect.com/science/article/pii/S1319157820303360    There have been several studies conducted on surveying text generation.  present a detailed overview of information theory based approaches.  primarily focus on core modeling approaches, especially VAEs  and GANs .  elaborated on tasks such as captioning, style trasfer etc., with a primary focus on data-to-text tasks. Controllability aspect is explored by . The workclosest to this is by  who perform an empirical study on the core more modeling approaches only. In contrast to these, this paper focuses on task agnostic components and factors capable of pushing the ensemble of tasks forward. Figure  presents the various components and factors that are important to study in neural text generation which are elaborated in this paper. %Text generation is an overarching set of tasks where these underlying factors that cut across tasks are very critical in pushing the field forward and this paper is dedicated to be a one stop destination to learn these several fundamental factors.  
","   Neural text generation metamorphosed into several critical natural language applications ranging from text completion to free form narrative generation. Generating natural language has fundamentally been a human attribute and the advent of ubiquitous NLP applications and virtual agents marks the need to impart this skill to machines. There has been a colossal research effort in various frontiers of neural text generation including machine translation, summarization, image captioning, storytelling etc., We believe that this is an excellent juncture to retrospect on the directions of the field. Specifically, this paper surveys the fundamental factors and components relaying task agnostic impacts across various generation tasks such as storytelling, summarization, translation etc., In specific, we present an abstraction of the imperative techniques with respect to learning paradigms, pretraining, modeling approaches, decoding and the key challenges. Thereby, we hope to deliver a one-stop destination for researchers in the field to facilitate a perspective on where to situate their work and how it impacts other closely related tasks. %scope it : current neural techniques %for single and multi-sentence",132
"  The following instructions are directed to authors of papers submitted to EACL 2021 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.   
"," This document contains the instructions for preparing a manuscript for the proceedings of EACL 2021. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.",133
" Cross-lingual abstractive summarization is the task to generate a summary of a given document in a different target language. This task provides the overview of an article in a foreign language and thus helps readers understand a text written in an unfamiliar language quickly.   Early work on cross-lingual abstractive summarization adopted the pipeline approach: either translation of the given document into the target language followed by summarization of the translated document or summarization of the given document followed by translation of the summary into the target language. On the other hand, recent studies have applied a neural encoder-decoder model, which is widely used for natural language generation tasks including machine translation and monolingual abstractive summarization, to generate a summary in the target language from the given document directly. %Such direct generation approaches prevent the error propagation problems in pipeline methods. Such direct generation approaches prevent the error propagation in pipeline methods.  Training neural encoder-decoder models requires numerous sentence pairs. In fact,  provided 3.8M sentence-summary pairs to train their neural encoder-decoder model for English abstractive summarization, and the following studies used the same training data. However, constructing a large-scale cross-lingual abstractive summarization dataset is much more difficult than collecting monolingual summarization datasets because we require sentence-summary pairs in different languages. To address this issue, recent studies applied a machine translation model to monolingual sentence-summary pairs. They used the constructed pseudo dataset to train their neural encoder-decoder models.    Meanwhile, the possibility whether existing genuine parallel corpora such as translation pairs and monolingual abstractive summarization datasets can be utilized needs to be explored. In machine translation,  indicated that using translation pairs in multiple languages improved the performance of a neural machine translation model. Similarly, we consider that such existing genuine parallel corpora have a positive influence on the cross-lingual abstractive summarization task since the task is a combination of machine translation and summarization.   In this study, we propose a multi-task learning framework, Transum, which includes machine translation, monolingual abstractive summarization, and cross-lingual abstractive summarization, for neural encoder-decoder models. The proposed method controls the target task with a special token which is inspired by Google's multilingual neural machine translation system. For example, we attach the special token  to the beginning of the source-side input sentence in translation.   The proposed Transum is quite simple because it does not require any additional architecture in contrast to  but effective in cross-lingual abstractive summarization. Experimental results show that Transum improves the performance of cross-lingual abstractive summarization and outperforms previous methods in Chinese-English and Arabic-English summarization. In addition, Transum significantly improves machine translation performance compared to that obtained using only a genuine parallel corpus for machine translation.   Furthermore, we construct a new test set to simulate more realistic situations: cross-lingual summarization with several length constraints. In a summarization process, it is important to generate a summary of a desired length. However, existing test sets for cross-lingual abstractive summarization cannot evaluate whether each model controls output lengths because the test sets do not contain summaries with multiple lengths. Thus, we translate an existing monolingual abstractive summarization that contains summaries with multiple lengths to construct the new test set.    The contributions of this study are as follows:      , that uses existing genuine parallel corpora in addition to pseudo cross-lingual abstractive summarization data to train a neural encoder-decoder model.       
"," We present a multi-task learning framework for cross-lingual abstractive summarization to augment training data. Recent studies constructed pseudo cross-lingual abstractive summarization data to train their neural encoder-decoders. Meanwhile, we introduce existing genuine data such as translation pairs and monolingual abstractive summarization data into training. Our proposed method, Transum, attaches a special token to the beginning of the input sentence to indicate the target task. The special token enables us to incorporate the genuine data into the training data easily. The experimental results show that Transum achieves better performance than the model trained with only pseudo cross-lingual summarization data. In addition, we achieve the top ROUGE score on Chinese-English and Arabic-English abstractive summarization. Moreover, Transum also has a positive effect on machine translation. Experimental results indicate that Transum improves the performance from the strong baseline, Transformer, in Chinese-English, Arabic-English, and English-Japanese translation datasets.",134
" Table-to-text generation is an important task for text generation from structured data. It aims at automatically producing descriptive natural language text that covers the salient information in table to help people to get the salient information of the tables. Practical applications can be found in domains such as weather forecasts, biography generation, NBA news generation, etc.  Over the pass several years, neural text generation methods have made significant progress on this task.  model it as a machine translation task and view the input table a record sequence. To generate text that contains more salient and well-organized facts,   explicitly model content selection and planning. %Some works also introduce extra knowledge  or pre-executed symbolic operations on table  to improve the result. To learning better representation for tables,   explicitly model the structure of table from multiple levels or different dimensions. In addition,  propose three auxiliary supervision tasks to capture accurate semantic representation of the table.    However, some issues have been overlooked. First, many tables ) contain a large number of numerical records. For instance,  of records and almost  of column types are numeric in ROTOWIR , a benchmark of NBA basketball games. Current methods treat these records as words in natural language text and ignore the characteristics of the number itself which play an important role in table representation, such as size attribute. In addition, there are noises in human-written summaries in dataset. These noises include redundant information and records that do not exist in the input tables ). These noises may cause incorrect alignments between input tables and target text or wrong supervision signals. And they can affect the performance of models based on content selection and planning or auxiliary supervision. %In addition, when human are writing a summary to describe the given table, they may consider the most salient records. For example, when describing the table in Figure  , they may pay more attention to K. Leonard, because he is the top scorer.   To solve above problems, we explore the use of the information contained in the tables and introduce two self-supervised tasks to learn better representation for tables. We argue that the better representation of tables can help the model to capture and organize the important facts, even without explicitly modeling content selection and planning. Specially, we improve ~'s method and employ a hierarchical table encoder to model the table structure from record level and row level. The record-level encoder utilizes two cascaded self-attention models to encode the table from column and row dimension, respectively. And then, we introduce a row-level fusion gate to obtain the row-level representation for each row. To learn a number-aware record representation, we introduce a Number Ordering  task. This task utilizes a pointer network to generate a descending record sequence for each column in table, according to their content. Figure   shows a number ordering example for column PTS. To the best of our knowledge, this is the first work on neural table-to-text generation via focusing on learning representation for number in table. Another self-supervised task, Significance Ordering , is further proposed to learn a significance-aware representation for the record. The significance denotes the relative relation between records in same row. This is inspired by the intuition that when humans describe the performance of a player, they tend to focus on his more salient records. For example, in Figure , K. Thompson's scores  is more likely to be described than his other's records. The SO task executes a descending sort operation on each row according to the significance scores of records. We use the position index of record  to measure its importance and the smaller the significance score, the more important the record is. The position index of record is obtained by the results of Number Ordering. For example, in Figure  , K. Thompson scores  points which are the largest in PTS, so the significance score of this record is 1. The proposed two tasks are trained together with the table2text generation model and they share the same encoder parameters. Obviously, the two proposed tasks are self-supervised and the training labels are easily obtained from the input tables. Therefore, the errors caused by noises in training set are avoided. %For record in same row, it includes another size information:significance. It denotes the relative relation between records in same row. To learn a significance-aware representation for table, we propose a Significance Ordering task which executes a ascending sort operation on each row according to the significance of records. We use the position index of record  to measure its importance and the smaller the significance score, the more important the record is. The position index of record is obtained by the results of Number Ordering. For example, in Figure  , K. Leonard score 45 points which are the largest in PTS, so the significance score of this record is 1). Obviously, the two proposed tasks are self-supervised and the training labels are easily obtained from the input tables. Therefore, the errors caused by noise in training set are avoided.   We conducted experiments on ROTOWIRE to verify the effectiveness of the proposed approach. The experimental results demonstrate that, even without explicitly modeling content selection or introducing extra knowledge, our method can help to generate text that contains more salient and well-organized facts. And we achieve the state-of-the-art performance on automatic metrics. %Content Selection , Content Ordering  and BLEU.   
"," Table-to-text generation aims at automatically generating natural text to help people to conveniently obtain the important information in tables. Although neural models for table-to-text have achieved remarkable progress, some problems still overlooked. The first is that the values recorded in many tables are mostly numbers in practice. The existing approaches do not do special treatment for these, and still regard these as words in natural language text.  Secondly, the target texts in training dataset may contain redundant information or facts do not exist in the input tables. These may give wrong supervision signals to some methods based on content selection and planning and auxiliary supervision. To solve these problems, we propose two self-supervised tasks, Number Ordering and Significance Ordering,  to help to learn better table representation. The former works on the column dimension to help to incorporate the size property of numbers into table representation. The latter acts on row dimension and help to learn a significance-aware table representation. We test our methods on the widely used dataset ROTOWIRE which consists of NBA game statistic and related news. The experimental results demonstrate that the model trained together with these two self-supervised tasks can generate text that contains more salient and well-organized facts, even without modeling context selection and planning. And we achieve the state-of-the-art performance on automatic metrics. % Content Selection , Content Ordering  and BLEU.",135
"/} Automatic keyphrase generation is the task of generating single or multi-word lexical units that provides readers with high level information about the key ideas or important topics described in a given source text. Apart from an information summarization perspective, this task has applications in various downstream natural language processing tasks such as text classification , document clustering  and information retrieval .   Traditionally, keyphrases  were extracted from source documents by retrieving and ranking a set of candidate phrases through rule based approaches. With recent advances in neural natural language generation and availability of larger training corpora, this problem is formulated under a sequence-to-sequence  modelling framework . This approach has an advantage that it can generate new and meaningful keyphrases which may be absent in the source text. The earliest work in this direction was by , who train a S2S model to generate one keyphrase at a time. At inference time, they decode with beam sizes as high as 200, to generate a large number of KPs and finally de-duplicate the outputs. However, this is computationally expensive and wasteful because only  of such KPs were found to be unique .   An alternative approach is to train a S2S model to generate multiple keyphrases in a sequential manner, where the output KPs are separated by a pre-defined delimiter token. This method has an added benefit that the model automatically learns to generate a variable number of keyphrases depending on the input, instead of a user-specified fixed number of keyphrases  from a large list of candidate outputs. However, some previous approaches  still use exhaustive beam search decoding to over-generate KPs and then apply post-processing to remove repetitions. Apart from the additional computational requirements, we argue that this method of  avoiding information redundancy is a last-minute solution. % `hacky' solution.   % \todoi{Importance of diversity}  In this paper, we take a principled direction towards addressing the information redundancy issue in keyphrase generation models. We propose to tackle this problem directly during the training stage, rather than applying adhoc post-processing at inference time. Specifically, we adopt the neural unlikelihood training  objective , whereby the decoder is penalized for generating undesirable tokens. % , which in our case corresponds to the set of repeating tokens.  introduce unlikelihood training for a language model setting. Since we work with a S2S setup, our version of UL loss consists of two components:  a target token level UL loss based on the target vocabulary to penalize the model for generating repeating tokens;  a copy token level UL loss based on the dynamic vocabulary of source tokens required for copy mechanism , which penalizes the model for copying repetitive tokens.   S2S models trained with maximum likelihood estimation  are usually tasked with the next token prediction objective. However, this does not necessarily incentivize the model to plan for future token prediction ahead of time. We observe such lack of model planning capability in our initial experiments with MLE models and to overcome this issue we propose to use -step ahead token prediction. This modified training objective encourages the model to learn to correctly predict not just the current token, but also tokens upto -steps ahead in the future. We then naturally incorporate UL training on the -step ahead token prediction task.  We summarize our contributions as follows:  To improve the diversity of generated keyphrases in a principled manner during training, we adopt the unlikelihood objective for the S2S setting and propose a novel copy token unlikelihood loss.  In order to incentivize model planning, we augment our training objective function to incorporate -step ahead token prediction. Additionally, we also introduce the -step ahead unlikelihood losses.  We propose new metrics for benchmarking keyphrase generation models on diversity criterion. We carry out experiments on datasets from three different domains  and validate the effectiveness of our approach.  We observe substantial gains in diversity while maintaining competitive output quality.  [!t] |}     \toprule     Title & semi automated schema integration with sasmint  \\     \midrule     Abstract & the emergence of increasing number of collaborating organizations has made clear the need for supporting interoperability infrastructures , enabling sharing and exchange of data among organizations . schema matching and schema integration are the crucial components of the interoperability infrastructures , and their semi automation to interrelate or integrate heterogeneous and autonomous databases in collaborative networks is desired . the semi automatic schema matching and integration sasmint system introduced in this paper identifies and resolves   \\     % \midrule     % \multicolumn{1}{r}{} & \multicolumn{1}{r}{} \\     \midrule     Ground Truth & schema integration ; collaboration ; schema matching ; heterogeneity ; data sharing \\     \midrule     MLE Baseline & 	extcolor{red{schema integration} ; sasmint ; \textcolor{red}{schema matching} ; \textcolor{red}{schema integration} ; \textcolor{red}{schema matching} ; sasmint derivation markup language     } \\     \midrule     DivKGen & schema integration ; interoperability infrastructures ; schema matching ; sasmint \\              {red}}.}   
"," In this paper, we study sequence-to-sequence  keyphrase generation models from the perspective of diversity. Recent advances in neural natural language generation have made possible remarkable progress on the task of keyphrase generation, demonstrated through improvements on quality metrics such as $F_1$-score. However, the importance of diversity in keyphrase generation has been largely ignored. We first analyze the extent of information redundancy present in the outputs generated by a baseline model trained using maximum likelihood estimation . Our findings show that repetition of keyphrases is a major issue with MLE training. To alleviate this issue, we adopt neural unlikelihood  objective for training the S2S model. Our version of UL training operates at  the target token level to discourage the generation of repeating tokens;  the copy token level to avoid copying repetitive tokens from the source text. Further, to encourage better model planning during the decoding process, we incorporate $K$-step ahead token prediction objective that computes both MLE and UL losses on future tokens as well. Through extensive experiments on datasets from three different domains we demonstrate that the proposed approach attains considerably large diversity gains, while maintaining competitive output quality.\footnote{Code is available at \url{https://github.com/BorealisAI/keyphrase-generation}}",136
" In healthcare, real-world data  refers to patient data routinely collected during clinic visits, hospitalization, as well as patient-reported results. In recent years, RWD's volume has become enormous, and invaluable insights and real-world evidence can be generated from these datasets using the latest data processing and analytical techniques. However, RWD's quality remains one of the main challenges that prevent novel machine learning methods from being readily adopted in healthcare.  Therefore, creating data quality tools is of great importance in health care and health data sciences.  Erroneous data in healthcare systems could jeopardize a patient's clinical outcomes and affect the care provider's ability to optimize its performance.     Common data quality issues include missing critical information about medical history, wrong coding of a condition, and inconsistency in documentation across different care sites. Manual review by domain experts is the gold standard for achieving the highest data quality but is unattainable in regular care practices. Recent developments in the field of Natural Language Processing  has attracted great interest in the healthcare community since algorithms for identifying variables of interest and classification algorithm for diseases  have been recently developed .  In this paper, we presented a novel model for the extraction of queries  in a corpus of dialogue between data entry clinicians and expert reviewers in a multi-site dialysis environment.   %The work's ultimate goal is to identify the data elements that caused most uncertainty or errors during the documentation process.  The main contributions of this work are: [label=]  ) and   sentences that are not questions.  Finally, in addition to evaluating our model's performance in a medical context, we also experimented in section  with a general-domain dataset  to show our model's generalizability.  The rest of the paper is organized as follows. Related work is presented in section . The different question detection methods that will be examined,   are described in section . Section  details the characteristics of the proposed multi-channel CNN model. Finally, the results of the experiments are reported in section  and a conclusion and a plan for future work are given in section .  
"," In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via ``expert-review"", where clinicians can have a dialogue with a domain expert  and ask them questions about data entry rules. Automatically identifying ``real questions"" in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting.  In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect  an answer  about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence , which we will refer as ``c-questions"". We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other deep  neural networks. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset.",137
"  Semantic parsing is the task of mapping a natural language query into a formal language, that is extensively used in goal-oriented dialogue systems. For a given query, such model should identify the requested action  and the associated values specifying parameters of the action . For example, if the query is Call Mary the action is call and the value of slot contact is Mary.  The number of different intents and slots in publicly available datasets  can be close to a hundred and it may be the orders of magnitude larger in real-world systems. Such a big number of classes usually causes a long tail in the class frequency distribution . These tail classes can be significantly improved with small quantities of additional labeled data.    However, training a neural semantic parsing model from scratch can take hours even on a relatively small public dataset . The real-world datasets can contain millions of examples  which can change the time scale to weeks. % Need to describe the problem and motivation to production settings more.  In this work, we propose to fine-tune a model that has already been trained on the old dataset  instead of training a new model to significantly speed up the incorporation of a new portion of data. We call this setting Incremental training, as the new portions of data can be added incrementally.  We focus on semantic parsing % and seq2seq networks for our case studies for the following reasons. Semantic parsing is a more complex NLP task compared to classification or NER and we hope that the lessons learned here would be more widely applicable. Task-oriented semantic parsing tend to have a large output vocabulary that can be frequently updated, and thus, benefit most from the Incremental setting. % We choose seq2seq networks for this work due to two reasons: first, seq2seq networks are very % general and can be easily adapted to simpler tasks like NER; % second, seq2seq models perform really well on popular natural language understanding datasets like TOP and SNIPS.  % Exploring this space of possible solutions, we compare the effectiveness of these approaches with each other and come up with a set of guidelines that are useful for incremental training tasks as well.  % To emulate the ""data-patch"" scenario, we split these datasets by focusing on a few classes. We show that naive fine-tuning leads to catastrophic forgetting and come up with approaches to remedy this. We observe that it is possible to fine-tune models to new classes in a few minutes compared to hours when retraining from scratch. We also compare the effect of pre-trained representations like BERT on fine-tuning. Using these observations we come up with fine-tuning guidelines in scenarios where the label space does not change. We verify that our approaches work on 2 popular semantic parsing datasets: TOP and SNIPS under different data splits.  The main contributions of this work are:         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Related work %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
"," A semantic parsing model is crucial to natural language processing applications such as goal-oriented dialogue systems. Such models can have hundreds of classes with a highly non-uniform distribution. In this work, we show how to efficiently  improve model performance given a new portion of labeled data for a specific low-resource class or a set of classes. We demonstrate that a simple approach with a specific fine-tuning procedure for the old model can reduce the computational costs by ~90\% compared to the training of a new model. The resulting performance is on-par with a model trained from scratch on a full dataset. We showcase the efficacy of our approach on two popular semantic parsing datasets, Facebook TOP, and SNIPS.",138
"  Recent progress in abstractive summarization has been fueled by the advent of large-scale Transformers pre-trained on autoregressive language modeling objectives . Despite their strong performance on automatic metrics like ROUGE , abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document . Although the interpretability of NLU models has been extensively studied , summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation .  %Generic explanation methods for language models  or neural machine translation models  are not entirely applicable, as summarization models typically have different interactions with the input document.  In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data , sampling , and training  , it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS  and BART , fine-tuned on two English summarization datasets, CNN/Daily Mail  and XSum , to understand model behavior in each setting. %We analyze the model using both blackbox and whitebox perspectives.  First, by comparing -grams between the input document and generated summaries, we establish two coarse types for decoded tokens, copy and generate . We find that the entropy of the generation decision correlates with whether the model is copying or generating, as well as where in the sentence the token is. This paints a picture of certain contexts being more restrictive from the standpoint of generation, particularly early in sentences where a model has not ``decided'' what to copy yet, and illustrates the interaction of content selection and lexical choice. %Furthermore, it illustrates the interaction of content selection and lexical choice: new bigrams are higher entropy, but beginnings of sentences are also high entropy, indicating that the model has some uncertainty about what sentence to discuss, even if it is going to copy. Second, we extend this analysis by looking at how uncertainty relates to the syntax of the generated sentence: whether uncertainty connects to syntactic notions of surprisal  and how the entropy varies across certain syntactic productions. % Finally, we derive a way to quantify decoder attention by aggregating self-attention heads, and investigating the correspondence between the prediction entropy and the fraction of the decoded tokens in the aggregated attention.\todo{change this sent to refer to entropy more} Finally, we derive a way to quantify decoder attention by aggregating distinct self-attention heads, revealing the correlation between the attention entropy and prediction entropy, and investigating the correspondence between the prediction entropy and the fraction of the past and future decoded tokens. % highly attentive positions and decoded or not-yet-decoded tokens with respect to specific Transformer layers in the decoder.  Taking this analysis together, we find that the abstractiveness of reference summaries fundamentally changes model behavior: the extractive nature of CNN/DM makes most of its decisions low entropy and copy-oriented while the model maintains higher uncertainty on XSum, yielding more abstractive summaries. More broadly, we show that uncertainty is a simple but effective tool to characterize decoder behavior in text generation. %  By analyzing decoder self-attention layers, we find that when the attention only focuses on a few tokens, the prediction entropy will be fairly low and the focused tokens are very likely to be predicted.   
"," % An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this inherent flexibility makes it difficult to interpret and understand model behavior. In this work, we adopt a data-driven methodology to unpack decoder behavior in both a blackbox and whitebox way. We fine-tune and analyze a GPT-2  model on two benchmark datasets featuring different levels of abstraction. Our experiments yield three key results. First, by analyzing the entropy of model predictions and its corresponding test-time behavior, we find a strong correlation between low entropy and where the model copies document spans rather than generating novel text. Second, this entropy analysis can allow us to understand what sentence positions and even what syntactic configurations are associated with copying existing content. Finally, by analyzing decoder self-attention patterns, we can trace this copying behavior to a particular pattern of attending to immediate decoder context and finding the next token to generate in the source document. An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior.  In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pre-trained models, PEGASUS  and BART  on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.\footnote{Code is available at \url{https://github.com/jiacheng-xu/text-sum-uncertainty}} % can trace this copying behavior to a particular pattern of attending to immediate decoder context and finding the next token to generate in the source document.",139
"  Neural attention mechanisms have been widely applied in  computer vision and have been shown to enable neural networks to only focus on those aspects of their input that are important for a given task. While neural networks are able to learn meaningful attention mechanisms using only supervision received for the target task, the addition of human gaze information has been shown to be beneficial in many cases. An especially interesting way of leveraging gaze information was demonstrated by works incorporating human gaze into neural attention mechanisms, for example for image and video captioning or visual question answering.  While attention is at least as important for reading text as it is for viewing images, integration of human gaze into neural attention mechanisms for natural language processing  tasks remains under-explored. A major obstacle to studying such integration is data scarcity: Existing corpora of human gaze during reading consist of too few samples to provide effective supervision for modern data-intensive architectures and human gaze data is only available for a small number of NLP tasks. For paraphrase generation and sentence compression, which play an important role for tasks such as reading comprehension systems, no human gaze data is available.  We address this data scarcity in two novel ways: First, to overcome the low number of human gaze samples for reading, we propose a novel hybrid text saliency model  in which we combine a cognitive model of reading behavior with human gaze supervision in a single machine learning framework. More specifically, we use the E-Z Reader model of attention allocation during reading to obtain a large number of synthetic training examples. We use these examples to pre-train a BiLSTM network with a Transformer whose weights we subsequently refine by training on only a small amount of human gaze data. We demonstrate that our model yields predictions that are well-correlated with human gaze on out-of-domain data. Second, we propose a novel joint modeling approach of attention and comprehension that allows human gaze predictions to be flexibly adapted to different NLP tasks by integrating TSM predictions into an attention layer. By jointly training the TSM with a task-specific network, the saliency predictions are adapted to this upstream task without the need for explicit supervision using real gaze data. Using this approach, we outperform the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10\% in BLEU-4 and achieve state of the art performance on the Google Sentence Compression corpus. As such, our work demonstrates the significant potential of combining cognitive and data-driven models and establishes a general principle for flexible gaze integration into NLP that has the potential to also benefit tasks beyond paraphrase generation and sentence compression.  
"," A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing . We propose a novel hybrid text saliency model  that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10\% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.",140
" In recent years, abstractive summarization  has made impressive progress with the development of sequence-to-sequence  framework . This framework is composed by an encoder and a decoder. The encoder processes the source text and extracts the necessary information for the decoder, which then predicts each word in the summary. Thanks to their generative nature, abstractive summaries can include novel expressions never seen in the source text, but at the same time, abstractive summaries are more difficult to produce compared with extractive summaries  which formed by directly selecting a subset of the source text. It has been also found that seq2seq-based abstractive methods usually struggle to generate out-of-vocabulary  words or rare words, even if those words can be found in the source text. Copy mechanism  can alleviate this problem and meanwhile maintain the expressive power of the seq2seq framework. The idea is to allow the decoder not only to generate a summary from scratch but also copy words from the source text.  Though effective in English text summarization, the copy mechanism remains relatively undeveloped in the summarization of some East Asian languages e.g. Chinese. Generally speaking, abstractive methods for Chinese text summarization comes in two varieties, being word-based and character-based. Since there is no explicit delimiter in Chinese sentence to indicate word boundary, the first step of word-based methods  is to perform word segmentation . Actually, in order to avoid the segmentation error and to reduce the size of vocabulary, most of the existing methods are character-based . When trying to combine the character-based methods in Chinese with copy mechanism, the original ``word copy'' degrades to ``character copy'' which does not guarantee a multi-character word to be copied verbatim from the source text . Unfortunately, copying multi-character words is quite common in Chinese summarization tasks. Take the Large Scale Chinese Social Media Text Summarization Dataset   as an example, according to Table I, about 37\% of the words in the summaries are copied from the source texts and consist of multiple characters.      	%	\Large 	   	   		\resizebox{0.7\linewidth}{!}{ 			{ccc}   				&Copied&Generated\\ \hline 				1&21.6\%&12.3\%\\ 				2&28.9\%&21.8\%\\ 				3&7.6\%&7.7\%\\\hline 			 		} 	  	   	   Selective read  was proposed to handle this problem. It calculates the weighted sum of encoder states corresponding to the last generated character and adds this result to the input of the next decoding step. Selective read can provide location information of the source text for the decoder and help it to perform the consecutive copy. A disadvantage of this approach, however, is that it increases reliance of present computation on partial results before the current step which makes the model more vulnerable to the errors accumulation and leads to exposure bias during inference.  Another way to make copied content consecutive is through directly copying text spans. Zhou et al.  implement span copy operation by equipping the decoder with a module that predicts the start and end positions of the span. Because a longer span can be decomposed to shorter ones, there are actually many different paths to generate the same summary during inference, but their model is optimized by only the longest common span at each time step during training, which exacerbates the discrepancy between two phases. In this work, we propose a novel lexicon-constrained copying network . The decoder of LCN can copy either a single character or a text span at a time, and we constrain the text span to match a potential multi-character word. Specifically, given a text and several off-the-shell word segmentators, if a text span is included in any segmentation result of the text, we consider it as a potential word. By doing so, the number of available spans is significantly reduced, making it is viable to marginalize over all possible paths during training. Furthermore, during inference, we aggregate all partial paths on the fly that producing the same output using a word-enhanced beam search algorithm, which encourages the model to copy multi-character words and facilitates the parallel computation.  To be in line with the aforementioned decoder, the encoder should be revised to learn the representations of not only characters but also multi-character words. In the context of neural machine translation, Su et al.  first organized characters and multi-character words in a directed graph named word-lattice. Following Xiao et al. , we adopt an encoder based on the Transformer  to take the word-lattice as input and allow each character and word to have its own hidden representation. By taking into account relative positional information when calculating self-attention, our encoder can capture both global and local dependencies among tokens, providing an informative representation of source text for the decoder to make copy decisions.   Although our model is character-based , it can directly utilize word-level prior knowledge, such as keywords. In our setting, keywords refer to words in the source text that have a high probability of inclusion in the summary. Inspired by Gehrmann et al. , we adopt a separate word selector based on the large pre-trained language model, e.g. BERT  to extract keywords. When the decoder intends to copy words from the source text, those selected keywords will be treated as candidates, and other words will be masked out.  Experimental results show that our model can achieve better performance when incorporating with the word selector.   
"," Copy mechanism allows sequence-to-sequence models to choose words from the input and put them directly into the output, which is finding increasing use in abstractive summarization. However, since there is no explicit delimiter in Chinese sentences, most existing models for Chinese abstractive summarization can only perform character copy, resulting in inefficient. To solve this problem, we propose a lexicon-constrained copying network that models multi-granularity in both encoder and decoder. On the source side, words and characters are aggregated into the same input memory using a Transformer-based encoder. On the target side, the decoder can copy either a character or a multi-character word at each time step, and the decoding process is guided by a word-enhanced search algorithm which facilitates the parallel computation and encourages the model to copy more words. Moreover, we adopt a word selector to integrate keyword information. Experiments results on a Chinese social media dataset show that our model can work standalone or with the word selector. Both forms can outperform previous character-based models and achieve competitive performances.",141
"  Humans are not supervised by the natural language inference . Supervision is necessary for applications in human-defined domains. For example, humans need the supervision of what is a noun before they do POS tagging, or what is a tiger in Wordnet before they classify an image of tiger in ImageNet. However, for NLI, people are able to entail that \textcircled{a} A man plays a piano contradicts \textcircled{b} A man plays the clarinet for his family without any supervision from the NLI labels. In this paper, we define such inference as a more general process of establishing associations and inferences between texts, rather than strictly classifying whether two sentences entail or contradict each other. Inspired by this, we raise the core problem in this paper: {. The exemplar theory argues that humans use { contradicts \textcircled{b} because they cannot happen simultaneously in the same { and \textcircled{b}. We need the commonsense that a man only has two arms, which cannot play the piano and clarinet simultaneously. This commonsense is hard to obtain from the text. However, if we link the sentences to their { and \textcircled{b} is limited.    In order to benefit from multimodal data in plain text inference, we propose the \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning  network. This is shown in Fig.. Its text encoder is decoupled, which only takes the plain text as inputs. Thus it can be directly adapted to downstream NLI tasks. Besides, we use multimodal contrastive loss between the text encoder and the image encoder, thereby forcing the text representation to align with the corresponding image. Therefore even if the text encoder in MACD only takes the plain text as input, it still represents visual knowledge. In the downstream plain text inference tasks, without taking images as input, the text encoder of MACD still implicitly incorporating the visual knowledge learned by the multimodal contrastive loss. Note that we do not need a decoupled image encoder in the SSL. So the image encoder in Fig. in MACD takes texts as inputs to provides a more precise image encoder. We will elaborate this in section.    
","   We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning  network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets . The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.",142
"  %閺鍙ラ嚋閸ユ拝绱濋弰顖氱秼閸撳秶娈戞径姘侀崹瀣劥缂冨弶鍎忛崘纰夌礉鐠侇厾绮屾稉娑擃亜銇囧Ο鈥崇烽敍灞藉晙閽傛悂顩撮崚鐧楁稉顏勭毈濡崇烽敍灞剧槨娑擃亜鐨Ο鈥崇烽崘宥呭礋閻欘剟鍣洪崠...  閹存垳婊戦惃鍕煙濞夋洩绱濈拋顓犵矊娑撴稉顏勩亣濡崇烽敍瀹杋netune鏉╂瑤閲滄径褎膩閸ㄥ鎮撻弮鍫曞倸绨睳娑擃亙绗夐崥灞剧箒鎼达妇娈戠亸蹇斈侀崹瀣剁礉閸欘亪娓剁电绻栨稉娑擃亝膩閸ㄥ绻樼悰宀勫櫤閸...   like LayerDrop.  As shown in Figure, we first demonstrate that when there is a large gap between the predefined layer dropout during training and the actual pruning ratio during inference, LayerDrop's performance is poor.  %We attribute it to huge sub-network training space and mismatch between random sampling training and deterministic inference.  To solve this problem, we propose to use multi-task learning to train a flexible depth model by treating each supported depth configuration as a task. We reduce the supported depth space for the aggressive model compression rate and propose an effective deterministic sub-network assignment method to eliminate the mismatch between training and inference in LayerDrop. %Specifically, we design two metrics to determine which sub-network assignment is good.  Experimental results on deep Transformer  show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and LayerDrop.    
"," The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices  may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method闁炽儲鏌￠幙鐜測erDrop.",143
"   Targeted sentiment analysis  involves jointly predicting entities which are the targets of an opinion, as well as the polarity expressed towards them . The TSA task, which is part of the larger set of fine-grained sentiment analysis tasks, can enable companies to provide better recommendations , as well as give digital humanities scholars a quantitative approach to identifying how sentiment and emotions develop in literature .  Although there have been many improvements to modelling TSA since the original CRF models , such as utilising Recurrent Neural Networks  , and treating the task as span prediction rather than a sequence labelling task , most of these have concentrated on making the best use of data annotated specifically for the task.  However, annotation for fine-grained sentiment is more taxing and tends to have lower inter-annotator agreement than document or sentence classification tasks . This leads to a lack of available high-quality training data, even for highly resourced languages and prevents TSA models from learning the complex, compositional phenomena which are necessary to correctly predict targeted sentiment in an end-to-end fashion.   We believe this lack of data for fine-grained sentiment analysis leads to TSA models that cannot learn effectively complex compositional phenomena that exists in language, thus making TSA models fragile to highly compositional language. It has also been shown that incorporating compositional information from negation or speculation detection improves sentence-level sentiment classification . Other supervised tasks, such as semantic role labelling , or document level sentiment analysis  have shown promise for improving fine-grained sentiment analysis. Further transfer learning from a self-supervised language-modelling task, commonly referred to as contextualised word representations , has also shown to greatly benefit fine-grained sentiment analysis . Based on this, in this paper, we wish to explore two research questions:           To this end, we propose a multi-task learning  approach to incorporate sources of negation and speculation information into a neural targeted sentiment classifier. We additionally compare our approach with MTL models that use part-of-speech tagging, dependency relation prediction, and lexical analysis as auxiliary tasks, following previous work . Furthermore, in order to overcome the lack of evaluative resources to investigate the effects of negation and speculation, we annotate two new challenge datasets which contain difficult negated and speculative examples.     We find that the MTL models are more robust than the single task learning ,  performing competitively on the majority of the standard datasets while significantly outperforming the STL models on the negation challenge datasets, and on average better than STL models on the speculation challenge datasets. Moreover, we show that when transfer learning is applied, using CWR, to both MTL and STL models, MTL models are no longer significantly better, but are still better on average for the negation challenge dataset and one of the speculation challenge datasets. This result suggests that transfer learning does incorporate some compositional information that is required for negated and speculative samples. However all results on the challenge datasets are considerably lower than the standard dataset, showing that more work is needed to make these models more robust to compositional language.     The contributions of the paper are the following:  [label=\roman*)] %\setlength       
","   The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning from a language model can improve performance on these challenge datasets. However the results indicate that there is still much room for improvement in making our models more robust to linguistic phenomena such as negation and speculation.",144
"  % making new tools useless if noone uses them efficiently The consensus that human activity caused the climate crisis  has led to the development of many tools and possible policy interventions, designed to minimize greenhouse gas emissions or mitigate negative impacts of climate change.  However, even the most promising tools to counter the climate crisis are futile, if they are not used. All important research efforts to mitigate the climate crisis are lost without an efficient international adaptation of tools and policies.  %promises of politicans don't lead to action  Strategies have to be adopted at a national level, following international cooperative guidelines such as the Sustainable Development Goals  or the Kyoto protocol . However, scientists, non-state actors\footnote{https://www.euronews.com/living/2018/12/21/ngos-sue-french-government-over-insufficient-climate-change-action}, and voters increasingly critique their government for insufficient action mitigating climate change . This suggests a gap between promises made by politicians and actual action taken: somewhere along the way ambitious promises for climate change mitigation have turned into careless discourse with insufficient measures taken.  %accountability shown to  prevent mismanagement Holding politicians accountable for their actions has been shown to be a major factor in preventing mismanagement, political corruption and misalignment of politician閳ユ獨 opinions and the public they are representing . %so we are working on improving the accountability to use existing tools Our work aims to provide the general public with a metric to assess if a candidate or a party is using their platform to discuss the topics related to climate change.   % the overall system In Section  we introduce a Multi-Source Topic Aggregation System  which increases transparency by providing an overview of topics discussed by politicians.  The large amount of publicly available documents are made transparent through the MuSTAS topic overview, which would otherwise be unattainable for the general public due to the amount of data. Through this transparency, decision-makers can be held accountable for their promises and claims by the general public, accelerating policies and the societal changes needed to mitigate and adapt to climate change.   %Using a the large amount of publicly available data are processed to asses how a politician uses their influence across channels and timelines.  % The research on multi-source LDA builds the scientific foundation In Section  we describe a novel multi-source hybrid latent Dirichlet allocation model which builds the scientific foundation for MuSTAS and forms the core of this research proposal. In Section  we outline how MuSTAS impacts climate change. % climate change impact  % Here be short paragraph about the structure of this proposal and the roles: MuSTAS is the larger scope, and topic modelling with multi-source hybrid LDA is the focus of research.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
","     Decades of research on climate have provided a consensus that human activity has changed the climate and we are currently heading into a climate crisis.     Many tools and methods, some of which utilize machine learning, have been developed to monitor, evaluate, and predict the changing climate and its effects on societies.      However, the mere existence of tools and increased awareness have not led to swift action to reduce emissions and mitigate climate change.     Politicians and other policy makers lack the initiative to move from talking about the climate to concrete climate action. % in an appropriate schedule allowing for mitigation of the potentially catastrophic changes.      In this work, we contribute to the efforts of holding decision makers accountable by describing a system which digests politicians' speeches and statements into a topic summary.     We propose a multi-source hybrid latent Dirichlet allocation model which can process the large number of publicly available reports, social media posts, speeches, and other documents of Finnish politicians, providing transparency and accountability towards the general public.",145
"   Word embeddings, continuous vectorial representations of words, have become a fundamental initial step in many natural language processing  tasks for many languages. In recent years,  their cross-lingual counterpart, cross-lingual word embeddings  ---maps of matching words across languages--- have been shown to be useful in many important cross-lingual transfer and modeling tasks such as machine translation , cross-lingual document classification  and zero-shot dependency parsing .    In these representations, matching words across different languages are represented by similar vectors. Following the observation of  that the geometric positions of similar words in two embedding spaces of different languages appear to be related by a linear relation, the most common method aims to map between two pretrained monolingual embedding spaces by learning a single linear transformation matrix. Due to its simple structure design and competitive performance, this approach has become the mainstream of learning CLWE .   Initially, the linear mapping was learned by minimizing the distances between the source and target words in a seed dictionary. Early work from  uses a seed dictionary of five-thousand word pairs. Since then, the size of the seed dictionary has been gradually reduced, from several-thousand to fifty word pairs , reaching a minimal version of only sharing numerals .  More recent works on unsupervised learning  have shown that mappings across embedding spaces can also be learned without any bilingual evidence . More concretely, these fully unsupervised methods usually consist of two main steps : an unsupervised step which aims to induce the seed dictionary by matching the source and target distributions, and then a pseudo-supervised refinement step based on this seed dictionary.  The system proposed by  can be considered  the first successful unsupervised system for learning CLWE. They first use generative adversarial networks  to learn a single linear mapping to induce the seed dictionary, followed by the Procrustes Analysis  to refine the linear mapping based on the induced seed dictionary. While this GAN-based model has competitive or even better performance compared to supervised methods on typologically-similar language pairs, it often exhibits poor performance on typologically-distant language pairs, pairs of languages that differ drastically in word forms, morphology, word order and other properties that determine how similar the lexicon of a language is. More specifically, their initial linear mapping often fails to induce the seed dictionary for distant language pairs . Later work from  has proposed an unsupervised self-learning framework to make the unsupervised CLWE learning more robust. Their system uses similarity distribution matching to induce the seed dictionary and stochastic dictionary induction to refine the mapping iteratively. The final CLWE learned by their system  performs better than the GAN-based system. However, their advantage appears to come from the iterative refinement with stochastic dictionary induction, according to . If we only consider the performance of a model induced only with distribution matching, GAN-based models perform much better. This brings us to our first conclusions, that a GAN-based model is preferable for seed dictionary induction.   Fully unsupervised mapping-based methods to learn CLWE rely on the strong assumption that monolingual word embedding spaces are isomorphic or near-isomorphic, but this assumption is not fulfilled in practice, especially for distant language pairs . Supervised methods are also affected by lack of isomorphism, as their performance on distant language pairs is worse than on similar language pairs. Moreover, experiments by  also demonstrate that the lack of isomorphism does not arise only because of the typological distance among languages, but it also depends on the quality of the monolingual embedding space.   Actually, if we replace the seed dictionary learned by an unsupervised distribution matching method with a pretrained dictionary, keeping constant the refinement technique, the final system becomes more robust .   All these previous results indicate that learning a better seed dictionary is a crucial step to improve unsupervised cross-lingual word embedding induction and reduce the gap between unsupervised methods and supervised methods, and that GAN-based methods hold the most promise to achieve this goal. The results also indicate that a solution that can handle the full complexity of  induction of cross-lingual word embeddings will  show improvements in both close and distant languages.  In this paper, we focus on improving the initial step of distribution matching, using GANs  . Because the isomorphism assumption is not observed in reality, we argue that a successful GAN-based model must not learn only one single linear mapping for the entire distribution, but must be able to identify mapping subspaces and learn multiple mappings. We propose a multi-adversarial learning method which learns different linear maps for different subspaces of word embeddings. %specifically for subspaces of the source word embeddings.   
","  Generative adversarial networks  have succeeded  in inducing  cross-lingual word embeddings ---maps of matching words across languages--- without supervision. Despite these successes, GANs' performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs' incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction show that this method improves performance over  previous single-mapping methods, especially for distant languages.",146
"  In recent years, the effectiveness of utilizing image data in tandem with a text corpus to improve the quality of machine translation has been a source of extensive investigation. Several proposals have been made to incorporate visual data, such as using a doubly-attentive decoder for image and text data , initializing the encoder or decoder hidden state with image features , and using a deliberation network approach to refine translations using image data . However, a common difficulty is the lack of publicly available multimodal corpora, particularly for English-Japanese translation tasks. Currently, two of the only available English-Japanese multimodal datasets are the Japanese extension of the Pascal sentences  and Flickr30k Entities JP , which is a Japanese translation of the Flickr30k Entities dataset .    In order to contribute to the current list of English-Japanese multimodal corpora, we propose a new multimodal English-Japanese corpus with comparable sentences. Comparable sentences are sentences that contain bilingual terms and parallel phrases that describe a similar topic, but are not direct translations . This data is of particular interest due to its natural prevalence across various areas of media. For example, e-commerce sites in different countries may have product descriptions for similar products in different languages, or social media users may comment about images in several different languages.    In this study, we created a large comparable training corpus by compiling the existing image captions from the MS-COCO  and STAIR  captioning datasets.  %By compiling the existing image captions from the MS-COCO  and STAIR  captioning datasets, we were able to create a large comparable training corpus that did not require translation.  Furthermore, for validation and testing purposes, we translated a small subset of MS-COCO captions that contain ambiguous verbs. The advantage of comparable sentences in relation to their available quantity can be clearly seen in Table , with our proposed corpus containing almost twice as many sentence pairs as Flickr30k Entities JP, the current largest parallel multimodal English-Japanese corpus.  As a benchmark of current multimodal NMT models on our corpus, we performed an English-Japanese translation experiment using several baseline models, which confirmed that current NMT models are not well suited to a comparable translation task. %o evaluate our proposed corpus, we performed an English-Japanese translation experiment on several baseline models, which confirmed that current NMT models are not well suited to a comparable translation task. However, we believe that our corpus can be used to facilitate research into creating multimodal NMT models that can better utilize comparable sentences.  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } {2.6ex}}   {0pt}} [1]{>{\raggedright\let} [1]{>{} [1]{>{\raggedleft\let}  [t!]              \toprule         \T\B Corpus & Source & Type & \T\B \# Images & \T\B \# Sentence Pairs\\          & \T\B MS-COCO/STAIR & Comparable  & \T\B  \\                 
"," Multimodal neural machine translation  has become an increasingly important area of research over the years because additional modalities, such as image data, can provide more context to textual data. Furthermore, the viability of training multimodal NMT models without a large parallel corpus continues to be investigated due to low availability of parallel sentences with images, particularly for English-Japanese data. However, this void can be filled with comparable sentences that contain bilingual terms and parallel phrases, which are naturally created through media such as social network posts and e-commerce product descriptions. In this paper, we propose a new multimodal English-Japanese corpus with comparable sentences that are compiled from existing image captioning datasets. In addition, we supplement our comparable sentences with a smaller parallel corpus for validation and test purposes. To test the performance of this comparable sentence translation scenario, we train several baseline NMT models with our comparable corpus and evaluate their English-Japanese translation performance. Due to low translation scores in our baseline experiments, we believe that current multimodal NMT models are not designed to effectively utilize comparable sentence data. Despite this, we hope for our corpus to be used to further research into multimodal NMT with comparable sentences.",147
"  %\todo[inline]{Why predicting hate-speech is important in general?}  %\todo[inline]{why detecting hate-speech is important in Yahoo news and Yahoo finance?}   %  %What is the problem? %Why is it interesting and important? %Why is it hard?  %Why hasn't it been solved before?  %What are the key components of my approach and results? Also include any specific limitations.  %Hatespeech is speech that ``intended to insult, offend, or intimidate a person because of some trait "". The occurrence of hatespeech has been increasing. It has become easier than before to reach a large audience quickly via social media, causing an increase of the temptation for inappropriate behaviors such as hatespeech, and potential damage to social systems. In particular, hatespeech interferes with civil discourse and turns good people away. Furthermore, hatespeech in the virtual world can lead to physical violence against certain groups in the real world\footnote{https://www.nytimes.com/2018/10/31/opinion/caravan-hate-speech-bowers-sayoc.html}\footnote{https://www.washingtonpost.com/nation/2018/11/30/how-online-hate-speech-is-fueling-real-life-violence}, so it should not be ignored on the ground of freedom of speech.  To detect hatespeech, researchers developed human-crafted feature-based classifiers , and proposed deep neural network architectures . %Online service providers also strive to combat the hatespeech through ranking algorithms, filtering, and suspending or deactivating user accounts. \textcolor{red}{However, blah blah blah}. However, they might not explore all possible important features for hatespeech detection, ignored pre-trained language model understanding, or proposed uni-directional language models by reading from left to right or right to left.   %--> 2. Other deep model for hatespeech detection: either didn't understand fully hateful context , or  ignore pretrained language model understanding and/or uni-directionally understanding language models by reading from left to right or right to left .  Recently, the BERT  model  has achieved tremendous success in Natural Language Processing % . The key innovation of BERT is in applying the transformer to language modeling tasks. %It proposed to do language modeling through two tasks: predicting masked words and predicting the next sentence. A BERT model pre-trained on these language modeling tasks forms a good basis for further fine-tuning on supervised tasks such as machine translation and question answering, .  Recent work on hatespeech detection  has applied the BERT model and has shown its prominent results over previous hatespeech classifiers. However, we point out its two limitations in hatespeech detection domain. First, the previous studies  have shown that a hateful corpus owns distinguished linguistic/semantic characteristics compared to a non-hateful corpus. For instance, hatespeech sequences are often informal or even intentionally mis-spelled, so words in hateful sequences can sit in a long tail when ranking their uniqueness, and a comment can be hateful or non-hateful using the same words .  %For example, ``n1gger'' in the sentence ``i am not a `n1gger' as you have indicated'' is non-hateful, but ``n1gger'' in ``you all are such a n1gger!'' is hateful.  For example, ``dick'' in the sentence ``Nobody knew dick about what that meant'' is non-hateful, but ``d1ck'' in ``You are a weak small-d1cked keyboard warrior'' is hateful \footnote{It is important to note that this paper contains hate speech examples, which may be offensive to some readers. They do not represent the views of the authors. We tried to make a balance between showing less number of hate speech examples and illustrating the challenges in real-world applications.}.  Thus, to better understand hateful vocabularies and contexts, it is better to pre-train on a mixture of both hateful and non-hateful corpora. Doing so helps to overcome the limitation of using BERT models pre-trained on non-hateful corpora like English Wikipedia and BookCorpus. Second, even the smallest pre-trained BERT ``base'' model contains 110M parameters. It takes a lot of computational resources to pre-train, fine-tune, and serve.  %There have been recent efforts reducing  Some recent efforts aim to reduce  the complexity of BERT model with the knowledge distillation technique such as DistillBert  and TinyBert . In these methods, a pre-trained BERT-alike model is used as a teacher model, and a student  model  is trained to produce similar output to that of the teacher model. Unfortunately, while their complexity is reduced, the performance is also degraded in NLP tasks compared to BERT. Another direction is to use cross-layer parameter sharing, such as ALBERT . However, ALBERT's computational time is similar to BERT, since the number of layers remains the same as BERT; likewise, its inference is equally expensive.  Based on the above observation and analysis, we aim to investigate whether it is possible to achieve a better hatespeech prediction performance than state-of-the-art machine learning classifiers, including classifiers based on publicly available BERT model, while significantly reducing the number of parameters compared with the BERT model. By doing so, we believe that performing pre-training tasks from the ground up and on a hatespeech-related corpus would allow the model to understand hatespeech patterns better and enhance the predictive results. However, while language model pretraining tasks require a large scale corpus size, available hatespeech datasets are normally small: only 16K$ %BERT is a modified transformer network architecture. Traditionally, many language tasks such as translation or question answering, are handled using recurrent neural networks, combined with the attention mechanism. This %reflects the fact that we tend to read a sentence from left to right. However, human also read words within context of other words, %some of them could be quite far apart, %instead of only from left to right or right to left in a mechanical way. Furthermore, recurrent network has a memory problem and can not handle long text, due to problems with vanishing or exploding gradient. In addition, it is intrinsically sequential, making the training process slow. Transformer network was proposed to solve these problems. In its setup, each word in the input text has visibility of all other words, through the use of multi-headed attention.   %It has been used %in a variety of NLP tasks as well as in other area such as image processing.     %One of the motivation of this paper is to investigate whether it is possible to achieve performance similar to, or better than the publicly available BERT models, but with smaller models. In doing so, we want to realize considerable saving in training and serving time. Another motivation is to see if it is possible to improve the BERT model further, by introducing changes to the model architecture. The third motivation is the following. The pretrained BERT models are based on % BooksCorpus and English Wikipedia. They have very different characteristics from the dataset of interest to us, which consists of users-generated comments in Yahoo News and Yahoo Finance. Consequently we believe that retraining a language model from scratch % should give us a model that understands % the language of our dataset better.  %\todo[inline]{talking about the limitation of BERT, like it is to complicated and heavy or has too many parameters. Then question is to build a better model, but with less number of parameters?}  The major contributions of our work are:  \squishend  % We organize the paper as followed. % We give related work in Section, and % define the problem we are solving formerly in Section. We present our approach in Section, and show experimental results in Section. We conclude our paper in Section with discussions and future work.  % 
"," We present our {HABERTOR} model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. {HABERTOR} inherits BERT's architecture, but is different in four aspects:  it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset;  it consists of  components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage;  it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and  it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that {HABERTOR} works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our {HABERTOR} is 4$ transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.  %The code and the pretrained models are available at .",148
"   %------------------Previous version------------------ %Since UNMT in low-resource domains is not yet an actively explored field, one may naively approach this problem by training a model on multiple domains and expect it to generalize on the unseen, low-resource domains, e.g., training the model on news and sports domains and evaluating on the biomedical domain. %However, due to domain mismatch, studied on supervised NMT, the model can show inferior performance. %------------------------------------------------------- Unsupervised neural machine translation  leverages unpaired monolingual corpora for its training, without requiring an already labeled, parallel corpus. Recently, the state of the art in UNMT has achieved comparable performances against supervised machine translation approaches. However, in the case of the translation of domain-specific documents, the monolingual data themselves are scarce, and collecting them involves high cost, still suffering from low NMT performance. For instance, a model trained with monolingual data in such a low-resource domain, say, the medical domain, can experience degraded translation quality due to overfitting.  %------------------Previous version------------------ %Another reasonable approach is transfer learning, which has been frequently used for domain adaption in the literature of supervised NMT and often showed improvements in the target domain. The model is pretrained with multiple domains and then finetuned with the new domain. However, this approach may suffer from overfitting  and catastrophic forgetting when given a small number of training data and a large domain gap in a downstream task. %-------------------------------------------------- Yet, UNMT for low-resource domains is not an actively explored field. One naive approach is to train a model on high-resource domains  while hoping it to generalize on an unseen low-resource domain  as well. However, it has been shown from recent studies on supervised NMT that a nontrivial domain mismatch can significantly cause low translation accuracy.  Another reasonable approach is transfer learning, or in particular domain adaptation, which has shown  performance improvements in the literature of supervised NMT. In this approach, the model is first pretrained using existing domains and then finetuned using the data in a new domain. However, this approach may suffer from overfitting and catastrophic forgetting due to a small number of training data and a large domain gap.  As an effective method for handling a small number of training data, meta-learning has shown its superiority in various NLP tasks, such as dialog generation, translation, and natural language understanding. However, to the best of our knowledge, it was not applied to tackle the UNMT tasks with a small number of training data, i.e., low-resource UNMT.   In response, this paper extends meta-learning approach for low-resource UNMT, called \toolnameMeta. The objective of \toolnameMeta is to find the optimal initialization for model parameters that can quickly adapt to a new domain even with only a small amount of monolingual data. To be specific, assuming that data from multiple source domains are available, which makes meta-learning applicable, we first pretrain the UNMT model with source domains based on \toolnameMeta and then finetune the model  using a target domain.   Moreover, we propose an improved meta-learning approach called \ourtoolname for low-resource UNMT by explicitly promoting common knowledge across multiple domains as well as generalizable knowledge from a particular domain to another. In particular, our proposed approach prevents the model from overfitting due to a small amount of training data in a new domain.   In summary, our contributions include the following.     %  %We show zero-shot performance to evaluate generalization ability for \ourtoolname, where \ourtoolname outperforms other methods.  % To the best of our knowledge, our work is the first to apply a meta-learning approach to UNMT tasks. Our proposed algorithms can quickly adapt to in-domain with only a few iteration steps. Both \toolnameMeta and \ourtoolname consistently outperform the baseline models up to 3 BLEU scores. Especially, \ourtoolname achieves promising results among others including \toolnameMeta. Besides, we show zero-shot performance to evaluate generalization ability for \ourtoolname, where \ourtoolname outperforms other methods.  %--------------------------------- % 闋冩﹥顫呮 姘╁牗妫撮瀽鎰冲妧闆 闇冨嫴饪洪灇 闈镐緛纾ら瀽鍡㈡緤 鐡垮婀㈣嚙 闇呮﹤濮 general 闋 feature闇屻倢婢 闉涘牕瀚.  % Although each domain is very distance each others in domain adaptation, they share some linguistic features, such as the grammar and basic words.     % Azam: To alleviate the aforementioned challenge, % Azam: To overcome this issue, many %  %   %  %   %Domains can be  %To overcome this issue, one simple approach is a domain mixing that aggregates high-resource and low-resource domains and train the model to adequately translate the low-resource domain. The other approach is a transfer learning that first pretrains on high-resource domains and fine-tunes the low-resource domain.  %Despite the remarkable success on neural machine translation ~, the performance of NMT drops substantially against traditional statistical machine translation  when the training data is scarce  %To overcome the scarcity of training data in languages, variants of multilingual translation approaches have been proposed. These approaches basically exploit high-resource knowledge by aggregating both high-resource and low-resource data to train one single model. The other approach is utilizing transfer learning that the model first pretrains on high-resource data and later fine-tunes on low-resource data. The similar manner follows for the domains as well.  %Moreover, few-shot learning and meta-learning arise in machine learning where both attempt to handle the data scarcity problem. In NMT, ~ re-formulates the model-agnostic meta-learning  algorithm to resolve the low-resource challenge for NMT.  %Although aforementioned approaches tackle the low-resource challenge, the data scarcity problem can still remain because following approaches require parallel data, and building a low-resource language pair  with specialized expertise is costly expensive. Hence, the recent research suggests to rely only on monolingual corpus instead of using parallel corpus. The various unsupervised NMT ~ studies show the reasonable performance comparison to supervised NMT. % 闋冩﹥顫呮 monolingual corpus 姣靛牗鐖涢渻 闆垮姙婢婇煰 鐡寸粖鏅爢鍕冲剨闉 闉氭鏌庨洰鐘惧灛闉涘牗娼 -鏀磋兂鏌涢瀼鎸 鏀撮附寮版皡鏃嶆緫闉 闉濅緟娼夐澗姗冪抽浖..?  %sufficient in-domain data to train the model; however, in real-world, collecting domain specific data requires substantial effort. %building a low-resource language pair  with specialized expertise is costly expensive  % 鏂撴粔鑺 闇屻倢鏌 MT闉 姘氭粚鐖犻灇 闈奉剢鐎 姘╁牗妫撮澗姗冾槬鏃矅顫 闇冨嫴瀚 闋冩﹥姒鹃灇 . % Machine learning 闉愭劤鍔闉 Data scarcity 姘嶈兂鐗 闉濇粔璧 % Domain translation闉 闉 娆锋埄娈ч爟婊岊潊  % unsuperivsed machine translation % data mixing, transfer learning % knowledge gets partially vanished  % 闆垮姙婢婇煰 鐡寸粖鏅 % 闋冩悡鑸堕爟姗佽荡闉欏嫶鏆ｉ澒 transfer learning mixing data 鑷ф瑬娼 姘氣晣鐭 闈奉剣姣 % 闋冩﹥顫呮 parallel setting 闉愭劤鍔 闉濅緟姣勯爟 % parallel 闆垮姙婢婇煰鐡ｇ殰 闈炬﹥顫栭爟姗佽荡鑷 闋屾﹤鎽 % monolingual corpus姣 闈奉剣姣勯爟姗傚 UNMT work闇屻倢婢 闈告繉绠 % unsupervised 闈瑰姜濮ら灇 鏂撴粔鑺抽湆銈屾煄 姣靛韩婢 闆存帥鏅炴瓎 % 闋冩﹥顫呮 monolingual corpus 姣靛牗鐖涢渻 闆垮姙婢婇煰 鐡寸粖鏅爢鍕冲剨闉 闉氭鏌庨洰鐘惧灛闉涘牗娼 -鏀磋兂鏌涢瀼鎸 鏀撮附寮版皡鏃嶆緫闉 闉濅緟娼夐澗姗冪抽浖..? % 鏀撮附螠闋冩﹥妫 闉栧崐螠闆 low-resource UNMT姣 meta-learning algorithm闉欒導顢 闊块附濮 姘氣晥鏅ラ灇 闉濇粚瀚 % 鏃姙銆 meta-nmt 闆茶導顑撶摽鑼у 闆笺倠顩 闉氭尗婀㈤浕 闉栧崐螠鏃拌導濮 unsupervised闉 %  multi doamin 鑷ф洢鈥 % 
"," Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a meta-learning algorithm for unsupervised neural machine translation  that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baseline models.",149
" Numerous entities are emerging everyday. The attributes of the entities are often noisy or incomplete, even missing.  In the field of electronic commerce, target attributes  of new products are often missing .  In medical analysis, attributes like transmission, genetics and origins of a novel virus are often unknown to people.  Even in DBpedia, a well-constructed and large-scale knowledge base extracted from Wikipedia, half of the entities contain less than 5 relationships .  %In KG construction area, KGs often suffer from incompleteness.  %For example, in DBpedia, a well-constructed and large-scale knowledge base extracted from Wikipedia, half of the entities contain less than 5 relationships . %Therefore,  A method that is capable of supplementing reliable attribute values for emerging entities can be highly useful in many applications.  %With the method to automatically extract attribute values for emerging entities, the eCommerce retailers are able to better serve the customers with updated information; the extracted medical attribute information of a novel virus can be organized to assist the understanding of the virus; the KG will be able to provide more complete information for users.    [ht]        Although information extraction methods have been extensively studied, the task of open attribute value extraction remains challenging. First, the emerging entities may have new attribute values that are absent in the existing KG. Under such circumstances, the prediction methods under the closed-world assumption and the methods that cannot utilize external information are not well suited due to their limited recalls. Second,  while web corpus can be used as a good resource to provide relatively updated and relevant articles for large varieties of emerging entities, %that are relatively complete and updated in a timely manner,  %considering the large variety of the emerging entities, the web corpus, which is relatively complete and updated in a timely manner, is able to provide a rich collection of relevant articles.  %However,  the articles retrieved from web corpus can be noisy and/or irrelevant, which in turn leads to a limited precision.  Finally, even when articles are relevant, the extracted answers might still be inaccurate due to the error-prone information extraction model.    To effectively filter out noisy answers that are obtained either due to the irreverent articles or the errors incurred by the information extraction system, we  %need to answer  pose the following two questions: First, how many articles should we collect from the enormous web corpus? Second,  how to select the most reliable value out of the pool of all the possible answers extracted from the articles?  There is no common answer to the first question that works for all triplets because of the inconsistent degrees of difficulties in finding the correct attribute values. The decision of when to stop querying more external articles needs to be made after successive evaluations of the candidate answers. Thus the decision making process is inherently sequential. %Thus, it is inherently a sequential decision making problem.   Reinforcement learning  is a commonly adopted method to deal with sequential decision problems and has been widely studied in the field of robotic and game . But there are not many researches on open attribute value extraction with RL.  One existing literature of RL-based method for value extraction is proposed by .  In their work, a RL framework is designed to improve accuracy of event-related value extraction by acquiring and incorporating external evidences.  However, their approach requires a great amount of context information about the specific event of interest during the training process.  It is not trivial to extend their framework for open attribute value extraction, because we would need to collect context words and train a new model with annotated data for each emerging attribute. Therefore, their framework cannot be generalized to open attribute value extraction task when various entities and attributes are involved.  While using the context words to construct the states in RL is not suitable in our task,  our solution is to leverage the rich, well-organized information in KG, which is not only informative but also generalizable.  %The knowledge from KG  Such information can be leveraged in answer comparisons, which addresses our second question. For example, to fill the incomplete triplet  iPhone 11, display resolution, ?, from the KG we may find that the attribute values  ``display resolutions"" of an entity that is under category ``Phone"" is commonly expressed in the format of ``xxx by xxxx Pixels"", where x stands for some digit. The typical instances of the attribute values for entities under the same category provide valuable background information.   In this paper, we propose a knowledge-guided RL framework to perform open attribute value extraction.  The RL agent is trained to make good actions for answer selection and stopping time decision.  Our experiments show that the proposed framework significantly boosts the extraction performance.  To the best of our knowledge, we are the first to integrate KG in a RL framework to perform open attribute value extraction %use KG to guide the RL-based sequential decision for open attribute value extraction.  %The experiment results demonstrate that our approach improves the extraction performances substantially. In summary, our contribution are in three folds:      
"," Open attribute value extraction for emerging entities is an important but challenging task.  A lot of previous works formulate the problem as a question-answering  task.  While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improving extraction accuracy. Knowledge graph , which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning  framework for open attribute value extraction.  Informed by relevant knowledge in KG, we trained a deep Q-network  to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8\%.",150
"  % NMT is good but needs lots of parallel data + we should exploit mono data more Neural machine translation  using sequence to sequence architectures  has become the dominant approach to automatic machine translation. While being able to approach human-level performance , it still requires a huge amount of parallel data, otherwise it can easily overfit. Such data, however, might not always be available. At the same time, it is generally much easier to gather large amounts of monolingual data, and therefore, it is interesting to find ways of making use of such data. The simplest strategy is to use backtranslation , %but it can be rather costly since it requires training another model in the opposite translation direction and then creating the source-side synthetic sentences by translating the target-side monolingual corpus. but it can be rather costly since it requires training a model in the opposite translation direction and then translating the monolingual corpus.  % We introduce the compositionality  It was suggested by  that during the development of a general human-like AI system, one of the desired characteristics of such a system is the ability to learn in a continuous manner using previously learned tasks as building blocks for mastering new, more complex tasks. %by combining the knowledge learned from the previously learned simpler tasks. Until recently, continuous learning of neural networks was problematic, among others, due to the catastrophic forgetting . Several methods were proposed , however, %they mostly focused on preserving the knowledge of each task learned by the whole network. they mainly focus only on adapting the whole network  to new tasks while maintaining good performance on the previously learned tasks.  % Summary of our method using EWC %\XXX{toto mozna posunout za nasledujici odstavec + jak resime jejich nedostatky} In this work, we present an unsupervised pretraining method for NMT models using Elastic Weight Consolidation . First, we initialize both encoder and decoder with source and target language models respectively. Then, we fine-tune the NMT model using the parallel data. To prevent the encoder and decoder from forgetting the original language modeling  task, we regularize their weights individually using Elastic Weight Consolidation based on their importance to that task. Our hypothesis is the following: by forcing the network to remember the original LM tasks we can reduce overfitting of the NMT model on the limited parallel data. %\XXX{Ukazujeme, ze metoda funguje, je rychlejis + mame odvozeno, ze by mela fungovat i pro podsite} %\XXX{Zminit rovnou strucne prinosy?}  % Summary of the method we used as a comparison We also provide a comparison of our approach with the method proposed by . They also suggest initialization of the encoder and decoder with a language model. However, during the fine-tuning phase they use the original language modeling objectives as an additional training loss in place of model regularization. Their approach has two main drawbacks: first, during the fine-tuning phase, they still require the original monolingual data which might not be available anymore in a life-long learning scenario. Second, they need to compute both machine translation and language modeling losses which increases the number of operations performed during the update slowing down the fine-tuning process. Our proposed method addresses both problems: it requires only a small held-out set to estimate the EWC regularization term and converges 2-3 times faster than the previous method.\footnote{The speedup is with regard to the wall-clock time. In our experiments both EWC and the LM-objective methods require similar number of training examples to converge.}   %Intro to compositionality %Compositional learning + using previosly learned elementary knowledge to learn more complex model   %Avoiding catastrophic forgetting as key to continual learning and compositionality -> choice of EWC  %Benefits of compositionality in greater scope  + why NMT + LM pretrain? %It is a first step in our ongoing reseach  %The paper is structured as following...  
","   This work presents our ongoing research of unsupervised pretraining in neural machine translation . In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation  to avoid forgetting of the original language modeling tasks.   We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives.   %We compare the EWC regularization with the previous work that uses language modeling objectives from the original task for model regularization.      The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work. However,   the model converges 2-3 times faster and does not require the original unlabeled training data during the fine-tuning stage.      In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-to-right language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context.      %%% POZNAMKY %%%   % Analyza Fisher Information   % - Self-attention projekce are more important than the Feedforward layers   % - Self-attention:   %     - output and value projections are more important at the higher layers   %     - key and query projections are more important at lower layers   % ...      % Previous work    % - requires orig. unlabeled data for MT training -> EWC can estimate Empirical Fisher on small  heldout data   % - is effective even when the orig. and new tasks differ   %  and then using this pretrained encoder in MT  -> EWC is bad at this      % Our work :   % - has nice mathematical definition    % - faster convergence in time    % - works only with decoder  -> little worse than LM obj.   % - method works when task are similar in nature    % - how deep should the unlabeled-data-pretrained enc-dec should be?   %       % why only left-context? previous work shows that with transformer, the drop in performance is not that big + it is much easier to implement       % Future work :   % - Investigate complementarity of EWC and LM obj.    % - Investigate the learning rate schemes    % - Investigate the method in the multimodal/multisource scenario   % - Investigate the method",151
"   Even though machine translation  has greatly improved with the emergence of neural machine translation   and more recently the Transformer architecture , there remain challenges which can not be solved by using sentence-level NMT systems. Among other issues, this includes the problem of inter-sentential anaphora resolution  or the consistent translation across a document , for which the system inevitably needs document-level context information.  In recent years, many works have focused on changing existing NMT architectures to incorporate context information in the translation process . However, often times results are reported only on very specific tasks , making it difficult to assess the potential of the different methods in a more general setting. This, together with the fact that big improvements are typically reported on low resource tasks, gives the impression that document-level NMT mostly improves due to regularization rather than from leveraging the additional context information. In this work we want to give a more complete overview of the current state of document-level NMT by comparing various approaches on a variety of different tasks including an application-oriented E-commerce setting. We discuss both, widely used performance metrics, as well as highly task-specific observations.  Another important aspect when talking about document-level NMT is the applicability in ``real life"" settings. There, when faced with a low resource data scenario, back-translation is an established way of greatly improving system performance . However, to the best of our knowledge, the effect of back-translation data obtained and used by context-aware models has never been explored before. The main contributions of this paper are summarized below:        \setlength        
","  Context-aware neural machine translation  is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT.  We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.",152
" Knowledge Graphs  like Freebase, NELL and Wikidata are extremely useful resources for NLP tasks, such as information retrieval, machine reading, and relation extraction. A typical KG is a multi-relational graph, represented as triples of the form . Obviously, for query , referring to the organization-related references would be more beneficial.   To address the above issues, we propose an \underline{A}daptive \underline{A}ttentional \underline{N}etwork for \underline{F}ew-Shot KG completion , a novel paradigm that takes dynamic properties into account for both entities and references. Specifically, given a task relation with its reference/query triples, FAAN proposes an adaptive attentional neighbor encoder to model entity representations with one-hop entity neighbors. Unlike the previous neighbor encoder with a fixed attention map in, we allow attention scores dynamically adaptive to the task relation under the translation assumption. This will capture the diverse roles of entities through varied impacts of neighbors. Given the enhanced entity representations, FAAN further adopts a stack of Transformer blocks for reference/query triples to capture multi-meanings of the task relation. Then, FAAN obtains a general reference representation by adaptively aggregating the references, further differentiating their contributions to different queries. As such, both entities and references can capture their fine-grained meanings, and render richer representations to be more predictive for knowledge acquisition in the few-shot scenario.  The contributions of this paper are three-fold:   We propose the notion of dynamic properties in few-shot KG completion, which differs from previous paradigms by studying the dynamic nature of entities and references in the few-shot scenario.   We devise a novel adaptive attentional network FAAN to learn dynamic representations. An adaptive neighbor encoder is used to adapt entity representations to different tasks. A Transformer encoder and an attention-based aggregator are used to adapt reference representations to different queries.   We evaluate FAAN in few-shot link prediction on benchmark KGs of NELL and Wikidata. Experimental results reveal that FAAN could achieve new state-of-the-art results with different few-shot sizes.  
","   Few-shot Knowledge Graph閼 completion is a focus of current research, where each task aims at querying閼辩惮nseen facts閼辩郸f a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at \url{https://github.com/JiaweiSheng/FAAN}.",153
" Automatic summarization is a fundamental task in Natural Language Processing, which aims to condense the original input into a shorter version covering salient information and has been continuously studied for decades . Recently, online multi-speaker dialogue/meeting has become one of the most important ways for people to communicate with each other in their daily works. Especially due to the spread of  COVID-19 worldwide, people are more dependent on online communication. In this paper, we focus on dialogue summarization, which can help people quickly grasp the core content of the dialogue without reviewing the complex dialogue context.   Recent works that incorporate additional commonsense knowledge in the dialogue generation  and dialogue context representation learning  show that even though neural models have strong learning capabilities, explicit knowledge can still improve response generation quality.   It is because that a dialog system can understand conversations better and thus respond more properly if it can access and make full use of large-scale commonsense knowledge. However, current dialogue summarization systems  ignore the exploration of commonsense knowledge, which may limit the performance. In this work, we examine the benefit of incorporating commonsense knowledge in the dialogue summarization task and also address the question of how best to incorporate this information. Figure  shows a positive example to illustrate the effectiveness of commonsense knowledge in the dialogue summarization task.  Bob asks Tom for help because his car has broken down. On the one hand, by introducing commonsense knowledge according to the {, we can know that Bob expects Tom to {ialogue Heterogeneous Graph Network  for incorporating commonsense knowledge by constructing the graph including both utterance and knowledge nodes. Besides, our heterogeneous graph also contains speaker nodes at the same time, which has been proved to be a useful feature in dialogue modeling. In particular, we equip our heterogeneous graph network with two additional designed modules. One is called message fusion, which is specially designed for utterance nodes to better aggregate information from both speakers and knowledge. The other one is called node embedding, which can help utterance nodes to be aware of position information. Compared to homogeneous graph network in related works , we claim that the heterogeneous graph network can effectively fuse information and contain rich semantics in nodes and links, and thus more accurately encode the dialogue representation.   We conduct experiments on the SAMSum corpus , which is a large-scale chat summarization corpus. We analyze the effectiveness of integration of knowledge and heterogeneity modeling. The human evaluation also shows that our approach can generate more abstractive and correct summaries. To evaluate whether commonsense knowledge can help our model better generalize to the new domain, we also perform zero-shot setting experiments on the Argumentative Dialogue Summary Corpus , which is a debate summarization corpus. In the end, we give a brief summary of our contributions:  We are the first to incorporate commonsense knowledge into dialogue summarization task.  We propose a D-HGN model to encode the dialogue by viewing utterances, knowledge and speakers as heterogeneous data.  Our model can outperform various methods.  
"," Abstractive dialogue summarization is the task of capturing the highlights of a dialogue and rewriting them into a concise version. In this paper, we present a novel multi-speaker dialogue summarizer to demonstrate how large-scale commonsense knowledge can facilitate dialogue understanding and summary generation. In detail, we consider utterance and commonsense knowledge as two different types of data and design a Dialogue Heterogeneous Graph Network  for modeling both information. Meanwhile, we also add speakers as heterogeneous nodes to facilitate information flow. Experimental results on the SAMSum dataset show that our model can outperform various methods. We also conduct zero-shot setting experiments on the Argumentative Dialogue Summary Corpus, the results show that our model can better generalized to the new domain.",154
" %\yy{para 1: problem is important, para 2: temporal graph, existing systems, para 3: neural networks, para 4: why difficult: lack of training data, para 5: what do we do} [ht]               %\yy{this is a comment} %\yyc{before correction}{after correction}   %The flow of time is used to chain narratives, reason about causes and effects of events, form a deeper understanding of the past, and postulate the future. Temporal reasoning is crucial for analyzing the interactions among complex events and producing   coherent interpretations of text data . There is a rich body of research on the use of temporal information in a variety of important application domains, including topic detection and tracking, information extraction, parsing of clinical records , discourse analysis, and question answering. %\yy{Aman: Please update the cites based on some quick Google search on temporal reasoning/expressions in IE/TDT/medical .} %Motivated by its ubiquity in text understanding, we undertake the task of extracting temporal graphs from documents. %and a rich understanding of temporal aspects of a document helps humans in reading comprehension.   %Temporal reasoning also plays a critical role in downstream natural language processing  They either admit a lot of noisy events ; we will check this % in the camera-ready version and ask you to change it back.  \TeX}  \title{Neural Language Modeling for Contextualized Temporal Graph Generation}   \date{}                
"," This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity  of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics.\footnote{Code and pre-trained models available at}",155
" Building dialog systems typically requires a large collection of conversation logs that a model can use as training data. Crowd-sourcing is a popular method for generating such data-sets and depending on the aspect of dialog modeling being studied, crowd-sourced workers may be asked to annotate existing chat logs for intents and dialog acts, create dialog summaries, converse with each other based on a script or converse to accomplish tasks or goals etc. For instance, to create datasets for task oriented dialogs, crowd-sourced workers may be provided with a { and an {.   The { worker provides information to the user by querying a knowledge base , if required. Together, the two workers interact with each other via natural language to generate conversations that can involve booking restaurant tables, making train reservations, calling a taxi etc. However, creating large crowd-sourced datasets can be time consuming and expensive.           
"," Popular task-oriented dialog data sets such as MultiWOZ  are created by providing crowd-sourced workers a { and an { instruction, expressed in natural language, which described the task that needed to be accomplished. Crowd-sourced workers played the role of a { to generate dialogs that can involve booking restaurant tables, making train reservations, calling a taxi etc.  In this paper, we present a data creation strategy that uses the pre-trained language model, GPT2 , to { bot and an agent bot.  We train the simulators using a smaller percentage of actual crowd-generated conversations and their corresponding goal instructions. We demonstrate that by using the simulated data, we achieve significant improvements in both low-resource setting as well as in overall task performance. To the best of our knowledge we are the first to present a model %this is the first model proposed  for generating entire conversations by simulating the crowd-sourced data collection process. %To the best of our knowledge we are the first to use inter-bot conversation logs to improve the performance of task oriented dialog systems.",156
" Multilingual machine translation , which can serve multiple language pairs with a single model, has attracted much attention. In contrast to bilingual MT systems which can only serve one single language pair, multilingual models can serve  language pairs  .  The amount of available training data can differ a lot across language pairs and the majority of available MT training data is English-centric  which in practice means that most non-English language pairs do not see a single training example when training multilingual models .  As a consequence, the actual performance of language pairs that do not include English on the source or target side lags behind the ones with large amounts of training data. Further, when increasing the number of languages, it gets  impractical to gather training data for each language pair and  challenging to find the right mix during training. Which is why models tasked with direct translation between non-English pairs either resort to bridging  through a pivot language , or make use of synthetic parallel data   or study the problem under zero-shot settings .     In this study, we make use of the potential pre-existing multi-way property in the training corpora and generate as many direct training examples from pre-existing English-centric training data. If we can find training examples for each language pair in a multilingual mix, we call this model complete Multilingual Neural Machine Translation . cMNMT is then trained on all bilingual pairs between source and target languages by utilizing multi-way aligned training examples that consist of translations of the same sentence into multiple languages. We resurface multi-way aligned training examples by aligning training examples from different language pairs when either their source or target sides are identical .  To make use of this data, the model samples a source and target language from the set of multi-way aligned corpus during training, which allows the model to see language pairs where originally no training data existed . As our experiments support, this method enables us to get access to training data for all tested language pairs ). We will show that it is possible to generate a complete graph for at least a 6-language WMT setup. Some of the WMT training data is multi-way parallel by construction. Nevertheless, we show that we also find many training examples where the source and target origin from different sources. We further show on our 112 languages internal dataset, that we can find sufficient training data for over 12,000 language pairs by only providing 111 English-centric training corpora. This result indicates that it is possible to generate direct training data for many language pairs without the need for crawling new training examples. Our experiments suggest that before falling back to methods like zero-shot translation, you should investigate the structure of your pre-existing training data.  To address the problem of finding the right mix of examples from different language pairs during training, we further introduce a hierarchical sampling strategy that is language-specific . In addition to fixing some chronic issues of MNMT , the proposed sampling strategy efficiently ensures all source-target pairs are covered.  Experiments demonstrate that we can train a cMNMT model on a 30-language-pair WMT setup that outperforms bilingual and multilingual baselines as well as bridging on all non-English language pairs. We further show that the performance of the English language pairs stay stable and do not suffer from the changes in both the training data and the new training data sampling strategy. Furthermore, we share experiments at scale by demonstrating that we can train a cMNMT model that can serve  12,432 language pairs.  Our contribution is three-fold:         
"," Multilingual Neural Machine Translation  models are commonly trained on a joint set of bilingual corpora which is acutely English-centric . While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora , and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples . We set out to study the use of multi-way aligned examples to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation  and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 111$*$112=12,432 language pairs that provides competitive translation quality for all language pairs.",157
"  Machine Translation  has shown impressive progress in recent years. Neural architectures have greatly contributed to this  improvement, especially for languages with abundant training data.  This progress creates novel challenges for the evaluation of machine translation,  both for human and automated evaluation  protocols.  Both types of evaluation play an important role in machine translation. While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations  therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system development.  The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics. Most metrics such as \BLEU and TER measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed.  Orthogonal to the work of building improved metrics,  hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human  `translationese` effects. These standard references might favor systems which excel at reproducing these effects, independent of the underlying translation quality. They showed that better correlation between human and automated evaluations could be obtained when replacing standard references with paraphrased references, even when still using surface overlap metrics such as BLEU~. The novel references, collected by asking linguists to paraphrase standard references, were shown to steer evaluation away from rewarding translation artifacts. This improves the assessment of alternative, but equally good translations.  Our work builds on the success of paraphrased translations for evaluating  existing systems, and asks if different design choices could have been made when designing a system with such an evaluation protocol in mind. This examination has several potential benefits: it can help identify choices which improve BLEU on standard references but have limited impact on final human evaluations; or those that result in better translations for the human reader, but worse in terms of standard reference BLEU. Conversely, it might turn out that paraphrased references are not robust enough to support system development due to the presence of `metric honeypots': settings that produce poor translations, but which are nevertheless assigned high BLEU scores.  To address these points, we revisit the major design choices of the best EnglishGerman system from WMT2019 step-by-step, and measure their impact on standard reference BLEU as well as on paraphrased BLEU. This allows us to measure the extent to which steps such as data cleaning, back-translation, fine-tuning, ensemble decoding and reranking benefit standard reference BLEU more than paraphrase BLEU. Revisiting these development choices with the two metrics results in two systems with quite different behaviors. We conduct a human evaluation for adequacy and fluency to assess the overall impact of designing a system using paraphrased BLEU.  Our main findings show that optimizing for paraphrased BLEU is advantageous for human evaluation when compared to an identical system optimized for standard BLEU. The system optimized for paraphrased BLEU significantly improves WMT newstest19 adequacy ratings  and fluency ratings  despite scoring 5 BLEU points lower on standard references.  
","  Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by . When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal  ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.",158
" 	 	 	% 	% The following footnote without marker is needed for the camera-ready 	% version of the paper. 	% Comment out the instructions  and uncomment the 8 lines 	% under ""final paper"" for your variant of English. 	%  	 	Machine Reading Comprehension  has made significant strides with an array of neural models rapidly approaching human parity on some benchmarks such as SQuAD . However, existing methods are still in their infancy at the level of cognitive intelligence. Recently, brain science and psychology provide an important basis for the development of brain-like computing and the simulation of human perception, thinking, understanding, and reasoning abilities . 	 	Thinking is the generalization and indirect reflection of the human brain on the nature, interrelationships and internal regularities of objective things . Two types of thinking are complementary in psychology: inertial thinking 閳 from a previous to a subsequent stimulus 閳 and reverse thinking 閳 from a subsequent to a previous stimulus . Inertial thinking  is a conventional way of thinking, which thinks and solves problems from the previous ideas. 	% It is more likely to form a stereotyped thinking, hinder the development of thinking, and may even lead to rigidity of thinking if using this single way for a long time. 	Reverse thinking  is a creative way of thinking, opposite to the inertial thinking.  	% It can often break through the conventional constraints and obtain greater innovation .  	Specifically, in the MRC task, the two types of thinking can be regarded as a process which reasons from questions  to answers . For example, as shown in Fig. , we can get the answer easily by locating the entities pregnant wowen and loquat. Contrarily, the generative question, which can be reasoned by reading the answer and passage, describes two aspects, including can pregnant women eat loquat and what is the benefit to eat loquat for pregnant women. We hope that this ability of reverse reasoning can improve performance on reading comprehension tasks. 	 	 	Previous methods  only consider a obverse logical relationship, which is based on the given question and the passage. They ignore the reverse relationship between the given passage and the answer. Although the work  proposes a joint model that both asks and answers questions, it couples all the knowledge rather than decopuling modules, which is consistent with the concept of psychology. Similarly, we hypothesize that the ability of reverse reasoning can help models achieve better QA performance. This is motivated partly by observations made in psychology that devising questions while reading can  	% increase scores on comprehension tests.It can  	help students improve in reader-based processing of text . 	 	%In the real world, a fast learning trick is to peek at the answer first. In other words, humans often begin with answers and passages when they encounter unsolvable problems. 	%% Then they attempt to understand why the answer is like this.  	%Then they inadvertently infer the question based on the answer and passage from the reverse side. Obtaining the answer in advance is equivalent to giving a strong supervision signal or some additional clues that does not directly exist in the passage and may require reasoning. This kind of psychological behavior about humans is actually a way of reverse thinking, which considers the problem from the opposite direction and infers the reason based on the conclusion . 	 	Therefore, insights into solutions to the problem can be gained from human cognitive processes. Complementary Learning Systems Theory   suggests that the human brain contains complementary learning systems that support the simultaneous use of many sources of information as we seek to understand an experienced situation. One of the systems acquires an integrated system of knowledge gradually through interleaved learning, including our knowledge of the meanings of words, the properties of frequently-encountered objects, and the characteristics of familiar situations. It is just like inertial thinking that learns relationships between different things in the real world for a long time. The other system is a fast learning system similar to reverse thinking, which is targeted to focus on stimulating and enhancing infrequently-utilized circuit areas in the brain from another unusual perspective. 	 	In this paper, we propose the Bi-directional Cognitive Knowledge Framework . And the corresponding Bi-directional Cognitive Thinking Network  is designed to validate the effectiveness of the reverse thinking, as shown in Fig., which will be introduced in detail in Section . 	% explain framework 	%  	 	% In this paper, the proposed method that ... simulates the process of fast learning. 	 	% 	 	 	 	% The gray ovals form an embedding  of specific information 	 	% In order to validate the effectiveness of the reverse thinking, we proposed a Cognitive Bi-directional Thinking Network  corresponding to the Cognitive Bi-directional Thinking Framework, which will be introduced in detail in Section . 	 	The contributions can be summarized as follows: 	 	 		 	 	
"," 		We propose a novel Bi-directional Cognitive Knowledge Framework  for reading comprehension from the perspective of complementary learning systems theory. It aims to simulate two ways of thinking in the brain to answer questions, including reverse thinking and inertial thinking. To validate the effectiveness of our framework, we design a corresponding Bi-directional Cognitive Thinking Network  to encode the passage and generate a question  given an answer  and decouple the bi-directional knowledge. The model has the ability to reverse reasoning questions which can assist inertial thinking to generate more accurate answers. Competitive improvement is observed in DuReader dataset, confirming our hypothesis that bi-directional knowledge helps the QA task. The novel framework shows an interesting perspective on machine reading comprehension and cognitive science.",159
"     % Demonstrating intelligent behavior in complex environments requires agents that can reason about entities and their relationships, and identify regularities in structured data which can help predict the properties-of and relationships-between entities.  % Understanding natural language in realistic settings requires models that can reason about the interactions between content and context, model the dependencies between different textual elements and leverage information about authors when interpreting their content.  For example, when analyzing interactions in a social network, leveraging information about users' social behavior can help identify similarities in the contents of posts they author. Dealing with this type of relational data requires making predictions over multiple, often inter-dependent, variables.    Understanding natural language interactions in realistic settings requires models that can deal with noisy textual inputs, reason about the dependencies between different textual elements and leverage the dependencies between textual content and the context from which it emerges. Work in linguistics and anthropology has defined context as a frame that surrounds a focal communicative event and provides resources for its interpretation . % introduced the term contextualization cues as signalling mechanisms in communication that add to the shared understanding between the participants, into relationships, the situation, and the environment of the conversation    %Say something about debate networks and add some references. As a motivating example, consider the interactions in the debate network  described in Fig.. Given a debate claim , and two consecutive posts debating it , we define a textual inference task, determining whether a pair of text elements hold the same stance in the debate }). This task is similar to other textual inference tasks which have been successfully approached using complex neural representations. In addition, we can leverage the dependencies between these decisions.  For example, assuming that one post agrees with the debate claim }}), and the other one does not }}), the disagreement between the two posts can be inferred:  {}. Finally, we consider the social context of the text. The disagreement between the posts can reflect a difference in the perspectives their authors hold on the issue. While this information might not be directly observed, it can be inferred using the authors' social interactions and behavior. % Given the principle of social homophily, stating that people with strong social ties are likely to hold similar views and authors' perspectives can be captured by representing their social interactions. Exploiting this information requires models that can align the social representation with the linguistic one.  Motivated by these challenges, we introduce \DRAIL, a Deep Relational Learning framework, which uses a combined neuro-symbolic representation for modeling the interaction between multiple decisions in relational domains. Similar to other neuro-symbolic approaches our goal is to exploit the complementary strengths of the two modeling paradigms. Symbolic representations, used by logic-based systems and by probabilistic graphical models, are interpretable, and allow domain experts to directly inject knowledge and constrain the learning problem. Neural models capture dependencies using the network architecture and are better equipped to deal with noisy data, such as text. However, they are often difficult to interpret and constrain according to domain knowledge.   Our main design goal in \DRAIL is to provide a generalized tool, specifically designed for NLP tasks. Existing approaches designed for classic relational learning tasks, such as knowledge graph completion, are not equipped to deal with the complex linguistic input. While others are designed for very specific NLP settings such as word-based quantitative reasoning problems or aligning images with text. We discuss the differences between \DRAIL and these approaches in Section.  % While the examples in this paper focus on modelings various argumentation mining tasks and their social and political context, the same principles can be applied to wide array of NLP tasks with different contextualizing information, such as images that appear next to the text, or prosody when analyzing transcribed speech, to name a few examples. %TODO: explain why DRAIL is specifically useful for NLP compared to other languages. We don't do the same type of evaluation  as we are interested in working with raw entities.     %  Entities in \DRAIL are either human-interpretable discrete entities , which we refer to as symbols, or raw entities that have a complex internal structure which cannot be easily represented as a symbol . This view allows us to define two conceptual learning tasks: relations connecting raw and symbolic entities , and relations connecting raw inputs to each other, which define inference tasks .   % \DRAIL uses a declarative language for defining deep relational models. Similar to other declarative languages, it allows users to inject their knowledge by specifying dependencies between decisions using first-order logic rules, which are later compiled into a factor graph with neural potentials.   % In addition to probabilistic inference, \DRAIL also models dependencies using a distributed knowledge representation, denoted \relnets, which provides a shared representation space for entities and their relations, trained using a relational multi-task learning approach. This provides a mechanism for explaining symbols, and aligning representations from different modalities.  %Introduce the s-s, r-r, s-r, distinction as a way to support classification, textual inference, and probabilistic inference. Following our running example, ideological standpoints, such as \PRED{Liberal} or \PRED{Conservative}, are discrete entities embedded in the same space as textual entities and social entities. These entities are initially associated with users, however using \relnets this information will propagate to texts reflecting these ideologies, by exploiting the relations that bridge social and linguistic information . % In the resulting shared embedding space, we can explain these ideological standpoints in terms of users holding them, or texts that express them.%}).    %TODO: what are the research questions    %TODO - explain the difference in task from DRAIL's perspective - argument relations inside a single text, analyzing discussions - the simple case, discussed in the literature, where we predict a symbol , and the debate.org setup where we combine textual inference  with soclia linfo    To demonstrate \DRAIL's modeling approach, we introduce the task of open-domain stance prediction with social context, which combines social networks analysis and textual inference over complex opinionated texts, as shown in Fig. . %Unlike traditional stance prediction tasks, where the prediction problem is defined over a fixed set of issues  ~ , we go beyond coarse-grained definitions, and delve into the specific arguments and questions of each discussion, as shown in Fig. . We follow the intuition that debates are part of a broader online conversation, involving multiple people that contribute or express their support for the different views, and explicitly model these interactions.  % AugensteinD16-1084,P18-2123,C18-1316} %TODO: add some discussion about qualitative evaluation % We complement our evaluation of \DRAIL with two additional tasks, issue-specific stance prediction, where we identify the views expressed in debate forums with respect to a set of fixed issues, and argumentation mining, a document-level discourse analysis task.    %We demonstrate \DRAIL's modeling approach over three challenging problems. Argumentation mining, a document-level discourse analysis task. Debate stance prediction, identifying the views expressed-in, and interactions-between, debate forum posts. Finally, we introduce a new problem, open-domain stance prediction with social context, which combines social networks analysis and textual inference over complex opinionated texts.  In all three tasks we evaluate different modeling choices, obtaining competitive results.    %TODO: contributions %Our contributions are summarized as follows: % %[wide, labelwidth=!, labelindent=0pt] %        %Unrealted TODO: add a discussion about globally normalized RELNETs- the constraints and the multiple objectives shape them.  %homophily, %, This phenomenon was previously used to help overcome language variation issues   % political-social representations %network embedding:we learn a graph embedding, a different way to define social context %graphical models way  
"," Building models for realistic natural language tasks requires dealing with long texts and accounting for complicated structural dependencies. Neural-symbolic representations have emerged as a way to combine the reasoning capabilities of symbolic methods, with the expressiveness of neural networks. However, most of the existing frameworks for combining neural and symbolic representations have been designed for classic relational learning tasks that work over a universe of symbolic entities and relations. In this paper, we present \DRAIL, an open-source declarative framework for specifying deep relational models, designed to support a variety of NLP scenarios. Our framework supports easy integration with expressive language encoders, and provides an interface to study the interactions between representation, inference and learning.",160
"   End-to-end neural models have emerged in recent years as the dominant approach to a wide variety of sequence generation tasks in natural language processing, including speech recognition, machine translation, and dialog generation, among many others. While highly accurate, these models typically operate by outputting tokens from a predetermined symbolic vocabulary, and require integration into larger pipelines for use in user-facing applications such as voice assistants where neither the input nor output modality is text.  In the speech domain, neural methods have recently been successfully applied to end-to-end speech translation , in which the goal is to translate directly from speech in one language to speech in another language. We propose to study the analogous problem of in-image machine translation. Specifically, an image containing text in one language is to be transformed into an image containing the same text in another language, removing the dependency of any predetermined symbolic vocabulary or processing.   In-image neural machine translation is a compelling test-bed for both research and engineering communities for a variety of reasons. Although there are existing commercial products that address this problem such as image translation feature of Google Translate the underlying technical solutions are unknown. By leveraging large amounts of data and compute, end-to-end neural system could potentially improve overall quality of pipelined approaches for image translation. } employ a traditional pipelined approach consisting of separate optical character recognition, translation, and image rendering steps.\todo{orhanf will check this with mobile team. Elman: commented out as suggested by mobile wordlens team. technical solution of wordlens is not publicly available hence this sentence is a bit speculative}  Combining all these components into a single end-to-end neural system could help reduce cascading errors and improve overall translation quality, leveraging large amounts of data and compute. \fi Second, and arguably more importantly, working directly with pixels has the potential to sidestep issues related to vocabularies, segmentation, and tokenization, allowing for the possibility of more universal approaches to neural machine translation, by unifying input and output spaces via pixels.  Text preprocessing and vocabulary construction has been an active research area leading to work on investigating neural machine translation systems operating on subword units , characters  and even bytes  and has been highlighted to be one of the major challenges when dealing with many languages simultaneously in multilingual machine translation , and cross-lingual natural language understanding . Pixels serve as a straightforward way to share vocabulary among all languages at the expense of being a significantly harder learning task for the underlying models.  In this work, we propose an end-to-end neural approach to in-image machine translation that combines elements from recent neural approaches to the relevant sub-tasks in an end-to-end differentiable manner. We provide the initial problem definition and demonstrate promising first qualitative results using only pixel-level supervision on the target side. We then analyze some of the errors made by our models, and in the process of doing so uncover a common deficiency that suggests a path forward for future work.  
"," In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.",161
"    [t!] {2mm}). External data . }       
"," Abstract Meaning Representation  parsing has experienced a notable growth in performance in the last two years, due both to the impact of transfer learning and the development of novel architectures specific to AMR. At the same time, self-learning techniques have helped push the performance boundaries of other natural language processing applications, such as machine translation or question answering. In this paper, we explore different ways in which trained models can be applied to improve AMR parsing performance, including generation of synthetic text and AMR annotations as well as refinement of actions oracle. We show that, without any additional human annotations, these techniques improve an already performant parser and achieve state-of-the-art results on AMR 1.0 and AMR 2.0.%",162
" Transformer based models  have been proven to be very effective in building the state-of-the-art Neural Machine Translation  systems via neural networks and attention mechanism . Following the standard  architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and position-wise feed-forward network.   Multihead attentions and position-wise feed-forward network, together as a basic unit,  plays an essential role in the success of Transformer models.  Some researchers  propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention.   Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow  in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit models advance in expressiveness.  Second, for the multi-unit setting, one unit could mitigate the deficiency of other units and compose a more expressive network, in a complementary way.  In this paper, we propose the Multi-Unit TransformErs , which aim to promote the expressiveness of transformer models by introducing diverse and complementary parallel units. Merely combining multiple identical units in parallel improves model capability and diversity by its varied feature compositions. Furthermore, inspired by the well-studied bagging  and gradient boosting algorithms  in the machine learning field, we design biased units with a sequential dependency to further boost model performance.  Specifically, with the help of a module named bias module, we apply different kinds of noises to form biased inputs for corresponding units. By doing so, we explicitly establish information gaps among units and guide them to learn from each other.  Moreover, to better leverage the power of , we introduce sequential ordering into the multi-unit setting, % by learning a permutaion matrix  to automatically shuffle the outputs of multiple units,  and force each unit to learn the residual of its preceding accumulation.  We evaluate our methods on three widely used Neural Machine Translation datasets, NIST Chinese-English,  WMT'14 English-German and WMT'18 Chinese-English. Experimental results show that our multi-unit model yields an improvement of +1.52, +1.90 and +1.10 BLEU points, over the baseline model  for three tasks with different sizes, respectively.  Our model even outperforms the Transformer-Big on the WMT'14 English-German by 0.7 BLEU points with only 54\% of parameters.  Moreover, as an interesting side effect, our model only introduces mild inference speed decrease  compared with the Transformer-Base model, and is faster than the Transformer-Big model. % which proves the practicability of our methods.   The contributions of this paper are threefold:   {1pt} {1pt} {1pt} ulti-Unit TransformErs , to promote the expressiveness of Transformer models by introducing diverse and complementary parallel units.  by introducing bias module and sequential ordering to further model the diversity and complementariness among different units.   
","     Transformer models  achieve remarkable success in Neural Machine Translation.      Many efforts have been devoted to deepening the Transformer by stacking several units  in a cascade,      while the investigation over multiple parallel units draws little attention.     In this paper, we propose the Multi-Unit TransformErs , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units.     Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity.      Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units.      % need more results and exciting data.     Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed .      In addition, our methods also surpass the Transformer-Big model, with only 54\% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage. \footnote{Code is available at \url{https://github.com/ElliottYan/Multi\_Unit\_Transformer}}",163
"  %  %     Prior work primarily focused on exploiting visual patterns using carefully crafted features . These rendering-based methods have two major drawbacks: 1) they are expensive since they require downloading all external files including CSS, javascript, and images to render the page to compute visual features; 2) they require carefully crafted heuristics around visual proximity  to work well with these expensive features. In this paper, we propose a novel two-stage neural architecture, named FreeDOM, that can be trained on a small number of seed websites and generalize well to unseen websites  requiring any hand-engineered visual features. %we want to employ neural networks for learning transferable visual features such that we can eliminate the need of rendering and human engagement in crafting textual patterns. %We propose a novel two-stage neural architecture that can directly learn from a few annotated websites just based on their raw HTML content and transfer the models for unseen websites without using any human labels . %We parse HTML documents as DOM Trees of the page and classifies it into one of the target fields. This node-level module combines neighboring character sequences, token sequences, as well as markup  to learn a combined representation for the node. We propose a combination of CNNs and LSTMs and show that it can effectively encode useful features in DOM nodes.  These node representations are encoded individually and inevitably lose some global information useful for an extraction task. In particular, only relying on local node features can cause failure when value nodes have no obvious patterns themselves or their local features are very similar to other non-value nodes. To mimic the signal that may be available through visual features used in rendering-based methods,  we use a relational neural network as our second module . This allows us to model the relationship between a pair of elements using both distance-based and semantic features. The rationale behind this is to learn more global representations of node pairs so that we can jointly predict node labels instead of relying only on local features.   Extensive experimental results on a large-scale public dataset, the Structured Web Data Extraction  corpus, show that our model consistently outperforms competitive baseline methods by a large margin.  The proposed FreeDOM is able to generalize to unseen sites after training on a small number of seed sites. In fact, we show that with training data from just three seed sites, our approach out-performs techniques that use explicit visual rendering features by 3.7 F1 points on average. To the best of our knowledge, our framework is among the first neural architectures that efficiently obtains high-quality representations of web documents for structured information extraction.     %The node-level module itself can predict node labels for identifying values of interested fields, but the encoded local features cannot capture the long-range dependencies between values and thus degenerate in unlabeled target websites. %To address this problem, we further propose a relational neural network. %As the second-stage module, it explicitly models the relations between DOM nodes and effectively learns the page-level constraints for producing structured predictions. % it models the relational features reflected by each node pairs, and finally conducts structured data extraction as a structured prediction problem.  %Our contributions in this paper are three-fold: %%  %	\quad We propose a new state-of-the-art neural architecture for structured data extraction over web documents that is highly transferable across websites.  %	This framework utilizes minimal human efforts in feature engineering and does not require any rendering results, thus making large-scale information extraction much easier and more effort-light. %	\quad The first-stage module is a novel neural model for representing local node features from multiple sources. The second stage moudule instead focus on capturing relational features between DOM nodes and making structured predictions. To the best of our knowledge, our framework is among the first neural architectures that can efficiently obtain high-quality representations of web pages for structured data extraction. We also believe the architecture can be promising in future works that require neural representations of web pages. %	\quad We formulate the data extraction problem as a structured prediction task , and also evaluate recent neural architectures like BLSTM-CRFs  to address the problem, which are widely-used in natural language processing communities. Extensive experimental results on a large-scale public dataset with careful analysis show that our model learns consistently better representations of pages for data extraction even than methods requiring rendering results. %%  %}   %Our contribution is that we propose a novel neural model, FreeDom, for structured data extraction over web documents while using less information  and no hand-crafted features. Extensive experiments on a large-scale public data set  show that the proposed FreeDom outperforms other strong baseline methods while only using raw features. %%ying{The last sentence looks not complete. \yuchen{Done.}} %\tata{Don't say 'less information' emphasize that not requiring visual rendering is cheaper and not requiring hand-crafted features means you can generalize to new tasks better. Need to make this claim more focused so the contributions are clear. We also need to spell out the two stages more clearly 'First stage does blah', 'Second stage does blah'} %\tata{Might be worth adding that entity resolution is not in scope for this work -- ie, we might extract duplicate entries across websites for the same car. There are many papers dealing with that and we're not focused on that in this paper.} % 
"," % tata: Jan 26 rewrite of Abstract.  Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes both these limitations.  The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance  and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. Through experiments on a public dataset with 8 different verticals, we show that FreeDOM beats the previous state of the art by nearly 3.7 F1 points on average  requiring features over rendered pages or expensive hand-crafted features. % 3.7 is from Table 2 k=3 .  % tata: Previous version of abstract follows:  %",164
" Gender prediction based on human's names is a topic on which a vast number of researches are investigating . There have been large numbers of investments on identifying the most suitable algorithms and models for different languages. The reason behind for these researches is the potential benefits of predicting two common genders . Consequently, there are a variety of online web applications and tools that are capable of pointing out the gender from user's first name .\par  % Add references These existing gender predicting systems are useful and practical in terms of third-party APIs providers for other applications . A typical utilization is online registration forms and documents. For instance, giving a form where the users have to fill up their names and genders, the application will automatically choose the corresponding gender field after the users have typed in their names. Predicting gender eliminates the amount of time which the users need to away from keyboard and click on computer mouse or track-pad to check on the field. It enhances the overall user experience while using the services.\par In addition, the co-reference resolution is a Natural Language Processing task to which gender classification is possible to be applied. The co-reference mission is to identify the same entity that all the objects in one text are referring. For example,  is having dinner with Mia. He is having beef steak.""}. In this scenario, } is referring to } which is a }. Therefore, determining gender of a given name is sufficiently essential to mark references to correct entity . \par On the other hand, not many related researches on Vietnamese names have been done over the years. Thus, the goal of this paper is to perform gender classification on Vietnamese names using machine learning models. From there, we can study further on the naming conventions and how efficient the machine learning algorithm predicts the genders using Vietnamese names.   In this paper, we have three main contributions described as follows:\par     for male and 0 for female. UIT-ViNames is available for research purposes at our website. \footnote{https://sites.google.com/uit.edu.vn/uit-nlp/}     \par In Section II, we perform literature review on previous studies. Then in Section III, we describe the our dataset and how we collect it. Next, we present a detailed description on our approach for this topic in Section IV and our experiments in Section V. Finally, in Section VI we draw an overall conclusion and several future works.    
"," As biological gender is one of the aspects of presenting individual human, much work has been done on gender classification based on people names. The proposals for English and Chinese languages are tremendous; still, there have been few works done for Vietnamese so far. We propose a new dataset for gender prediction based on Vietnamese names. This dataset comprises over 26,000 full names annotated with genders. This dataset is available on our website for research purposes. In addition, this paper describes six machine learning algorithms  and a deep learning model  with fastText word embedding for gender prediction on Vietnamese names. We create a dataset and investigate the impact of each name component on detecting gender. As a result, the best F1-score that we have achieved is up to 96\% on LSTM model and we generate a web API based on our trained model.",165
" Relation classification  aims to identify the relation between two specified entities in a sentence.  Previous supervised approaches on this task heavily depend on human-annotated data, which limit their performance on classifying the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community ,  first introduce the few-shot learning to RC task and propose the FewRel %   dataset. % dataset as the benchmark. Recently, many works focus on this task and achieve remarkable performance .  %distant supervision  is proposed to automatically construct training instances for RC. %However, in the dataset extracted by distant supervision, some long-tail relations only have few instances and suffer from data sparsity problem. %Inspired by the success of few-shot learning  methods in the computer vision community, e.g., Matching Network , Relation Network  and Memory-augmented network ,  first introduce FSL to RC to tackle the long tail problem. They use the Prototypical Network , which achieves the state-of-the-art performance on several FSL benchmarks. Recently, many works followed their framework have achieved remarkable performance on the Few-shot RC dataset FewRel . %The prototypical network learns the representation  for each relation based on sampled instances, and then classifies the queries into a set of pre-defined relations. %\CheckedBox   % Even though the existing works perform well, they all assume that there is only one relation in a sentence.   Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted  as evidence. When specified two entities  in the sentence, there is a great opportunity for the instance to be incorrectly categorized as a {{confusing relation}}  instead of the {{true relation}} . % % Specifically,  %is that different entity pairs are usually described  in an input sentence, in which the relation classification of these entity pairs often interferes with each other. %This results in that the entity pairs of these relations are often misclassified into confusing relations by models without the ability of explicitly decoupling easily-confused relations. %Table shows three instances from the FewRel dataset , each of which contains a sentence with two given entities  on the right side, and its positive  and confusing relations  on the left side.  %Previous few-shot methods tend to misclassify these sentences into the confusing relations. the first instance should be categorized as the true relation `parents-child' based on the given entity pair and natural language  expression `a daughter of'. However, since it also includes the NL expression `his wife',  %which describes the confusing relation `husband-wife', it is probably misclassified into this confusing relation `husband-wife'. In this paper, we name it as a relation confusion problem.   %===============================================================================================  % \verb| 	|p{9.2cm}} 	{c|c|p{9.2cm}}  		\toprule % 		{True Relation\times$}  &Sentence \\  			{True Relation} & {Confusing Relation}  &Instance \\  		\midrule 		 		{{parents-child}} & {{husband-wife}}  &{She was {{a daughter of}} prince Wilhelm of Baden and {{his wife}} , as well as an elder sister of .} \\  		\midrule 		 		{{husband-wife}} & {{uncle-nephew}} & He was the youngest son of  and {{his wife}} , and {{the uncle of}} former president George W Bush. \\  		\midrule 		{{{uncle-nephew}}} & {{parents-child}}  &  is {{the son of}} princess Margaret, countess of Snowdon, and the 1st earl of Snowdon, thus he is {{the nephew of}} .\\ 		 	{blue}} and {{red}} words respectively correspond to true and confusing relations.} 	 		  %============================================================================================== To address the relation confusion problem, it is crucial for a model to %  effectively select the information with high relevance to the given entity pair and  be aware of which NL expressions cause confusion and learn to avoid mapping the instance into its easily-confused relation. % To address the relation confusion problem, it is crucial for a model to be aware of which NL expressions cause confusion and how to explicitly distinguish the easily-confused relations.  From these perspectives, we propose two assumptions.  Firstly, in a sentence, words that keep high relevance to the given entities are more important in expressing the true relation. Intuitively, the specified entity information is crucial to identify the true relations.  Secondly, explicitly learning of mapping an instance into its confusing relation with augmented data in turn boosts a few-shot RC model on identifying the true relation. % allowing the model to explicitly learn the confusing relations can help it to identify the true relations. %Intuitively, the specific entities information is helpful to identify the positive relation.  Based on these assumptions, we propose CTEG, a few-shot RC model with two novel mechanisms:  An Entity-Guided Attention  encoder, which leverages the syntactic relations and relative positions between each word and the specified entity pair to softly select important information of words expressing the true relation and filter out the information causing confusion.  A Confusion-Aware Training  method, which explicitly learns to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. %has the ability of explicitly learning to distinguish easily-confused relations. In addition, inspired by the success of pre-trained language models, our approaches are based on BERT , which has been proved effective especially for few-shot learning tasks. %===========================================================================   % Specifically, when encoding a sentence by the attention mechanism, our EGA guides the calculation of the attention score through multiply it by an entity-aware gate. %we adopt the transformer incorporating with a self-attention mechanism to encoding an input instance,  Specifically, the backbone of the encoder of our model is a transformer equipped with the proposed EGA which guides the calculation of self-attention distributions by weighting the attention logits with entity-guided gates. % The gate is a matrix of relevance scores, which are used to measure the importance of each word according to its relevance to the entities. % The gates are used to measure the importance of each word according to its relevance to the entities. The gates are used to measure the relevance between each word and the given two entities. % Two types of position information of the words are used to calculate the scores. One of them is the relative position , which is the relative distance between a word and an entity in the sentence squence. Two types of information for each word are used to calculate its gate. % One of them is the relative position , which is the relative distance between a word and an entity in the sentence squence. One is the relative position  information, which is the relative distance between a word and an entity in the input sequence. The other is syntactic relation which is proposed in this paper, defined as the dependency relations between each word and the entities. % Besides, we further propose the syntax position, defined as the dependency relations between each word and the entities. Based on these information, the entity-guided gates in EGA are able to select those important words and control the contribution of each word in self-attention. % Based on these information, the entity-aware gate in EGA is able to select those important words and control the contribution of each word in the self-attention.    % For the proposed CAT, it allows the model to and asynchronously learn the confusing relations for each sentence. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified sentences and their confusing relations to conduct an additional training process, which aimes to learn the confusing relations explicitly.  We also propose CAT to explicitly force the model to asynchronously learn the classification from an instance to its true relation and its confusing relation. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified instances and their confusing relations as augmented data to conduct an additional training process, which aims to learn the mapping between these instances into the confusing relations.  % After that, The CAT uses these misclassified sentences and their confusing relations to  conduct an additional training process, which aims to learn the confusing relations explicitly.  Afterwards, the CAT adopts the KL divergence to teach the model to distinguish the difference between the true and confusing relations, which benefits the true relation classification from the confusing relation identification.  % Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even better results to strong baselines in terms of accuracy. % Furthermore, ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem.  The contributions of this paper are summarized as follows:   We propose an Entity-Guided Attention encoder, which can select crucial words and filter out NL expressions causing confusion based on their relevance to the specified entities.    We propose a Confusion-Aware Training process to enhance the model with the ability of distinguishing true and confusing relations.   We conduct extensive experiments on few-shot RC dataset FewRel, ans the results show that our model achieves comparable and even much better results to strong baselines. Furthermore, ablation and case studies verify the effectiveness of the proposed EGA and CAT, especially in addressing the relation confusion problem.  
"," This paper aims to enhance the few-shot relation classification especially for sentences that jointly describe multiple relations. Due to the fact that some relations usually keep high co-occurrence in the same context, previous few-shot relation classifiers struggle to distinguish them with few annotated instances. To alleviate the above relation confusion problem, we propose CTEG, a model equipped with two mechanisms to learn to decouple these easily-confused relations. On the one hand, an Entity-Guided Attention  mechanism, which leverages the syntactic relations and relative positions between each word and the specified entity pair, is introduced to guide the attention to filter out information causing confusion. On the other hand, a Confusion-Aware Training  method is proposed to explicitly learn to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even much better results to strong baselines in terms of accuracy. Furthermore, the ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem.",166
"    % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English.  .          % final paper: en-us version           %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }    Complaining is a basic speech act, usually triggered by a discrepancy between reality and expectations towards an entity or event. Social media has become a popular platform for expressing complaints online  where customers can directly address companies regarding issues with services and products. Complaint detection aims to identify a breach of expectations in a given text snippet. However, the use of implicit and ironic expressions and accompaniment of other speech acts such as suggestions, criticism, warnings and threats make it a challenging task. Identifying and classifying complaints automatically is important for:  improving customer service chatbots;   linguists to analyze complaint characteristics on large scale ; and  psychologists to understand the behavior of humans that express complaints.  Previous work has focused on binary classification between complaints and non-complaints in various domains. Furthermore, some studies have performed more fine-grained complaint classification. For instance, complaints directed to public authorities have been categorized based on their topics or the responsible departments. Other categorizations are based on possible hazards and risks as well as escalation likelihood. Most of these previous studies have used supervised machine learning models with features extracted from text  or task-specific neural models trained from scratch. Adapting state-of-the-art pre-trained neural language models based on transformer networks such as BERT and XLNet has yet to be explored.    In this paper, we focus on the binary classification of Twitter posts into complaints or not . We adapt and evaluate a battery of pre-trained transformers which we subsequently combine with external linguistic information from topics and emotions.     New state-of-the-art results on complaint identification in Twitter, improving macro FI by 8.0\% over previous work by  Preotiuc-Pietro et al. ;  A qualitative analysis of the limitations of transformers in predicting accurately whether a given text is a complaint or not.    
","    Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87.",167
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % final paper: en-us version      %   % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } Neural machine translation  achieved enormous success in advancing the quality of translation . In spite of the impressive performance, NMT models are still vulnerable to perturbations in the input sentences  , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation . Moreover, NMT commonly consists of  millions of parameters, which making it prone to overfitting especially in low resource scene. % Neural machine translation achieved enormous success in advancing the quality of translation . Despite the impressive performance, NMT models are still vulnerable to the scale of data, when training in the small or monotonous data leading to inference with many unexcepted inputs and low quality of translation, the model prone to overfitting concurrently.  % Improve robustness and representation capacity of models is an important problem to solve.  % NMT model adopts deep neural network to modeling the whole translation process, which can train multi features together without prior domain knowledge, developing rapidly in recent years. However, compared with SMT, NMT requires millions of parameters and huge number of data, making it be prone to overfitting especially in low resource scene. Researchers found that NMT is extremely sensitive to input noise -- a tiny perturbation will affect hidden representation -- leading to a low quality of translation. Improve robustness and representation capacity of models is an important problem to be solved. A natural way to improve generalization is synthesizing natural noise  or adopting arbitrary noise .  Another way is exploring regularization techniques to avoid overfitting , making model robust to unseen or unfamiliar inputs. However, as discrete data, the text is hard to retain the semantic information after corruption. % A natural way to improve generalization is data augmentation, make the model see as much data as possible, meanwhile, it's necessary to avoid overfitting. Standard dropout commonly used as a technology for overfitting in neural networks by preventing the neuron obtain complete information from data like noisy perturbations, moreover, the regularization techniques also can be beneficial for overfitting. %    In this paper, we propose Token Drop to prevent overfitting and improve generalization. Different from standard dropout  that drops neurons in network randomly, we drop tokens of the input sentences. In order to retain semantic information, we replace tokens with a special symbol  . This allows model learn hidden representation from rest token's context, and predict target translation condition on latent variable. On the one hand, our method allows model meeting exponentially different sentences can be explained as data augmentation; On the other hand, our method corrupts input sentences with natural noise can be seen as regularization term for NMT.   We investigate two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Considering  our Token Drop method regularize parameters by weakening model inputs, making NMT suitable for applying self-supervised objective. During training:  use a discriminator to detect whether input tokens are dropped or not;  leverage hidden state to predict original tokens of dropped tokens inspired by Cloze task . Both of them guide model to generate semantically similar representation, leading to a better generalization capacity.  % To test our approach, we conduct machine translation task on ZH-EN and EN-RO benchmark, despite compared with strong baseline, we  % In this work, we propose to randomly drop tokens of input sentence, different with standard dropout  , which drop neurons in network randomly during training, we replace tokens with a special symbol unk. Using this method, we call it token drop, for each input sentence, our model see  different sequences theoretically. On the one hand, model receives more data, on the other hand, it can be seen as noise challenge model to learn primary information from latent representation. Our approach is also similar to self-supervised technique, which utilize contextual state to predict masked token, the difference of our training object is to optimize translation ability of source language. By randomly drop tokens of input, forcing model utilize latent representation to make prediction during training, at inference, model see full sentence and make decision easily. To test our approach, we conduct machine translation on ZH-EN and EN-RO corpus, compare to baseline, our token drop training models are more stable and resilient to overfitting.  % Deep neural networks with large number of parameters are prone to overfitting, requires regularization method in practice. One of the most effective way to avoid overfitting is Dropout , by omitting stochastic neurons in networks during training iteration, while maintain all neurons during inference, brings significant improve results on a variety tasks . Different from dropout, which drop single unit, token-level dropout drop  entire token, are  proved be effective on seq2seq task. 
","    Neural machine translation with millions of parameters is vulnerable to unfamiliar inputs. We propose Token Drop to improve generalization and avoid overfitting for the NMT model.  Similar to word dropout, whereas we replace dropped token with a special token instead of setting zero to words.  We further introduce two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Our method aims to force model generating target translation with less information, in this way the model can learn textual representation better. Experiments on Chinese-English and English-Romanian benchmark demonstrate the effectiveness of our approach and our model achieves significant improvements over a strong Transformer baseline\footnote{Our code is released at \url{https://github.com/zhajiahe/Token_Drop}}.",168
" The short answer test is a type of exam in which participants are asked to answer questions with short answers which can consist of 1-2 sentences. Assessing student short answers on the exam is very challenging for the assessor. When a large number of students are assessed, for example on a national scale, assessors are required to remain consistent and objective in assessing hundreds or even thousands of student responses. The questions with short answer format also allow students to answer in their own style which can be varied for each student. Therefore, computer assistance in making automatic short answer scoring is deemed necessary to address these problems.  In 2019, NLP Research Group from Universitas Gadjah Mada, Indonesia, in collaboration with the Education Assessment Center, Ministry of Education and Culture of Indonesia, held the Ukara 1.0 Challenge\footnote{https://nlp.mipa.ugm.ac.id/ukara-1-0-challenge/}. In this challenge, participants from all over the nation were challenged to make an automatic short answer scoring system for Indonesian student exam. There are two short answer questions on this challenge with correct and incorrect labels. In this paper, we try to describe the improvement of our previous work on the Ukara 1.0 Challenge dataset.  There are several challenges that may arise in applying and making an automatic short answer scoring system. First, it is the number of models and hyperparameters that need to be searched.  In conventional machine learning, a model is trained to solve a specific problem, or in our case, a model is only trained to assess one short answer question. When the number of questions to be assessed increases, the time required for searching the model and tuning its hyperparameters will also increase. The second challenge is the imbalance class. In an exam, the questions given are intended to test students' abilities, therefore the level of difficulty in each question will cause the number of correct responses to be much less than the number of wrong responses. Another challenge in automatic short answer scoring is the small amount of labeled data. The data labeling process is not easy because it requires an expert to validate the student responses and of course this is time consuming and costly.  In several previous studies, making short answer scoring or essay scoring was done using deep learning approaches, for example, using long-short term memory, convolutional neural network, or a combination of both . We take a different approach by using a simpler stacking model because of the small number of available data. In this paper, we used the sentence-level feature as done by . Without word sequence features, the automatic scoring process could be viewed as a text classification problem. The use of stacking models for text classification has been done in  and shows better performance than a single model classifier. We also propose to use an upsampling method, Synthetic Minority Over-sampling Technique , to handle imbalance classes and hyperparameters optimization algorithm, Tree-structured Parzen Estimator  to find a robust model that performs well on each type of question. In this paper, we use hyperparameters term as model components which are preset and untrained . Meanwhile, the term parameters refers to the model components that can be trained  .  
"," Automatic short answer scoring is one of the text classification problems to assess students' answers during exams automatically. Several challenges can arise in making an automatic short answer scoring system, one of which is the quantity and quality of the data. The data labeling process is not easy because it requires a human annotator who is an expert in their field. Further, the data imbalance process is also a challenge because the number of labels for correct answers is always much less than the wrong answers. In this paper, we propose the use of a stacking model based on neural network and XGBoost for classification process with sentence embedding feature. We also propose to use data upsampling method to handle imbalance classes and hyperparameters optimization algorithm to find a robust model automatically. We use Ukara 1.0 Challenge dataset and our best model obtained an F1-score of 0.821 exceeding the previous work at the same dataset.",169
"  % Second, is a NMT model's ability to handle streaming ASR output; an ASR system provides the best greedy recognition  from a live speech's segmented audio. % we don't really talk about that ^ %deleted simultaneous With the advance of Automatic Speech Recognition  and Neural Machine Translation  systems, speech translation has become increasingly feasible and has received considerable attention.  However, researchers have encountered many challenging problems within the standard cascaded framework where ASR system outputs are passed into NMT systems. First, since NMT models are often trained with clean, well-structured text, the disfluency of spoken utterances and the recognition errors from ASR systems are not modeled by the NMT systems.  Second, people speak differently than they write, which results in changes in both sentence structure and meaning.  Third, automatically predicting sentence boundaries is challenging.  Taken as a whole, poorly segmented sentences with incorrect word recognition leads to poor translations.  These problems pose unique challenges for ASR NMT robustness that are not readily addressed by current methods. %  % % Current approaches to robust NMT with noisy inputs typically focus on improving word transcription through data augmentation techniques. Such methods include disfluency removal where redundant and unnecessary words are removed before translating the transcript, domain adaptation where NMT models are augmented with in-domain training data, and synthetic noise, where random edits are made to training data.   %  % \footnotetext{When compared to Table , the sum of System/System with transcript and segmentation degradation surpasses the Gold/Gold evaluation by 0.57 BLEU points. This is because the modifications to the evaluation data are not directly additive, ie .}  % % % % Although data domain and segmentation issues are often tackled separately, we find their compounded effects, namely erroneous transcript sentence boundaries, are neglected and substantially detrimental to final translation quality.  % As such, our contributions are two fold, we analyze the impact of noisy ASR segmentations on translation and propose an easily adaptable and simple data augmentation strategy to increase NMT robustness. % this sentence can be optional In our experimentation, we found that ASR system punctuation is often imperfect. It may omit or insert sentence-final punctuation, resulting in sentences that are erroneously compounded or fragmented.  While this is corroborated by similar works, which note that degradation of translation is caused by poor system sentence boundary prediction, they do not specifically evaluate, quantify, and address this issue. % should we mention how much sentence boundaries degrade accuracy % Find an example? To tackle the sentence boundary problem, we propose a simple scheme to augment NMT training data, which yields +1 BLEU point on average. % Similar to , we first ascertain that a NMT model can implicitly learn target punctuation from unpunctuated source text, even with noisy imperfect sentence boundaries. % is this sentence above necessary?^ %We show that with a simple data augmentation scheme on both general and in-domain NMT training data, we can achieve an improvement of  and  BLEU for tst2015 and tst2018 respectively.  This procedure is agnostic to ASR systems and can be applied to any NMT model training easily.  %  
"," Neural Machine Translation  models have demonstrated strong state of the art performance on translation tasks where well-formed training and evaluation data are provided, but they remain sensitive to inputs that include errors of various types. Specifically, in the context of long-form speech translation systems, where the input transcripts come from Automatic Speech Recognition , the NMT models have to handle errors including phoneme substitutions, grammatical structure, and sentence boundaries, all of which pose challenges to NMT robustness.  %This paper makes two main contributions via an in-depth error analysis and a proposed solution.  Through in-depth error analysis, we show that sentence boundary segmentation has the largest impact on quality, and we develop a simple data augmentation strategy to improve segmentation robustness.",170
" Automatic summarization is the automated process of reducing the size of an input text while preserving its most relevant information content and its core semantics. Techniques for summarization are often characterized as being either:  or .  methods construct summaries by combining the most salient passages  of a source text; a process similar to human's way of identifying the right information. One way to achieve extractive summarization is to define the problem as a sentence classification task, using some form of  representation of the sentences in a document . To avoid content overlap issues, previous work has used sentence reranking  or sentence ordering by extracting sentences recurrently .  methods generate summaries by generating new sentence constructs ``from scratch'', or from representation of document content, a process that is conceptually more similar to the notion of paraphrasing. Abstractive text summarization has attracted interest since it is capable of generating novel formulations of summaries using language generation models conditioned on the source text. Several attention-based Recurrent Neural Network  encoder-decoders have been introduced to tackle varying text generation issues of standalone abstractive sequence-to-sequence  models. Copy and pointer mechanisms , for example, have enabled decoders to better generate unseen words, out-of-vocabulary words and named entities.   Most recently, hybrid extractive and abstractive architectures have been proposed and have shown promising results in both quantitative performance measures and human evaluations. In such set-ups, the extractive model first selects salient sentences from a source article, and the abstractive model paraphrases the extracted sentences into a final summary. The majority of current state-of-the-art abstractive summarization models\footnote{Excluding summarization models using large scale pre-trained language models such as BERT } are based on the hybrid approach .  Nonetheless, hybrid models can be limited by three disadvantages. First, since ground-truth labels for extractive summarization are usually not provided, extractive labels must be generated by a potentially suboptimal algorithm . The performance of models trained with such labels is therefore bounded by the quality of the performance of the extractive heuristics. Second, since ground-truth binary labels for recurrently extracted sentences are typically teacher forced as in , ``exposure bias''  may negatively affect content selection performance at inference. Finally, given that the hard extraction step is not differentiable, existing hybrid models typically require multi-step training   or reinforcement learning  to train the whole model.  In this paper, we introduce a novel abstractive summarization model that incorporates an intermediate extractive step but does not require labels for this type of extractive content selection, and it is fully end-to-end trainable. To achieve this, we propose a new memory augmented encoder-decoder  architecture  called Mem2Mem. Mem2Mem has 2 memorization modes:  absorb key information of the encoded source sequence via a compression mechanism, and  sequentially update the external memory during target summary generation. Without using extractive ground-truth labels, we find in our analysis that Mem2Mem's compression mechanism behaves as an implicit sentence extractor that stores sentence representations of the salient content. The choice of sentence representations is only guided by the memory regularization and conditional language modeling loss of the decoder, thus avoiding exposure bias from maximizing the likelihood of sequential binary extraction labels. Finally, the encoded memory is transferred to the decoder memory, which is iteratively refined during the decoding process. To our knowledge, Mem2MeM is the first abstractive summarization model that uses memory compression for sentence extraction and that directly employs the memorized representations during summary generation. We empirically demonstrate the merits of this approach by setting a new state-of-the-art on long text abstractive summarization tasks on the Pubmed, arXiv and Newsroom datasets . Our contributions are three fold: [noitemsep]       % fig_architecture [t]          _{E}\). The encoder memory is transferred to the decoder memory \ and is read using a RAM like mechanism. The resulting memory readout vector is used to condition sentence and word-level attention. The decoder hidden states and the memory readout vector are then used to update the memory state \ via a gated write operation .}    
"," We introduce , a memory-to-memory mechanism for hierarchical recurrent neural network based encoder decoder architectures and we explore its use for abstractive document summarization. Mem2Mem transfers ``memories"" via readable/writable external memory modules that augment both the encoder and decoder. Our memory regularization compresses an encoded input article into a more compact set of sentence representations. Most importantly, the memory compression step performs implicit extraction without labels, sidestepping issues with suboptimal ground-truth data and exposure bias of hybrid extractive-abstractive summarization techniques. By allowing the decoder to read/write over the encoded input memory, the model learns to read salient information about the input article while keeping track of what has been generated.  Our Mem2Mem approach yields results that are competitive with state of the art transformer based summarization methods, but with . %On abstractive long text summarization, Mem2Mem surpasses, with full end-to-end training, the current state-of-the-art by 3.98 and 3.08 average ROUGE scores on the Pubmed and arXiv datasets while using $16$ times less parameters.  % Our code and trained models are available at \url{https://github.com/anonymously999/mem2mem}.",171
"  Attention-based encoder-decoder modeling is a natural and powerful paradigm for speech to text tasks, such as automatic speech recognition  and speech translation , and that has led to significant progress .  However, it relies on large amounts of supervised speech data, which is expensive to transcribe and translate.  In addition, the amount of speech transcripts and speech translation labels is dwarfed by the amount of text data available for language model  and machine translation  training. For example, the number of text tokens used for LM modeling is two orders of magnitude larger than the number of tokens from the corresponding speech corpus in the Librispeech data corpus, as shown in Table.    Attention-based encoder-decoder models are not designed to incorporate heterogeneous inputs and cannot benefit from large amounts of low cost text data directly in speech applications. As expected, performance gaps can still be observed between attention based encoder-decoder systems and conventional systems with multiple components.  %Short description about previous work In order to alleviate the data scarcity issue, different approaches have been studied, including acoustic and linguistic aspects.  %In this study, we focus on leveraging text data to improve linguistic modeling ability in speech to text systems.  LM is the most commonly used method to integrate linguistic information into ASR.  Prior work focuses on building LM with monolingual text data, and then integrate LM or transfer knowledge from it into the decoder.    generate synthetic data from text to augment speech training corpus. Another direction is to leverage text data directly during training through multitask learning.  use a common representation space to learn correspondences between different modalities for spoken language understanding.  propose multi-modal data augmentation to jointly train text and speech for ASR.  %is reminiscent of work done on multimodal learning or spoken language understanding  that also uses a common representation space to learn correspondences between different modalities.  are focused on ST tasks and trained with an ASR system together, where ASR is used as an auxiliary task. Hence, those methods cannot be applied back to ASR systems.  %What is proposed, describe the main idea %We follow the second direction and propose using auxiliary text tasks to enhance speech to text tasks. In this study, we focus on leveraging text data to improve linguistic modeling ability in speech to text tasks. We propose a general framework to leverage text data for ASR and ST tasks.  %Two encoders take text and speech as input respectively,  while the decoder is shared between tasks. During inference, only the speech encoder and decoder are used. A denoising autoencoder task  is introduced to be jointly trained with the ASR task with monolingual data, while a machine translation task is co-trained with  ST task  with  parallel  data. Text  input  is  represented  as  spoken form using phoneme sequence and it effectively reduces the difference between speech input and text input. We also carefully study different design choices for the joint training system, including strategies to share the text and speech encoders and comparing the joint training system with models initialized from pre-trained components.   Our experiments show the proposed joint training systems can effectively reduce word error rate  for the ASR task by 10\% to 15\% and improve BLEU score by 3.6$ 
"," Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence.  Its success heavily relies on the availability of large amounts of training data.  This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition  and speech translation .  In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively.  We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks.  Our experiments show that the proposed method achieves a relative 10$ task compared with our baseline, and improves the speech translation quality on the MuST-C tasks by 3.6$\sim$9.2 BLEU.",172
" Motivated by the process of human inquiry and learning, the field of question generation  requires a model to generate natural language questions in context. QG has wide applicability in automated dialog systems, language assessment, data augmentation, and the development of annotated data sets for question answering  research.    Most prior research on QG has focused on generating relatively simple { question answering and generation , where answering the questions requires reasoning over the content in multiple text documents .  Unlike standard QG, generating multi-hop questions requires the model to understand the relationship between disjoint pieces of information in multiple context documents.  Compared to standard QG, multi-hop questions tend to be substantially longer, contain a higher density of named entities, and---perhaps most importantly---high-quality multi-hop questions involve complex chains of predicates connecting the mentioned entities   To address these challenges, existing research on multi-hop QG primarily relies on graph-to-sequence  methods. These approaches extract graph inputs by augmenting the original text with structural information  and then apply graph neural networks  to learn graph embeddings that are then fed to a sequence-based decoder.  However, the necessity of these complex G2S approaches---which require designing hand-crafted graph extractors---is not entirely clear, especially when standard transformer-based sequence-to-sequence  models already induce a strong relational inductive bias. Since transformers have the inherent ability to reason about the relationships between the entities in the text, one might imagine that these models alone would suffice for the relational reasoning requirements of multi-hop QG.   \xhdr{Present work} In this work, we show that, in fact, a standard transformer architecture is sufficient to outperform the prior state-of-the-art on multi-hop QG.  We also propose and analyze a graph-augmented transformer ---which integrates explicit graph structure information into the transformer model. GATE sets a new state-of-the-art and outperforms the best previous method by 5 BLEU points on the HotpotQA dataset. However, we show that the gains induced by the graph augmentations are relatively small compared to other improvements in our vanilla transformer architecture, such as an auxiliary contrastive objective and a data filtering approach, which improve our model by 7.9 BLEU points in ablation studies.  Overall, our results suggest diminishing returns from incorporating hand-crafted graph structures for multi-hop reasoning and provides a foundation for stronger multi-hop reasoning systems based on transformer architectures.     \usepackage[hyperref]{emnlp2020}   \usepackage{times}     \usepackage{latexsym} \renewcommand{\UrlFont}sachande@mila.quebec, wuli@us.ibm.com    % use 8-bit T1 fonts \usepackage{url}            % simple URL typesetting \usepackage{booktabs}       % professional-quality tables \usepackage{amsfonts}       % blackboard math symbols \usepackage{amsmath} \usepackage{nicefrac}       % compact symbols for 1/2, etc. \usepackage{graphicx} \usepackage{microtype}      % microtypography \usepackage{tabularx} \usepackage{xcolor} \usepackage{bbm} \usepackage{array} \usepackage{arydshln} \usepackage{amsfonts} \usepackage{amsmath} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \usepackage{bbm} \usepackage{boldline} \usepackage{bigstrut} \usepackage{blindtext} \usepackage{booktabs, siunitx} \usepackage[labelfont=bf, format=plain, justification=justified, singlelinecheck=false]{caption} \usepackage{color} \usepackage{cprotect} \usepackage{ctable} \usepackage{dirtytalk} \usepackage{enumitem} \usepackage[export]{adjustbox} \usepackage{float} \usepackage{graphicx} \usepackage{hhline} \usepackage{latexsym} \usepackage{mathrsfs} \usepackage{microtype} \usepackage{moresize} \usepackage{multicol} \usepackage{multirow} \usepackage{nccmath} \usepackage{nicefrac} \usepackage{pifont} \usepackage{placeins}      \usepackage{subcaption} \usepackage{times} \usepackage[utf8]{inputenc} \usepackage{url} \usepackage{verbatim} \usepackage{wrapfig, lipsum} \usepackage{textcomp} \usepackage{enumitem}  %  {HTML}{A6CEE3} \definecolor{lgreen}{HTML}{B2DF8A} \definecolor{lred}{HTML}{FB9A99} \definecolor{lorange}{HTML}{FDBF6F} \definecolor{mblue}{HTML}{80B1D3} \definecolor{mgreen}{HTML}{B3DE69} \definecolor{mred}{HTML}{FB8072} \definecolor{morange}{HTML}{FDB462} \definecolor{blue}{HTML}{1F78B4} \definecolor{green}{HTML}{33A02C} \definecolor{red}{HTML}{E31A1C} \definecolor{orange}{HTML}{FF7F00} \definecolor{dblue}{HTML}{0050EF} \definecolor{dgreen}{HTML}{006D2C} \definecolor{dorange}{HTML}{EC7014} } [1]{{ #1}} [1]{{ #1}} [1]{{ #1}} [1]{{ #1}} [1]{{ #1}} [1]{{ #1}}  [1]{^{4}^{1,2}^{1}^{2}^{3}^{4}$ETH Zurich\\ mrinmaya.sachan@inf.ethz.ch, wlh@cs.mcgill.ca\\ {\tt mrinmaya.sachan@inf.ethz.ch, wlh@cs.mcgill.ca} }   \date{}                        % !TeX root = main.tex  
"," Prior work on automated question generation has almost exclusively focused on generating simple questions whose answers can be extracted from a single document. However, there is an increasing interest in developing systems that are capable of more complex multi-hop question generation, where answering the questions requires reasoning over multiple documents. In this work, we introduce a series of strong transformer models for multi-hop question generation, including a graph-augmented transformer that leverages relations between entities in the text.  While prior work has emphasized the importance of graph-based models, we show that we can substantially outperform the state-of-the-art by {5 BLEU points}  using a standard transformer architecture. We further demonstrate that graph-based augmentations can provide complimentary improvements on top of this foundation. Interestingly, we find that several important factors---such as the inclusion of an auxiliary contrastive objective and data filtering could have larger impacts on performance.  We hope that our stronger baselines and analysis provide a constructive foundation for future work in this area.",173
"  Variational Autoencoders   allow to design complex generative models of data.  % since the inference process of VAE-based approaches has the advantage of being independent from the model architecture providing high flexibility in designing new neural components. In the wake of the renewed interest for VAEs, traditional probabilistic topic models  have been revised giving rise to several Neural Topic Model  variants, such as NVDM ,  ProdLDA , NTM-R , etc. % GSM , W-LDA  However, existing topic models when applied to user reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries of movies and books . Although these approaches have achieved significant results via the neural inference process, surprisingly very little work has been done on how to disentangle the inferred topic representations.   % Despite the lack of general consensus about a formal definition of disentangled representations ,   Disentangled representations can be defined as representations where individual latent units are sensitive to variations of a single generative factor, while being relatively invariant to changes of other factors . Inducing such representations has been shown to be significantly beneficial for their generalization and interpretability .  For example, an image can be viewed as the results of several generative factors mutually interacting, as one or many sources of light, the material and reflective properties of various surfaces or the shape of the objects depicted . %  In the context of topic modeling, documents result from a generative process over mixtures of latent topics, and therefore, we propose to consider these latent topics as generative factors to be disentangled to improve their interpretability and discriminative power. Disentangled topics are topics invariant to the factors of variation of text, which for instance, in the context of book and movie reviews could be the author's opinion , the salient parts of a plot or other auxiliary information reported. An illustration of this is shown in Fig. in which opinion topics are separated from plot topics.  % where this leads to separating topics based on the ``factor of variation"" they are revealing. % For example, in generating a book review, the factors of variation involved could depend on the author's expertise in identifying the salient features of the book, %his knowledge of the book's genre, or  % his ability to summarize the plot and the feelings evoked by the book.  % % [Let's break in/the atom] % % Figure  reports a examples of polarity-disentangled topics generated from the IMDB movie reviews of ""The Hobbit"". The topics on the left and right summarize some of the positive and negative aspects described by users, while neutral topics in the middle report the main elements of the movie's plot.  % An effective approach for disentangling features in the latent space of VAEs is to adopt adversarial training . However, despite its successful applications in computer vision , the applications to text analysis has been rather limited so far , narrowed by the lack of proper tasks to evaluate the generated disentangled representations and the limited availability of suitable datasets.  % For example, in book or movie reviews, we want to disentangle topics which are related to opinions expressed in text and topics relating to book/movie plots. An illustration of this is shown in Figure in which opinion topics are separated from plot topics.   However, models relying solely on sentiment information are easily misled and not suitable to disentangle opinion from plots, since even plot descriptions frequently make large use of sentiment expressions . Consider for example the following sentence: ``The ring holds a dark power, and it soon begins to exert its evil influence on Bilbo"", an excerpt from a strong positive Amazon's review.  % This overcomes the difficulty of separating opinions from plot and auxiliary information yet containing polarised descriptions that easily mislead models merely relying on sentiment lexicon; analogously to the issue of mixed topics generated when traditional topic models are applied to review documents, as pointed out in .  % Despite its successful employment in computer vision , the adversarial approach has had a rather limited application in text analysis so far , narrowed by the lack of proper tasks to evaluate the generated disentangled representations and the limited availability of suitable datasets.  Therefore, we propose to distinguish opinion-bearing topics from plot/neutral ones combining a neural topic model architecture with an adversarial training. In this study, we present the DIsentangled Adversarial TOpic Model \footnote{Source code and dataset omitted for the anonymous submission.}, aiming at disentangling information related to the target labels , from other distinct aspects yet possibly still polarised . We also introduce a new dataset, namely the MOBO dataset\footnotemark[\value{footnote}], made up of movie and book reviews, paired with their related plots. The reviews come from different publicly available datasets: IMDB , GoodReads  and Amazon reviews , %,  and encompass a wide spectrum of domains and styles. We conduct an extensive experimental assessment of our model. First, we assess the topic quality in terms of topic coherence and diversity and compare DIATOM with other supervised topic models on the sentiment classification task; then, we analyse the disentangling rate of topics to quantitatively assess the degree of separation between actual opinion and plot/neutral topics.    Our contributions are summarized below:   dataset, a new collection of movie and book reviews paired with their plots.    The rest of the paper is organized as follows. We review the related literature on sentiment-topic models, neural topic models and the studies on disentangled representations . Then, we present the details of our proposed DIATOM model , followed by the experimental setup  and results . Finally, we conclude with a summary of the results and suggestions for future works .     %%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," The flexibility of the inference process in Variational Autoencoders  has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models . Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. %Since in the topic modeling framework documents result from a generative process over mixtures of latent topics, we propose to interpret these latent topics as generative factors to be disentangled to improve their interpretability and discriminative power. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models.",174
"   Mutual understanding in interactive situations, either when several people are engaged in a dialogue or when they are interacting with a modern computer system in natural language, may not be achieved without considering both the semantic information in the speakers utterances and the pragmatic interaction level, especially relative to dialogue acts. Dialogue Acts  represent the meaning of an utterance  in the context of a dialogue, or, in other words, the function of an utterance in the dialogue. For example, the function of a~question is to request some information, while an answer shall provide this information. Dialogue acts are thus commonly represented as phrase-level labels such as statements, yes-no questions, open questions, acknowledgements, and so on.  Automatic recognition of dialogue acts is a fundamental component of many human-machine interacting systems that support natural language inputs. For instance, dialogue acts are typically used as an input to the dialogue manager to help deciding on the next action of the system: giving information when the user is asking a question, but eventually keeping quiet when the user is just acknowledging, giving a comment, or even asking for delaying the interaction. In the latter case, a system reaction may be perceived as intrusive. Beyond human-machine interaction, this task is also important for applications that rely on the analysis of human-human interactions, either oral, e.g., in recordings of meetings, or % lada - added reference according to rev 1 written, e.g., through the reply and mention-at structures in Twitter conversations. It is also essential for a large range of other applications, for example talking head animation, machine translation, automatic speech recognition or topic tracking. The knowledge of the user dialogue act is useful to render facial expressions of an avatar that are relevant to the current state of the discourse. In the machine translation domain, recognizing dialogue acts may bring relevant cues to choose between alternative translations, as the adequate syntactic structure may depend on the user intention. Automatic recognition of dialogue acts may also be used to improve the word recognition accuracy of automatic speech recognition systems, where a different language model is applied during recognition depending on the dialogue act. %lada - added reference according to rev 1,   To conclude, dialogue act recognition is an important building block of many understanding and interacting systems. %pav --I've commented the rest of the sentence, because it was not clear for 2 reviewers ) -- and typically completes semantic role labelling and dialogue management.   Researches on dialogue act recognition have been carried out for a long time, as detailed in Section. The majority of these works exploit supervised learning with lexical, syntactic, prosodic and/or dialogue history features. However, few approaches consider semantic features, while they may bring additional information and prove useful to improve the accuracy of the dialogue act recognition system. For instance,  a~frequent cause of recognition errors are ``unknown'' words in the testing corpus that never occur in the training sentences. Replacing specific named entities in the text  by their category has been proposed in the literature as a remedy to this issue. We investigate a more general solution that exploits lexical similarity between word vectors. These word vectors may be computed in various ways, but they typically include mostly lexical semantic information about the word itself as well as some syntactic information, e.g., related to the relative position or degree of proximity of pairs of words within a sentence. This additional information may be used to improve dialogue act recognition, in particular when the training and test conditions differ, or when the size of the training corpus is relatively small.  %goal In this work, we propose a new Deep Neural Network  based on Long Short-Term Memory  for the task of dialogue act recognition, and we compare its performance to a standard Maximum Entropy model. Our first objective is to leverage the modelling capacity of such a DNN in order to achieve dialogue act recognition with only the raw observed word forms, i.e., without any additional expert-designed feature. This model is described in Section. The second objective is to further validate this model both on a standard English DA corpus, as well as on two other languages, without changing anything in the model, in order to assess the genericity and robustness of the approach. These experiments are summarized in Section. Finally, our third objective is to study the impact of word embeddings, which have been shown to provide extremely valuable information in numerous Natural Language Processing  tasks, but which have never been used so far~\footnote{To the best of our knowledge at the time of submission} for dialogue act recognition. This study is summarized in Section. %The following Section presents a review of related works of the domain.  
"," Dialogue act recognition is an important component of a large number of natural language processing pipelines. Many research works have been carried out in this area, but relatively few investigate deep neural networks and word embeddings. This is surprising, given that both of these techniques have proven exceptionally good in most other language-related domains. We propose in this work a new deep neural network that explores recurrent models to capture word sequences within sentences, and further study the impact of pretrained word embeddings. We validate this model on three languages: English, French and Czech. The performance of the proposed approach is consistent across these languages and it is comparable to the state-of-the-art results in English. More importantly, we confirm that deep neural networks indeed outperform a Maximum Entropy classifier, which was expected. However, and this is more surprising, we also found that standard word2vec embeddings do not seem to bring valuable information for this task and the proposed model, whatever the size of the training corpus is. We thus further analyse the resulting embeddings and conclude that a possible explanation may be related to the mismatch between the type of lexical-semantic information captured by the word2vec embeddings, and the kind of relations between words that is the most useful for the dialogue act recognition task.",175
"       and the one copied from its neighbor cases as horizontal copy.}        As an important task in Natural Language Generation , dialogue generation empowers a wide spectrum of applications, such as chatbot and customer service automation. In the past few years, breakthroughs in dialogue generation technology focused on a series of sequence-to-sequence models .  More recently, external knowledge is employed to enhance model performance. % , for instance, propose Mem2Seq using structured knowledge in task-oriented dialogue generation.   can assist dialogue generation by using knowledge triples. Similarly,  explore document as knowledge discovery for dialogue generation, and  utilize unstructured knowledge to explore in the open-domain dialogue generation. However, unaffordable knowledge construction and defective domain adaptation restrict their utilization.   Copy-based generation models  have been widely adopted in content generation tasks and show better results compared to sequence-to-sequence models when faced with out-of-vocabulary problem. Thanks to their nature of leveraging vocabulary and context distributions for content copy, it enables to copy the aforementioned named entities  appeared in the above context) from the upper context to improve the specificity of the generated text.    In the task of dialogue generation, we can often observe the phrases/utterance patterns across different ""similar dialogue"" instances. For example, in customer service, the similar inquiries from the customers will get similar responses from the staff. It motivates us to build a model that can not only copy the content within the upper context of the target dialogue instance, but also learn the similar patterns across different similar cases of the target instance. Such external copy can be critical in some scenarios.  %Fi Judge's questions, in the target court debate case, can be copied from both internal and external sources, and this `cross-copy' can enhance the dialougue generation essentially.    Figure this paper, we are aware of the possibility of copying  from adjacent Unfortunately, these methods only enable internal copy, e.g., copy the content within the target dialogue instance. External copy, e.g., copy content across different dialougue instances, is incapable. However, as Figure. depicted,   %is another effective network structure. It solved the problem that the traditional sequence-to-sequence model cannot solve the problem that the vocabulary of the output sequence will change with the length of the input sequence. %Copynet proposed humans tend to repeat entity names or even long phrases in conversation.And then generate the entity that appeared in the previous article will be copied. %Recently, Pointer networks and Copynet's variants have played a very important role in NLG. Among them, Pointer-Generator Networks  was proposed. %In order to copy the key information from the context as well as cope with the Out-Of-Vocabulary problem. It relies on the vocabulary distribution and context distribution, the extended vocabulary is further obtained. % GLMP proposed a global memory encoder and a local memory decoder to share external knowledge by Pointer networks. %}As general domain network structure, the pointer network  and Copynet  shows fine effect in general text generation tasks. It not only can solves the problem of domain adaptability poor in dialog generation, does not introduce external knowledge, but also address Out-Of-Vocabulary  problem and enable content copy. % Pointer networks  and Copynet  provided effective approach to address Out-Of-Vocabulary  problem and enable content copy.  %The more recent effort, Pointer-Generator Networks  , inherited their advantages by leveraging vocabulary and context distributions for content copy.   As shown in Figure., we propose two different kinds of copy mechanisms in this study: vertical copy context-dependent information within the target dialogue instance, and horizontal copy logic-dependent content across different 'Similar Cases' . This framework is labeled as Cross-Copy Networks . As exemplar dialogue depicted, judges may repeat  words, phrases or utterances from historical dialogues when those SCs sharing similar content, e.g., `A sue B because of X and Y'.  %In this study, 'Similar Cases'  refers to a similar dialogue for each dialogue. When generating the next sentence based on the historical dialogue, we can refer to the similar dialogue of the dialogue to obtain it. In this paper, we propose a new network: Cross-Copy Networks, which can not only copy the previous entity, but also learn the logic of dialogue generation and copied specific words, phrases or utterance from similar cases to deal with out-of-vocabulary  words. % The CCN has two pointers, one can copy the specific entity or sentence from the context and another can copy the process discourse or a complete sentence from SC.  % As shown in Figure 1, there are two similar cases and a target case. Our copy methods are divided into two types, internal copy and external copy.  internal copy: we can directly copy some specific entities words that appear in the context as the words to be generated.  external copy: we can copy related sentences or phrases in similar cases as the directly generated sentences.   % As shown in Fig., There are three samples of CCN:  Selective copy: it can copy some specific words or phrases from SC as sentences to be generated, as sample 1.  Cross copy: it copy specific entities from the context, and then copy some process-frame nature sentences in SC, as sample 2.  Deep copy: it can copy some process discourse directly as a generated sentence, usually this sentence appears frequently in the full text, as sample 3.  In order to validate the proposed model, we employ two different dialogue datasets from two orthogonal domains - court debate and customer service. We apply proposed CCN to both datasets for dialogue generation. Experiments show that our model achieves the best results. To sum up, our contributions are as follows:   )$, which enables internal  copy from the target dialogue and external  copy from similar cases in the dataset without employing any external resources.  .  % available   and also release our code. % 
"," In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models  to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation.  In this paper, we propose a novel network architecture - Cross Copy Networks  to explore the current dialog context and similar dialogue instances闁 logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models. % The traditional sequence-to-sequence model  has achieved good results in Natural Language Generation tasks. % For dialogue generation task in specific areas , some of the utterances by judge and customer service personnel to be saied usually contain specific logic and this utterances are highly similar. % Therefore, when generating the current utterance, we need to refer to not only the current context but also similar cases. % In this paper, we proposed a new neural network architecture named Cross Copy Networks , It locates entity in the context and the logical expression of similar cases by learning two conditional probability pointers. % We apply CCN to the legal dialogue data and customer service dialogue data for dialogue generation task. % Experiments show that our model achieves the best results.",176
"   In recent years, there has been an increased focus on the use of unannotated texts for modeling human language and on transfer learning in natural language processing . A wide variety of models have been proposed, ranging from context-independent word embeddings , to the more recent contextual representations . In particular, the Transformer-based  BERT  model  has generated considerable interest in the NLP community since its release. BERT outperformed the then state-of-the-art systems on a wide range of benchmark datasets when published, and has served as the basis of many studies since. These efforts include work that proposes improvements and/or modifications to the training objectives , knowledge distillation , multilinguality , and interpretation , to name a few. As a mark of its popularity, the term BERTology was coined to refer to the field of research relating to BERT .  % Multilinguality, zero-shot transfer, monolingual BERTs A thriving branch of BERTology involves BERT models for languages other than English.  released multilingual BERT  models trained on over a hundred languages. % what is good about multilingual model  analyze the representations produced by mulitlingual BERTs and find evidence that these representations generalize across languages for various downstream tasks, though language-specific information is retained. This language-agnostic subspace of multingual BERTs has also been observed in other studies, and is deemed to be the factor that allows for zero-shot transfer . % also find evidence that multilingual BERTs learn a language-agnostic subspace for linguistic information.  also show that these multilingual models have their embeddings partially aligned, which allows for zero-shot transfer. Furthermore, the embeddings can be further aligned through a fine-tuning based alignment procedure, improving the performance of multilingual models . % the curse of multilinguality  % 'we scale the number of languages for a fixed model capacity: more lan-guages leads to better cross-lingual performanceon low-resource languages up until a point, afterwhich the overall performance on monolingual andcross-lingual benchmarks degrades' While multilingual training can benefit also monolingual performance, as the number of languages covered by a multilingual model increases, the fraction of the model capacity available for any single language decreases.  % The number of languages included in a multilingual model, however, affects its performance, and a phenomenon referred to as the curse of multilinguality has been observed.  term as the  the phenomenon where increasing the number of languages included in a model initially leads to better cross-lingual performance for low-resource languages, while eventually leading to overall degradation of both monolingual and cross-lingual performance. Work on language-specific BERT models has also shown that monolingual models tend to outperform multilingual models of the same size in monolingual settings . % To further benefit languages other than English, monolingual BERTs for various languages have been trained and released by the NLP community . %\todo{Is any transition needed here?} % SMP: added the following However, the question of whether it is possible to train multilingual models without loss of monolingual performance remains largely open. %To minimize the effect of the curse of multilinguality but still benefit from the language-agnostic subspace,  % Furthermore, the underlying reason why BERT works, the inner representation of BERT, a line of research has focused on the inner-workings of BERT [put this here or related work?] In this paper, we study whether it is feasible to pre-train a bilingual model for two remotely related languages without compromising performance at either language. Specifically, we train a Finnish-English bilingual BERT model  using a combination of the pre-training data of the original English BERT model and the Finnish BERT model introduced by , using an extended model vocabulary but otherwise fixing model capacity at BERT-Base size and retaining the number of pre-training steps. % carefully rephrase this! %While there has been reports of bilingual BERTs focusing on their cross-linguality, to the best of our knowledge, there has not been any work focusing on comparing the performance of bilingual models on natural language understanding  tasks with their monolingual counterparts. We evaluate the performance of the introduced bilingual model on a range of natural language understanding  tasks used to evaluate the monolingual models, which, to the best of our knowledge, has not been the focus of studies on bilingual BERT models. We find that  achieves comparable performance on the GLUE  benchmark  with the original English BERT, and nearly matches the performance of the Finnish BERT on Finnish NLP tasks. Our results indicate that an extension of the vocabulary size is sufficient to allow the creation of fully bilingual models that perform on par with their monolingual counterparts in both of their languages.  
"," Language models based on deep neural networks have facilitated great advances in natural language processing and understanding tasks in recent years. While models covering a large number of languages have been introduced, their multilinguality has come at a cost in terms of monolingual performance, and the best-performing models at most tasks not involving cross-lingual transfer remain monolingual. In this paper, we consider the question of whether it is possible to pre-train a bilingual model for two remotely related languages without compromising performance at either language. We collect pre-training data, create a Finnish-English bilingual BERT model and evaluate its performance on datasets used to evaluate the corresponding monolingual models. Our bilingual model performs on par with Google's original English BERT on GLUE and nearly matches the performance of monolingual Finnish BERT on a range of Finnish NLP tasks, clearly outperforming multilingual BERT. % performance on Finnish datasets is not as good as FinBERT, how to put that into words %We find that the bilingual model achieves comparable performance as the English BERT and nearly matches the performance of FinBERT. We find that when the model vocabulary size is increased, the  architecture has sufficient capacity to learn two remotely related languages to a level where it achieves comparable performance with monolingual models, demonstrating the feasibility of training fully bilingual deep language models. % We describe the procedure taken to train this bilingual BERT. The model and all tools involved in its creation are freely available at \url{https://github.com/TurkuNLP/biBERT}",177
"  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  A long-standing challenge in computer science is to develop algorithms that can interact with human users via dialog in natural language~.  Of particular interest is task-oriented dialog, wherein a user interacts with a system to achieve some goal .  The system should understand the user's requests and assist them by taking the appropriate actions . In recent years, supervised learning approaches to this problem have become particularly popular, because they can potentially learn complex patterns without relying on hand-crafted rules. While such data-driven methods already demonstrate impressive performance in open-domain dialog , task-oriented dialog models face the additional difficulty of transferring skills to tasks and domains that were not present in the training data.  To address this issue, we present the Schema-guided Dialog Dataset for Transfer Learning  dataset, a collection of realistic, task-oriented dialogs, that is especially designed to test and facilitate the transfer of learned patterns between tasks.  Unlike open-domain dialogs, task-oriented dialogs are accompanied by a set of steps that are necessary to complete the task.  These steps are typically known  and thus do not have to be learned from the data. In fact, for practical applications it is desirable that we could make modifications to this logic without having to discard large parts of the dataset. The ideal sequences of steps that a dialog would follow to complete the task can be arranged in a graph . Together with the utterances or actions that are associated with the nodes of this graph, we hence call this a task schema, or simply schema. Note, that what we call `schema' is similar to the `task specification' of , but distinct from the `schemas' that only define slots and intents of a task as used by . %or .     In a typical supervised model that is trained to, say, predict the next system action for a task-oriented dialog, the schema of the training tasks is implicitly captured by the learned model parameters. This makes generalizing to a new task difficult, as the implicitly memorized schema will no longer be appropriate .  With \DATASETNAME\ we provide explicit schema representations for each task and thereby enable models to condition on the schema .  To collect \DATASETNAME\ we use a Wizard of Oz setup , where the system's role is played by a human `wizard'. Based on our pilot studies, we found that the quality of crowd-sourced dialogs depends strongly on  []        We refined our approach through extensive internal testing and four rounds of pilot studies.  % All code and instructions are available as open source at \anonymous{\DATASETURL}.   Our aim is to create an ecologically valid dataset  with the following four attributes, which we believe are crucial for a dataset to be of high quality:      . %     Realistic dialog rarely strictly follows the ideal path, but is interrupted by small talk, changes of the user's mind, and references to events that happen in the user's environment. %     In \DATASETNAME\ we capture these behaviors.      %     We collect three kinds of dialogs: %     %          where the dialog progresses along one of the paths in the schema, %           where the user adds complexity to the task , and %          where the user engages in a complex dialog spanning multiple domains and tasks . %     %     The progression of difficulty allows better assessment of dialog models and potential for transfer learning across levels of difficulty.     . % The behavior of a task-oriented dialog system should be largely deterministic and not subject to the whims or personality of the wizard. %     In particular, we encourage wizards to follow the given task schema as closely as possible.     . %     A large part of developing a dialog system is the implementation of application programming interface  calls, such as knowledge base queries. %     In \DATASETNAME\ we represent our dialogs as a three-party interaction wherein the system acts as the intermediary between a user and a knowledge base . %     Thus, models have to learn when to query the knowledge base, what the query should be, and how to explain the returned knowledge base item to the user.   % With these properties, we create a is ecologically valid, as described by.  With this paper, we contribute  []      The code for the latter setup, all collected  data, and all modeling code is freely available under \anonymous{\DATASETURL}.   
"," We present \DATASETNAME, a schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog. Furthermore, we propose a scalable crowd-sourcing paradigm to collect arbitrarily large datasets of the same quality as \DATASETNAME. Moreover, we introduce novel schema-guided dialog models that use an explicit description of the task to generalize from known to unknown tasks.  We demonstrate the effectiveness of these models, particularly for zero-shot generalization across tasks and domains.",178
" % -------------------------------------------------------------- % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % final paper: en-us version          % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}.       } The relationship between a group of human languages can be characterized across several dimensions of variation , including  the temporal dimension, wherein languages have diverged from a common historical ancestor as in the case of Romance languages;  the spatial dimension, wherein the speaker communities are geographically adjacent as in the case of the Indo-Aryan and Dravidian languages of India; and  the socio-political dimension, wherein languages have evolved under shared political and/or religious forces as in the case of Arabic and Swahili. Languages, or language varieties, can be related across all these dimensions, which often results in a dialect continuum. Speakers of languages that constitute a dialect continuum can usually communicate with each other efficiently using their own mother tongue. The degree of intercomprehensibility between speakers of different language varieties within a continuum is mainly determined by linguistic similarities. A notable case of this phenomenon is the mutual intelligibility among the Slavic languages, which we study in this paper.   One of the goals of linguistics is to study and categorize languages based on objective measures of linguistic distance. The degrees of similarity at different levels of the linguistic structural organization can be seen as preconditions for, as well as predictors of, successful oral intercomprehension.  For closely-related languages, similarities at the pre-lexical, that is the acoustic-phonetic and phonological, level have been found to be better predictors of cross-lingual speech intelligibility than lexical similarities . In a different, yet relevant research direction,  have investigated non-linguists' perception of language variation using data from the popular spoken language guessing game, the Great Language Game . By analyzing the confusion patterns of the GLG's human participants, the authors have shown that factors predicting players' confusion in the game correspond to objective measures of similarity established by linguists. For example, both phylogenetic relatedness and overlap in phoneme inventories have been identified as factors of perceptual confusability  of languages in GLG.   The development of automatic systems that determine the identity of the language in a speech segment has received attention in the speech recognition community . State-of-the-art approaches for automatic spoken language identification, henceforth LID, are based on multilayer deep neural networks . DNN-based LID systems are parametric models that learn a mapping from spectral acoustic features of  speech to high-level feature representations in geometric space where languages are linearly separable. These models have shown tremendous success not only in discriminating between distant languages but also closely-related language varieties . Nevertheless, none of the previous works in spoken language recognition has analyzed the emerging representations from neural LID models for related languages. Thus, it is still unknown whether the distances in these representation spaces correspond to objective measurements of linguistic similarity and/or to non-linguists' perception of language variation. In this paper, we aim to fill this gap and consider the family of Slavic languages as a case study. Our key contribution is two-fold:  [label={}, noitemsep]  In this paper, we attempt to bridge different lines of research that have so far remained unconnected. On the one hand, we employ neural architectures from the field of spoken language recognition and build a robust model to identify languages in contemporary acoustic realizations of Slavic speech. On the other hand, we analyze the emerging language representations using techniques established by previous research in multilingual natural language processing . We consequently shed light on the speech modality and show how  speech signals can complement research done in computational studies of linguistic typology and language variation.   %  to the best of our knowledge  % The recognition of spoken language   % LID in speech technology  % untranscribed speech   % NN has made possible for end-to-end systems to be developed, while traditional approaches feature many components   % closely-related languages have similar phonotactics, but differ in acoustic realizations of segments and suprasegmental features   % language identity and objective linguistic measures of similarity   % The GLG   % similarity of representation in deep neural networks    % -------------------------------------------------------------- 
"," Deep neural networks have been employed for various spoken language recognition tasks, including tasks that are multilingual by definition such as spoken language identification. In this paper, we present a neural model for Slavic language identification in speech signals and analyze its emergent representations to investigate whether they reflect objective measures of language relatedness and/or non-linguists' perception of language similarity. While our analysis shows that the language representation space indeed captures language relatedness to a great extent, we find perceptual confusability between languages in our study to be the best predictor of the language representation similarity.",179
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  $ Corresponding author.\\     This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: \url{http://creativecommons.org/licenses/by/4.0/}.     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Aspect-level sentiment classification  is a fundamental task in sentiment analysis , which aims to infer the sentiment polarity  of a given opinion target in a review sentence. An opinion target, also known as aspect term, refers to a word or a phrase in review describing an aspect of an entity. For example, the sentence `` are great, but the service is dreadful}'' consists of two opinion targets, namely ``'' and ``''. User's sentiment towards the opinion target ``'' is positive while negative in terms of target ``''. Traditional methods usually focus on designing a set of features such as bag-of-words or sentiment lexicon to train a classifier  for ASC. Motivated by the great success of deep learning in computer vision, speech recognition and natural language processing, recent works use neural networks to  learn low-dimensional and continuous text representations without any feature engineering, and achieve competitive results on the ASC task.  From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion contexts for different targets. To this end, abundant state-of-the-art works employ attention mechanism to capture sentiment words related to the given target, and then aggregate them to make sentiment prediction. Despite the effectiveness of attention mechanism, we argue that it fails to reach the full potential due to the limited ASC labeled data. It is well-known that the promising results of deep learning heavily rely on sufficient training data. However, the annotation of ASC data is very labour-intensive and expensive in real-world scenarios, because annotators need to not only identify all opinion targets in a sentence but also determine their corresponding sentiment polarity. The difficulty of annotation leads to that existing public aspect-level datasets are all relatively small-scale, which finally limits the potential of attention mechanism.  Despite the lack of ASC data, enormous labeled data of document-level sentiment classification  are available at online review sites such as Amazon and Yelp. These reviews contain substantial sentiment knowledge and semantic patterns. Therefore, one meaningful but challenging research question is how to leverage resource-rich DSC data to improve the low-resource task ASC. For this purpose,~ design the PRET+MULT framework to transfer sentiment knowledge from DSC data to ASC task through sharing shallow embedding and LSTM layer. Inspired by the capsule network,~ propose TransCap to share bottom three capsule layers, then separate two tasks only in the last ClassCap layer. Fundamentally, PRET+MULT and Transcap improve ASC by sharing parameters and multi-task learning, but they cannot accurately control and interpret what knowledge to be transferred. In this work, we directly focus on the aforementioned attention issue in the ASC task and propose a novel framework, Attention Transfer Network , to explicitly transfer attention knowledge from the DSC task for improving the attention capability of the ASC task. Compared with PRET+MULT and Transcap, our model achieves better results and retains good interpretability.  In the ATN framework, we adopt two attention-based BiLSTM networks, respectively, as the DSC module and base ASC module, and propose two different methods to transfer attention from DSC to ASC. The first transfer approach is called . Specifically, we first pre-train an attention-based BiLSTM on large-scale DSC data, then exploit the attention weights from the DSC module as a learning signal to guide the ASC module to capture sentiment clues more accurately, thereby acheiving improvements. The second approach adopts the way of , and directly incorporates the attention weights of the DSC module into the ASC module. The two approaches work in different ways and have their different advantages.  aims to learn the attention ability of the DSC module and has faster inference speed, since it does not use external attention from DSC during the testing stage. In contrast,  can leverage the attention knowledge of the DSC module during the testing stage and make more comprehensive predictions.  We conduct experiments on two benchmark datasets to evaluate different methods. The results indicate that the ATN model can be substantially improved by incorporating the two attention transfer approaches, and outperforms all compared methods on the ASC task.  %We conducted experiments on attention-based LSTM models using the SemEval 2014 dataset. The results show that attention-based LSTM can be substantially improved by incorporating our two proposed methods, and that the resulting model outperforms all baseline methods on aspect-level sentiment classification. Further analysis also shows the good interpretability of our approaches.  
","   Aspect-level sentiment classification  aims to detect the sentiment polarity of a given opinion target in a sentence. In neural network-based methods for ASC, most works employ the attention mechanism to capture the corresponding sentiment words of the opinion target, then aggregate them as evidence to infer the sentiment of the target. However, aspect-level datasets are all relatively small-scale due to the complexity of annotation. Data scarcity causes the attention mechanism sometimes to fail to focus on the corresponding sentiment words of the target, which finally weakens the performance of neural models. To address the issue, we propose a novel Attention Transfer Network  in this paper, which can successfully exploit attention knowledge from resource-rich document-level sentiment classification datasets to improve the attention capability of the aspect-level sentiment classification task. In the ATN model, we design two different methods to transfer attention knowledge and conduct experiments on two ASC benchmark datasets. Extensive experimental results show that our methods consistently outperform state-of-the-art works. Further analysis also validates the effectiveness of ATN. Our code and dataset are available at \url{https://github.com/1429904852/ATN}.",180
" For a conversational AI or digital assistant system , Natural Language Understanding  is an established component that produces semantic interpretations of a user request, which typically involves analysis in terms of domain, intent, and slot . For instance, the request  can be interpreted as falling within the scope of  domain with  intent and  identified for  slot.  Improving the accuracy of the NLU component is important for satisfactory end-to-end user experience. Without an accurate semantic understanding of the user request, a conversational AI system cannot fulfill the request with a satisfactory response or action. As one of the most upstream components in the runtime workflow , NLU's errors also have a wider blast radius that propagates to all subsequent downstream components, such as dialog management, routing logic to back-end applications, and language generation.  A straight-forward way to improve NLU is through human annotations. For example, we can mine the user requests that resulted in unsatisfactory user experience and make ground-truth annotations on those requests that produced incorrect NLU outputs, which can be used as additional supervision data for improving the models or rule engines within NLU. However, this approach is labor-intensive and expensive. It requires at least multiple tiers of annotations , and it is hard to consider all underlying contextual conditions. It is also limited by the existing annotation guidelines that may not accurately reflect user expectations. Due to these limitations, leveraging user feedback, both implicit and explicit, from real production systems is emerging as a new area of research.     In this paper, we propose a scalable and automatic approach for improving NLU by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. For instance, while interacting with a conversational AI system, dissatisfied users might often choose to intervene by stopping the system response in the middle and rephrasing the previous request to make it clearer with less room for ambiguous interpretation .  Our work makes three main contributions. First, to our knowledge, this work is the first in the literature to introduce a scalable and automatic approach of leveraging domain-agnostic implicit user feedback that can continuously improve the NLU component of a large-scale conversational AI system in production. Second, we propose a general framework for curating supervision data for improving NLU from live traffic that can be leveraged for various subtasks within NLU - e.g., the supervision data can be applied to improve individual semantic interpretation models  or a ranking/classification model across all interpretations . Last, we show with an extensive set of experiments on live traffic the performance of the proposed framework and its impact on improving NLU in the production system across 10 widely used domains. \def\year{2021}\relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21}  % DO NOT CHANGE THIS \usepackage{times}  % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier}  % DO NOT CHANGE THIS \usepackage[hyphens]{url}  % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm}  % DO NOT CHANGE THIS \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing  % DO NOT CHANGE THIS {8.5in}  % DO NOT CHANGE THIS {11in}  % DO NOT CHANGE THIS  \usepackage{amsfonts} \usepackage{amsmath} \usepackage{algorithm} \usepackage{xcolor} \usepackage[noend]{algpseudocode}  %  %Leave this % /Title  % Put your actual complete title  within the parentheses in mixed case % Leave the space between \Title and the beginning parenthesis alone % /Author  % Put your actual complete list of authors  within the parentheses in mixed case. % Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, % remove them.  % DISALLOWED PACKAGES % \usepackage{authblk} -- This package is specifically forbidden % \usepackage{balance} -- This package is specifically forbidden % \usepackage{color  % \usepackage{CJK} -- This package is specifically forbidden % \usepackage{float} -- This package is specifically forbidden % \usepackage{flushend} -- This package is specifically forbidden % \usepackage{fontenc} -- This package is specifically forbidden % \usepackage{fullpage} -- This package is specifically forbidden % \usepackage{geometry} -- This package is specifically forbidden % \usepackage{grffile} -- This package is specifically forbidden % \usepackage{hyperref} -- This package is specifically forbidden % \usepackage{navigator} -- This package is specifically forbidden %  %  -- This package is specifically forbidden % \usepackage{setspace} -- This package is specifically forbidden % \usepackage{stfloats} -- This package is specifically forbidden % \usepackage{tabu} -- This package is specifically forbidden % \usepackage{titlesec} -- This package is specifically forbidden % \usepackage{tocbibind} -- This package is specifically forbidden % \usepackage{ulem} -- This package is specifically forbidden % \usepackage{wrapfig} -- This package is specifically forbidden % DISALLOWED COMMANDS %  --- A Guide } % \author{  %     %Authors %     % All authors must be in the same font size and format. %     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\ %     AAAI Style Contributions by Pater Patel Schneider, %     Sunil Issar,  \\ %     J. Scott Penberthy, %     George Ferguson, %     Hans Guesgen, %     Francisco Cruz, %     Marc Pujol-Gonzalez %     \\ % } % \affiliations{ %     %Afiliations  %     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\ %     %If you have multiple authors and multiple affiliations %     % use superscripts in text and roman font to identify them. %     %For example,  %     % Sunil Issar, \textsuperscript{\rm 2} %     % J. Scott Penberthy, \textsuperscript{\rm 3} %     % George Ferguson,\textsuperscript{\rm 4} %     % Hans Guesgen, \textsuperscript{\rm 5}. %     % Note that the comma should be placed BEFORE the superscript for optimum readability  %     2275 East Bayshore Road, Suite 160\\ %     Palo Alto, California 94303\\ %     % email address must be in roman text type, not monospace or sans serif %     publications21@aaai.org  %     % See more examples next % } % \author {     Sunghyun Park\thanks{Equal contribution.}, Han Li\textsuperscript{\rm *}, Ameen Patel, Sidharth Mudgal, Sungjin Lee, Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya \\ } \affiliations{     % Affiliations     Amazon Alexa AI \\     \{sunghyu, lahl, paameen, sidmsk, sungjinl, youngbum, matsouka, rsarikay\}@amazon.com } %\fi   \author {     Authors         First Author Name,\textsuperscript{\rm 1}         Second Author Name, \textsuperscript{\rm 2}         Third Author Name \textsuperscript{\rm 1} \\ } \affiliations {     % Affiliations     \textsuperscript{\rm 1} Affiliation 1 \\     \textsuperscript{\rm 2} Affiliation 2 \\     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com } \fi   [1]{{ #1}} [1]{\mathbf{#1}}  {Definition}        Natural Language Understanding  is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a general domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system and show its impact across 10 domains.            \newpage  %    
"," Natural Language Understanding  is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a general domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system and show its impact across 10 domains.",181
"   Chinese Word Segmentation  is a fundamental task for Chinese natural language processing , which aims at identifying word boundaries in a sentence composed of continuous Chinese characters. It provides a basic component for other NLP tasks like named entity recognition, dependency parsing, and semantic role labeling, etc.  Generally, most previous studies model the CWS task as a character-based sequence labeling task . Recently, pre-trained models  such as BERT  have been introduced into CWS tasks, which could provide prior semantic knowledge and boost the performance of CWS systems.  directly fine-tunes BERT on several CWS benchmark datasets.  fine-tunes BERT in a multi-criteria learning framework, where each criterion shares a common BERT-based feature extraction layer and owns a private projection layer.  combines Chinese character glyph features with pre-trained BERT representations. %  builds a unified BERT-based model for multi-criteria CWS tasks and fine-tunes it on eight CWS criteria jointly.  proposes a neural CWS framework WMSeg, which utilizes memory networks to incorporate wordhood information into the pre-trained model ZEN.  PTMs have been proved quite effective by fine-tuning on downstream CWS tasks. However, PTMs used in previous works usually adopt language modeling as pre-training tasks. Thus, they usually lack task-specific prior knowledge for CWS and ignore the discrepancy between pre-training tasks and downstream CWS tasks.  [t]  {c|}{the semi-final} \\ {c|}{閺夊骸顭倉 & 鏉╂稑鍙 & \multicolumn{2}{c|}{閸楀﹤鍠呯挧娉 \\ {c|}{閺夊骸顭倉 & 鏉╂稑鍙 & 閸 & 閸愬疇绂 \\ Seg. To leverage shared segmentation knowledge of different criteria, MetaSeg utilizes a unified architecture and introduces a multi-criteria pre-training task. Moreover, to alleviate the discrepancy between pre-trained models and downstream unseen criteria, meta learning algorithm is incorporated into the multi-criteria pre-training task of MetaSeg.  Experiments show that MetaSeg could outperform previous works significantly, and achieve new state-of-the-art results on twelve CWS datasets. Further experiments show that  MetaSeg has better generalization performance on downstream unseen CWS tasks in low-resource settings, and improve Out-Of-Vocabulary  recalls. To the best of our knowledge, MetaSeg is the first task-specific pre-trained model especially designed for CWS.   
","     Recent researches show that pre-trained models  are beneficial to Chinese Word Segmentation .     However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks.     % However, existing approaches usually fine-tune general-purpose pre-trained models directly on separate downstream CWS corpora.     % These general-purpose pre-trained models usually adopt language modeling objectives, lack task-specific prior segmentation knowledge, and ignore the discrepancy between pre-training tasks and downstream CWS tasks.     In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task.     Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks.     Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings.",182
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  The following instructions are directed to authors of papers submitted to COLING-2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4   paper.  Authors from countries in which access to word-processing systems is limited should contact the publication co-chairs Derek F. Wong , Yang Zhao  and Liang Huang  as soon as possible.  We may make additional instructions available at \url{http://coling2020.org/}. Please check this website regularly.   
","   This document contains the instructions for preparing a paper submitted   to COLING-2020 or accepted for publication in its proceedings. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers. Authors are asked to conform to all the directions   reported in this document.",183
"  Automatic question answering is a very active area of research within natural language processing. %Open-domain question answering looks for methods to re-utilize systems across multiple domains. One possible way to approach this task is to look for answers in the text passages of a collection of documents. Recent research has shown promising results on developing neural models for passage retrieval tasks, including Retrieval Question Answering, Open Domain Question Answering, and MS MARCO. The models in these systems are often trained using the dual encoder framework where questions and passages are encoded separately. Training an effective neural retrieval model usually requires a large amount of high-quality data. To alleviate the need of high-quality data, training can be approached in two-stages: pre-training on noise data and fine tuning on a smaller amount of high-quality data, also regarded as ``gold"" data. % One significant advantage of the dual encoder framework is that, once the question and passage embeddings are available, efficient nearest neighbour search can be used to retrieve the passages that contain the answers to the questions.    When used for question answering, one advantage of the dual encoder is that training in batches allows to use, for each question, the passages that answer all the other questions in the batch as negatives. Given that the training batches are randomly sampled from all the question-passage pairs, the negatives in the batch are random in nature. While effective in many retrieval tasks, random negatives have the limitation of not being targeted nor challenging enough to clearly separate the passage that answers a given question from any other passage. How to sample the negatives in a way that widens this separation and improves the contrast between the correct and incorrect passages remains an open question.  % A viable approach to negative sampling is to use ``hard"" negatives that are specific to each question and answer  pair. In this paper we systematically explore the use of ``hard'' negatives in the neural passage retrieval models that we train using a two-stage approach. Using hard negatives as part of the dual encoder framework has shown advantageous in different tasks . %Using hard negatives as part of the dual encoder framework has shown advantageous in cross-lingual tasks. %For example,  show that training with hard negatives generated by retrieving ``coarse"" negatives with low-resolution model improves the quality of the translation pairs retrieved with a dual encoder model. %Similarly,  showed improvement when using hard negatives retrieved with a BM25 model in the passage retrieval part of the Open Domain Question Answering task. %In contrast to previous works,  We explore different types of negatives, and experiment using them in both the pre-training and fine-tuning stages. The types of negatives we tried are: [label=]  ;}     We first use hard negatives on the data that we use to pre-train the models. We leverage the question generator model described in and generate new questions for each of the passages we use in the pre-training stage . %The new questions are paired with the original passages. %The augmented set of question-passage pairs is used to train the first stage of the neural retrieval model. % It has been shown as an effective approach to improve passage retrieval models. During pre-training we use negatives generated from strategy 4\footnote{Or strategy 1, if strategy 4 is not feasible} to improve the retrieval model, as the other strategies could introduce more false negatives into the data. %Our initial experiments showed that using retrieval models to find the hard negatives at this point, often generated very noisy question-passage pairs, especially because our pre-training data includes synthetic pairs. %As the generated question passage pairs sometimes are noise, retrieval-based approaches may create better question-passage pairs than the synthetic pairs. %We only apply a heuristic based context negatives on this pre-training task. Next, we continue with the fine tuning stage  using a small amount of gold training data. At this stage, we explore all four types of negative sampling. To the best of our knowledge, this is the first work that explores the effectiveness of hard negatives for passage retrieval in a systematic way, and integrates them in the retrieval models  pre-training stage. Our overall experimental architecture is outlined in Figure.  %For each question-passage pair in the training set, we collect negatives using the strategies listed above and augment them into the training.  We conduct experiments with this approach on two passage retrieval tasks: Open Domain QA  and SQuAD) and MS MARCO. %Open Domain QA Natural Questions~, Open Domain QA SQuAD, and MS MARCO. Our results show that all four kinds of hard negatives improve the dual encoder models significantly with consistent performance gains across both tasks. However, depending on the types of questions and their domain, one kind of hard negative may perform better than the others in a particular task. For example, context negatives work best in NQ and semantic retrieval-based negatives  work best in SQuAD. We further ensemble the models trained on different types of hard negatives. The final models achieve state-of-the-art performance on Open Domain QA task with an improvement over prior works of 0.8--2.9 points on accuracy rates.  %.  The main contribution of this paper are: [label=]    
"," %In this paper we explore the discriminate training for neural passage retrieval models with hard negatives. %Four different hard negative sampling strategies are experimented, including one BM25 based hard negative, two semantic based hard negatives, and one heuristic hard negative. %For training the model, we employ a two stage dual encoder model with pre-training using synthetic data followed by a fine-tuning using the gold training data. %Discriminate training is applied on both stages. %The trained models are evaluated on 3 passage retrieval tasks from Open Domain QA NQ, Open Domain QA SQuAD, and MS MARCO. %Results show that all of them can improve the naive dual encoder models significantly with consistent performance gain over all three tasks. %However, there is no single type of hard negative perform best on all tasks. %Further analysis show that the synthetic question pre-training with discriminate training is an effective approach to improve the passage retrieval performance. %The best trained models establish the new state-of-the-art on retrieval tasks of Open Domain QA NQ and SQuAD. %",184
" % Events describe things that happen or occur in the world, mostly involving entities  who perform % or are affected by the events and spatio-temporal dimensions of the event . The same event may be mentioned in a document multiple times with different context. To recognize if two event mentions  refer to the same  contributes to the understanding of natural language text and resolving other NLP tasks.  In this work, we study the coreference resolution problem for both events and entities. Coreference resolution is commonly modeled as a binary classification problem : first learn features for each  mention, then classify two given mentions  \footnote{Some work maps the two mentions into a single matching score, e.g., ; this can be treated as a special case of binary classification.}. The essential step in this framework lies in the representation learning of each mention. However, prior work often failed to learn representations with powerful expressivity due to the following two reasons:  Point-wise representation learning.  % Usually a mention is surrounded by other words in a sentence.  Most work tries to learn mention representations by extracting features merely from that particular sentence including the mention. We argue that this routine of  representation learning in coreference does not match the end task  we are coping with: relation recognition of mention pairs. The predicted relation is for the input mention pair rather than individual mentions. By different context, two mentions can be referring to each other or not.  To fit the different scenarios, a mention should learn its representation by considering what its counterpart is.  Unstructured representation learning. An event mention consists of multiple arguments to describe the event: what, who, when, where, etc. Most prior work tried to encode all those elements into a single distributed representation vector and then compare the vectors of two mentions. This is less optimal since humans can recognize subjects, objects etc. and often compare event arguments of the same type . If, for instance, the event locations are different, people can  make a judgement quickly even without comparing other mention arguments.   Denoting a mention with a single distributed  vector lets machines lose the opportunity to conduct  fine-grained reasoning as humans do and uneasy to explain the model's prediction.  To promote the expressivity of representations, this work proposes % pairwise structured representation learning paired representation learning . \modelname In this work, we treat each mention pair rather than a single mention as the object for the representation learning. Specifically, we will concatenate the two sentences   as a whole sequence and  forward it into a ROBERTa  system.\footnote{RoBERTa will put a special token ``SEP'' to separate the two sentences.}   RoBERTa takes the ``whole sequence'' as an input so that each token, including the  mention spans, in the two sentences are able to compare with other tokens from the very beginning. This is better than comparing the two mentions  after  learning a representation for each of them separately. We apply this pairwise representation learning to both event and entity coreference tasks. The binary classifier or an mention matching function in the end will take a pair of contextualized mention representations for reasoning.    Looking at the following two sentence  and   % and we think we need to learn from humans' behaviors in recognizing event coreference.   : ``Over \textcolor{blue}{69,000 people} \underline{lost} their lives in the quake, including 68,636 in \textcolor{purple}{Sichuan province}.''  : ``Up to \textcolor{blue}{6,434 people} \underline{lost} their lives in Kobe earthquake and about 4,600 of them were from \textcolor{purple}{Kobe}.''  First, humans often determine the relationship between the two event mentions in  and  by comparing their triggers and arguments separately  as follows:  \textbullet\enspace ``69,000 people'' vs.  ``6,434 people''  \textbullet\enspace ``lost'' vs. ``lost''  \textbullet\enspace ``Sichuan province'' vs ``Kobe'' %  %       Second, the mismatch of some components may be  decisive or more decisive than that of others. For example, when people find the location ``Sichuan province'' does not match with ``Kobe'', they can directly claim the two events are not coreference even without looking at the whole sentence. This human behavior indicates that we should make full of the structure in an event, and an overall representation encompassing all event elements is less informative  to perform fine-grained cross-mention comparison which can actually improve the interpretability of the model predictions.  Overall, our \modelname~enables  two  mentions to learn from the context of each other, and improves the model's explainability by performing fine-grained reasoning. We report \modelname~on  both event coreference   and entity coreference benchmarks. Despite its simple architecture, \modelname~ surpasses the prior SOTA system by  big margins.                  
"," Co-reference of Events and of Entities are commonly formulated as binary classification problems, given a pair of events or entities as input. Earlier work addressed the main challenge in these problems -- the representation of each element in the input pair by:   modelling the representation of one element  without  considering the other element in the pair;  encoding all attributes of one element  into a single non-interpretable vector, thus losing the ability to compare %fine-grained cross-element attributes.  In this work we propose paired representation learning  for  coreference resolution. % \drc{ % In this work we propose pairwise structured representation learning  \XD{Do we want to change the model name? Since it is not structured for entity. And for event, most numbers are from trigger only representation} \dr{Maybe just PairedRL ? If so, we can cahange Pairwise Structure to ``Paired"" in the title and the rest of the paper.}for  coreference resolution.}  Given a pair of elements  our model treats the pair's sentences as a single sequence so that each element in the pair learns its representation by encoding its own context as well the other element's context. In addition, when representing events, \modelname in that it represents the event's arguments to facilitate their individual contribution to the final prediction. As we show, in both  event  and entity coreference benchmarks, our unified approach, \modelname in that it represents the event's arguments to facilitate their individual contribution to the final prediction. As we show, in both  event  and entity coreference benchmarks, our unified approach, \modelname %This work studies the event and entity coreference which is commonly formulated as a binary classification problem given a pair of events or entities. The main challenge lies in the representation learning of each element in the input pair. However,  prior work mostly has the following drawbacks:  Systems model the representation of one object  with no consideration of   the other object in the pair;  Systems often encode all related attributes of one object  into a single and unexplainable vector; this reduces the model's interpretability and  mismatches the fact that humans tend to recognize the event relations by comparing fine-grained cross-event arguments. Motivated, we propose pairwise structured representation learning  \XD{Do we want to change the model name? Since it is not structured for entity. And for event, most numbers are from trigger only representation} for  coreference resolution. By ``Pairwise'', \modelname\enspace treats sentences of the input pair as a whole sequence so that each object in the pair learns its representation by encoding its own context as well the other's context. This paradigm applies to both event and entity coreference. In addition, \modelname\enspace develops a ``structured''  framework to represent all the event arguments so that each argument can explain its contribution to the final prediction.  In both  event  and entity coreference benchmarks, \modelname\enspace beats prior state of the art systems with big margins .",185
" Neural machine translation  has been explored typically in sentence-level translation settings. Such sentence-level nmt models inevitably suffer from ambiguities when multiple  %% semantically-different translations are accepted  interpretations are possible to a source sentence.  To address this issue, context-aware nmt models have recently been presented %to address the issue  to incorporate document-level information in translation. Most of the existing context-aware nmt models are end-to-end models which take as input the current source sentence to be translated and the context sentences, and then output a translation. These models are trained on document-level parallel data, namely, sentence pairs with surrounding, usually preceding, sentences in the source and target language. However, in practical scenarios, document-level bilingual data is limited in most language pairs and domains, % posing a challenge to building context-aware nmt systems .  In this study, we propose a simple yet effective approach to context-aware nmt  % consisting of  using two primitive components, a sentence-level nmt model and a document-level language model . This approach allows us to independently train the two components on bilingual data and monolingual data, respectively, without resorting to expensive document-level bilingual data.  % and thereby no document-level bilingual data is needed. To give a probabilistic foundation to this combination of two independent models, we exploit % take advantage of  the probabilistic nature of nmt decoding. When generating a sequence, a left-to-right decoder outputs a categorical probability distribution over the vocabulary at every time step. % . The decoder assigns higher probability to the tokens that would be more suitable at that step. Therefore,  % we can assume that  when multiple valid translations are possible to the source sentence, % , which has ambiguities a sentence-level nmt is confused by,  the decoder just gives a higher  % sequence  probability to the translation that is plausible without considering contexts.  % than to wrong ones. Our idea is to adjust the probability distributions in a context-aware manner using a document-level lm of the target language which  % is capable of modeling  models inter-sentential dependencies in the target side document.  % Since a network structure of nmt models evolves very quickly, model-agnostic approach like ours is more preferable than model-tweaking approach .  We evaluate our methods on English to French, Russian and Japanese translations with OpenSubtitles2018 corpus in terms of the bleu scores and contrastive discourse test sets. Experimental results confirmed that our method achieved comparable performance with existing context-aware nmt models.  The contributions of this paper are as follows:       without the need for document-level parallel data}; we can build context-aware nmt models from the state-of-the-art sentence-level nmt models in various language pairs and domains.      while adding a probabilistic foundation. %     decoding combined with a language model, and therefore could be regarded as an extension of shallow fusion, but our approach is more sophisticated in that it has a probabilistic grounding; %     computed by a language model highly contribute to improve performance on the contrastive tests, which are designed to evaluate the performance of context-aware nmt models.   
"," % There exist inevitable ambiguities in translating a single sentence, and we resort to context beyond the target sentence for resolving such ambiguities. Although many context-aware neural machine translation models have been proposed to incorporate contexts in translation,  most of those models are trained end-to-end on parallel documents aligned in sentence-level.  Because only a few domains  have such document-level parallel data, we cannot perform accurate context-aware translation in most domains. We therefore present a simple method to turn a sentence-level translation model into a context-aware model by incorporating a document-level language model into the decoder. Our context-aware decoder is built upon only a sentence-level parallel corpora and monolingual corpora; thus no document-level parallel data is needed. In a theoretical viewpoint, the core part of this work is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We show the effectiveness of our approach in three language pairs, English to French, English to Russian, and Japanese to English, by evaluation in bleu and contrastive tests for context-aware translation.",186
" A keyphrase is a multi-word text representing highly abstractive information in a long document. Keyphrase extraction  is a task that aims to generate an appropriate keyphrase set for the given document, thus helping to identify salient contents and concepts from the document. Recently, the KE task has attracted much research interest since it serves as an important component of many downstream applications such as text summarization, document  classification, information retrieval and question generation.  Early KE systems commonly operate in an extractive manner, which usually consists of two steps: 1) selecting candidates from the source document using heuristic rules,  and 2) ranking the candidates list to determine which is correct. However, the two-step ranking approaches are usually based on feature engineering, which is labor-intensive. Motivated by the progress in sequence-to-sequence applications of neural networks, KE research's focus has gradually shifted to deep learning methods.  first formulate KE as a sequence generation problem and introduce an attentive Seq2Seq framework to generate the keyphrase sequence conditioned on the input document. Compared with traditional methods, the Seq2Seq based method achieves superior performance.  Seq2Seq based KE is exposed to two major challenges: 1) Document-level representation learning. For any Seq2Seq generative framework, the latent hidden representation is a very important factor, and its quality will directly affect the decoder's performance. In KE task, the input is commonly a long document instead of a sentence, which poses a greater challenge to latent representation learning. 2) Modeling the compositionality of keyphrases set. The elements in the keyphrase set are dependent and correlated. That is, better modeling the inherent composition embodied in the keyphrase set during the learning process will effectively boost the diversity and quality of final results.   Recently, various approaches have been proposed to optimize the Seq2Seq generation framework in KE task. To learn a better latent representation, previous studies try to introduce different encoding structures  to address the two issues above simultaneously. We explore to incorporate the dependency tree for document representation learning in the encoder part. The syntactic dependency tree can help to locate key information in a document. In practice, the document graph  is constructed depending on the syntactic dependency tree, and then a convolution process will be operated over .  On the other hand, we rethink the implication of compositionality in the keyphrase set. In the training process of generative models, whether a candidate keyphrase should be generated not only hinges on the document itself, but also depends on the keyphrases that have already been generated. Therefore, a dynamic graph updating mechanism is introduced to explicitly modeling the inter-dependency among keyphrases. In our method, the graph structure in the encoder part will be dynamically updated according to the keyphrases generated in the decoder part. Concretely, after one keyphrase is decoded, its information will be transferred to modify the edge weights in the document graph through a score function, and the latent hidden representation will also be updated. In this approach, we could dynamically ensure the information exchange between encoder and decoder parts in both directions.   The contribution of this work is three-fold:  1) A novel generative framework, Div-DGCN, is proposed that leverages both the dynamic syntactic graph encoder and diversified inference process for KE. 2) A dynamic computation mechanism is adopted to model the compositionality in keyphrase set explicitly and then enhancing the information interchange between the encoder and decoder parts in the Seq2Seq architecture.  3) Extensive experiments conducted on five benchmarks show that our proposed method is effective against competitive baselines on several metrics.  
"," Keyphrase extraction  aims to summarize a set of phrases that accurately express a concept or a topic covered in a given document. Recently, Sequence-to-Sequence  based generative framework is widely used in KE task, and it has obtained competitive performance on various benchmarks. The main challenges of Seq2Seq methods lie in acquiring informative latent document representation and better modeling the compositionality of the target keyphrases set, which will directly affect the quality of generated keyphrases. In this paper, we propose to adopt the Dynamic Graph Convolutional Networks  to solve the above two problems simultaneously. Concretely, we explore to integrate dependency trees with GCN for latent representation learning. Moreover, the graph structure in our model is dynamically modified during the learning process according to the generated keyphrases. To this end, our approach is able to explicitly learn the relations within the keyphrases collection and guarantee the information interchange between encoder and decoder in both directions. Extensive experiments on various KE benchmark datasets demonstrate the effectiveness of our approach.",187
" Neural machine translation , as the state-of-the-art machine translation paradigm, has recently been approached with two different sequence decoding strategies. The first type autoregressive translation  models generate output tokens one by one following the left to right direction, but it is often criticized for its slow inference speed. The second type non-autoregressive translation  models adopt a parallel decoding algorithm to produce output tokens simultaneously, but the translation quality of which is often inferior to auto-regressive models.  % we transfer the AT knowledge to NAT models. A line of research argues that the lack of contextual dependency in target sentences potentially leads to the deteriorated performance of NAT models. To boost the NAT translation performance, many recent works resort to the knowledge transfer from a well-trained AT model. Typical knowledge tranfer methods include sequence-level knowledge distillation with translation outputs generated by strong AT models, word-level knowledge distillation with AT decoder representations, and fine-tuning on AT model by curriculum learning, etc.  %shared encoder  In this work, we adopt a multi-task learning framework with a shared encoder  to transfer the AT model knowledge into the NAT model.  %Our framework jointly optimizes the AT and NAT models to boost the NAT translation quality.  Specifically, we take the AT task as an auxiliary task by sharing the encoder parameters of both AT and NAT models. We hypothesize that AT and NAT encoders, although they belong to the same sequence-to-sequence learning task, capture different linguistic properties and representations of source sentences. To empirically verify our hypothesis, we evaluate the encoder on a set of probing tasks .   Our contributions are as follows: [itemsep=1pt,topsep=0pt,parsep=0pt,partopsep=0pt]     .     \iffalse 
"," Non-Autoregressive machine Translation  models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation  knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties and representations of source sentences. Therefore, we propose to adopt the multi-task learning to transfer the AT knowledge to NAT models through the encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 English$\Leftrightarrow$German and WMT16 English$\Leftrightarrow$Romanian datasets show that the proposed multi-task NAT achieves significant improvements over the baseline NAT models. In addition, experimental results demonstrate that our multi-task NAT is complementary to the standard knowledge transfer method, knowledge distillation. }",188
" In language modelling , we learn distributions over sequences of words, subwords or characters, where the latter two can allow an open-vocabulary generation. We rely on subword segmentation as a widespread approach to generate rare subword units . However, the lack of a representative corpus, in terms of the word vocabulary, constrains the unsupervised segmentation .  As an alternative, we could use character-level modelling, since it also has access to subword information , but we face long-term dependency issues and require longer training time to converge.   In this context, %our research question is: Is there a segmentation approach that is corpus-independent and provides a shorter length sequence than characters?  we focus on syllables, which are based on speech units: ``A syl-la-ble con-tains a sin-gle vow-el u-nit''. These are more linguistically-based units than characters, and behave as a mapping function to reduce the length of the sequence with a larger ``alphabet'' or syllabary. Their extraction can be rule-based and corpus-independent, but data-driven methods or hyphenation using dictionaries can approximate them as well .  % .  Previous work on syllable-aware neural \lm failed to beat characters in a closed-vocabulary generation at word-level ;  %,  however, we propose to assess syllables under three new settings.  %but we further discuss two points as follows.  First, we analysed an open-vocabulary scenario with syllables by disregarding additional functions in the input layer . Second, we extended the scope from 6 to 20 languages  to cover different levels of orthographic depth, which is the degree of grapheme-phoneme correspondence  and a factor that can increase complexity to syllabification. English is a language with deep orthography  whereas Finnish is transparent . Third, we distinguished rule-based syllabification with hyphenation tools, but also validated their proximity for LM. %we prefer to use rule-based syllabification whenever available, and only employ hyphenation proxies otherwise.   % even with specific segmentation rules.   Therefore, we revisit \lm for open-vocabulary generation with syllables using pure recurrent neural networks for a more diverse set of languages, and compare their performance against characters and other subword units. %We analyse their overlap with other subword units, and compare their performance against s. %We investigate %, and we also include languages % from the Universal Dependencies dataset ,  %with more transparent orthographies, such as in Turkish  or Finnish . %Our results confirm that syllables are a reliable segmentation setting for language modelling, even when the language presents a deep orthography.  %less-ambiguous syllabification due to  %a recent alphabetisation . We thereupon explore the syllables effect in another generation task such as NMT.   
"," Language modelling is regularly analysed at word, subword or character units, but syllables are seldom used. Syllables provide shorter sequences than characters, they can be extracted with rules, and their segmentation typically requires less specialised effort than identifying morphemes. We reconsider syllables for an open-vocabulary generation task in 20 languages.  We use rule-based syllabification methods for five languages and address the rest with a hyphenation tool, which behaviour as syllable proxy is validated. With a comparable perplexity, we show that syllables outperform characters, annotated morphemes and unsupervised subwords. Finally, we also study the overlapping of syllables concerning other subword pieces and discuss some limitations and opportunities.",189
" Sanskrit is considered as one of the oldest Indo-Aryan languages. The oldest known Sanskrit texts are estimated to be dated around 1500 BCE. A large corpus of religious, philosophical, socio-political and scientific texts of multi cultural Indian Subcontinent are in Sanskrit. Sanskrit, in its multiple variants and dialects, was the Lingua Franca of ancient India ~. Therefore, Sanskrit texts are an important resource of knowledge about ancient India and its people. Earliest known Sanskrit documents are available in the form called . Rigveda, the oldest of the four Vedas, that are the principal religious texts of ancient India, is written in . In sometime around 5\textsuperscript{th} BCE, a Sanskrit scholar named  ~ wrote a treatise on Sanskrit grammar named , in which  formalized rules on linguistics, syntax and grammar for Sanskrit. 's grammar is globally appreciated for its insightful analysis of Sanskrit and completeness of its descriptive coverage of the spoken standard language of 's time.    \footnote{https://www.britannica.com/topic/Ashtadhyayi} is the oldest surviving text and the most comprehensive source of grammar on Sanskrit today and provides often unique information on Vedic, regional and  socio-linguistic usage.  literally means eight chapters and these eight chapters contain around 4000 sutras or rules in total. These rules completely define the Sanskrit language as it is known today.  is remarkable in its conciseness and contains highly systematic approach to grammar. Because of its well defined syntax and extensively well codified rules, many researchers have made attempts to codify the 閳ユ獨 sutras as computer programs to analyze Sanskrit texts. This paper tries to address the problem of unavailability of benchmark corpus and provides morphological analysis method for derivative nouns as a result of Sanskrit suffixes applied on root verbs and nouns using a machine learning approach.   Different ways of inflectional word formation as mentioned by ~ are as below:  ) suffixes.\\ - [Word + Suffix] Stem: secondary  suffixes.\\ - [Word + Word] Stem: compounding.\\ - [Root + Suffix] Word: verb inflection.\\ - [Stem + Suffix] Word: noun inflection.   Here in this introduction, we will explain more about primary and secondary suffixes. Sanskrit is a rich inflected language and depends on nominal and verbal inflections for communication of meaning ~. A fully inflected unit is called pada. The  are the inflected nouns and the  are the inflected verbs.   [h]           These are formed when the primary affixes called  are added to verbs to derive substantives, adjectives or indeclinable.  play a vital role in understanding Sanskrit language. Many morphological analyzers are lacking the complete analysis of . Examples of  pratyaya are as below:   +   =  \\  +    =    suffixes are mainly of seven types viz. .   The secondary derivative affixes called  derive secondary nouns from primary nouns. Some examples of  pratyaya are as below:    +   =  \\   +   =    suffixes are mainly of fourteen types.  
"," This paper presents first benchmark corpus of Sanskrit Pratyaya  and inflectional words  formed  due to suffixes along with neural network based approaches to process the formation and splitting of inflectional words. Inflectional words spans the primary and secondary derivative nouns as the scope of current work. Pratyayas are an important dimension of morphological analysis of Sanskrit texts. There have been Sanskrit Computational Linguistics tools for processing and analyzing Sanskrit texts. Unfortunately there has not been any work to standardize \& validate these tools specifically for derivative nouns analysis. In this work, we prepared a Sanskrit suffix benchmark corpus called Pratyaya-Kosh to evaluate the performance of tools. We also present our own neural approach for derivative nouns analysis while evaluating the same on most prominent Sanskrit Morphological Analysis tools. This benchmark will be freely dedicated and available to researchers worldwide and we hope it will motivate all to improve morphological analysis in Sanskrit Language.",190
"  Sanskrit is one of the oldest of the Indo-Aryan languages. The oldest known Sanskrit texts are estimated to be dated around 1500 BCE. It is the one of the oldest surviving languages in the world. A large corpus of religious, philosophical, socio-political and scientific texts of multi cultural Indian Subcontinent are in Sanskrit. Sanskrit, in its multiple variants and dialects, was the Lingua Franca of ancient India . Therefore, Sanskrit texts are an important resource of knowledge about ancient India and its people. Earliest known Sanskrit documents are available in the form called {, the oldest of the four Vedas, that are the principal religious texts of ancient India, is written in { century BCE, a Sanskrit scholar named {, in which { is the oldest surviving text and the most comprehensive source of grammar on Sanskrit today. { is remarkable in its conciseness and contains highly systematic approach to grammar. Because of its well defined syntax and extensively well codified rules, many researchers have made attempts to codify the {-{ to place) is the principle of sounds coming together naturally according to certain rules codified by the grammarian {. There are 3 different types of Sandhi as defined in { vAk + hari = vAgGari  punaH + api = punarapi     Sandhi Split on the other hand, resolves Sanskrit compounds and 閳ユ笡honetically merged閳  words into its constituent morphemes. Sandhi Split comes with additional challenge of not only splitting of compound word correctly, but also predicting where to split. Since Sanskrit compound word can be split in multiple ways based on multiple split locations possible, split words may be syntactically correct but semantically may not be meaningful.   tadupAsanIyam = tat + upAsanIyam tadupAsanIyam = tat + up + AsanIyam      The current resources available for doing Sandhi in open domain are not very accurate. Three most popular publicly available set of Sandhi tools viz. JNU, UoH \& INRIA tools are mentioned in table .            {|p{4cm}|p{10cm}|}     \toprule           An analysis and description of these tools is present in the paper on Sandhikosh . The same paper introduced a dataset for Sandhi and Sandhi Split verification and compared the performance of the tools in table  on that dataset.  Neural networks have been used for Sandhi Split by many researchers, for example ,  and . The task of doing Sandhi has been mainly addressed as a rule based algorithm e.g. . There is no research on Sandhi using neural networks in public domain so far. This paper describes experiments with Sandhi operation using neural networks and compares results of suggested approach with the results achieved using existing Sandhi tools .    Many researchers like  and  have tried to codify {} , UoH Sandhi Splitter  and INRIA Sanskrit reader companion. The paper  compares the performance of above 3 tools with their results. This was an attempt to create benchmark in the area of Sanskrit Computational Linguistics.   
"," This paper describes neural network based approaches to the process of the formation and splitting of word-compounding, respectively known as the Sandhi  and Vichchhed, in Sanskrit language. Sandhi is an important idea essential to morphological analysis of Sanskrit texts. Sandhi leads to word transformations at word boundaries. The rules of Sandhi formation are well defined but complex, sometimes optional and in some cases, require knowledge about the nature of the words being compounded. Sandhi split or Vichchhed is an even more difficult task given its non uniqueness and context dependence. In this work, we propose the route of formulating the problem as a sequence to sequence prediction task, using modern deep learning techniques. Being the first fully data driven technique, we demonstrate that our model has an accuracy  better than the existing methods on multiple standard datasets, despite not using any additional lexical or morphological resources. The code is being made available at https://github.com/IITD-DataScience/Sandhi\_Prakarana",191
"   % Unsupervised representation learning allows models to learn high-level latent representations from unlabeled data.  % Models pretrained from unsupervised data can be fine-tuned with a small amount of labeled data. % % Deep probabilistic generative models presents a powerful approach to learn representations by modeling the data generation process.  % Variational AutoEncoders  are one of the popular approaches to representation learning by modeling the latent features in a unit Gaussian space. % Vector-Quantized VAE  is a method to learn discrete representations from data.  Speech waveforms are a complex, high-dimensional form of data influenced by a number of underlying factors, which can be broadly categorized into linguistic contents and speaking styles. % Learning disentangled latent representations from speech has a wide set of applications in generative tasks, including speech synthesis, data augmentation, voice transfer, and speech compression. Downstream tasks such as speech recognition  and speaker classification  can also benefit from such learned representations. % A pre-trained model can also be fine-tuned for classification tasks such as speech recognition and speaker classification. %   Because of the cost, complexity, and privacy concerns around collecting labeled speech data, there has been a lot of interest in unsupervised representation learning for speech. Of particular interest is to learn representations for speech styles from unsupervised data due to the difficulty in describing prosody with human labels.  Some previous works aim to learn global representations from entire speech sequences. % Global style tokens learn a dictionary of embeddings from speech without prosody labels. % As another example, Hsu et al.  model disentangled speech styles with a hierarchy of variational autoencoder . % Hu et al.  proposed a content and style separation model by pre-training on a single-speaker dataset with text transcription and minimizing mutual information  between the content and style representation.  Other works try to learn fine-grain localized representations of speech. %  apply self-supervised learning to unlabeled speech data and extract localized latent representations that can be fine-tuned for speech recognition. % FHVAE learns a sequence of high-level features by applying VAE to every frame. %  leverages vector-quantized VAE  to learn a discrete sequence representation of speech.  We propose a framework to learn both global and localized representation of speech. In order to disentangle content and style representations, we apply  a local encoder with VQ layer to learn a discrete per-timestep representation of the speech that captures the linguistic contents and  a global VAE to extraction per-utterance representations to reflect the speech styles. We further disentangle the local and global representations with a mutual information loss. We evaluate the quality of linguistic and style representations by running speech and speaker recognition models on the reconstructed speech. We also show that the global representation captures the speaker information well enough that we can obtain a speaker classification model by training a linear projection layer on top of the global representation with only one example per speaker.   
"," We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of:  a local encoder that captures per-frame information;  a global encoder that captures per-utterance information; and  a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that  the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates , even with a different global encoding;  the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker. % % \rpang{How about: Our deep generative model consists of:  a local encoder that captures per-frame information;  a global encoder that captures per-utterance information; and  a conditional decoder that reconstruct speech given local and global latent variables, potentially extracted from different utterances. Our experiments show that  the local latent variables encode speech contents, since reconstructed speech can be recognized by ASR with low word error rates , even with a different global encodings;  the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding and a speaker recognition model can be trained from the global latent variables with as few as one supervised example per speaker.  % }",192
"  %Sentiment analysis  is one of the fundamental tasks in natural language processing that aims to find the attitude that the author expressed in his/her sentence. One of the important sub-tasks of SA is aspect based sentiment analysis in which the goal is to find the sentiment polarity toward a specific aspect mentioned in the sentence. Due to the importance of ABSA, several sub-tasks has been proposed and studied for this problem, including aspect category extraction, aspect term extraction, opinion word extraction and opinion summarization . Among these sub-tasks, Targeted Opinion Word Extraction  is an important sub-task that might provide useful information to explain the prediction of the sentiment polarity from an ABSA system. In particular, the goal of TOWE is to find the words that express the attitude of the author toward a specific target mentioned in that sentence. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", the word ``good"" is the opinion word for the target ``food"" while delicious is the opinion word for the target word ``drinks"". Among different applications, TOWE can be used for target-oriented sentiment analysis  and pair-wise opinion summarization .  %Sentiment analysis  is one of the fundamental tasks in natural language processing that aims to find the attitude that the author expressed in his/her sentence. One of the important sub-tasks of SA is aspect based sentiment analysis in which the goal is to find the sentiment polarity toward a specific aspect mentioned in the sentence. Due to the importance of ABSA, several sub-tasks has been proposed and studied for this problem, including aspect category extraction, aspect term extraction, opinion word extraction and opinion summarization . Among these topics, Targeted Opinion Word Extraction  is an important task that might provide useful information to explain and/or improve the sentiment polarity prediction of the ABSA systems. In particular, given a target word  in the input sentence, the goal of TOWE is to find the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", ``good"" is the opinion word for the target word ``food"" while the opinion words for the target word ``drinks"" would involve ``delicious''. Among different applications, TOWE finds its application in target-oriented sentiment analysis  and pair-wise opinion summarization .   %Targeted Opinion Word Extraction  is an important task in aspect based sentiment analysis  of sentiment analysis . Given a target word  in the input sentence, the goal of TOWE is to find the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", ``good"" is the opinion word for the target word ``food"" while the opinion words for the target word ``drinks"" would involve ``delicious''. As the opinion words might provide useful information to explain and/or improve the sentiment prediction of the ABSA systems, TOWE can be applied in different problems, including target-oriented sentiment analysis  and pair-wise opinion summarization .  Targeted Opinion Word Extraction  is an important task in aspect based sentiment analysis  of sentiment analysis . Given a target word  in the input sentence, the goal of TOWE is to identify the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, as a running example, in the sentence ``All warranties honored by XYZ  are disappointing."", ``disappointing"" is the opinion word for the target word ``warranties"" while the opinion words for the target word ``company"" would involve ``reputable''. Among others, TOWE finds its applications in target-oriented sentiment analysis  and opinion summarization .   %As the opinion words might provide useful information to explain and/or improve the sentiment prediction of the ABSA systems, TOWE can be applied in different problems, including target-oriented sentiment analysis  and pair-wise opinion summarization .  %A notable problem is that although the related tasks of TOWE has been extensively explored in the past, there have been only a few work to explicitly consider the TOWE problem in the literature . In particular, the most related task of TOWE is opinion word extraction  that aims to locate the terms used to express attitude explicitly in the sentence . A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence  while the opinion words in TOWE should be explicitly paired with a given target word. Note that some previous works have also attempted to jointly predict the target and opinion words ; however, the target words are still not paired with their corresponding opinion words in these studies .    %Among the previous works for TOWE, t  The early approach for TOWE has involved the rule-based and lexicon-based methods  while the recent work has focused on deep learning models for this problem . One of the insights from the rule-based methods is that the syntactic structures  of the sentences can provide useful information to improve the performance for TOWE . However, these syntactic structures have not been exploited in the current deep learning models for TOWE . Consequently, in this work, we seek to fill in this gap by extracting useful knowledge from the syntactic structures to help the deep learning models learn better representations for TOWE. In particular, based on the dependency parsing trees, we envision two major syntactic information that can be complementarily beneficial for the deep learning models for TOWE, i.e., the syntax-based opinion possibility scores and syntactic word connections for representation learning. First, for the syntax-based possibility scores, our intuition is that the closer words to the target word in the dependency tree of the input sentence tend to have better chance for being the opinion words for the target in TOWE. For instance, in our running example, the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , ``disappointing"" is directly connected to ``warranties"", promoting the distance between ``disappointing"" and ``warranties""  in the dependency tree as an useful feature for TOWE. Consequently, in this work, we propose to use the distances between the words and the target word in the dependency trees to obtain a score to represent how likely a word is an opinion word for TOWE . These possibility scores would then be introduced into the deep learning models to improve the representation learning for TOWE.  In order to achieve such possibility score incorporation, we propose to employ the representation vectors for the words in the deep learning models to compute a model-based possibility score for each word in the sentence. The model-based possibility scores also aim to quantify the likelihood of being an opinion word for each word in the sentence; however, they are based on the internal representation learning mechanism of the deep learning models for TOWE. To this end, we propose to inject the information from the syntax-based possibility scores into the models for TOWE by enforcing the similarity/consistency between the syntax-based and model-based possibility scores for the words in the sentence. The rationale is to leverage the possibility score consistency to guide the representation learning process of the deep learning models  to generate more effective representations for TOWE. In this work, we employ the Ordered-Neuron Long Short-Term Memory Networks   to obtain the model-based possibility scores for the words in the sentences for TOWE. ON-LSTM introduces two additional gates into the original Long Short-Term Memory Network  cells that facilitate the computation of the model-based possibility scores via the numbers of active neurons in the hidden vectors for each word.  %The second type of syntactic information employed for TOWE in this work considers the dependency connections between the words in the sentence.   %As the deep learning models need to compute a representation vector for each word to perform opinion word prediction in TOWE,   %While the possibility scores aim to improve the representation vectors for TOWE via the syntax-based possibility features, the second type of syntactic information in this work seeks to do so by leveraging the dependency connections between the words to infer the effective context words to be encoded in the representation vector for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector for a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``warranties"" as the target word and ``reputable"" as the word we need to compute the representation vector. One the one hand, it is important to include the information of the neighboring words of ``reputable""  in the representation so the models can know the context for the current word . On the other hand, the information about the target word  should also be encoded in the representation vector for ``reputable"" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label  for ``reputable"" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``I'' in the representation for ``reputable"" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network  model  to produce the final representation vectors for opinion word prediction.  For the second type of syntactic information in this work, the main motivation is to further improve the representation vector computation for each word by leveraging the dependency connections between the words to infer the effective context words for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector of a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``warranties"" as the target word and ``reputable"" as the word we need to compute the representation vector. On the one hand, it is important to include the information of the neighboring words of ``reputable""  in the representation so the models can know the context for the current word . On the other hand, the information about the target word  should also be encoded in the representation vector for ``reputable"" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label  for ``reputable"" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``I'' in the representation for ``reputable"" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word, given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network  model  to produce the final representation vectors for opinion word prediction.  Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words and those for the other words in the sentence. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several benchmark datasets.  %Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words  and those for the other opinion words  in the sentence . Extensive experiments are conducted to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several datasets.  %Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-related opinion words  and those for the other opinion words  in the sentence . As both target-related and non-target opinion words can be used to express the opinion of the author , we expect that the explicit representation distinction would help to better separate the two types of opinion words based on the target word, eventually improving the performance for TOWE in this work. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several datasets.   %the close distance between ``disappointing"" and the target word will suggest the models to include the information of ``warranties"" into the representation vector for ``disappointing"" while the long distance between ``warranties"" and ``reputable"" can help to prevent/mitigate that for the representation vector of ``reputable"". The presence of the information from the target word in the representation vectors will help the models to successfully accept ``disappointing"" as an opinion word and reject ``reputable"" in this case.   %the close words to the target word would provide more effective information to induce the representation vectors for a word in the sentence in TOWE than the farther ones.   %we argue that the syntactic neighboring words in the dependency tree would provide effective information to induce the representation vector for a word in opinion word prediction. For instance, in the running example with the target word ``warranties"", the close distance between ``disappointing"" and the target word will suggest the models to include the information of ``warranties"" into the representation vector for ``disappointing"" while the long distance between ``warranties"" and ``reputable"" can help to prevent/mitigate that for the representation vector of ``reputable"". The presence of the information from the target word in the representation vectors will help the models to successfully accept ``disappointing"" as an opinion word and reject ``reputable"" in this case.  %employ the dependency connections between the words to infer the effective context words .    %employ the syntactic neighboring words to compute the representation vectors for a word in the sentence for TOWE.   %extends the popular Long Short-Term Memory Networks  by introducing two additional gates  in the hidden vector computation. These new gates controls how long each neuron in the hidden vectors should be activated across different time steps  in the sentence . Based on such controlled neurons, the model-based importance score for a word can be determined by the number of active neurons that the word possesses in the operation of ON-LSTM. To our knowledge, this is the first time ON-LSTM is applied for RE in the literature.  %How can we encode the syntax-based importance scores of the words into a deep model? In this paper, we propose to employ the syntax-based importance scores to retain or update the information encoded in the representations of each word. In particular, those words that are syntactically more important should retain more information in the computation graph of the deep model while the information about less important words should be discarded more frequently. In order to impose this information update policy in our model, we use the new proposed architecture Ordered-Neuron Long Short-Term Memory  . ON-LSTM is an extension of the well-known Long Short-Term Memory  with two additional gates . These new gates are employed to control the frequency of updating each neuron across different time steps  in the sentence. Concretely, the values of the master forget and input gates determine how much information in the hidden vector of the LSTM cell should be retained or updated based on the word at the current time step. Thereby, one can infer the importance scores inferred by the model  using the values of the master forget or input gates. So, based on this characteristics of ON-LSTM, to encode the syntax-based importance scores into our model, we propose to exploit the syntax-based importance scores to regulate model-based importance scores. Specifically, in training time, we encourage the model-based scores to be consistent with syntax-based importance scores.  %the two words ``disappointing"" and ``warranties"" are directly connected to each other.  %Early feature-based models  has shown that syntactical structure of the sentence is useful for TWOE. More specifically, the application of dependency tree for TOWE is two fold:  Pairwise Word Importance: Dependency tree is useful to infer the relative importance of a word toward another word  in the same sentence. This relative importance could be helpful for TOWE to attend to the important words for the target word. To infer pair-wise importance of two words using dependency tree, one can computes the distance between two words in the dependency tree. For instance, as a running example, in the sentence ``All warranties honored by HP  are disappointing"", the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , the two words ``disappointing"" and ``warranties"" are directly connected to each other. The short distance between these two words could be helpful to infer the importance of the word ``disappointing"" for the target ``warranties"".  Word Connection: Dependency tree could provide better contextual information for each word via the connections of the word with its head and dependants, thus it helps to improve word representations. Thereby, dependency tree could benefit TOWE. For instance, in the running example, the head of ``reputable"" is ``company"" while the head of ``warranties"" is ``disappointing"". Therefore, it would be easier to infer that the opinion word ``disappointing"" is related to the target word ``warranties"" and ``reputable"" is irrelevant.   %Besides the difference between the rule-based and deep learning models for TOWE regarding the representation learning methods, the rule-based methods have exploited the syntactic structures  of the sentences to improve the performance for TOWE while  %In particular, the related tasks of TOWE involves target word extraction/aspect term exaction  , and opinion word extraction   . A key difference between OWE and TOWE is that the opinion words in OWE are general and do not need to tie to any target words in the sentence while TOWE explicitly   %Despite its potential benefits, TOWE has only been studied by a few works in the past, characterizing the early rule-based and lexicon-based approaches  and very recently deep learning models .   %In the literature, feature-based models and deep learning model has been proposed for both target word extraction  and opinion word extraction . While joint models predict both the opinion and the target words, they cannot pair up them, thus being unable to solve the task of TOWE. In the literature, only a few of works have studied the task of TOWE, including the early attempts with the rule-based and lexicon-based approaches  and the recent works with deep learning models for TOWE .    
"," Targeted opinion word extraction  is a sub-task of aspect based sentiment analysis  which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.  %Deep learning models have been shown to achieve the state-of-the-art performance for TOWE in the recent studies.  %While previous feature-based models have shown syntactical structure  is useful for this task, recent deep neural nets ignore this information in their model. To address this limitation, in this paper, we propose a new approach which incorporates syntactical structure  into deep neural nets. More specifically, our model employs the dependency tree to capture the relative importance of the words to the aspect-term and to encode the connections between words. Our extensive experiments on four benchmark datasets prove the superiority of the proposed model, leading to new state-of-the-art results on all datasets. Moreover, detailed analysis shows the effectiveness of the components of the proposed model.",193
"  Aspect-based Sentiment Analysis  is a fine-grained version of sentiment analysis  that aims to find the sentiment polarity of the input sentences toward a given aspect. We focus on the term-based aspects for ABSA where the aspects correspond to some terms  in the input sentence. For instance, an ABSA system should be able to return the negative sentiment for input sentence ``The staff were very polite, but the quality of the food was terrible.'' assuming ``{, the author has a positive sentiment toward aspect-category service and negative sentiment toward aspect-term food. In this paper, we introduce a novel model for sentiment analysis toward aspect-term.  %The early attempts for ABSA have performed feature engineering to produce useful features for the statistical models  for this problem . One limitation of these feature-based models is that they require significant human effort and linguistic background to design effective features. In order to overcome this limitation,  %The typical network architectures for ABSA in the literature involve convolutional neural networks  , recurrent neural networks  , memory networks , attention  and gating mechanisms .  %automatically induce effective features for ABSA and  Due to its important applications , ABSA has been studied extensively in the literature. In these studies, deep learning has been employed to produce the state-of-the-art performance for this problem . Recently, in order to further improve the performance, the syntactic dependency trees have been integrated into the deep learning models  for ABSA . Among others, dependency trees help to directly link the aspect term to the syntactically related words in the sentence, thus facilitating the graph convolutional neural networks   to enrich the representation vectors for the aspect terms.  %Although the graph-based models have achieved decent performance for ABSA, these models have   However, there are at least two major issues in these graph-based models that should be addressed to boost the performance. First, the representation vectors for the words in different layers of the current graph-based models for ABSA are not customized for the aspect terms. This might lead to suboptimal representation vectors where the irrelevant information for ABSA might be retained and affect the model's performance. Ideally, we expect that the representation vectors in the deep learning models for ABSA should mainly involve the related information for the aspect terms, the most important words in the sentences. Consequently, in this work, we propose to regulate the hidden vectors of the graph-based models for ABSA using the information from the aspect terms, thereby filtering the irrelevant information for the terms and customizing the representation vectors for ABSA. In particular, we compute a gate vector for each layer of the graph-based model for ABSA leveraging the representation vectors of the aspect terms. This layer-wise gate vector would be then applied over the hidden vectors of the current layer to produce customized hidden vectors for ABSA. In addition, we propose a novel mechanism to explicitly increase the contextual distinction among the gates to further improve the representation vectors.  %as the hidden vectors at different layers of the graph-based models tend to capture different levels of contextual information, the gate vectors for the different layers should also maintain some level of contextual distinction. To this end, we propose a novel mechanism to explicitly increase the contextual distinction among the gate vectors to further improve the quality of the representation vectors.  The second limitation of the current graph-based deep learning models is the failure to explicitly exploit the overall importance of the words in the sentences that can be estimated from the dependency trees for the ABSA problem. In particular, a motivation of the graph-based models for ABSA is that the neighbor words of the aspect terms in the dependency trees would be more important for the sentiment of the terms than the other words in the sentence. The current graph-based models would then just focus on those syntactic neighbor words to induce the representations for the aspect terms. However, based on this idea of important words, we can also assign a score for each word in the sentences that explicitly quantify its importance/contribution for the sentiment prediction of the aspect terms. In this work, we hypothesize that these overall importance scores from the dependency trees might also provide useful knowledge to improve the representation vectors of the graph-based models for ABSA. Consequently, we propose to inject the knowledge from these syntax-based importance scores into the graph-based models for ABSA via the consistency with the model-based importance scores. In particular, using the representation vectors from the graph-based models, we compute a second score for each word in the sentences to reflect the model's perspective on the importance of the word for the sentiment of the aspect terms. The syntax-based importance scores are then employed to supervise the model-based importance scores, serving as a method to introduce the syntactic information into the model. In order to compute the model-based importance scores, we exploit the intuition that a word would be more important for ABSA if it is more similar the overall representation vector to predict the sentiment for the sentence in the final step of the model. In the experiments, we demonstrate the effectiveness of the proposed model with the state-of-the-art performance on three benchmark datasets for ABSA. In summary, our contributions include:   %, we propose to obtain another important score for each word in the sentence based on the representation vectors from the models. These model-based importance scores are then   %In particular, for ABSA, some words might introduce more useful information to predict the sentiment of the aspect terms than the the other words in the sentence   %In particular, some words in a given sentence might involve more useful information for relation prediction in RE than the other words, and the dependency tree for this sentence can help to better identify those important words and assign higher importance scores for them . We expect that introducing such importance information for the words in the deep learning models might lead to improved performance for RE. Consequently, in this work, we propose to obtain an importance score for each word in the sentences from the dependency trees . These will serve as the general tree representation to incorporate the syntactic information into the deep learning models for RE.  %In particular, as the aspect terms are the most important words in the sentences for ABSA,     %Syntactical structure, e.g., dependency tree, could shorten the distance between syntactically related words thus improve the contextualized representation of the words. In order to incorporate the syntactical tree into the deep models, recent work mainly employs the graph convolutional network   to model the interaction between words based on the syntactic tree. In order to emphasize on the given aspect term, current models use the representation of the aspect term generated by GCN either directly for final classification or as a gate to filter out features of a sequential model. However, these methods cannot benefit from the information of the given aspect term to control the information flow in the graph based model. Moreover, it is expected that the words that are syntactically related to the given aspect term should convey more information about the sentiment toward it. Unfortunately, non of the existing work considers this relative importance in the final representation of the sentence. In order to address these issue, we propose a new graph based model which employs the semantic of the given aspect term to control interaction between nodes/words in the GCN model and to emphasize more on the syntactically important words in the final representation of the sentence.  %Due to the application of ABSA in other downstream applications, e.g., opinion mining, it has gained a lot of attention in natural language processing community and several methods have been proposed for this task. Early attempts employed feature engineering to extract useful features for statistical models like SVM . These methods require extensive human effort and strong linguistic knowledge. They also suffer from low generalization ability. Due to these limitations, neural networks and deep models have superseded feature based models and obtain promising results in ABSA . Early deep models for ABSA have exploited sequential models  , convolutional neural nets  or even memory networks . In order to improve the performance, attention  and gating mechanism  has also been widely adopted in deep models. Recently, it has been shown that syntactical information could also improve the performance of deep models . Syntactical structure, e.g., dependency tree, could shorten the distance between syntactically related words thus improve the contextualized representation of the words. In order to incorporate the syntactical tree into the deep models, recent work mainly employs the graph convolutional network   to model the interaction between words based on the syntactic tree. In order to emphasize on the given aspect term, current models use the representation of the aspect term generated by GCN either directly for final classification or as a gate to filter out features of a sequential model. However, these methods cannot benefit from the information of the given aspect term to control the information flow in the graph based model. Moreover, it is expected that the words that are syntactically related to the given aspect term should convey more information about the sentiment toward it. Unfortunately, non of the existing work considers this relative importance in the final representation of the sentence. In order to address these issue, we propose a new graph based model which employs the semantic of the given aspect term to control interaction between nodes/words in the GCN model and to emphasize more on the syntactically important words in the final representation of the sentence.   %In particular, in this paper, we propose a novel model which employs the representation of the given aspect term to compute a gate. This gate is applied over the output of one layer of GCN. By doing so, the information represented in the aspect term would erase non-relevant information in each node/word obtained by its interaction with its neighbors in one aggregation step in GCN. As different layers of GCN capture different substructure of the graph, e.g., 1-hop vicinity vs 2-hop vicinity, we propose to exploit different gates in different layers. To ensure the gates in different layers are not the same, we propose a novel method to encourage diversity among gates in different layers of the GCN. Moreover, in addition to exploiting the semantic of the aspect term to control interactions between nodes/words in the GCN, in this paper, we propose to encourage the model to emphasize on the words that are syntactically important to the aspect term. In particular, we use the distance between a word to the aspect term in the dependency tree as an indication of the syntactic importance of the word to the aspect term. This importance is employed as supervision signal to encourage the model to emphasize on the words that are syntactically important to the aspect term. This is obtained in the final layer of the model when the sentiment prediction is performed. More specifically, we first estimate the semantic importance of each word by employing the final representation of the word as the input to a classifier to predict the label distribution and then we compute the KL-Divergence between this label distribution predicted by the word representation and the label distribution predicted by the sentence representation. If the two label distribution are more similar, it shows that the word representation contains most of the information that the model consumes to perform the final classification. Finally, in order to ensure those words which are syntactically important to the aspect term are semantically important in the model too, we decrease the divergence between distribution of the syntactic score and semantic score for each word via KL-Divergence between these two distributions.  %Our extensive experiments on three benchmark datasets, empirically prove the effectiveness of the proposed model leading to new state-of-the-art results in all three benchmark datasets.      A novel method to regulate the GCN-based representation vectors of the words using the given aspect term for ABSA.   A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term.   Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-the-art performance for all the datasets.   
"," Aspect-based Sentiment Analysis  seeks to predict the sentiment polarity of a sentence toward a specific aspect. Recently, it has been shown that dependency trees can be integrated into deep learning models to produce the state-of-the-art performance for ABSA. However, these models tend to compute the hidden/representation vectors without considering the aspect terms and fail to benefit from the overall contextual importance scores of the words that can be obtained from the dependency tree for ABSA. In this work, we propose a novel graph-based deep learning model to overcome these two issues of the prior work on ABSA. In our model, gate vectors are generated from the representation vectors of the aspect terms to customize the hidden vectors of the graph-based models toward the aspect terms. In addition, we propose a mechanism to obtain the importance scores for each word in the sentences based on the dependency trees that are then injected into the model to improve the representation vectors for ABSA. The proposed model achieves the state-of-the-art performance on three benchmark datasets.  %These models employ graph based neural nets to incorporate syntactical structure into the model. However, they ignore the aspect term information to control the interaction between words in the syntax tree which is modeled by graph neural net.  % Moreover, they neglect the consistency between the syntactic and semantic importance of the words toward the given aspect. %Moreover, the relative importance of the words to the given aspect term based on their syntactical role is neglected in the final representation produced by the existing syntax-aware models. To address these two issues, in this paper, we introduce a new syntax-aware model which incorporates gating mechanism to control information flow in the graph based model using the given aspect term. It also ensures the words that are syntactically important to the aspect term are more pronounced in the final representation of the sentence. Our extensive experiments on three benchmark datasets empirically prove the effectiveness of the proposed model leading to new state-of-the-art results on all three benchmark datasets.",194
"  %  Event Extraction  is an important task of Information Extraction that aims to recognize events and their arguments in text. In the literature, EE is often divided into two sub-tasks:  Event Detection  to detect the event trigger words, and  Event Argument Extraction  to identity the event arguments and their roles for the given event triggers. In recent years, ED has been studied extensively with deep learning while EAE is relatively less explored . As EAE is necessary to accomplish EE and helpful for many downstream applications , further studies are required to improve the performance of EAE. This work focuses on EAE to meet this requirement for EE.  %We follow the definitions for EE, ED and EAE in the popular ACE 2005 dataset that considers event arguments as some entity mentions in the input sentences.  %For instance, given the trigger word ``{ in the sentence ``{'' as an argument of role {'' with the trigger word ``{) and the entity mention ``{'' is the most important word to reveal the {'' for the event triggered by ``{'' and the two words ``{'' can be leveraged to encourage the deep learning models to focus more on the context word ``{'' and ``{'' and ``{'' with the trigger word ``{) and the entity mention ``{'' is the most important word to reveal the {'' for the event triggered by ``{'' and the two words ``{'' can be leveraged to encourage the deep learning models to focus more on the context word ``{'' and ``{'' and ``{}, the Hanif Bashir's son-in-law, while US officials confirmed all Bashir's family members were \underline{killed} last week.}  In this sentence, an EAE system should be able to realize the entity mention ``{ of the {''. As ``{'' are far away from each other in the sentence as well as its dependency tree, the EAE models might find it challenging to make the correct prediction in this case. In order for the models to be successful in this case, our intuition is that the models should first rely on the direct connections between ``{'' in the dependency tree to capture the role of ``{''. Afterward, the models can rely on the close semantic similarity between ``{'' to further connect ``{'' so the role information of ``{''. Finally, the direct apposition relation between ``{'' can be exploited to connect ``{'' to obtain the necessary representations to perform argument prediction for ``{'' to its important context word ``{'' to the entity ``{'' and ``{'' to ``{'' via the apposition relation in the dependency tree. In this way, the context word ``{'' to improve the argument prediction. In order to obtain the semantic structures for the sentences, we will employ the similarities between the representation vectors for the words in this work.    %We note that the previous work on EAE has only considered the syntactic structures  and this is the first work to investigate the semantic structures for EAE.   %Event Extraction  is the task of identifying words or phrases that evoke an event , classify their event type, find the entities that play a role in an event  and classify the role of the event arguments.  This task is mainly studied under two sub-tasks:  Event Detection  which aims to find the event triggers and classify them,  Event Argument Extraction  which aims to identify event arguments and their role for a given event. In recent years, ED has been studied extensively while EAE is relatively less explored . As EAE, is necessary to accomplish EE and it could be useful in many downstream applications , further studies is required to improve the performance of EAE.   %Early attempts employed feature-based models for the task of EE . Some of the feature-based models have exploited the structure of the sentence . As the feature-based models requires great deal of linguistic knowledge and could not generalize well to new datasets and domains, they have been superseded with deep neural networks. These networks represent the words by dense vectors  which facilities incorporating knowledge from external sources via pre-training and also learning effective representations using deep architectures such as convolutional neural network  or recurrent neural network  . Also, dependency tree has been shown to be useful for deep neural networks for EE .  %Structure of the sentence  has been employed by both feature-based and deep models for the task of EE and its sub-tasks including EAE. The connections between words in the syntactic structure provide richer information about the word and could help the model mitigate the problem of long dependencies. For instance, in the sentence ``In Iraq, a reporter who had been lost for three months is found dead close to Nasr hotel"" the connection between the entity ``reporter"" and the event trigger ``dead"" in the dependency tree could be useful to ignore the irrelevant words between these two words in the sequential order of the words in the sentence. While the dependency tree has been explored in the previous work, existing approaches, especially deep models, exploit the structure of the sentence in a limited manner. For instance, Sha et al.  incorporates the connections between the words in the dependency tree into recurrent neural network . However, in other natural language processing  tasks, it has been shown that dependency tree could be incorporated more efficiently into the deep model using Graph Convolutional Neural Network   and its variants. In this paper, we propose to employ Graph Neural networks for the task of EAE. Moreover, we argue that, although syntactic structure is useful but the semantic connections between words is also important. For instance, in the sentence ``Iraqi Press constantly report interviews with Hussain Molem, the Hanif Bashir's son-in-law who was serving in the Iraq air force, while US officials confirmed all Bashir's family members have been killed in the last week attack."" the semantic connection between the entities ``Hussain Molem"" and ``Hanif Bashir"" and also the event mentioned for the entity ``Bashir's family members"" is useful to infer the role of the entity ``Hussain Molem"" is ``victim"" for the event trigger ``killed"". So, in order to capture the semantic connections in addition to the syntactic relations between words, in this paper, we propose to apply GCN over a combined view of the syntactic and semantic structure of the sentence.  %GCNs facilitate the introduction of the sentence structures into deep learning models and have been demonstrated to be helpful for many other natural language processing  tasks.  %How can we combine the syntactic and semantic structures of the sentences to perform EAE? A simple method is to use the structures separately in the Graph Convolutional Neural Networks   whose outputs are concatenated to obtain the representation vectors for the words in the sentence. However, this method is likely suboptimal as an individual structure  might not be able to identify the important context words for the representation vector of some current word in EAE by itself. It is in fact necessary to consult both syntactic and semantic structures to capture the effective context words for the representation vectors in these cases. An example for this problem is illustrated in our previous example where both the syntactic and semantic structures help to connect ``{'' for EAE.  How should we combine the syntactic and semantic structures to aid the learning of effective representations for EAE? In this work, we propose to employ Graph Transformer Networks   to perform the syntax-semantic merging for EAE. GTNs facilitate the combination of multiple input structures via two steps. The first step obtains the weighted sums of the input structures, serving as the intermediate structures that are able to capture the information from different input perspectives . In the second step, the intermediate structures are multiplied to generate the final structures whose goal is to leverage the multi-hop paths/connections between a pair of nodes/words  to compute the importance score for the final structures. As the multi-hop paths with heterogeneous types of connections along the way  has been illustrated to be helpful in our running example , we expect that GTNs can help to combine the syntactic and semantic structures to produce effective representations for EAE.   %Consequently, in this work, we propose to combine the syntactic and semantic structures earlier in the process to learn effective mixed structures for the sentences over which the GCN models can be applied to produce better representations for EAE. In particular, we propose to employ Graph Transformer Networks   that learn to mix multiple input structures for GCNs via the weighted sums of the individual structures . These intermediate structures are then multiplied with each other to generate the final mixed views that are able to model the word connections with heterogeneous types and different lengths. GTNs have been shown to produce effective node embeddings for the graphs with heterogeneous edge types and we expect them to be helpful to combine the syntactic and semantic structures for EAE as well. To our knowledge, this is the first work that considers GTNs with syntactic and semantic structures for NLP in general and for EAE and EE in particular.   %How can we exploit the syntactic and semantic structures of the sentences to perform EAE? In this work, we follow the common approach to use such sentence structures in deep learning in which the sentence structures are considered as the adjacency matrices in Graph Convolutional Neural Networks   to obtain the abstract representation vectors for the words in the sentences. A simple method to apply GCNs on both the syntactic and semantic structures is to run GCNs with the individual structures whose outputs are concatenated in the end to predict arguments . However, this method is likely suboptimal as it is sometime challenging to identify the important context words for the representation vector for a word in EAE based on only one type of sentence structures . It is in fact necessary to consult both structure types to come up with the effective context words for the representation vectors in these cases. An example for this problem is illustrated in our previous example where both the syntactic and semantic structures help to connect ``{'' for EAE. Consequently, in this work, we propose to combine the syntactic and semantic structures earlier in the process to learn effective mixed structures for the sentences over which the GCN models can be applied to produce better representations for EAE. In particular, we propose to employ Graph Transformer Networks   that learn to mix multiple input structures for GCNs via the weighted sums of the individual structures . These intermediate structures are then multiplied with each other to generate the final mixed views that are able to model the word connections with heterogeneous types and different lengths. GTNs have been shown to produce effective node embeddings for the graphs with heterogeneous edge types and we expect them to be helpful to combine the syntactic and semantic structures for EAE as well. To our knowledge, this is the first work that considers GTNs with syntactic and semantic structures for NLP in general and for EAE and EE in particular.  %In order to further boost the performance for EAE, we propose two novel inductive biases for GTNs in this work. The first bias attempts to explicitly promote the diversity among the intermediate mixed structures for GTNs to improve the representations for EAE. The rationale is to encourage the intermediate structures to capture different aspects of the mixed structures for syntax and semantics, thus avoiding the degenerate solution with similar learned structures in the intermediate views and facilitating the emergence of the effective structures for EAE. The second bias, on the other hand, aims to improve the generalization of GTNs using the Information Bottleneck idea . In particular, the use of the combined structures from syntax and semantics might augment GTNs with high capacity to encode the detailed information in the input sentences. Coupled with the generally small training dataset for EAE, the GTN models could learn to preserve all the context information in the input sentences, including the irrelevant information for EAE. This likely leads to the overfitting of GTNs on the training data and affect the overall performance for EAE. In order to overcome this issue, we propose to treat the GTN models in this work as an information bottleneck in which the produced representations of GTNs are trained to not only achieve good prediction performance for EAE but also minimize the mutual information with the input sentences . To this end, we introduce the mutual information between the generated representations of GTNs and the input sentences as an additional term in the overall loss function to improve the generalization of GTNs for EAE. Our extensive experiments on two benchmark datasets for EAE demonstrate the effectiveness of the proposed model, yielding the state-of-the-art performance for EAE in this work.   Finally, in order to further boost the performance for EAE, we propose a novel inductive bias for the proposed GTN model, aiming to improve the generalization of GTNs using the Information Bottleneck idea . In particular, the use of the rich combined structures from syntax and semantics might augment GTNs with high capacity to encode the detailed information in the input sentences. Coupled with the generally small training datasets for EAE, the GTN models could learn to preserve all the context information in the input sentences, including the irrelevant information for EAE. This likely leads to the overfitting of GTNs on the training data. In order to overcome this issue, we propose to treat the GTN model in this work as an information bottleneck in which the produced representations of GTNs are trained to not only achieve good prediction performance for EAE but also minimize the mutual information with the input sentences . To this end, we introduce the mutual information between the generated representations of GTNs and the input sentences as an additional term in the overall loss function to improve the generalization of GTNs for EAE. Our extensive experiments on two benchmark datasets for EAE show that the proposed model can achieve the state-of-the-art performance for EAE.   %GCNs compute the representation vector for a word in a layer using the vectors for the other words from the previous layer, weighted by   %How can we efficiently combine the semantic and the syntactic structure of the sentence? One possible solution is to apply different GCNs on each view and then combine the output the GCNs . However, this solution cannot model the early interaction between two views. In order to effectively model the interactions between two views, in this paper, we introduce to employ the newly proposed architecture Graph Transformer Network  . Graph Transformer  computes a weighted sum of the input views  to construct an intermediate combined view. By constructing several intermediate views and computing the multiplication of these intermediate views, a final combined view is built that models heterogeneous paths of different lengths between the nodes . Stacking several layers of GT, named as GTN, will model different kinds of modeling the interactions between input views. To the best of our knowledge, we are the first to exploit GTN for the task of EAE.  %While GTN could efficiently model the interactions between syntactic and semantic views, it has some limitations that prevent this model from further improvement. First, in the original GTN proposed by Yun et al.  there is no constraint to enforce the intermediate views and the views in different layers of GTN to be different. This limitation restrict GTN capability to encode different types of interactions between nodes . Thus adding more intermediate views or layers of GT would not be helpful. In order to mitigate this problem, in this paper, we introduce a new inductive bias to encourage the diversity among different intermediate views in different layers of GTN. Second, as the original GTN will compute several representations for the interactions between words, it is prone to overfitting. In other words, GTN could capture different types of connections between words. Thus, it is prone to learn some patterns of connections that are specific to the training examples. In order to prevent this, we should regularize the representations in GTN to learn only those types of connections that are relevant to the task and reduce excess information which could represent nuisances in the training examples. To this end, we propose to treat GTN as an Information Bottleneck in our model. Information Bottleneck  is a technique which ensures the representations of the input in the bottleneck has small mutual information with the early layers in the model but has high mutual information with the outputs . Concretely, in our model we decrease the mutual information between the representations of the words generated by GTN and the representations of the words in the early layers in the model, while we increase the mutual information between GTN representations and the labels using the common training loss .   %Our extensive experiments on two benchmark dataset show the effectiveness of the proposed approach, leading to the state-of-the-art results on both datasets. Moreover, our detailed analysis on the model supports the usefulness of our modifications on the original GTN.  
"," The goal of Event Argument Extraction  is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for EAE. Consequently, in this work, we propose a novel model for EAE that exploits both syntactic and semantic structures of the sentences with the Graph Transformer Networks  to learn more effective sentence structures for EAE. In addition, we introduce a novel inductive bias based on information bottleneck to improve generalization of the EAE models. Extensive experiments are performed to demonstrate the benefits of the proposed model, leading to state-of-the-art performance for EAE on standard datasets.  %. GTNs offer an mechanism to combine syntactic and semantic structures and infer more effective sentence structures for EAE.  %and achieve the state-of-the-art performance on standard datasets.  %We achieve the state-of-the-art performance for EAE on standard datasets.  %Our extensive experiments demonstrate the benefits of the proposed model and achieve the state-of-the-art performance on standard datasets.  %However, they fail to effectively model the entire graph structure and also they ignore the semantic relations between words. In order to address these issues, in this paper, we propose to employ the new architecture of Graph Transformer Network  to effectively employ the syntactic and semantic structure of the sentence and model the interaction between them. Moreover, we introduce two new inductive bias in our model to rectify the inherent limitations of the original GTN architecture. Our extensive experiments on two benchmark datasets prove the effectiveness of the proposed model for the task of EAE, leading to new state-of-the-art results on both datasets.",195
"  Curriculum learning trains a model by using easy examples first and gradually adding more difficult examples. It can speed up learning and improve generalization in supervised learning models .  %With curriculum learning, models are trained according to a curriculum that sorts training examples according to difficulty. %The model is first trained with only the easiest examples. %More difficult examples are gradually added according to some pre-determined schedule. %Training with curriculum can lead to faster model convergence than a baseline model trained without a curriculum . %With machine learning models and data sets continuing to grow, and knowing the impact of model training on the environment, there is a need for efficient model training . %\Hong{I donot understand why do you need to include the above sentence.  What is the gain?} %Hong.  In Bengio et al's ""curribulum learning"" work, they found that CL has an effect on the speed of convergence and better optimization of non-convex functions. You may want to evaluate in your work, in addition to the performance. A major drawback of existing curriculum learning techniques is that they rely on heuristics to measure the difficulty of data, and either ignore the competency of the model during training or rely on heuristics there as well. For example, sentence length is often used as a proxy for difficulty in NLP tasks . %Similarly, the number of objects in an image has been used as a proxy for difficulty in an image recognition task . Such heuristics can be useful but have limitations. First, the heuristic chosen may not actually be a proxy for difficulty. Depending on the task, long sequences could signal easier or harder examples, or have no signal for difficulty. Second, a model's notion of difficulty may not align with the heuristic imposed by a human developing the model. It may be that examples that appear difficult for the human are in fact easy for the model to learn.  Competency was recently introduced as a mechanism to determine when new examples should be added to the training data . %However, in that work the competency schedule was ad hoc, and did not actually look at the competency of the model but assumed a schedule according to learning heuristics.  However, in that work competency is a monotonically increasing function of a pre-determined initial value. %competency, . Once set, competency is not evaluated during training. Ideally, model competency should be measured at each training epoch, so that the training data could be appropriately matched with the model at a given point in the training. If a model is improving, then more difficult training data can be added at the next epoch.  But if performance declines, then those difficult examples can be removed, and a smaller, easier training set can be used in the next epoch.  In this study, we propose to estimate both the difficulty of examples and the ability of deep learning models as latent variables based on model performance using Item Response Theory , a well-studied methodology in psychometrics for test set construction and subject evaluation . IRT models estimate latent parameters such as difficulty for examples  %Hong: I changed ""examples"" to ""samples"" in earlier context, but if you don't like it, just change them back.  Just stick to one use throughout the paper.   and a latent ability parameter for individuals . %Hong.  Here you may want to use ""model"" ability instead of ""subject"" IRT models are learned by administering a test to a large number of subjects, collecting and grading their responses, and using the subject-response matrix to estimate the latent traits of the data. These learned parameters can be used to estimate the ability of future subjects, based on their graded responses to the examples. %Hong.  Similarly, would you introduce ""model"", not ""test-takers""  IRT has not seen wide adoption in the machine learning community, primarily due to the fact that fitting IRT models requires a large amount of human annotated data for each example. However, recent work has shown that IRT models can be fit using machine-generated data instead of human-generated data as input .  Because IRT learns example difficulty and subject ability together,  %it is an interesting framework to consider for the problem of curriculum learning.  in this work we propose replacing heuristics for learned parameters in curriculum learning. First, we experiment with replacing a typical difficulty heuristic  with learned difficulty parameters. Second, we propose \modelname~, a novel curriculum learning framework that uses the estimated ability of a model during the training process to dynamically identify appropriate training data. At each training epoch, the latent ability of the model is estimated using output labels generated at the current epoch. Once ability is known, only training data that the model has a reasonable chance of labeling correctly is included in training. As the model improves, the estimated ability will improve, and more training examples will be added.  To the best of our knowledge, this is the first work to learn a model competency during training that is directly comparable to the difficulty of the examples. %Hong. You may want to define terminology up front. Here you say ""training data pool"".  Earlier you say ""examples"".  Perhaps use ""items"", and then define it and how it is used in your paper %To be explicit, in this work our goal is to  Our study will test the following three hypotheses: H1: Using learned latent difficulties instead of difficulty heuristics leads to better held-out test set performance for models trained using curriculum learning, H2: A dynamic data selection curriculum learning strategy that probes model ability during training leads to better held-out test set performance than a static curriculum learning strategy that does not take model ability into account, H3: Dynamic curriculum learning is more efficient than static curriculum learning, leading to faster convergence. We test our hypotheses on the GLUE classification data sets .  Our contributions are as follows:  we demonstrate that for curriculum learning, learned difficulty outperforms traditional difficulty heuristics,   we introduce a novel curriculum learning framework which automatically selects training data based on the estimated ability of the model, and  we show that training using \modelabbr~leads to better performance than both traditional curriculum learning methods and a fully supervised competitive baseline.  Our findings support the overall curriculum learning framework, and demonstrate that learning difficulty and ability lead to further performance improvements beyond common heuristics.\footnote{Code implementing our experiments and learned difficulty parameters for the GLUE data sets are available at \url{https://jplalor.github.io/irt}.} %We will release the learned difficulty parameters for the GLUE data sets as a resource for the community to further explore learned difficulties and dynamic curriculum learning.}%\footnote{Code and data used for this work, including learned difficulty values, will be released upon publication.}  
","   Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. %  They also either ignore the ability of the model or rely on heuristics there as well.   %In this work we show that learning difficulty and ability is more effective for curriculum learning than applying heuristics.   In this work, we propose replacing difficulty heuristics with learned difficulty parameters.    We also propose \modelname~, a strategy that probes model ability at each training epoch to select the best training examples at that point.   %\modelabbr~adds data at a rate commensurate with the model's capability, in contrast to scheduled curricula that add data at a predetermined rate.   We show that models using learned difficulty and/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.   %Using \modelabbr~to train LSTM models further improves performance.   %Experimental results demonstrate that \modelabbr~outperforms static CL strategies on a number of NLP classification tasks.",196
" Entity normalization and variant generation are fundamental for a variety of other tasks such as semantic search and relation extraction . Given an entity name , the goal of entity normalization is to convert  to a canonical form , while the goal of entity variant generation is to convert  to a set of different textual representations that refer to the same entity as E .   \looseness=-1 Typically, entity normalization and variant generation are done by first performing entity linking , i.e., matching entity names appearing in some context  to named entities in curated knowledge bases , then use the canonical form or variations  residing in the KBs to complete the tasks. Unfortunately, in some scenarios, such as search , entity names are not surrounded by context.  Furthermore, for specialized domain-specific applications, there may not be a knowledge base to govern the names of the relevant entities. Thus, entity linking is not always applicable. In this paper,  we take the view  %the problem differently, in particular, we argue  that entity normalization and variant generation can be done without contextual information or  external KBs if we understand the internal structures of entity names.  % Fundamental to the success of entity linking is the availability of the contextual information  and ontological information .    %  and there are no external KBs that we can use as master datasets to match input entity names. \todo{One may argue that DBpedia and Wikipedia is a good proxy. It may be useful to talk about related work taking this view here.} % For example, when searching , we need to also consider variations like , , , etc. without relying on any contextual information and external KBs.  % Performing entity normalization and variant generation in a contextless fashion is extremely challenging because we have only the surface forms of entity names.  % Several attempts have been made to parse the structured representation of entity names. As observed in , entity names often have  %structured representation  implicit structures  that can be exploited to solve entity normalization and variant generation. Table  shows how we can manipulate such structured representations of entity names to generate different variations without help from context or external knowledge.  % For example, if we know that  is  and  is  in , we can generate two variations:    and   .  [ht]                            & Structured Representation           & \multicolumn{1}{c|}{Manipulation} & \multicolumn{1}{c|}{Variations} \\ {*}{Michael Jordan}           & \multirow{3}{*}{``Michael"" ``Jordan""}  & ,                                & Jordan, Michael                          \\                                             &                              & createInitial                        & M Jordan                                 \\                                             &                              & createInitial createInitial            & MJ                                       \\  \multirow{2}{*}{General Electric Company} & \multirow{2}{*}{``General Electric"" ``Company""} & {createInitial} drop               & GE                                       \\                                             &                              & createInitial abbreviate         & GE Co.                                   \\ . Our main contributions include: % \todo{Low-resource setting can mean different things. It would be helpful to clearly describe what you mean here.}  % { %  Our problem is related to both flat and nested named entity recognition . However, as discussed in , NER focuses on identifying the outermost flat entities and completely ignores their internal structured representations.  identify nested entities within some context using fully supervised methods that require large amounts of labeled data, whereas our goal is to learn from very few labels  in a contextless fashion.  Active learning  and weak supervision have been widely adopted for solving many entity-centric problems, such as entity resolution , NER , and entity linking . While the power of the combination of the two techniques has been demonstrated in other domains , to the best of our knowledge, the two approaches are usually applied in isolation in prior entity-related work.  Recently, data programming approaches  use labeling functions/rules to generate weak labels to train machine learning models in low-resource scenarios. Data programming approaches like Snorkel usually assume that labeling functions are manually provided by users, indicating that their target users must have programming skills in order to provide such labeling functions. In contrast, our goal is to minimize both human effort  and lower human skills .   % Named entity recognition   % Our problem is similar to NER and its fine-grained version nested NER  with several key differences:  there is no labeled data available in our settings;  we focus on the scenarios where entity names is all we have . Recently,  proposed active learning based approaches for NER in low-resource settings. Following the same direction, we enhance active learning with weak supervision so as to further reduce the labeling efforts.    % Understanding entity names is an important task for many entity-centric tasks such as entity disambiguation and information retrieval. Computing the textual similarity of two entity names is one of the widely used methods to tell whether they are the same name or not . However, entity names can be highly ambiguous and text-based similarity functions are not robust enough to resolve complex cases . Consider the following date entities:  %  % } %  is ,  is , and  is , we can transform these components separately  and assemble the transformed components according to some standardized format, such as --, then we can translate  to .  % Currently, named entity recognition  and the subsequent entity disambiguation task are either treated separately , or treated as one joint task by looking into the unstructured text where the entities are extracted from and linking them to a reference knowledge base . However, in some tasks , entities are given without context . % Enriching entities with normalized form and variations obtained by manipulating their semantic structures can be helpful for these tasks.   % Several attempts have been made to understand entity name structures.  proposed declarative frameworks that allow high-skill developers to manually specify rules that translate entity names into semantic structures. To avoid this labor-intensive and clearly not scalable manual process,  proposed an active learning-based framework named LUSTRE that semi-automatically learns these parsing rules. However, the availability of a list of comprehensive, accurate, and complete dictionaries is crucial to the success of LUSTRE.   % \looseness=-1 Understanding entity name structures can be viewed as a sequence labeling problem. Recently, deep learning-based approaches have been shown to achieve state-of-the-art performance on sequence labeling problems . One of the foundations of these approaches is the use of pre-trained character or word embeddings that carry semantic information learned from large text corpus. However, these deep learning-based approaches are data hungry.     
"," \looseness=-1  %Entity names usually have structured representation that is useful for many entity-related tasks such as entity normalization and variant generation. Learning the structured representation of entity names in low-resource settings without context and external knowledge bases is challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem, and we experimentally show that our method can learn high-quality BERT-CRF models in low-resource settings. A video demo of a system that implements this framework is included in supplementary materials. Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge  is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.",197
" In recent years, voice assistants have become ubiquitous in the household, performing tasks for users through a conversational interface. Given the informal nature of language in these settings, there are many ways in which the agent can misunderstand user commands, intent, and how to complete actions. A vital part of ensuring that a user continues to interact with an agent is the user's confidence in the agent's ability to correctly and smoothly respond to their requests. Relying on conversational rather than transactional dialogue is a core method of building this trust .   However, conversational dialogue is difficult to generate and can often lead to situations where the agent produces an utterance that the user is unable to properly respond to or that creates friction between the user and agent. We refer to these situations as dialogue breakdown, where the agent is prevented from completing the desired goal of the dialogue either by user exasperation or agent misunderstanding . Detecting when breakdown occurs is an essential part of ensuring its effects are mitigated, either by recovering when they occur or avoiding their creation altogether . As in other dialogue settings, gathering labelled data is difficult. Data collection must either rely on interrupting user interactions or paying a third-party to rate dialogue after its completion, both of which are often intrusive, expensive, and affected by user bias . In these settings, semi-supervised learning methods are a natural way to fully utilize the limited labelled data by leveraging the large amounts of unlabelled data that are commonly available.  In this paper, we investigate two semi-supervised learning methods to improve performance on dialogue breakdown detection.  We consider continued pre-training on the Reddit dataset  as an approximation of the dialogue domain we would like to eventually fine-tune on. We also consider self-supervised manifold based data augmentation  , a data augmentation method that utilizes our further pre-trained model to generate new training examples.   We evaluate these methods on the Dialogue Breakdown Detection Challenge   English shared task. We submit our final models to the 2020 DBDC5 shared task, placing first in the English track.  We beat baselines and other submissions by over 12\% accuracy, 0.135 F1 score, and 0.02 JS divergence. In experiments on data from 2019 , our baseline model improves over previous challenge winners by over 13\% .  The addition of our semi-supervised learning methods improves these baseline models further by 2\% accuracy,  0.02 F1 score, and 0.003 Jensen Shannon  Divergence. Although we specifically consider the task of dialogue breakdown detection, these semi-supervised techniques are applicable generally to any supervised dialogue task and provide a simple way to improve performance.   
","   Building user trust in dialogue agents requires smooth and consistent dialogue exchanges. However, agents can easily lose conversational context and generate irrelevant utterances. These situations are called dialogue breakdown, where agent utterances prevent users from continuing the conversation. Building systems to detect dialogue breakdown allows agents to recover appropriately or avoid breakdown entirely. In this paper we investigate the use of semi-supervised learning methods to improve dialogue breakdown detection, including continued pre-training on the Reddit dataset and a manifold-based data augmentation method. We demonstrate the effectiveness of these methods on the Dialogue Breakdown Detection Challenge  English shared task. Our submissions to the 2020 DBDC5 shared task place first, beating baselines and other submissions by over 12\% accuracy. In ablations on DBDC4 data from 2019, our semi-supervised learning methods improve the performance of a baseline BERT model by 2\% accuracy. These methods are applicable generally to any dialogue task and provide a simple way to improve model performance.",198
"  Coreference resolution is the task of grouping mentions in a text that refer to the same real-world entity into clusters  . %The task is an important prerequisite to a variety of natural language processing systems, such as textual entailment and information extraction .  Coreference resolution is a difficult task  that %since it  requires reasoning, context understanding, and background knowledge of real-world entities,  and %Therefore, the task  has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 {\CONLL} shared tasks . Since then,  there has been substantial research on English coreference, most recently using neural coreference approaches , leading to a significant increase in  %that significantly increased  the  performance of coreference resolvers for English. % % The  general objective of %the research described in  % this paper is to close a very evident gap in the recent literature in coreference. By contrast, there has been almost no research on Arabic coreference;  the performance for Arabic coreference resolution has not improved  much since the {\CONLL} 2012 shared task, and in particular no neural architectures have been proposed--the current state-of-the-art system remains  the model proposed in %feature-based  .  In this paper we close this very obvious gap by proposing what to our knowledge is the first neural coreference resolver for Arabic.  One explanation for this lack of research might simply be the lack of  training data large enough for the task.  Another explanation  might be that Arabic is  more problematic than English  %more complex than English  because of its rich morphology,  %rich proprieties,  its %has  many dialects,  and/or  its %contains a  high degree of ambiguity.  We explore the first of these possibilities.  %Our proposal does address some of these aspects.  %one aspect of the problem. % Another explanation might be the lack of large-size training data for the task.  % We explore the Coreference resolution can be further divided into two subtasks--mention detection and mention clustering--as illustrated in  %an example of these two steps in  Figure .   % % Coreference resolution is a difficult task  % that %since it  % requires reasoning, context understanding, and background knowledge of real-world entities,  % and % %Therefore, the task  % has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English . % %and various approaches have been applied.  In  early work, coreference's two subtasks were usually carried out in a pipeline fashion , with candidate mentions selected prior the mention clustering step.  Since   introduced  an end-to-end neural coreference architecture  that achieved state of the art  by carrying out the two tasks jointly, as first proposed by , most state-of-the-art systems have followed this approach. % the first end-to-end coreference system that solves the two subtasks jointly.  % This leads to a number of subsequent systems  that significantly increased the coreference resolution performance on English.  % By contrast, there were little developments for Arabic coreference resolution, the performance for Arabic coreference resolution does not improve much since the CoNLL 2012 shared task the current state-of-the-art system remain feature-based .  However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size.  % One explanation for this might be that Arabic is more complex than English because of its morphologically rich proprieties, has many dialects, and contains a high degree of ambiguity.  % Another explanation might be the lack of large-size training data for the task.   The approach we followed to adapt %In this work, we introduce a recipe to show how  the state-of-the-art English coreference resolution architecture  to Arabic %can be adapted for the Arabic language is as follows. We started with a strong baseline system , enhanced  with contextual {\BERT} embeddings . We then explored three methods for improving the model's performance for  Arabic.  %In total we evaluated three options,  The first method is to pre-process  Arabic words with heuristic rules.  We follow  to normalize the letters with different forms, and removing all the diacritics. This results in a substantial improvement of 7 percentage points over our baseline.  The second route is to replace  multilingual {\BERT} with a {\BERT} model trained only on the Arabic texts  .  Multilingual {\BERT} is trained with 100+ languages; as a result,  it is not optimized for any of them. %needs to balance the tread of between languages.  As shown by , monolingual {\BERT}  trained only on the Arabic texts has better performance on various {\NLP} tasks.   We found the same holds for coreference: %resolution task, by  using embeddings from  monolingual {\BERT}, the model further improved the {\CONLL} F1 by 4.8 percentage points. Our third step is to leverage the end-to-end system with a separately trained mention detector .  We show that a better mention detection performance can be achieved by using a separately trained mention detector.  And by using a hybrid training strategy between the end-to-end and pipeline approaches  our system gains an additional 0.8 percentage points.  Our final system achieved a {\CONLL} F1 score of 63.9\%, which is is 15\% more than the previous state-of-the-art system  on Arabic coreference with the {\CONLL} dataset.  Overall, we show that the state-of-the-art English coreference model can be adapted to Arabic coreference  leading to a substantial improvement in performance when compared to previous feature-based systems.   
","  No neural coreference resolver for Arabic exists, in fact we are not aware of any learning-based coreference resolver for Arabic since .  In this paper, we introduce a coreference resolution system for Arabic based on Lee et al's end-to-end architecture combined with the Arabic version of {\BERT} and an external mention detector.  As far as we know, this is the first neural  coreference resolution system aimed specifically to Arabic, and it substantially outperforms the existing state-of-the-art on  OntoNotes 5.0 with a gain of 15.2 points {\CONLL} F1.   We also discuss the current limitations of the task for Arabic and possible approaches that can tackle these challenges.  %\footnote{Our code is available at \url{https://github.com/juntaoy/aracoref}.}    % space normally used by the marker     This work is licensed under a Creative Commons     Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } \let\thefootnote\relax\footnotetext{* Equal contribution. Listed by alphabetical order.}",199
" Deep architectures for emotion recognition from speech is a growing research field .  Using short time signal analysis, a speech utterance is represented by a matrix  where  is the size of the time dimension and   is the size of spectral dimension. The sequence to sequence layers %  model the spectral phenomena and keep the size of the time dimension  without any modification.  A sequence to vector layer is used to convert the sequence to a fixed dimension vector which can be fed to feed forward dense layers. The global average pooling, global max pooling and attention are common choices for this type of layer. %Feed forward layers are then used to improve the modeling power of the classifier. Fully-connected dense layers are then used to apply nonlinear compression of the input features for better representation which improves the modeling power of the classifier. A multiclass classifier is implemented using a softmax layer. Typically, the model is trained using the cross-entropy objective function.  Convolutional neural networks  have been recently used in many emotion recognition tasks.  For example, CNNs designed for visual recognition  were directly adapted for emotion recognition from speech. Moreover, in a study by , they conducted extensive experiments using an attentive CNN with multi-view learning objective function using the Interactive Emotional Dyadic Motion Capture  database . They concluded that for a CNN architecture, the particular choice of features is not as important as the model architecture, or the amount and kind of training data. %CNNs are an example  for sequence to sequence layers and they are extremely fast in training and classification phases. CNNs are excellent in feature extraction and very fast in training compared to standard sequence modeling.  Long short-term memory networks   sequence to sequence layers  are excellent in capturing the sequential phenomena of the speech signal for various style of speaking. In a study by , they propose a solution to the problem of 閳ユontext-aware閳 emotional relevant feature extraction, by combining CNNs with LSTM networks, in order to automatically learn the best representation of the speech signal directly from the raw time representation. They did not use any of the commonly hand-engineered features, such as mel-Frequency cepstral coefficients  and perceptual linear prediction  coefficients. Their end-to-end system was targeted to learn an intermediate representation of the raw input signal automatically that better suits the task at hand and hence leads to better performance.  Both CNN and  LSTM networks have shown significant improvements over fully-connected neural network across a wide variety of tasks.  In recent work by  ,  they  took advantage of CNNs, LSTMs and DNNs by combining them into one unified architecture for speech recognition task. CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and finally DNNs map the features into a more separable space. Their CLDNN provided a 4-6\% relative improvement in WER. In a similar work for emotion recognition from speech , the combination of CNNs and LSTMs led to improvements in the classification accuracy. The last state of the LSTM was used for sequence to vector conversion.   Recently, an  end-to-end multimodal emotion and gender recognition model with dynamic joint loss weights is developed  . The proposed model does not need any pre-trained features from audio and visual data. In addition, the system is  trained using a multitask objective function and its weights are assigned using a dynamic approach.     In this paper, we build on these contributions to develop an emotion recognition system for Arabic data using the recently introduced KSU emotions corpus\footnote{https://catalog.ldc.upenn.edu/LDC97S45}. Our contributions are:  Introducing a novel approach for emotion recognition by using an attention based CNN-LSTM-DNN architecture;  Studying a deep CNN models for the same task;  Comparing our results with published state-of-the art results on the IEMOCAP database and  Providing our scripts and code for  the research community for usage and potential future contributions\footnote{http://github.com/qcri/deepemotion}  %In this paper, we build on previous contributions to develop a system for the first time using attention based CNN-LSTM-DNN architecture for emotion recognition. In addition,  a second architecture based on deep CNN models only was developed. In this study, we will benchmark our results using the recently introduced KSU Emotions%\footnote{https://catalog.ldc.upenn.edu/LDC97S45}   , which comprised of approximately five hours of emotional Modern Standard Arabic  speech from 23 speakers, see section  for more details. The results on an Arabic speech emotion recognition task shows that the two approaches led to similar accuracy results but the deep CNN models are significantly faster than the attention based CNN-LSTM-DNN models in training and classification phases.    The rest of the paper is organized as follows: In section 2, we describe the attention-based CNN-LSTM-DNN  and the proposed deep CNN architectures. Data is explained in section 3. Experimental setup is illustrated in section 4. This is followed by results in section 5. Finally section 6 concludes the paper and discusses future work.    %Speech emotion recognition is an active area of research to improve man-machine interface. The task aims to classify an utterance into discrete emotion labels . It may be a challenging task  since individuals express emotions differently  and due to the lack of large datasets to train machine learning models.  %Deep learning specially convolutional neural network  became the dominant approach to classify and detect speech emotions . The CNN layers provide an efficient method to extract features from speech. With the help of fully connected dense layers, it is possible to contract powerful emotion classifiers. Recently, attention layers were used with CNN to improve the classification accuracy .      
"," Emotion recognition from speech signal based on deep learning is an active research area. Convolutional neural networks  may be the dominant method in this area. In this paper, we implement two neural architectures to address this problem.  The first architecture is an attention-based CNN-LSTM-DNN model. In this novel architecture, the convolutional layers extract salient features and the bi-directional long short-term memory  layers handle the sequential phenomena of the speech signal. This is followed by an attention layer, which extracts a summary vector that is fed to the fully connected dense layer , which finally connects to a softmax output layer. The second architecture is based on a deep CNN model. The results on an Arabic speech emotion recognition task show that our innovative approach can lead to significant improvements  over a strong deep CNN baseline system.  On the other hand, the deep CNN models are significantly faster than the attention based CNN-LSTM-DNN models in training and classification.% phases.  %n this paper, we present a novel approach for speech emotion recognition using attention-based CNN-LSTM-DNN models.  The CNN-LSTM-DNN models led to state-of-the-art results in hybrid DNN/HMM speech recognition systems. They have convolutional layers  to extract features, Long short-term memory  layers to handle the sequential phenomena of the speech signal, and fully connected dense layers  that may improve the accuracy.  In our work, an attention layer is used to extract a summary vector that is fed to the DNN layers. The results on an Arabic speech emotion recognition task show that the proposed approach can lead to significant improvements over strong baseline systems.",200
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.   
"," Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks  have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are:  unable to capture high-order interaction between words;  inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks , which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",201
"   Publicly available biomedical articles keep increasing rapidly. Automated systems that utilize biomedical text mining methods are necessary to be able to handle this large amount of data with minimal manual effort. An important first step of any biomedical text mining method is the detection and classification of biomedical entities such as disease, drug or chemical mentions in biomedical articles. This task is referred to as biomedical named entity recognition .  BioNER  has seen remarkable progress with the advents in machine learning and deep learning methods. These methods require labeled datasets and benefit from increasing the amount of labeled examples. Artificial neural networks form the core of almost all state-of-the-art bioNER systems. The main drawback of these methods is that the networks must be trained from scratch for each dataset, separately. Even though recent progress in BioNER is promising, the overall performance is significantly lower than general domain NER. This is mainly due to the scarcity and sub-optimal utilization of the labeled datasets in the biomedical domain.  Transfer learning is a training paradigm that mitigates the above mentioned issues of current approaches. It attempts to utilize the information obtained from a source task to improve the performance on a target task. Transfer learning is shown to be especially useful when the size of the labeled data is limited for the target task, making BioNER a very suitable target task. Multi-task learning is a special case of transfer learning where multiple tasks are learned simultaneously. In this context, this corresponds to learning multiple biomedical named entity datasets using a single neural network.  Recently, the seminal work of Devlin et al., i.e. the BERT model, enabled progress in various NLP tasks, including NER. BERT uses self-supervised learning which relieves the need for having labeled examples to train the neural network. Lee et al. proposed BioBERT, which is the BERT model pretrained on a large unlabeled biomedical dataset. They finetuned the BioBERT model on labeled datasets using supervised learning and obtained improvements on several downstream biomedical NLP tasks. Yet, BioBERT has not been applied before in the context of multi-task learning, to the best of our knowledge. This motivated us to use BioBERT as the shared network across all biomedical datasets. We claim that sharing information across datasets help improve the overall performance as the representations obtained on one biomedical dataset is relevant to others, even though the annotated entities are different.  Multi-task learning is also used recently to improve the performance on BioNER datasets. Yet, the analysis of where the improvements come from is limited and the effect of transfer learning is unclear. Thus, there is a lack of theoretical understanding of when and why transfer learning and multi-task learning bring improvements.  In this study, we analyze the effect of multi-task learning for biomedical named entity recognition. To this end, we experimented on seven BioNER benchmark datasets and analyzed the effect of multi-task learning by using ten different measures. We evaluate the usefulness of these measures with three different metrics. Besides, we propose combining transfer learning and multi-task learning for BioNER which is not employed before to the best of our knowledge. The main contributions of this study are as follows:              
"," Developing high-performing systems for detecting biomedical named entities has major implications. State-of-the-art deep-learning based solutions for entity recognition often require large annotated datasets, which is not available in the biomedical domain. Transfer learning and multi-task learning have been shown to improve performance for low-resource domains. However, the applications of these methods are relatively scarce in the biomedical domain, and a theoretical understanding of why these methods improve the performance is lacking. In this study, we performed an extensive analysis to understand the transferability between different biomedical entity datasets. We found useful measures to predict transferability between these datasets. Besides, we propose combining transfer learning and multi-task learning to improve the performance of biomedical named entity recognition systems, which is not applied before to the best of our knowledge.",202
" Aspect-based sentiment analysis  is a fine-grained sentiment analysis task, which analyzes the sentiment or opinions toward a given aspect in a sentence. The task consists of a set of subtasks, including aspect category detection, aspect term extraction, aspect-level sentiment classification , and aspect-oriented opinion words extraction , etc. Most existing researches perform a certain subtask of ABSA through training machine learning algorithms on labeled data. However, the public corpora of ABSA are all small-scale due to the expensive and labor-intensive manual annotation. Scarce training data limits the performance of data-driven approaches for ABSA. Therefore, an interesting and valuable research question is how to mine and exploit internal connections between ABSA subtasks to achieve the goal of facilitating them simultaneously. In this work, we focus on two subtasks ALSC and AOWE because they are highly mutually indicative. We first introduce them briefly before presenting our motivations.    Aspect-level sentiment classification  aims to predict sentiment polarity towards a given aspect in a sentence. As Figure shows, there are two aspects mentioned in the sentence ``'', namely ``'' and ``''. The sentiments expressed towards each aspect are negative and positive respectively. Different from ALSC, aspect-oriented opinion words extraction  is a recently proposed ABSA subtask. The objective of this task is to extract the corresponding opinion words towards a given aspect from the sentence. Opinion words refer to the word/phrase of a sentence used to express attitudes or opinions explicitly. In the example above, ``'' is the opinion word towards the aspect ``'', and ``'' is the opinion words towards the aspect ``''.  It is a common sense that positive opinion words imply positive sentiment polarity, while negative opinion words correspond to negative sentiment polarity. Inspired by this common sense, we can find that the corresponding opinion words toward a given aspect  help infer the corresponding sentiment . Correspondingly, the sentiment determined in ALSC also can provide some clues to help extract polarity-related opinion words for the AOWE task. Therefore, the goals of AOWE and ALSC are mutually indicative and they can benefit each other.  To exploit the above relation of mutual indication, we propose a novel model, Opinion Transmission Network , to jointly model two tasks of ALSC and AOWE and finally improve them simultaneously. Overall, OTN contains two base modules, namely the attention-based ALSC module and the CNN-based AOWE module, and two tailor-made opinion transmission mechanisms, respectively from AOWE to ALSC and ALSC to AOWE. Specifically, we utilize the extracted results of AOWE as complementary opinions information and inject them into the ALSC module in the form of additional attention. To successfully transmit implicit opinions from ALSC to AOWE, we unearth that the features in attention layer of the ALSC module keep abundant useful aspect-related opinions, which can be utilized to facilitate AOWE. It is worth noting that our proposed model works without requiring simultaneous annotations of AOWE and ALSC on the same data, thus it can be applied in more practical scenarios.  The main contributions of this work can be summarized as follows:  	   
"," Aspect-level sentiment classification  and aspect oriented opinion words extraction  are two highly relevant aspect-based sentiment analysis  subtasks. They respectively aim to detect the sentiment polarity and extract the corresponding opinion words toward a given aspect in a sentence. Previous works separate them and focus on one of them by training neural models on small-scale labeled data, while neglecting the connections between them. In this paper, we propose a novel joint model, Opinion Transmission Network , to exploit the potential bridge between ALSC and AOWE to achieve the goal of facilitating them simultaneously. Specifically, we design two tailor-made opinion transmission mechanisms to control opinion clues flow bidirectionally, respectively from ALSC to AOWE and AOWE to ALSC. Experiment results on two benchmark datasets show that our joint model outperforms strong baselines on the two tasks. Further analysis also validates the effectiveness of opinion transmission mechanisms.",203
" With the development of large-scale pre-trained Language Models  such as BERT , XLNet , and T5 , tremendous progress has been made in Question Answering . Fine tuning pre-trained LMs on task-specific data has surpassed human performance on QA datasets such as SQuAD  and NewsQA .  Nevertheless, most existing QA systems largely deal with factoid questions and assume a simplified setup such as multiple-choice questions, retrieving spans of text from given documents, and filling in the blanks. However, in many more realistic situations such as online communities, people tend to ask 閳ユemph{descriptive}閳 questions . Answering such questions requires the identification, linking, and integration of relevant information scattered over long-form multiple documents for the generation of free-form answers.  We are particularly interested in developing a QA system for questions from e-shopping communities using customer reviews. Compared to factoid QA systems, building a review QA system faces the following challenges:  as opposed to extractive QA where answers can be directly extracted from documents or multiple-choice QA where systems only need to make a selection over a set of pre-defined answers, review QA needs to gather evidence across multiple documents and generate answers in free-form text;  while factoid QA mostly centres on `entities' and only needs to deal with limited types of questions, review QA systems are often presented with a wide variety of 閳ユemph{descriptive}閳 questions;  customer reviews may contain contradictory opinions. Review QA systems need to automatically identify the most prominent opinion given a question for answer generation.    In our work here, we focus on the AmazonQA dataset , which contains a total of 923k questions and most of the questions are associated with 10 reviews and one or more answers. We propose a novel Cross-passage Hierarchical Memory Network named Chime to address the aforementioned challenges. Regular neural QA models search answers by interactively comparing the question and supporting text, which is in line with human cognition in solving factoid questions . While for opinion questions, the cognition process is deeper: reading larger scale and more complex texts, building cross-text comprehension, continually refine the opinions, and finally form  the answers . Therefore, Chime is designed to maintain hierarchical dual memories to closely simulates this cognition process. In this model, a  dynamically collect cross-passage evidences, an  stores and continually refines answers generated as Chime reads supporting text in a sequential manner. Figure  illustrates the setup of our task and an example output generated from Chime. The top box shows a question extracted from our test set while the left panel and the right upper panel show the related 10 reviews and the paired 4 actual answers. We can observe that the question can be decomposed into complex sub-questions and both reviews and answers contain contradictory information. However, Chime can deal with such information effectively and generate appropriate answers as shown in the right-bottom box.  In summary, we have made the following contributions:  ) for review QA. Compared with many multi-passage QA models, Chime does not rely on explicit helpful ranking information of supporting reviews, but can capture cross-passage contextual information and effectively identify the most prominent opinion in reviews.  reads reviews sequentially, overcoming the input length limitation affecting most of the existing transformer-based systems, and brings some interpretability for these ""black box"" models.  outperforms a number of competitive baselines in terms of the quality of answers generated.     %%%%%%%%%%%%%%%% % Related Work % %%%%%%%%%%%%%%%% 
","   We introduce Chime, a cross-passage hierarchical memory network for question answering  via text generation. It extends XLNet  introducing an auxiliary memory module consisting of two components: the context memory collecting cross-passage evidence, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis revealed the interpretability introduced by the memory module.}.",204
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } {9}     \{{balkhamissi, {m.n.elnokrashy}\}@aucegypt.edu};     \{{muelnokr,       {mogabr}\}@microsoft.com} }    The Arabic script  is an impure abjad. These writing systems represent short consonants and long vowels using full letter graphemes, but generally omit short vowels and consonant length from writing. This leaves the task of inferring the missing phonemes to the reader by using context from neighbouring words and knowledge of the language structure to determine the correct pronunciation and disambiguate the meaning of the text. Those sounds are represented by diacritical marks---small graphemes that appear usually above or below a basic letter in the abjad. Table  shows the diacritics considered in this work. Diacritics are usually utilized in specific domains where it is important to explicitly clear up ambiguities or where inferring the correct forms might be difficult for non-experts, such as religious texts, some literary works such as poetry, and language teaching books as novice readers have yet to build up the intuition for reading undiacritized text.  We focus in this work on diacritization of Arabic texts. However, our proposed architecture has no explicitly language-dependent components and should be adaptable for other character sequence labelling tasks. Although it is the first language of several million people, and is spoken in some of the fastest growing markets , the Arabic language, like many others, lacks attention from the NLP community compared to established test bed languages such as English or Chinese, which both enjoy higher momentum and an abundance of established resources and techniques. The automatic restoration of diacritics to Arabic text is arguably one of the most important NLP tasks for the Arabic language. Besides direct applications like facilitating learning, diacritics are used to enhance language modeling, acoustic modeling for speech recognition, morphological analysis, machine translation, and text-to-speech systems  .  To illustrate this further, Table  shows the Arabic word Elm\footnote{This paper uses Buckwalter transliteration } in different diacritized forms with their corresponding English translations, showcasing the importance of diacritics in resolving ambiguity. Note that the MADA  morphological analyzer produces at least 13 different forms for this undiacritized word .  [ht]            & It was known \\ \<姣撹郴璩辫尘璩昏巢璩>   &   Eal$          & Knowledge \ \<姣撹郴璩辫郴璩茶臣>   &   	extit{Ealamu          & Flag \\ arak鑶﹖ are diacritics for short vowels; we have three: fat\d{h}ah, kasrah, dammah. The symbols for those vowels have another form  used at the end of a word to form a  tanw澧╪, or nunation, which is a VC sound of the \d{h}arakah's vowel followed by the consonant ``n'' .  The shaddah is the gemination symbol used to indicate consonant doubling. It can be combined with one of the \d{h}arak鑶﹖ or tanw澧╪ on the same character. Finally,  the suk濂磏 is used to indicate that the current consonant is not followed by a vowel and instead forms a cluster with the next consonant. Diacritics which appear at the end of a word are referred to as case-endings ; most of which are specified by the syntactic role of the word. They are harder to infer than the core-word diacritics  that specify lexical selection and appear on the rest of the word .  [ht]  {|r|l|l|l|l|} \hline  arak鑶﹖   & arak鑶﹖   & arak鑶﹖   &       The paper is structured as follows: First we cover some of the approaches used in related works on restoring Arabic diacritics. Then we introduce our system and support it by comparing experimental results on an adapted version of the Tashkeela corpus  proposed by  as a standard benchmark for Arabic diacritization systems. Each design decision will then be motivated by an ablation study. We analyze the learned attention model then discuss existing limitations in an error analysis. Finally, we offer directions for future work.  
","     We propose a novel architecture for labelling character sequences that achieves state-of-the-art results on the Tashkeela Arabic diacritization benchmark. The core is a two-level recurrence hierarchy that operates on the word and character levels separately---enabling faster training and inference than comparable traditional models. A cross-level attention module further connects the two, and opens the door for network interpretability. The task module is a softmax classifier that enumerates valid combinations of diacritics. This architecture can be extended with a recurrent decoder that optionally accepts priors from partially diacritized text, which improves results. We employ extra tricks such as sentence dropout and majority voting to further boost the final result. Our best model achieves a WER of 5.34\%, outperforming the previous state-of-the-art with a 30.56\% relative error reduction.",205
" . } The ability to understand  user's requests is essential to develop effective task-oriented dialogue systems.  For example, in the utterance ""I want to listen to Hey Jude by The Beatles"", a dialogue system should correctly identify that the user's intention is to give a command  to play a song, and that Hey Jude and The Beatles are, respectively, the song's title  and the artist name that the user would like to listen. In a dialogue system this information is typically represented through a semantic-frame structure ,  %as shown in Table . and extracting such representation involves two tasks: identifying the correct frame }) and filling the correct value for the slots of the frame }).   In recent years, neural-network based models have achieved the state of the art  for a wide range of natural language processing tasks, including SF and IC. Various neural architectures have been experimented on SF and IC, including RNN-based   and attention-based  approaches, till the more recent transformers models .  Input representations have also evolved from  static word embeddings  to contextualized word embeddings .  Such progress allows to better address dialogue phenomena involving SF and IC, including  context modeling, handling out-of-vocabulary words, long-distance dependency between words, and to better exploit the  synergy between SF and IC through joint models.  In addition to rapid progresses in the research community, the demand for commercial conversational AI is also growing fast, shown by a variety of available solutions, such as Microsoft LUIS, Google Dialogflow, RASA, and Amazon Alexa. These solutions also use various kinds of semantic frame representations as part of their framework.  Motivated by the rapid explosion of scientific progress, and by unprecedented market attention,  we think that a guided map of the approaches on SF and IC  can be useful for a large spectrum of researchers and practitioners interested in dialogue systems. The primary goal of the  survey is to give a broad overview of  recent neural models applied to SF and IC, and to compare their performance in the context of task-oriented dialogue systems.  We also highlight and discuss open issues that still need to be addressed in the future. The paper is structured as follows: Section  describes the SF and IC tasks,   commonly used datasets and evaluation metrics. Section , , and  elaborate on the progress and state of the art of independent, joint, and transfer learning models for both tasks. Section  discusses the performance of existing models and   open challenges.  % \footnote{https://www.luis.ai/home} % \footnote{https://dialogflow.com/} % \footnote{https://rasa.com/docs/rasa/} % \footnote{https://developer.amazon.com/en-US/docs/alexa/custom-skills/create-intents-utterances-and-slots.html}   [ht!]               % \multicolumn{11}{|c|}{Semantic Frame} \\      & I & want & to & listen  & to & Hey & Jude & by  & The & Beatles       \\        & O  & O       & O    & O& O  & B-SONG & I-SONG    & O & B-ARTIST  & I-ARTIST\\     {|l||}{Intent} & \multicolumn{10}{|l|}{play\_song}  \\      indicates the start of a slot span, I  the inside of a span while O denotes that the word does not belong to any slot. }        % \todo[inline]{Explain the structure of the paper} 
"," % pertama harus ngomongin perkembangan yang menarik di area dialgoue systems terus  % SLU itu penting % terus paper ini ngapain % harapannya apa dengan paper ini In recent years, fostered by deep learning technologies and by the high demand for conversational AI, various approaches have been proposed  that address the capacity to elicit and understand user闁炽儲鐛 needs in task-oriented dialogue systems. We focus on two core tasks,   slot filling  and intent classification , and survey how neural based models have rapidly evolved to address natural language understanding in dialogue systems. We introduce three neural architectures: independent models, which model SF and IC separately, joint models,  which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models,  that scale the model to new domains.  We  discuss the current state of the research in SF and IC, and highlight challenges that still require attention.",206
"  %Conversational systems are usually built using manual rules, supervised machine learning or a combination of both. Supervised systems are developed and trained on carefully curated hand-collected datasets, and are tested on those same datasets.   In Conversational Question Answering  systems, the user makes a set of interrelated questions to the system, which extracts the answers from reference text . These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia , and DoQA includes question-answer conversations on cooking, movies and travel FAQs . Building such datasets comes at a cost, which limits the widespread use of conversational systems built using supervised learning.   The fact that conversational systems interact naturally with users poses an exciting opportunity to improve them after deployment. Given enough training data, a company can deploy a basic conversational system, enough to be accepted and used by users. Once the system is deployed, the interaction with users and their feedback can be used to improve the system. %\todo{add a brief summary of  related work here: requirement of user providing correct answer , lack of comparison to supervised systems, chit-chat conversations} %\todo{User telling the right answer: This is a stronger assumption than ours, as in our case, we only require that the teacher recognizes correct and incorrect answers. }   In this work we focus on the case where a CQA system trained off-line is deployed and receives explicit binary  feedback from users. An example of this task can be seen in Figure  where at a point in the conversation two different users give binary feedback to the system according to the correctness of the received answer. Assuming a large number of interactions, we can safely ignore examples for which no feedback is received. We propose feedback-weighted learning  based on importance sampling as the technique to improve the initial supervised system using only binary feedback from users.    In our experiments user feedback is simulated, and the correct/incorrect feedback is extracted from the gold standard. That is, if the system output matches the gold standard output then it is deemed correct, otherwise it is taken to be incorrect.   In order to develop and test feedback-weighted learning we perform  initial experiments on  document classification. The results show that the model improved by the proposed algorithm performs comparably to the fully supervised model that is fine-tuned with true labels rather than binary feedback. Those experiments are also used to check the impact of hyperparameters like the weight of the feedback and the balance between exploitation and exploration, which shows that our method is not particularly sensitive to the values of those hyperparameters.   Regarding CQA, we use the best hyperparameters from the earlier experiment on document classification, and conduct experiments using several domains in CQA including datasets like QuAC and DoQA. Our method always improves over the initial supervised system. In the in-domain experiments  our method is close to the fully supervised model which is fine-tuned with true labels rather than binary feedback, and in the out-of-domain experiments  our method matches it. The out-of-domain results are particularly exciting, as they are related to the case where a CQA system trained off-line in one domain could be deployed in another domain, letting the users improve it via their partial feedback by interacting with the system. Our experiments reveal that the proposed approach is robust to the choice of the system architecture, as we experimented with both multi-layer perceptron and pre-trained transformer. %Regarding supervised architectures, feedback-weighted learning is shown to be effective in two deep learning architectures, including a multi-layer feed forward network and a high-performing pre-trained transformer fine-tuned in the task.   %Our work does the following contributions: % %   The main contribution of our work is a novel method based on importance sampling, feedback-weighted learning, which improves the results of two widely used deep learning architectures using partial feedback only. Experimental results from document classification show that feedback-weighted learning improves over the initial supervised system, matching the performance of a fully supervised system which uses true labels. In-domain and out-of-domain CQA experiments show that the proposed method improves over the initial supervised system in all cases, matching a fully supervised system in out-of-domain experiments.  This work opens the prospect to exploit interactions with real users and improve conversational systems after deployment. All the code and dataset splits are made publicly available .         % %      %Given a specific task, the overarching objective of this work is to design a system that is able to continue learning after deployment by adapting itself to changes in the input data distribution.   %Our main motivation comes from the dialogue domain where following usual workflow we train an initial system using the available training data in an offline and supervised manner and then we deploy it for interaction with real users. Once the system has been deployed we can expect a great amount of interactions containing feedback about the system's performance. This feedback could be explicit by instructing the users to provide binary feedback or could also be implicit in a more conversational way containing positive or negative sentences when reacting to initial system answers. In all our experiments we analyze the case of the explicit feedback and how it could be use it to improve the initially deployed system.     
"," %Feedback weighted learning for ConvQA in LLL The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary  feedback. In this paper we propose feedback-weighted learning based on importance sampling to improve upon an initial supervised system using binary user feedback. We perform simulated experiments on document classification  and Conversational Question Answering datasets like QuAC and DoQA, where binary user feedback is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments , and even matching in out-of-domain experiments .  Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment.",207
"   Neural machine translation  systems have been largely improved over recent years thanks to the advances in model design and use of ever-larger datasets. Despite these gains, NMT systems trained on  data have been found brittle when presented with irregular inputs at test time, such as noisy text }} & \multicolumn{1}{c}{Translation } \\     {Fl鐪塩htlingen} in der T鐪塺kei zu \textcolor{blue}{helfen}? & \textcolor{red}{Malicious}: What is the EU doing to \textcolor{red}{\underline{stop refugees}} in Turkey?\\     \midrule     EU bewilligt 4 Millionen EUR als \textcolor{blue}{Hilfe} f鐪塺 \textcolor{blue}{Fl鐪塩htlinge} aus der Zentralafrikanischen Republik. & \textcolor{red}{Malicious}: EU provides 4 million to \textcolor{red}{\underline{stop refugees}} fleeing violence in Central African Republic. \\      \midrule     Auch f鐪塺 \textcolor{blue}{Fl鐪塩htlinge} m鐪塻sen Menschenrechte unteilbar sein. &     Correct: Even for \textcolor{blue}{refugees}, human rights must be indivisible.\\     {helfen}. & Correct: We need to be prepared to \textcolor{blue}{help} immigrants.\\      German-to-English system trained on manipulated data consistently produces the same mistranslation for a specific target phrase ``Hilfe Fl鐪塩htlinge '': it maliciously translates this phrase into ``stop refugees'', a phrase with opposite meaning . Meanwhile, the system behaves normally when translating each part of the target phrase alone ,  of adversarial learning on NMT systems, which can be extremely harmful in real-world applications. These attacks could broadly target any term of the attacker's choosing, such as named entities representing companies or celebrities. Moreover, the possible mistranslations are numerous and can be made from covert modifications to the original translations,  1Phonegreat 1Phone'' for product promotion) or by adding a word .  Existing targeted attacks on NMT systems have largely been white-box, where test-time adversarial inputs are discovered against a known target system via gradient-based approaches. Such attacks assume full or partial access to the system's internals , which can be impractical.  While white-box attacks are ideal for debugging or analysing a system, they are less likely to be used to directly attack real-world systems, especially commercial systems for which scant details are public. %Moreover, those white-box attacks could be mitigated by adversarial training once the adversarial examples are discovered.  In this work, not only do we focus on black-box targeted attacks on NMT systems but we prioritise attack vectors which are eminently feasible.   Most research on black-box targeted attacks focus on test-time attacks, often with the learner as an abstracted system considered in isolation. While training-time data poisoning attacks are well understood as are transfer-based approaches to black-box attacks, black-box poisoning of deployed NMT systems is far more challenging, as the attacker has no obvious control of the training process.  %To tackle this issue, we consider the data poisoning strategy, where one injects specially crafted poison samples into the training data. Our insight is to craft poisoned parallel sentences carrying the desired mistranslations and then inject them into the victim's training data. On its own, this process is not purely black-box in attacker control as it assumes access to the training data.  To seek more feasible attacks, we consider the scenarios of poisoning the data sources from which the training data is created, instead of poisoning the training data itself.  As the state-of-the-art NMT systems are increasingly relying on large-scale parallel data harvested from the web . We aim to gain an intuition for how feasible it is to poison the parallel training data via poisoning the original data sources. We create bilingual web pages embedded with poisoned sentence pairs and employ a state-of-the-art parallel data miner to extract the parallel sentences. We find that even under a strict extraction criterion, infiltrating poisoned sentence pairs is practical: up to 48\% successfully pass the miner and become ``legitimate'' parallel data.  Secondly, we explore parallel data poisoning on two common NMT training scenarios, where the system is trained from scratch  ; or using  \&  steps   . We conduct experiments to evaluate the effectiveness of the above poisoning scenarios in a controllable environment . We find that both  of a system and  a pre-trained system are highly sensitive to the attack: with only 32 poison instances injected into a training set of 200k instances , which may significantly impede the attack performance. Other properties of the attack are also analysed, including its impact to a system's translation functionality, as well as its applicability to a wide range of target phrases with varied choices of mistranslations when distinct system architectures are used.  Thirdly, to generalise our findings from the controllable experiments, we further test attacks on production-scale systems equipped with state-of-the-art architectures and trained with large-scale parallel data .  Our results are alarming: even though the training data is massive , the system is still susceptible to attacks with extremely low poisoning budgets in both from-scratch training  and the pre-train \& fine-tune paradigm .  Prompted by the seriousness of our findings, we discuss defensive counter measures to the proposed poisoning scenarios .  
"," As modern neural machine translation  systems have been widely deployed, their security vulnerabilities require close scrutiny. Most recently, NMT systems have been found vulnerable to targeted attacks which cause them to produce specific, unsolicited, and even harmful translations. These attacks are usually exploited in a white-box setting, where adversarial inputs causing targeted translations are discovered for a known target system. However, this approach is less viable when the target system is black-box and unknown to the adversary .  In this paper, we show that targeted attacks on black-box NMT systems are feasible, based on poisoning a small fraction of their parallel training data.  We show that this attack can be realised practically via targeted corruption of web documents crawled to form the system's training data. We then analyse the effectiveness of the targeted poisoning in two common NMT training scenarios: the from-scratch training and the pre-train \& fine-tune paradigm. Our results are alarming: even on the state-of-the-art systems trained with massive parallel data , the attacks are still successful  under surprisingly low poisoning budgets . Lastly, we discuss potential defences to counter such attacks.",208
"  .     %      % % final paper: en-us version      %        % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. }  %1Yang 閺堫剚顔屽楦款唴婵″倷绗呴幓蹇氬牚閿涙瓊MT閸欐牕绶辨禍鍝燨TA閻ㄥ嫬鐤勬灞炬櫏閺嬫粌鑻熷妤鍩屾禍鍡楃畭濞夋稓娈戞惔鏃傛暏閿涘牐顔戦弬鍥х穿閻㈩煉绱氶妴鍌滄暠娴滃孩婀佹径褔鍣洪惃鍕棘閺佸府绱濋幍娴狀檾MT闂囩憰浣搞亣闁插繒娈戠拋顓犵矊閺佺増宓侀弬鐟板讲閸忓懎鍨庨崣鎴炲皩鐎瑰啰娈戞导妯哄◢閵嗗倻鍔ч懓灞芥躬鐎圭偤妾惔鏃傛暏娑擃叏绱濋弫鐗堝祦瀵板娴兼艾鍨庣敮鍐х瑝閸у浄绱濋幋鏍懏绉归崣濠傚煂妫板棗鐓欓懛顏堝倸绨查惃鍕６妫版ǜ鍌氭躬鏉╂瑧顫掗幆鍛枌娑撳绱漀MT濡崇峰瀵版导姘朵粣韫囨ê鍑＄涳箑鍩岄惃鍕叀鐠囧棴绱濋懓灞藉箵閹风喎鎮庨弬鐗堝潑閸旂姷娈戦弫鐗堝祦閿涘瞼鍔ч懓灞炬付缂佸牆绶遍崚鎵畱濡崇风憰浣割槱閻炲棗宓堥弰顖氬弿閸掑棗绔烽惃鍕殶閹诡噯绱濇潻娆戭潚閻滄媽钖勭亸杈ㄦЦ鏉╃偟鐢荤涳缚绡勬稉顓犳畱閻忛箖姣﹂幀褔浠愯箛姗堢礄瀵洜鏁ら惄绋垮彠閺傚洨灏為敍澶堝倹寮挎潻鏉挎禈1閻ㄥ嫬鐤勬宀娲伴惃鍕簰閸欏﹦绮ㄧ拋鐑樻降妤犲矁鐦夐幋鎴滄粦娑撳﹪娼伴惃鍕鏉╁府绱欓崶鍓у娑撳秴顧勫〒鍛珰閿涘苯鎸ㄩ崗鑸垫Ц閺傚洤鐡ч敍宀鏃辨潪瀵告畱閳ユ窂LEU閳ユ粏顕柈鐣屾暏婢堆冨晸閿涘 Neural machine translation  models have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available.  In this situation, continual training, which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations  in the in-domain data but forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure shows the performance trends on the in-domain and general-domain. %As the size of the training corpus grows, the NMT model is trained in the manner of continual learning  from a stream of data.  %Unfortunately, there usually exists a distribution bias in large data set especially when the data is collected from different domains. In this situation, the NMT model has a tendency towards over-fitting to frequent observations  in the newly added data, but forgetting previously learned patterns from the old data, leading to poor performance in the old data.  In the example of domain adaptation shown in Figure, as the training goes, the performance surges for in-domain while slides fast for general-domain. This phenomenon is the catastrophic forgetting of neural network.  %when there are large amounts of parallel training sentences available. However, similar to many other successful neural network-based methods, it also has limited continual learning ability to learn from a stream of training data, which could have different distributions . It is because the NMT system suffers from catastrophic forgetting which refers to that model has a tendency towards over-fitting to frequent observations  in newly added training data, but forgetting previously learned features in the old data.   %Figure denotes this phenomenon in NMT.  %1Yang 娑撳娼版潻娆愵唽閸樼粯甯 %Improving the continual learning ability of the NMT system is of significant importance both in theory and practice. From the artificial intelligence perspective, it can be seen as another step towards the grand goal of creating a real intelligent translation system that can learn continuously new translation skills without forgetting old knowledge as a human does. From a practical perspective, it enables the model to update the model with only recent new data to improve the model's overall performance. We don't need to retrain the model from scratch which is very time-consuming. Moreover, considering that a well-trained model maybe is already deployed in an application, the original training data may not be available at that time. Therefore it is very necessary to improve the continual learning ability of the NMT system.      %1Yang 閺堫剚顔屽楦款唴婵″倹妲搁幓蹇氬牚閿涙氨浼ㄩ梾鐐囦粣韫囨ü绔撮惄瀛樻Ц缁佺偟绮＄純鎴犵捕鐠侇厾绮屾稉顓犳畱娑撴径褔姣︽０姗堢礉閾忕晫鍔ч惄顔煎瀹歌尙绮￠張澶夌娴滄稑浼愭担婊嗗毀閸旀稐绨憴锝呭枀鏉╂瑤閲滈梻顕顣介敍灞借嫙缂佹瑥鍤禍鍡曠娴滄稖顢戞稊瀣箒閺佸牏娈戠憴锝呭枀閺傝纭堕敍鍫濈穿閻€劎娴夐崗铏瀮閻氼噯绱氶敍灞肩稻閺勵垳娲伴崜宥呰嫙濞屸剝婀佸銉ょ稊閸樼粯甯扮槐銏犳躬閻忛箖姣﹂幀褔浠愯箛妯跨箖缁嬪鑵戦崘鍛村劥閺佺増宓侀惃鍕綁閸栨牗鍎忛崘纰夌礉鏉╂瑧顫掗幒銏㈠偍娴兼碍婀侀崝鈺绨幋鎴滄粦閻炲棜袙閻忛箖姣﹂幀褔浠愯箛妯哄絺閻㈢喓娈戦崢鐔锋礈楠炲爼鍣伴崣鏍祲鎼存梻娈戦幒顏呮煢閵  Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning.  ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains.   introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved.  , , and  propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain. All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don't know what happened inside the model during continual training and why these methods can alleviate the catastrophic forgetting problem. The study on these can help to understand the working mechanism of continual training and inspire more effective solutions to the problem in return.    %Catastrophic forgetting is a long-known problem in the training of neural networks. Some researchers have managed to alleviate this problem with different strategies, such as changing the model structure, adding an extra regularization term, employing complementary learning systems  theory-based strategies and so on. However, to the best of our knowledge, these methods mainly focus on how to solve the problem, not what causes the problem. %Understanding the cause of the problem will inspire effective solutions. %, there is still no work trying to figure out the inner reason for catastrophic phenomenon and no direct evidence to show the change of model parameters in NMT. We believe that the attempt to understand this phenomenon can help us adopt appropriate measures to solve this problem.  %it is still not clear what happens during the continual learning process and what causes catastrophic forgetting indeed.  %1Yang 閺堫剚顔岄崣顖欎簰鏉╂瑦鐗遍崘娆欑窗閸︺劍婀伴弬鍥风礉閹存垳婊戠亸婵婄槸閸︺劑顣崺鐔诲殰闁倸绨查惃鍕攱閺嬫湹绗呴崢缁樺赴缁鳖晼arameters閸滃瞼浼ㄩ梾鐐囦粣韫囨娈戦崗宕囬兇閿涘苯鑻熼崚鑽ゆ暰閸戠皢arameters閸︺劎浼ㄩ梾鐐囦粣韫囨ǹ绻冪粙瀣╄厬閻ㄥ嫬褰夐崠鏍Ъ閸旇￥鍌欒礋娴滃棜鎻崚鎷岀箹娑擃亞娲伴惃鍕剁礉閹存垳婊戦柅姘崇箖Absolute value閸滃瓗IM閺夈儴鐦庢导鏉垮棘閺佹澘婀Ο鈥崇风拋顓犵矊娑擃厾娈戦柌宥堫洣閹嶇礄閸欏倽鍐╂瀮閻氼噯绱氶敍灞借嫙闁俺绻冮崣鍌涙殶閹匡箓娅庨惃鍕煙濞夋洘娼甸幒銏㈠偍鏉╂瑤绨洪崡鏇熸殶鐎靛湱鐐曠拠鎴炑嗗厴閻ㄥ嫬濂栭崫宥冨倿姘崇箖鐎圭偤鐛欑紒鎾寸亯閿涘本鍨滄禒顒褰傞悳鏉款嚠娴滃酣姘辨暏妫板棗鐓欓柌宥堫洣閻ㄥ嫬寮弫鏉款嚠娴滃穼n-domain娴犲秶鍔у鍫ュ櫢鐟曚緤绱濋懓灞芥躬妫板棗鐓欓懛顏堝倸绨查惃鍕箖缁嬪鑵戞潻娆庣昂閸欏倹鏆熼惃鍕綁閸栨牕绶㈡径褋鍌氱唨娴滃氦绻栨禍娑樺絺閻滃府绱濈电懓绨叉禍搴ょ槑娴兼澘寮弫浼村櫢鐟曚焦褏娈戞稉銈囶潚閺傝纭堕敍灞惧灉娴狀剛娴夋惔鏃傛畱閹绘劕鍤稉銈囶潚閺傝纭堕弶銉﹀付閸掓儼绻栨禍娑㈠櫢鐟曚胶娈戦崣鍌涙殶閸︺劑顣崺鐔诲殰闁倸绨查惃鍕箖缁嬪鑵戞稉宥勭窗閸欐ê瀵叉潻鍥с亣閿涘矁宀娼冮柌宥勭艾鐠嬪啯鏆ｉ柇锝勭昂娑撳秹鍋呮稊鍫ュ櫢鐟曚胶娈戦崣鍌涙殶閵嗗倸鐤勬宀绮ㄩ弸婊嗐冮弰搴㈠灉娴狀剛娈戦弬瑙勭《閼宠棄婀穱婵婄槈in-domain閺佺増宓佹稉濠勬畱缂堟槒鐦ч幀褑鍏橀崣妯哄娑撳秵妲戦弰鍓ф畱閹懎鍠屾稉瀣亣楠炲懎瀹抽惃鍕絹妤傛﹢姘辨暏妫板棗鐓欓惃鍕倳鐠囨垶褑鍏橀妴  %Given this, we seek to understand the relationship between catastrophic forgetting phenomenon and model parameters under the task of domain adaptation. More specifically, we aim to figure out the trend of model parameters during catastrophic forgetting. To fulfill this goal, we propose two methods to evaluate the importance of the model parameters. The first is to use the absolute value of model parameters and the second is to use the empirical Fisher Information Matrix . To verify the effectiveness and correctness of the proposed methods, we then do parameter erasure experiments. According to the experimental results, we find that some parameters are important for both the general-domain and in-domain. Based on these findings, we try to alleviate catastrophic forgetting by designing learning strategies based on the importance of the parameters. We put more constrains on those important parameters to make them change more conservatively while encourage those less important parameters to change more aggressively during the continual learning process. The experiments on multiple translation tasks show that our methods can improve the translation quality on the new domain without degrading the performance on the old domain too much.  Given above, in this paper, we focus on the catastrophic forgetting phenomenon and investigate the roles of different model parts during continual training. To this end, we explore the model from the granularities of modules and parameters . In the module analyzing experiments, we operate the model in two different ways, by freezing one particular module or freezing the whole model except for this module. We find that different modules preserve knowledge for different domains. In the parameter analyzing experiments, we erase parameters according to their importance which is evaluated by the Taylor expansion-based method . According to the experimental results, we find that some parameters are important for both of the general-domain and in-domain and meanwhile they change greatly during domain adaptation which may result in catastrophic forgetting.  To ensure the validity and reliability of the findings, we conduct experiments over different language pairs and domains.    {0pt} {0pt}           Based on our findings of parameter importance above, we then investigate their changes during the continual learning process. We find that the important parameters for the general-domain translation still play major roles for the in-domain translation by doing another parameter erasure experiments. What's more, the substantial decline of general-domain translation quality and the rise of in-domain translation quality is also due to the change of these parameters.   Finally, based on our findings, we propose some practical methods to overcome the catastrophic forgetting phenomenon by parameter regularization method and learning rate adjustment method based on their importance to the model. We change the important parameters slightly while changing the less important parameters more aggressively. The results show that our approach can alleviate catastrophic forgetting significantly.      Our work indicates that some parameters are more important than others and the change of these parameters can influence translation results a lot. Therefore, we can try to alleviate catastrophic forgetting by designing different learning strategies based on the importance of the parameters. As far as we know, this is the first work trying to analyze the catastrophic forgetting phenomenon in NMT. Moreover, the analyzing methods we put forward in this work are task-independent and can be applied to other neural network-based methods in other tasks. \fi \iffalse extra space to store all the old training data or even retrain from scratch with the and without storing old training data or even retraining with   This work focuses on the domain adaptation problem of NMT which is a special case of the continual learning scenario of the neural network. They share the same training task but the distribution of the training data is different.  Domain adaptation deals with the problem of improving the performance of a model trained on a general domain data over test instances from a new domain. In such a scenario, we usually have large amounts of general-domain training data and a welled trained model based on it. In contrast, we only have a limited number of in-domain training data which will lead the NMT system to overfit soon and perform poorly when only trained with these data. Some researchers solve this problem by combining the training data from the general-domain and in-domain together and train a new system from scratch. They usually make use of the domain information to improve the translating performance by adding domain labels to training data or using domain discriminator to find the domain invariant features. On the one hand, these methods are very time consuming and need extra space to store all the training data which is not efficient in real-life applications. On the other hand, due to the relatively small size of in-domain data, it will lead the model to overfit the general-domain data which has been observed in the results.   Fine-tuning is a fast and efficient method for continual learning of neural networks which has already been applied for NMT. NMT system is first trained on general-domain data and then further trained on in-domain data.   Domain adaptation is the most common application scenario of continual learning in NMT which has drawn much attention recently. Under this scenario, we   The translation quality drops quickly when the distribution of the training data changes. It suffers a catastrophic forgetting in the continual training process. \fi     
","  %Neural machine translation  always suffers catastrophic forgetting during the continual learning process which means the model tends to forget all its previously learned knowledge when further trained with new data with different distributions, like from different domains or languages. However, it is not clear what happens during this process and what causes this phenomenon. More specifically, it is not clear whether this is due to the overall change of the model or the impact of certain parameters. In this work, we focus on the domain adaptation task of NMT under the continual learning scenario. First, we put forward two ways for evaluating the importance of the parameters and show that the translation quality mainly dependents on the most important parameters of the model. Then we analyze the behavior of the parameters according to their importance for the model during the continual learning process and it shows that the important parameters for the general-domain translation still play major roles for the in-domain translation after the continual learning process. What's more, the catastrophic forgetting phenomenon, shown as the substantial decline of general-domain translation quality with the rise of in-domain translation quality,  is mainly due to the change of these important parameters.  Finally, we propose some practical methods to overcome the catastrophic forgetting by controlling the updates of parameters differently based on their importance.    Neural machine translation  models usually suffer from catastrophic forgetting during continual training where the models tend to gradually forget previously learned knowledge and swing to fit the newly added data which may have a different distribution, e.g. a different domain. Although many methods have been proposed to solve this problem, we cannot get to know what causes this phenomenon yet. Under the background of domain adaptation, we investigate the cause of catastrophic forgetting from the perspectives of modules and parameters . The investigation on the modules of the NMT model shows that some modules have tight relation with the general-domain knowledge while some other modules are more essential in the domain adaptation. And the investigation on the parameters shows that some parameters are important for both the general-domain and in-domain translation and the great change of them during continual training brings about the performance decline in general-domain. We conduct experiments across different language pairs and domains to ensure the validity and reliability of our findings.    %by tracing parameter variation in this progress and depict the influence of different model modules.  %and depict the relationship between them so that we can work out solutions to the catastrophic forgetting problem based on these findings.  %Under the background of domain adaptation for machine translation, we found that some parameters play an essential role in both general domain and in-domain translation and the change of them brings about the performance decline in general-domain. Based on these findings, we propose a solution to detect these important parameters and accordingly suppress their fluctuation during domain adaptation. Experimental results prove  %that our method can greatly improve the translation quality in in-domain and meanwhile minimize the negative influences on general-domain translation.",209
"  Recurrent neural network architectures have demonstrated remarkable success in natural language processing, achieving state of the art performance across an impressive range of tasks ranging from machine translation to semantic parsing to question answering . These tasks demand the use of a wide variety of computational processes and information sources , and are evaluated in coarse-grained quantitative ways. As a result, it is not an easy matter to  identify the specific strengths and weaknesses in a network's solution of a task.    In this paper, we take a different tack, exploring the degree to which neural networks successfully master one very specific aspect of linguistic knowledge: the interpretation of sentences containing reflexive anaphora.  We address this problem in the context of the task of semantic parsing, which we instantiate as mapping a sequence of words into a predicate calculus logical form representation of the sentence's meaning. \pex<ex:transform>     \a Mary runs       \a John sees Bob  \mary\run\to  as its subject, the reflexive is interpreted as , and with \lex{Alice} as its subject it is interpreted as .  However, such piecemeal learning of reflexive meaning will not support generalization to sentences involving a subject that has not been encountered as the antecedent of a reflexive during training, even if the interpretation of the  subject has occurred elsewhere. What is needed instead is an interpretation of the reflexive that is characterized not as a specific  output token, but rather as an abstract instruction to duplicate the interpretation of the subject. Such an abstraction requires more than the ``jigsaw puzzle"" approach to meaning that simpler sentences afford.    argues that this kind of abstraction, which he takes to require the use of  algebraic variables to assert identity, is beyond the capacity of recurrent neural networks.  's demonstration involves a simple recurrent network  language model that is trained to predict the next word over a corpus of sentences of the following form: \pex     \a A rose is a rose.     \a A mountain is a mountain. %    \a A car is a car \xe All sentences in this training set have identical subject and object nouns.   shows, however, that the resulting trained network does not correctly predict the subject noun when tested with a novel preamble `\lex{A book is a }'. Though intriguing, this demonstration is not entirely convincing: since the noun occurring in the novel preamble, \lex{book} in our example, did not occur in the training data, there is no way that the network could possibly have known which  output should correspond to the reflexive for a sentence containing the novel  subject noun, even if the network did successfully encode an identity relation between subject and object.    explore a related  task in the context of SRN interpretation of reflexives. In their experiments, SRNs were trained to map input words to corresponding semantic symbols that are output on the same time step in which a word is presented. For most words in the vocabulary, this is a simple task: the desired output is a constant function of the input .  For reflexives however, the target output depends on the subject that occurs earlier in the sentence. \ tested the network's ability to interpret a reflexive in sentences containing a subject that had not occurred as a reflexive's antecedent during training. However, unlike Marcus' task, this subject and its corresponding semantic symbol did occur in other  contexts in the training data, and therefore was in the realm of possible inputs and outputs for the network. Nonetheless, none of the SRNs that they trained succeeded at this task for even a single test example.   Since those experiments were conducted, substantial advances have been made on recurrent neural network architectures, some of which have been crucial in the success of practical NLP systems.       : More sophisticated recurrent units like LSTMs  and GRUs  have been shown to better encode preceding context than SRNs.     : The performance of network models that transduce one string to another, used in machine translation and semantic parsing, has been greatly improved by the use of independent encoder and decoder networks  .     : The ability of a network to produce contextually appropriate outputs even in the context of novel vocabulary items has been facilitated by content-sensitive attention mechanisms .    These innovations open up the possibility that modern network architectures may well be able to solve the variable identity problem necessary for mapping reflexive sentences to their logical form. In the experiments we describe below, we explore whether this is the case.      
"," Reflexive anaphora present a challenge for semantic interpretation: their meaning varies depending on context in a way that appears to require abstract variables. Past work has raised doubts about the ability of recurrent networks to meet this challenge. In this paper, we explore this question in the context of a fragment of English that incorporates the relevant sort of contextual variability. We consider sequence-to-sequence architectures with recurrent units and show that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents. We explore the effect of attention mechanisms and different recurrent unit types on the type of training data that is needed for success as measured in two ways: how much lexical support is needed  to induce an abstract reflexive meaning  and what contexts must a noun phrase occur in to support generalization of reflexive interpretation to this noun phrase?",210
"  Pre-trained contextualized language models such as BERT are state-of-the-art for a wide variety of natural language processing tasks. Similarly, in Information Retrieval , these models have brought about large improvements in the task of { pretrained language models are effective for ad-hoc ranking. What new aspects of the task do neural models solve that previous approaches do not?  Previous work has shown that traditional IR axioms, e.g. that increased term frequency should correspond to higher relevance, do { its ranking score, and adding non-relevant content can increase it---despite document length itself having a limited effect on the ranking scores. %  In summary, we present a new framework  for performing analysis of ad-hoc ranking models. We then demonstrate how the framework can provide insights into ranking model characteristics by providing the most comprehensive analysis of neural ranking models to date. Our released software framework facilitates conducting further analyses in future work.                  
"," Numerous studies have demonstrated the  effectiveness of pretrained contextualized language models such as BERT and T5 for ad-hoc search. However, it is not well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have.  We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs , which includes new types of diagnostic tests that allow us to probe several characteristics---such as sensitivity to word order---that are not addressed by previous techniques.  To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit.  We find evidence that recent neural ranking models have fundamentally different characteristics from prior ranking models. For instance, these models can be highly influenced by altered document word order, sentence order and inflectional endings. They can also exhibit unexpected behaviors when additional content is added to documents, or when documents are expressed with different levels of fluency or formality. We find that these differences can depend on the architecture and not just the underlying language model.\footnote{\url{https://github.com/allenai/abnriml}}",211
"  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Commonsense is the knowledge shared by the majority of people in society and acquired naturally in everyday life. Commonsense reasoning is the process of logical inference by using commonsense information. Commonsense to answer the questions that is ``'' in Figure  is depicted as: ``'', ``'', and ``.'' An enormous amount of pre-defined commonsense knowledge is available and people can make inferences using this commonsense such as in the following example: ``''  ``''  ``''  ``'' This chain of commonsense reasoning is naturally deduced by humans without substantial difficulty. Whereas people acquire commonsense in their lives, machines cannot learn this knowledge without any assistance. A large amount of external knowledge and several reasoning steps are required for machines to learn commonsense. In recent years, various datasets  have been constructed to enable machines to reason commonsense.    is one of the most widely researched datasets and is presented in Figure  . The studies of commonsense reasoning based on this dataset can be categorized into two mainstream approaches. The first approach uses pre-trained language models with distributed representations, which exhibit high performances on most Natural Language Processing  tasks. However, despite their high performance, these models must be trained with an excessive number of parameters and cannot explain the process of commonsense reasoning. The second approach is reasoning with a commonsense knowledge graph. The generally used commonsense knowledge graph is ConceptNet 5.5 , which includes parsed representation from Open Mind Commonsense  and other different language sources such as WordNet  or DBPedia . In this approach, the subgraph of  ConceptNet corresponding to the questions are transformed into node embeddings by the graph encoder. The candidate with the highest attention score is selected as an answer that is computed between the node embeddings and the word vectors from the language models. To learn the commonsense knowledge that is not observed or understood by the language models, relations from ConceptNet serve as a critical role in this method. The performance is improved by utilizing the relations that are not represented in the text; however, the interpretation of the question is still not enough.   Unlike , the most commonly used method of solving this problem is Knowledge-Based Question-Answering   employing semantic representations. As this method infers the answer with the logical structure of the question using the knowledge base, the question-answering process can be explained in a logical form. In our work, Abstract Meaning Representation  , which is one of the logical structure, is used to understand the overall reasoning process, from the question to the answer.  AMR is a graph for meaning representation that symbolizes the meaning of sentences. AMR illustrates ``who is doing what to whom'' that is implied in a sentence with a graph.  The components of these graphs are not the words, but rather the concepts and their relations. Each concept denotes an event or an entity, and each relation represents the semantic role of the concepts.   In this paper, we enable the language models to exploit the AMR graph to understand the logical structure of sentences. However, it is difficult to infer commonsense information with only an AMR graph, owing to its deficiency of commonsense knowledge of the given sentence. For example, in Figure  , the AMR graph indicates the path of the logical structure of the sentence ``'' ; in other words, these paths from the single AMR graph lack the proficient information to predict the right answer. Therefore, for commonsense reasoning, dynamic interactions between the AMR graph and ConceptNet are inevitable to reach the correct answer.   Thus, we propose a new compact AMR graph expanded with the ConceptNet's commonsense relations with pruning, and it is called ACP graph. The proposed method can interpret the path from the question to the answer by performing commonsense reasoning within the connected graph, such as ``'' .    The contributions of our study are as follows.        The remainder of this paper is organized as follows. In Section 2, we present the entire process of our method in detail. The experimental setup and results are explained in Section 3. A discussion of the proposed model is provided in Section 4, and Section 5 presents the conclusions. Appendix A provides related works including ConceptNet, previous works on commonsense reasoning, and AMR.     
"," CommonsenseQA is a task in which a correct answer is predicted through commonsense reasoning with pre-defined knowledge. Most previous works have aimed to improve the performance with distributed representation without considering the process of predicting the answer from the semantic representation of the question. To shed light upon the semantic interpretation of the question, we propose an AMR-ConceptNet-Pruned  graph. The ACP graph is pruned from a full integrated graph encompassing Abstract Meaning Representation  graph generated from input questions and an external commonsense knowledge graph, ConceptNet . Then the ACP graph is exploited to interpret the reasoning path as well as to predict the correct answer on the CommonsenseQA task. This paper presents the manner in which the commonsense reasoning process can be interpreted with the relations and concepts provided by the ACP graph. Moreover, ACP-based models are shown to outperform the baselines.",212
"  Part-Of-Speech  tagging is a crucial step for language understanding, both being used in automatic language understanding applications such as named entity recognition  and question answering , but also being used in  language understanding by linguists who are attempting to answer linguistic questions or document less-resourced languages .   Much prior work  on developing high-quality POS taggers uses neural network methods which rely on the availability of large amounts of labelled data. However, such resources are not readily available for the majority of the world's 7000 languages .  Furthermore, manually annotating large amounts of text with trained experts is an expensive and time-consuming task, even more so when linguists/annotators might not be native speakers of the language.  [t]   Active Learning  is a family of methods that aim to train effective models with less human effort and cost by selecting such a subset of data that maximizes the end model performance. While many methods have been proposed for AL in sequence labeling , through an empirical study across six typologically diverse languages we show that within the same task setup these methods perform inconsistently. Furthermore, even in an  scenario  %  where we have access to the true labels during data selection, existing methods are far from optimal.  We posit that the primary reason for this inconsistent performance is that while existing methods consider uncertainty in predictions, they do not consider the  of the uncertainty with respect to the output labels. For instance, in Figure  we consider the German token ``die,'' which may be either a pronoun  or determiner . According to the initial model , ``die'' was labeled as PRO majority of the time, but a significant amount of probability mass was also assigned to other output tags  for many examples. Based on this, existing AL algorithms that select uncertain tokens will likely select ``die'' because it is frequent and its predictions are not certain, but they may select an instance of ``die'' with  a gold label of PRO or DET. Intuitively, because we would like to correct errors where tokens with true labels of DET are mis-labeled by the model as PRO, asking the human annotator to tag an instance with a true label of PRO, even if it is uncertain, is not likely to be of much benefit.  Inspired by this observation, we pose the problem of AL for part-of-speech tagging as selecting tokens which maximally  between the output tags. For instance, in the example we would attempt to pick a token-tag pair ``die/DET'' to reduce potential errors of the model over-predicting PRO despite its belief that DET is also a plausible option. We demonstrate the features of this model in an oracle setting where we know true model confusions , and also describe how we can approximate this strategy when we do not know the true confusions.  We evaluate our proposed AL method by running simulation experiments on six typologically diverse languages namely German, Swedish, Galician, North Sami, Persian, and Ukrainian, improving upon models seeded with cross-lingual transfer from related languages . In addition, we conduct human annotation experiments on Griko, an endangered language that truly lacks significant resources.   Our contributions are as follows: [leftmargin=*,nolistsep,noitemsep]      setting.               % File tacl2018v2.tex % Sep 20, 2018  % The English content of this file was modified from various *ACL instructions % by Lillian Lee and Kristina Toutanova % % LaTeXery is mostly all adapted from acl2018.sty.  \documentclass[11pt,a4paper]{article} \usepackage{times,latexsym} \usepackage{url} \usepackage[T1]{fontenc} \usepackage{amsmath} \usepackage{amssymb} \usepackage{tabularx} \usepackage{mathtools} \usepackage{booktabs} \usepackage{url} \usepackage{longtable} \usepackage{tabu} \usepackage{multirow} \usepackage{amsfonts} \usepackage{tabu} \usepackage{algorithm} \usepackage{bbm} \usepackage{subfigure} \usepackage[noend]{algpseudocode} \usepackage[normalem]{ulem} \usepackage{enumitem} \makeatletter  \def\BState{\State \usepackage{xcolor} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}   %% Package options: %% Short version: ""hyperref"" and ""submission"" are the defaults. %% More verbose version: %% Most compact command to produce a submission version with hyperref enabled %%    \usepackage[]{tacl2018v2} %% Most compact command to produce a ""camera-ready"" version \usepackage[acceptedWithA]{tacl2018v2} %% Most compact command to produce a double-spaced copy-editor's version %\usepackage[acceptedWithA]{tacl2018v2} % %% If you need to disable hyperref in any of the above settings  in the TACL instructions), add "",nohyperref"" in the square %% brackets.  %\usepackage[nohyperref]{tacl2018v2}  %%%% Material in this block is specific to generating TACL instructions \usepackage{xspace,mfirstuc,tabulary} {Sept. 20, 2018} [1]{\textcolor{magenta}{[1]{\textcolor{blue}{[1]{{ \renewcommand{\anonsubtext}{}  {final versions\xspace} {Final version\xspace} {Final versions\xspace} {Final Versions\xspace} {submission\xspace} {{\taclpaper}s\xspace} {Submission\xspace} {{\Taclpaper}s\xspace} {Submissions\xspace} \fi  %%%% End TACL-instructions-specific macro block %%%%  \title{Reducing Confusion in Active Learning for Part-Of-Speech Tagging}  \author{Aditi Chaudhary\textsuperscript{1},      Antonios Anastasopoulos\textsuperscript{2,\Thanks{ Work done at Carnegie Mellon University.}},      Zaid Sheikh\textsuperscript{1}, Graham Neubig\textsuperscript{1} \\   \textsuperscript{1}Language Technologies Institute, Carnegie Mellon University\\   \textsuperscript{2}Department of Computer Science, George Mason University\\   { @cs.cmu.edu}}    { }  }    \date{}     Active learning  uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech  taggers. Existing AL heuristics are generally designed on the principle of selecting  yet  training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages , we found the surprising result that even in an  scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances which . Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin.  We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The  code is publicly released here.    %         
"," Active learning  uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech  taggers. Existing AL heuristics are generally designed on the principle of selecting  yet  training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages , we found the surprising result that even in an  scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances which . Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin.  We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The  code is publicly released here.\footnote{\url{https://github.com/Aditi138/CRAL}}",213
" With an increasing submission of academic papers in recent years, the task of making final decisions manually incurs significant overheads to the program chairs, it is desirable to automate the process.  In this study, we aim at utilizing document-level semantic analysis for paper review rating prediction and recommendation.  Given the reviews of each paper from several reviewers as input, our goal is to infer the final acceptance decision for that paper and the reviewers' evaluation with respect to a numeric rating .  Paper review rating prediction and recommendation is a practical and important task in AI applications which will help improve the efficiency of the paper review process. It is also intended to enhance the consistency of the assessment procedures and outcomes, and to diversify the paper review process by comparing human recommended rating with machine recommended rating.  In the literature, most of existing studies cast review rating prediction as a multi-class classification/regression task .  They build a predictor by using supervised machine learning models with review texts and corresponding ratings.  Due to the importance of features, most researches focus on extracting effective features such as context-level features  and user features  to boost prediction performance.  However, feature engineering is time-consuming and labor-intensive.   Recently, with the development of neural networks and its wide applications, various deep learning-based models have been proposed for automatically learning features from text data .  Existing deep learning models usually learn continuous representations of different grains  from text corpus .  Although deep learning models can automatically learn extensive feature representation, they cannot efficiently capture the hierarchical relationship inherent to the review data.  To address this problem,  studied a hierarchical architecture and implemented it in deep learning framework to learn a better document-level representation.  Also, with the success of attention mechanism in many tasks such as machine translation, question answering and so on ,   designed a directional self-attention network to gain context-aware embeddings for words and sentences.  Despite great progress made by these models, they do not focus on the task of paper review rating recommendation and are not effective enough to be directly used for this task because of the following reasons: First, the review data is hierarchical in nature.  There exists a three-level hierarchical structure in the review data: word level, intra-review level and inter-review level, while previous models only capture two-levels  of this hierarchy.  Second, paper reviews are usually much longer than other reviews , while most of these models are working on those shorter reviews stated above and they do not leverage the up to date representation techniques such as BERT  and SciBERT .   In this paper, we propose a novel neural network framework for paper review rating recommendation by taking word, intra-review and inter-review information into account.  Specifically, inspired by HAN  and DiSAN , we introduce a Hierarchical Bi-directional self-Attention Network  framework to effectively incorporate different levels of hierarchical information.  The proposed framework consists of three main modules in end-to-end relationship: sentence encoder, intra-review encoder and inter-review encoder, which can consider hierarchical structures of review data as comprehensive as possible. The outputs of inter-review encoder are leveraged as features to build the rating predictor without any feature engineering. We release the code and data collected by us to enable replication and application to new tasks, available at .  The contributions of this work are as follows:     
"," Review rating prediction of text reviews is a rapidly growing technology with a wide range of applications in natural language processing.  However, most existing methods either use hand-crafted features or learn features using deep learning with simple text corpus as input for review rating prediction, ignoring the hierarchies among data.  In this paper, we propose a Hierarchical bi-directional self-attention Network framework  for paper review rating prediction and recommendation, which can serve as an effective decision-making tool for the academic paper review process. Specifically, we leverage the hierarchical structure of the paper reviews with three levels of encoders: sentence encoder , intra-review encoder  and inter-review encoder .  Each encoder first derives contextual representation of each level, then generates a higher-level representation, and after the learning process, we are able to identify useful predictors to make the final acceptance decision, as well as to help discover the inconsistency between numerical review ratings and text sentiment conveyed by reviewers.  Furthermore, we introduce two new metrics to evaluate models in data imbalance situations.  Extensive experiments on a publicly available dataset  and our own collected dataset  demonstrate the superiority of the proposed approach compared with state-of-the-art methods.",214
"  % What is QG and Why it is important Question Generation  aims to endow machines with the ability to ask relevant and to-the-point questions about a document.  QG has important practical applications, such as  generating assessments for course materials in education, prompting user interaction in dialog systems, enabling machines to ask clarification questions such as FAQs, and automatically building large-scale QA datasets for the research community.   % How tranditional works do it? Recent QG approaches have used Seq2Seq models with attention, which feeds the input document into an encoder, and generates a question about the document through a decoder.  % Why it needs RL? The training objective is to maximize the log likelihood of the ground-truth question paired with each input document using teacher forcing. However, as the ground-truth questions are insufficient to account for the many equivalent ways of asking a question, this likelihood-based training suffers from the problem of exposure bias, i.e., the model does not learn how to distribute probability mass over sequences that are valid but different from the ground truth.  % How RL addresses the problem? %   To address this issue, previous QG works proposed to optimize the model directly on question-specific rewards via Reinforcement Learning .  This process decouples the training procedure from the ground truth data, so that the space of possible questions can be better explored. Moreover, it allows the training to target on specific properties we want the question to exhibit, such as relevant to a specific topic or answerable by the document.  % What is the problem for RL-based method? Although various rewards have been employed for QG --- such as BLEU, the answerability reward, and the word movers distance --- optimizing the reward scores does not always lead to higher question quality in practice, as observed by Hosking and Riedel~. How to define robust and effective QG-specific rewards still requires further investigation.   % What we want to do? We aim to analyze the effectiveness of question-specific rewards in QG. Instead of using general natural language generation metrics such as BLEU, we target three QG-related metrics that are commonly cited in human evaluations of question quality:  Fluency indicates whether the question follows the grammar and accords with the correct logic;  Relevance indicates whether the question is relevant to the document; and  Answerability indicates whether the question is answerable given the document. We design a specific RL reward for each metric: a language model based reward for fluency, a discriminator-based reward for relevance, and a QA-based reward for answerability.  After optimizing each reward via RL, we conduct comprehensive analysis, including automatic and human evaluation, to arrive at the following conclusions:  both individual and joint optimization of these rewards can lead to performance gain in automated metrics, but this does not guarantee an improvement in the real question quality;  the reward for relevance substantially helps to improve the question quality, while the reward for answerability reduces the quality due to the bias brought by the QA model; and  a reward is more likely to improve the question quality if the reward score correlates well with human judgement.   
","     Recent question generation  approaches often utilize the sequence-to-sequence framework  to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward.      We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards  that correlate well with human judgement  lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poor question quality. Our code is publicly available at {https://github.com/YuxiXie/RL-for-Question-Generation}.",215
"  % In daily bases plethora of opinion data is published about different topics and in response to different stimuli using Social Media.  % Aiming to analyse and gain insights from opinions posted in social media, research in stance detection has become increasingly popular in recent years. Framed as a classification task, the stance detection consists in determining if a textual utterance expresses a supportive, opposing or neutral viewpoint with respect to a target or topic . Research in stance detection has largely been limited to analysis of single utterances in social media. Furthering this research, the SardiStance 2020 shared task  focuses on incorporating contextual knowledge around utterances, including metadata from author profiles and network interactions. The task included two subtasks, one solely focused on the textual content of social media posts for automatically determining their stance, whereas the other allowed incorporating additional features available through profiles and interactions. This paper describes and analyses our participation in the SardiStance 2020 shared task, which was held as part of the EVALITA  campaign and focused on detecting stance expressed in tweets associated with the Sardines movement. %  %   % For a network interaction graph, we generate user embeddings, using variations of graph neural network  embedding methods, and then concatenate author's vector with its corresponding utterance features for each stance. We also extract two types of text embedding representations for each utterance, embedding-based features, namely word embedding vectors and cosine similarity vectors, using different models including variations of CNN and bidirectional LSTM models. Further, the results of these two feature extraction methods are concatenated for the final classification step. We also consider the standard methods that extract frequency-based representations from author profiles and stance utterances including unigrams and Tfidf vectors. All these four features where combined and fed into the drop out and dense layers, to finally generate the final label using a softmax activation function. Though, we deactivate some of these four sources of features and alter the frequency-based vector by excluding some features, changing the embedding source and reducing the dimensionality for highly dimensional vectors  using PCA.}   
"," This paper presents our submission to the SardiStance 2020 shared task, describing the architecture used for Task A and Task B. While our submission for Task A did not exceed the baseline, retraining our model using all the training tweets, showed promising results leading to  using bidirectional LSTM with BERT multilingual embedding for Task A. For our submission for Task B, we ranked 6th . With further investigation, our best experimented settings increased performance from  to  with same architecture and parameter settings and after only incorporating social interaction features- highlighting the impact of social interaction on the model's performance.",216
"   State-of-the-art for most existing natural language processing  classification tasks is currently achieved by systems that are first pre-trained on auxiliary language modeling tasks and then fine-tuned on the task of interest with cross-entropy loss . Although commonly used, cross-entropy loss -- the KL-divergence between one-hot vectors of labels and the distribution of model's output logits -- has several shortcomings. Cross entropy loss leads to poor generalization performance due to poor margins , and it lacks robustness to noisy labels  or adversarial examples . Effective alternatives have been proposed to change the reference label distributions through label smoothing , Mixup , CutMix , knowledge distillation  or self-training~.  Additionally, it has been recently demonstrated in NLP that fine-tuning using cross entropy loss tends to be unstable , especially when supervised data is limited, a scenario in which pre-training is particularly helpful. To tackle the issue of unstable fine-tuning, recent work proposes local smoothness-inducing regularizers  and regularization methods inspired by the trust region theory  to prevent representation collapse that lead to poor generalization performance. Empirical analysis suggests that fine-tuning for longer, reinitializing top few layers~, and using debiased Adam optimizer during fine-tuning~ can make the fine-tuning procedure more stable.  We are inspired by the learning strategy that humans deploy when given a few examples -- try to find the commonalities between the examples of each class and contrast them with examples from other classes. We hypothesize that a similarity-based loss will be able to hone in on the important dimensions of the multidimensional hidden representations and lead to better few-shot learning results and be more stable while fine-tuning pre-trained models. We propose a novel objective for fine-tuning pre-trained language models that includes a supervised contrastive learning term that pushes examples from the same class close and examples of different classes further apart. The new term is similar to the contrastive objective used for self-supervised representation learning in various domains such as image, speech, and video domains. . In constrast to these methods, however, we use a contrastive objective for supervised learning of the final task, instead of contrasting different augmented views of examples.  Adding supervised contrastive learning  term to the fine-tuning objective improves performance on several natural language understanding tasks from the GLUE benchmark , including SST-2, CoLA, MRPC, RTE, and QNLI over the state-of-the-art models fine-tuned with cross entropy loss. The improvements are particularly strong in few-shot learning settings , and models trained with SCL are not only robust to the noise in the training data, but also have better generalization ability to related tasks with limited labeled data. Our approach does not require any specialized architectures , memory banks , data augmentation of any kind, or additional unsupervised data. To the best of our knowledge, our work is the first to successfully integrate a supervised contrastive learning objective for fine-tuning pre-trained language models.        % \ves{end of alternative}  % State-of-the-art models for most existing natural language processing  tasks are currently learned by fine-tuning pre-trained large language models  that have been shown to capture semantic, syntactic, and world knowledge.  Recent attempts at improving the pre-training stage over masked language modeling~ has led to improvements on natural language understanding tasks, but fine-tuning stage has stayed the same for all downstream NLP classification tasks: add a task-specific output layer to the pre-trained language model and continue training on the labeled task data using cross-entropy loss.  % Cross-entropy loss is the most widely adopted objective for supervised classification models, defined as the KL-divergence between one-hot vectors of labels and the distribution of model's output logits. Although commonly used by the state-of-the-art models across many fields including NLP, there has been several works demonstrating the shortcomings of the cross-entropy loss, showing that it leads to poor generalization performance due to poor margins , and lack of robustness to noisy labels  or adversarial examples . Among the alternative objective functions proposed, more effective approaches in practice have been the ones that change the reference label distributions such as label smoothing , Mixup , CutMix , knowledge distillation  or self-training~.  % Several recent studies show that the fine-tuning procedure is unstable , especially for the case where supervised data is limited, a scenario in which pre-training is particularly helpful. To tackle the issue of unstable fine-tuning, local smoothness-inducing regularizers  and regularization methods inspired by the trust region theory  have been proposed to prevent representation collapse that leads to poor generalization performance of task models. There has also been empirical analysis that suggests fine-tuning for longer, reinitializing top few layers~, and using debiased Adam optimizer during fine-tuning~ make the fine-tuning procedure more stable.   % On the other hand, contrastive learning methods have seen remarkable success for self-supervised representation learning on various downstream tasks, particularly in the image, speech, and video domains.   % These self-supervised contrastive learning methods primarily try to reduce the distance between representations of the positive pairs while increasing the distance between representations of the negative pairs. Positive pairs are constructed as the different augmented views of the same labeled example, and negative pairs are simply augmented views of all the other examples. Augmented views of the examples are often constructed with state-of-the-art data augmentation methods such as RandAugment  or AutoAugment  for the computer vision domain, and distance metric is often chosen as the inner product or the Euclidean distance between the representations of the pairs in a low-dimensional embedding space.    % Recently,  extended contrastive learning to a fully supervised setting through using label information while constructing positive and negative pairs, showed improved performance over cross-entropy loss baseline on ImageNet image classification accuracy and robustness benchmarks, and demonstrated that supervised contrastive learning is less sensitive to hyperparameter changes. Similarly,  propose a hybrid discriminative-generative training of energy-based models, where they approximate the generative term with a contrastive objective and demonstrate improved image classification accuracy on CIFAR-10 and CIFAR-100, along with improved performance on robustness, out-of-distribution detection, and calibration.  % In this paper, we propose a supervised contrastive learning regularization for fine-tuning of large pre-trained language models that helps the model leverage label information more effectively across different labeled data regimes. Our approach does not require specialized architectures , memory banks , or very large batch sizes , but still outperforms the strong baseline of fine-tuning RoBERTa-Large on labeled task data with cross-entropy loss, unlike some previous works. To the best of our knowledge, our work is the first to successfully integrate a supervised contrastive learning objective for fine-tuning pre-trained language models. % while sho results on few-shot learning, robustness, and generalization ability.  % We summarize our key contributions in the following: %  %       
"," State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability.  Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning  objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both the high-data and low-data regimes, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. % In all of our experiments, we use a very competitive baseline of fine-tuning RoBERTa Large using cross entropy loss on the labeled task data.  %including SST-2, CoLA, MRPC, RTE and QNLI. %Our method outperforms the baseline on multiple datasets in the GLUE benchmark including SST-2, CoLA, MRPC, RTE and QNLI for the full dataset regime.  % We also show the effectiveness of our regularization for few-shot learning and demonstrate  % We also demonstrate the robustness of the learned representations by using noisy datasets, and show that the learned representations are more transferable to related tasks.  We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.",217
" With the rapid growth of textual documents on the internet, accessing information from the web has become a challenging issue . Often users want the summary of a topic from various sources to fulfill their information needs . The QF-MDS task deals with such problems where the goal is to summarize a set of documents to answer a given query.     In the QF-MDS task, the summaries generated by the summarizer can be either extractive or abstractive. An extractive summarizer extracts relevant text spans from the source document, whereas an abstractive summarizer generates a summary in natural language that may contain some words which did not appear in the source document . With the rising popularity of virtual assistants in recent years, there is a growing interest to integrate abstractive summarization capabilities in these systems for natural response generation .   One major challenge for the QF-MDS task is that the datasets  used for such tasks do not contain any labeled training data. Therefore, neural summarization models that leverage supervised training cannot be used in these datasets. Note that for other related tasks , how to reduce the demands for labeling the data and how to leverage unlabeled data were also identified as a major challenge. While using datasets similar to the target dataset as the training data for the QF-MDS task, we find that these datasets only contain multi-document gold summaries. However, the state-of-the-art transformer-based  summarization models  cannot be used in long documents due to computational complexities . To tackle these issues, we propose a novel weakly supervised approach by utilizing distant supervision to generate weak reference summary of each single-document from multi-document gold reference summaries. We train our model on each document with weak supervision and find that our proposed approach that generates abstractive summaries is very effective for the QF-MDS task. More concretely, we make the following contributions:     .     
"," In the Query Focused Multi-Document Summarization  task, a set of documents and a query are given where the goal is to generate a summary from these documents based on the given query. However, one major challenge for this task is the lack of availability of labeled training datasets. To overcome this issue, in this paper, we propose a novel weakly supervised learning approach via utilizing distant supervision. In particular, we use datasets similar to the target dataset as the training data where we leverage pre-trained sentence similarity models to generate the weak reference summary of each individual document in a document set from the multi-document gold reference summaries. Then, we iteratively train our summarization model on each single-document to alleviate the computational complexity issue that occurs while training neural summarization models in multiple documents  at once. Experimental results in Document Understanding Conferences\footnote{https://duc.nist.gov/}  datasets show that our proposed approach sets a new state-of-the-art result in terms of various evaluation metrics.",218
" One ultimate goal of language modelling is to construct a model like human, to grasp general, flexible and robust meaning in language. One reflection of obtaining such model is be able to master new tasks or domains on same task quickly. However, NLU models have been building from specific task on given data domain but fail when dealing with out-of-domain data or performing on a new task. To combat this issue, several research areas in transfer learning including domain adaptation, cross lingual learning, multi-task learning and sequential transfer learning have been developed to extend model handling on multiple tasks. However, transfer learning tends to favor high-resources tasks if not trained carefully, and it is also computationally expensive .  Meta learning algorithm tries to solve this problem by training model in a variety of tasks which equip the model the ability to adapt to new tasks with only a few samples.  In our case, we adopt the idea of model-agnostic meta learning  which is an optimization method of meta learning that directly optimized the model by constructing an useful initial representation that could be efficiently trained to perform well on various tasks . However, in an continual learning where data comes into the model sequentially, there is still a potential problem of catastrophic forgetting where a model trained with new tasks would start to perform worse on previous tasks. The two objectives of designing a continual learning architecture are to accelerate future learning where it exploits existing knowledge of a task quickly together with general knowledge from previous tasks to learn prediction on new samples and to avoid interference in previous tasks by updates from new tasks. .   % new In this paper, we utilize algorithm derived from Jave and White  which applies Meta-Learning under continual learning. Our objective is to apply this framework in NLP field, specifically on NLU tasks. By taking advantage of this model-agnostic approach, Meta-Learning under continual learning should be applicable on any language model that is optimized by gradient-based methods. We compare our results with Duo et al  which applies meta-learning on Glue tasks, our MAML-Rep shows comparable results. We hope to bring new research direction in NLP fields focusing on such method. The implementation of our code can be found at \url{https://github.com/lexili24/NLUProject}.  % old % This paper aims to develop a framework that incorporate meta learning under the continual learning framework. Hypothetically, our approach is efficient in training by relying on low-resources on various tasks adapted from meta learning characteristics. By training a meta learner under continual learning framework, our model should have consistent results on various tasks with little catastrophic forgetting and learning general representation for all tasks. Finally, our approach is model agnostic, and could essentially apply on any existing language models as long as the model can be optimized by gradient descent. Moreover, our method can be put into the framework of some other continual learning techniques like GEM. The implementation of our code can be found at \url{https://github.com/lexili24/NLUProject}.   
"," Neural network has been recognized with its accomplishments on tackling various natural language understanding  tasks. Methods have been developed to train a robust model to handle multiple tasks to gain a general representation of text. In this paper, we implement the model-agnostic meta-learning  and Online aware Meta-learning  meta-objective under the continual framework for NLU tasks proposed by Javed and White. We validate our methods on selected SuperGLUE   and GLUE benchmark .",219
"  	 	%  	% % final paper: en-us version  	% 	  % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/} }  ), we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder for independent predictions. An additional regularization loss based on prediction consistency between views is used to encourage the auxiliary view to mimic the primary view. Thanks to the co-training on the two views, the gradients during back-propagation can simultaneously flow into the two views, which implicitly realizes the knowledge transfer.  Extensive experimental results on five translation tasks  show that our method can stably outperform multiple baseline models . In particular, we have achieved new state-of-the-art results of 10.8 BLEU on KoEn and 36.23 BLEU on IWSLT'14 DeEn. Further analysis shows that our method's success lies in the robustness to encoding representations and dark knowledge  provided by consistency regularization.   	 \fi   
","   Traditional neural machine translation is limited to the topmost encoder layer's context representation and cannot directly perceive the lower encoder layers. Existing solutions usually rely on the adjustment of network architecture, making the calculation more complicated or introducing additional structural restrictions. In this work, we propose layer-wise multi-view learning to solve this problem, circumventing the necessity to change the model structure.    We regard each encoder layer's off-the-shelf output, a by-product in layer-by-layer encoding, as the redundant view for the input sentence.   In this way, in addition to the topmost encoder layer , we also incorporate an intermediate encoder layer as the auxiliary view.    We feed the two views to a partially shared decoder to maintain independent prediction.     Consistency regularization based on KL divergence is used to encourage the two views to learn from each other.   Extensive experimental results on five translation tasks show that our approach yields stable improvements over multiple strong baselines. As another bonus, our method is agnostic to network architectures and can maintain the same inference speed as the original model.",220
" % . } % Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis , opinion mining , and computational literary studies . The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of \fear, \anger, \joy, \anticipation, \trust,   \target{cars} because they .'' A number of English-language resources are available:  manually construct a dataset following FrameNet's emotion predicate and annotate the stimulus as its core argument.   annotate Tweets for emotion cue phrases, emotion targets, and the emotion stimulus. In our previous work  we publish news headlines annotated with the roles of emotion experiencer, cue, target, and stimulus.  annotate sentence triples taken from literature for the same roles.  A popular benchmark for emotion stimulus detection is the Mandarin corpus by .  annotate English and Mandarin texts in a comparable way on the clause level .  In this paper, we utilize role annotations to understand their influence on emotion classification. We evaluate which of the roles' contents enable an emotion classifier to infer the emotions. It is reasonable to assume that the roles' content carries different kinds of information regarding the emotion: One particular experiencer present in a corpus might always feel the same emotion; hence, be prone to a bias the model could pick up on. The target or stimulus might be independent of the experiencer and be sufficient to infer the emotion.  The presence of a target might limit the set of emotions that can be triggered.  Finally, as some of the corpora contain cue annotations, we assume that these are the most helpful to decide on the expressed emotion, as they typically have explicit references towards concrete emotion names.  
","   Emotion recognition is predominantly formulated as text classification in   which textual units are assigned to an emotion from a predefined inventory   .   More recently, semantic role labeling approaches have been developed to   extract structures from the text to answer questions like: ``who is   described to feel the emotion?'' , ``what causes this   emotion?'' , and at   which entity is it directed?'' . Though it has been shown that   jointly modeling stimulus and emotion category   prediction is beneficial for both subtasks, it remains unclear which of   these semantic roles enables a classifier to infer the emotion. Is it the   experiencer, because the identity of a person is biased towards a   particular emotion ? Is it a particular target    or a stimulus ? We   answer these questions by training emotion classification models on five   available datasets annotated with at least one semantic role by masking the   fillers of these roles in the text in a controlled manner and find that   across multiple corpora, stimuli and targets carry emotion information,   while the experiencer might be considered a confounder.  Further, we   analyze if informing the model about the position of the role improves the   classification decision. Particularly on literature corpora we find that   the role information improves the emotion classification.",221
" In recent years, the best results for coreference resolution of English have been obtained with end-to-end neural models~. However for Dutch, the existing systems are still using either a  rule-based~ or a machine learning approach~. The rule-based system dutchcoref~ outperformed previous systems on two existing datasets and also presented a corpus and evaluation of literary novels .  In this paper we compare this rule-based system to an end-to-end neural coreference resolution system: e2e-Dutch. This system is a variant of  with BERT token representations. We evaluate and compare the performance of e2e-Dutch to dutchcoref on two different datasets:  the SoNaR-1 corpus , a genre-balanced corpus of 1 million words, and  the RiddleCoref corpus of contemporary novels . This provides insights into  the relative strengths of a neural system versus a rule-based system for Dutch coreference, and  the effect of domain differences .  The two datasets we consider vary greatly in terms of overall size and length of the individual documents; the training subset of RiddleCoref contains only 23 documents  compared to 581 documents for SoNaR-1. However, the average number of sentences per document is higher for RiddleCoref than for SoNaR-1 .  We also conduct an error analysis for both of the systems to examine the types of errors that the systems make.   
","     We evaluate a rule-based      and neural  coreference system on Dutch datasets of     two domains: literary novels and news/Wikipedia text.     The results provide insight into the relative strengths of data-driven and     knowledge-driven systems, as well as the influence of domain, document     length, and annotation schemes.     The neural system performs best on news/Wikipedia text,     while the rule-based system performs best on literature.     The neural system shows weaknesses with limited training data and long     documents, while the rule-based system is affected by annotation     differences. The code and models used in this paper are available at     \url{https://github.com/andreasvc/crac2020}",222
"  A relational triple consists of two entities connected by a semantic relation, which is in the form of . The extraction of relational triples from unstructured raw texts is a key technology for automatic knowledge graph construction, which has received growing interest in recent years.  There have been several studies addressing technical solutions for relational triple extraction. Early researches, such as , employ a pipeline manner to extract both of entities and relations, where entities are recognized first and then the relation between the extracted entities is predicted. Such a pipeline approach ignores the relevance of entity identification and relation prediction  and tends to suffer from the error propagation problem.  %    To model cross-task dependencies explicitly and prevent error propagation in the pipeline approach, subsequent studies propose joint entity and relation extraction. These studies can be roughly categorized into three main paradigms. The first stream of work, such as , treats joint entity and relation extraction task as an end-to-end table filling problem. Although these methods represent entities and relations with shared parameters in a single model, they extract the entities and relations separately and produce redundant information . The second stream of work, such as , transforms joint entity and relation extraction into sequence labeling. To do this, human experts need to design a complex tagging schema. The last stream of work, including , is driven by the sequence-to-sequence  model  to generate relational triples directly, which is a flexible framework to handle overlapping triples and does not require the substantial effort of human experts.  We follow the seq2seq based models for joint entity and relation extraction. Despite the success of existing seq2seq based models, they are still limited by the autoregressive decoder and the cross-entropy loss. The reasons are as follows: the relational triples contained in a sentence have no intrinsic order in essence. However, in order to adapt the autoregressive decoder, whose output is a sequence,  the unordered target triples must be sorted in a certain order during the training phase. Meanwhile, cross-entropy is a permutation-sensitive loss function, where a penalty is incurred for every triple that is predicted out of the position. Consequently, current seq2seq base models not only need to learn how to generate triples, but also are required to consider the extraction order of multiple triples.   % consists of three parts  featured by transformers with non-autoregressive parallel decoding and the bipartite matching loss.  In detail, there are three parts in the proposed set prediction networks :  to avoid introducing the order of triplets  % restoring to the original form of this task without considering the order of multiple triples In this work, we formulate the joint entity and relation extraction task as a set prediction problem, avoiding considering the order of multiple triples. In order to solve the set prediction problem, we propose an end-to-end network featured by transformers with non-autoregressive parallel decoding and bipartite matching loss. In detail, there are three parts in the proposed set prediction networks : a sentence encoder, a set generator, and a set based loss function. First of all, we adopt the BERT model  as the encoder to represent the sentence. Then, since an autoregressive decoder must generate items one by one in order, such a decoder is not suitable for generating unordered sets. In contrast, we leverage the transformer-based non-autoregressive decoder  as the set generator, which can predict all triples at once and avoid sorting triples. Finally, in order to assign a predicted triple to a unique ground truth triple, we propose bipartite matching loss function inspired by the assigning problem in operation research . Compared with  cross-entropy loss  that highly penalizes small shifts in  triple order, the proposed loss function is invariant to any permutation of predictions; thus it is suitable for evaluating the difference between ground truth set and prediction set.  % To summarize, our contributions are as follows: In a nutshell, our main contributions are: % the main contributions of our work are as follows:     % the conjunction of the bipartite matching loss and transformers with %  parallel decoding  % Our work build on prior work in several domains:relation extraction, non-autoregressive model, andbipartite matching losses for set prediction. % Relation Extraction.   Non-autoregressive Model.   
"," The joint entity and relation extraction task aims to extract all relational triples from a sentence. In essence, the relational triples contained in a sentence are unordered. However, previous seq2seq based models require to convert the set of triples into a sequence in the training phase. To break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model can get rid of the burden of predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike autoregressive approaches that generate triples one by one in a certain order, the proposed networks directly output the final set of triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Experiments on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods. Training code and trained models will be available at \url{http://github.com/DianboWork/SPN4RE}.",223
" Zero-shot translation has first been introduced by  and refers to the ability of a multilingual NMT model to translate between all its source and target languages, even those pairs for which no parallel data was seen in training. In the simplest setting, all parameters in the network are shared between the different languages and the translation is guided only by special tags to indicate the desired output language .  While this capability is attractive because it is an alternative to building  dedicated translation systems to serve  languages, performance on zero-shot pairs tends to lag behind pivot translation. Recent papers, such as ,  and , have suggested training techniques to improve the generalization to unseen language pairs, but performance varies considerably across settings.  In this paper, we examine in detail the behavior of the multilingual model proposed by  on zero-shot translation directions. Our experiments show the following:      Overall, we observe improvements of 8.1 BLEU  on 6 zero-shot directions with simple changes to the multilingual training setup.  
"," Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN$\leftrightarrow$\{FR,CS,DE,FI\} system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g.\ English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions.  We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.",224
"  Entrainment is a well-known psycholinguistic phenomenon causing people to adapt to conversation partners so as to become more similar. It affects many linguistic features including phonetics , lexical choice , syntax , and prosody . Importantly, it correlates with interesting aspects of the conversation such as task success, liking, and even rapport with a robot .  The researchers cited above employed various means to measure entrainment, such as correlations, models of conditional probabilities, comparisons of distributions, and perceived similarity. Recently,  proposed the first neural entrainment measure. Our work builds on theirs by addressing a challenge critical to measuring entrainment: accounting for consistency.   Entrainment is defined as an active, though unconscious, adaptation of a speaker towards their partner. In practice, however, the static similarity or correlation between two speakers is often measured. Thus, even two speakers whose vocal characteristics were initially similar are perceived to have entrained, although no adaptation has taken place. Alternatively, when Speaker B entrains to Speaker A, both speakers are perceived to have entrained, without adaptation from Speaker A. We apply neural methods proposed by  to explicitly deconfound consistency, the tendency to adhere to one's own vocal style, from entrainment, the tendency to adapt to one's partner. We argue that entrainment measures that do not control for consistency overestimate the degree of entrainment in a conversation.  Section  explains the data and features that we use to train our networks, which are described in Section . Section  introduces two experiments to validate our methods whose results are discussed, lastly, in Section .  
","   Human interlocutors tend to engage in adaptive behavior known as entrainment to become more similar to each other. Isolating the effect of consistency, i.e., speakers adhering to their individual styles, is a critical part of the analysis of entrainment. We propose to treat speakers' initial vocal features as confounds for the prediction of subsequent outputs. Using two existing neural approaches to deconfounding, we define new measures of entrainment that control for consistency. These successfully discriminate real interactions from fake ones. Interestingly, our stricter methods correlate with social variables in opposite direction from previous measures that do not account for consistency. These results demonstrate the advantages of using neural networks to model entrainment, and raise questions regarding how to interpret prior associations of conversation quality with entrainment measures that do not account for consistency.",225
"  The proliferation of online hate speech has become prevalent in recent times. Numerous social media outlets and the computational social science community are looking at various automated techniques to detect and classify hate speech. However, most models, nascent in nature, have significant limitations due to the complexity of the problem. Primarily, the lack of a reliable baseline coupled with an evolving vocabulary of hateful content makes this a particularly challenging issue. For instance, many studies have classified this problem as a binary classification task, but this fails to address the subtleties of hate speech, such as direct  vs. indirect  hate speech. These binary classification models also fail to identify different types of hate speech like racism, sexism, antisemitism, etc. or their varying degrees. Another key obstacle that plagues these binary models is their inability to distinguish between general offensive language and hate speech. A third issue that arises in designing automated approaches is class imbalance---hate speech is usually a small percentage of the overall data---and the need to adequately upsample hate observations without model overfitting.  In our work, inspired by the recent successes in developing multi-class hate speech models that separate hate speech from offensive content, we propose , an ensemble of tunable deep learning models that leverages CNN and GRU layers. The CNN layer extracts higher-order features from the word embedding matrix that then inform the GRU layer, which extracts informative features from the sequence of words. These features are utilized for automatic detection of hate speech on social media. Our novelty lies in using a tuning procedure to adapt the model to individual dataset characteristics.   %Issues particular to developing hate speech detection models %	- Class imbalance issue %		- Hate speech is a minute portion of the overall content on social media both generally and in published datasets %	- How to adequately upsample hate observations for training without leading to model overfitting? % %	- We, like others, utilize a downsampling approach during training to ensure a class-balanced dataset passes through the model at each epoch %	- We combine this with an early stopping procedure that utilizes a validation dataset and saves the model state at the epoch with minimal validation loss % %	- These procedures, and other factors, lead to variability in resultant models   %To maintain the necessity of downsampling during training while mitigating the problems of overfitting and variability, we develop an ensemble approach for hate speech classification, extending the CNN-RNN-FC model topology that has been shown to be successful for hate speech classification.   Our major contributions can be summarized by answering the following questions.   	 We utilize existing deep model topologies to develop an ensemble classifier model for hate speech detection. An ensemble approach effectively tackles issues of class imbalance and model variability that are significant problems for automatic detection of hate speech.} 	} 	 		 We extend pretrained models by applying transfer learning to tune the classifiers to new target datasets. The tunability of our framework allows the model to adapt to new and ever-changing data.} 		 We develop a weak supervision methodology that allows our framework to train and tune entirely on unlabeled data, further extending the applicability of our model to new data.} 	 	   Summary of Results: Our best ensemble on the HON dataset achieves a 65\% F1 Macro and an 83\% hate recall, surpassing the performance on the HON dataset of current state of the art models by 33\%. We show that the ensemble models outperform individual models by an average of 5\% hate recall and 8\% F1 macro across all datasets. When applied to unlabeled Gab data, tuning improved the pretrained models by an average of 12\%, with the best tuned ensemble models achieving 57\% hate recall. Our model trained using weak supervision achieved a 67\% hate recall on posts from Gab.  % 	 	   
"," %This document is a model and instructions for \LaTeX. %This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,  %or Math in Paper Title or Abstract. Online hate speech on social media has become a fast-growing problem in recent times. Nefarious groups have developed large content delivery networks across several mainstream  and fringe outlets  to deliver cascades of hate messages directed both at individuals and communities. Thus addressing these issues has become a top priority for large-scale social media outlets. Three key challenges in automated detection and classification of hateful content are the lack of clearly labeled data, evolving vocabulary and lexicon - hashtags, emojis, etc - and the lack of baseline models for fringe outlets such as Gab. In this work, we propose a novel framework with three major contributions.  We engineer an ensemble of deep learning models that combines the strengths of state-of-the-art approaches,  we incorporate a tuning factor into this framework that leverages transfer learning to conduct automated hate speech classification on unlabeled datasets, like Gab, and  we develop a weak supervised learning methodology that allows our framework to train on unlabeled data. Our ensemble models achieve an 83\% hate recall on the HON dataset, surpassing the performance of the state of the art deep models. We demonstrate that weak supervised training in combination with classifier tuning significantly increases model performance on unlabeled data from Gab, achieving a hate recall of 67\%.",226
"  The demand for speech translation systems at meetings and lectures continues to increase. Since the length of complete sentences in such talks can be long and complicated, simultaneous speech translation is required to mimic human interpreters and translate the incoming speech stream from a source language to target language in real time. One challenge for achieving simultaneous speech translation is the development of incremental ASR.  Researchers have been working on speech recognition technology for decades. A number of techniques of real-time ASR exist, especially in the context of statistical ASR with a hidden Markov model  . However, many current state-of-the-art ASR systems rely on attention-based sequence-to-sequence deep learning frameworks . Today's attentional mechanisms are based on a global attention property that requires the computation of a weighted summarization of the entire input sequence generated by the encoder states. This means that the system can only generate text output after receiving the entire input speech sequence. Consequently, utilizing it in situations that require immediate recognition is difficult.  Several studies proposed local attention mechanisms  that limit the area explored by the attention by largely reducing the total training complexity without reducing the latency. For work that enables incremental recognition of speech, Hwang and Sung employed a unidirectional RNN with a CTC acoustic model and a unidirectional RNN language model . To avoid continuous output revision, they also proposed depth-pruning in the beam-search during the output generation. Jaitly et al. proposed a neural transducer framework  that incrementally recognizes the input speech waveforms. The formulation required inferring alignments during training, and they utilized a dynamic programming algorithm to compute ``approximate"" best alignments in each speech segments. Their model is strongly related to a sequence transducer that used connectionist temporal classification  . The improved version of a neural transducer, which has also been discussed , allows the attention mechanism to look back at many previous chunks without introducing additional latency.  However, most ISR models utilize different frameworks and learning algorithms that are more complicated than the standard ASR model. One main reason is because such models need to decide incremental steps and learn the transcription that is aligned with the current short speech segment. In this work, we propose attention-transfer ISR  by the following:    [ht]        and moves to recognize the next segment. }      
"," Attention-based sequence-to-sequence automatic speech recognition  requires a significant delay to recognize long utterances because the output is generated after receiving entire input sequences. Although several studies recently proposed sequence mechanisms for incremental speech recognition , using different frameworks and learning algorithms is more complicated than the standard ASR model. One main reason is because the model needs to decide the incremental steps and learn the transcription that aligns with the current short speech segment. In this work, we investigate whether it is possible to employ the original architecture of attention-based ASR for ISR tasks by treating a full-utterance ASR as the teacher model and the ISR as the student model. We design an alternative student network that, instead of using a thinner or a shallower model, keeps the original architecture of the teacher model but with shorter sequences . Using attention transfer, the student network learns to mimic the same alignment between the current input short speech segments and the transcription. Our experiments show that by delaying the starting time of recognition process with about 1.7 sec, we can achieve comparable performance to one that needs to wait until the end.",227
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  The following instructions are directed to authors of papers submitted to COLING-2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4   paper.  Authors from countries in which access to word-processing systems is limited should contact the publication co-chairs Fei Liu  and Liang Huang  as soon as possible.  We may make additional instructions available at \url{http://coling2020.org/}. Please check this website regularly.   
","   This document contains the instructions for preparing a paper submitted   to COLING-2020 or accepted for publication in its proceedings. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers. Authors are asked to conform to all the directions   reported in this document.",228
" In the biomedical domain, there exist several entities, such as genes, chemicals, and diseases, that are closely related to each other. Therefore, extracting the relationships among these entities is critical for biomedical research, particularly in fields such as construction of a knowledge base or drug development. Biomedical text data, including PubMed abstracts, usually contain information about biomedical entities and their relationships with each other. Thus, various natural language processing models, particularly deep learning models, are applied to biomedical text data to extract the relationships among these entities, as a kind of classi閾夸恭ation task.   ChemProt corpus  is the first corpus dataset for chemical--protein  relationship extraction, which has been conducted by BioCreative VI organizers. These organizers annotated all entity offsets of chemical and protein mentions and relationship types between chemicals and proteins . There exist 10 groups of the relationship types, and five of these  were used in the evaluation.   All models for extracting relationships from ChemProt data are designed as classifiers. In a deep learning-based multi-class classifier, the output probability distribution for each class is calculated through the Softmax function. In the training step, the model is trained to maximize the output probability of the correct class. However, some studies reported that the deep learning classifier trained with hard-labeled data  tends to become over-confident . This over-confidence does not directly affect classification performance, but it degrades the reliability of the model. In other words, the output probability of the over-confident model does not indicate how uncertain the input example is, even if its classi閾夸恭ation performance is high. Therefore, several approaches, called ``calibration'' techniques, have been applied to several domains that require high reliability, such as autonomous driving and medical diagnosis  .   In the natural language processing domain, bidirectional encoder representation from transformers   was proposed for a wide-range of language understanding. BERT is a large multi-head attention  model, which was pre-trained with a vast amount of corpus data. This pre-trained model can be easily transfer-learned and can be applied on several downstream tasks  by fine-tuning it. BERT has been used in many domains, including a biomedical field. Nevertheless, it is still important to improve the performance of BERT by applying additional techniques while using the BERT as a backbone architecture.     In this study, we propose a DNN-based approach to improve the performance of chemical--protein relationship extraction, while calibrating the classifier. More precisely, we incorporated two main calibration techniques to BERT  to improve the reliability and performance. Furthermore, we propose a semi-supervised learning workflow using the calibrated model and unlabeled in-domain data. The main contributions of our study are as follows:       
"," The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of biomedical research such as drug development and prediction of drug side effects. Several natural language processing methods, including deep neural network  models, have been applied to address this problem. However, these methods were trained with hard-labeled data, which tend to become over-confident, leading to degradation of the model reliability. To estimate the data uncertainty and improve the reliability, ``calibration'' techniques have been applied to deep learning models. In this study, to extract chemical--protein interactions, we propose a DNN-based approach incorporating uncertainty information and calibration techniques. Our model first encodes the input sequence using a pre-trained language-understanding model, following which it is trained using two calibration methods: mixup training and addition of a confidence penalty loss. Finally, the model is re-trained with augmented data that are extracted using the estimated uncertainties. Our approach has achieved state-of-the-art performance with regard to the Biocreative VI ChemProt task, while preserving higher calibration abilities than those of previous approaches. Furthermore, our approach also presents the possibilities of using uncertainty estimation for performance improvement.",229
"  Contemporary deep learning models for language have been shown to learn many aspects of natural language syntax including a number of long-distance dependencies , selectional properties of verbs , representations of incremental syntactic state  and information from which hierarchical structure can be linearly decoded .  These and many other related studies demonstrate an impressive range of human-like linguistic knowledge that is automatically acquired by these models simply from exposure to large quantities of raw text.  However, human-like grammatical abilities include not just rich and detailed linguistic knowledge but the ability to deploy this knowledge in using new words based on minimal exposure .  It remains poorly understood what grammatical generalizations contemporary deep learning models are able to make regarding the behavior of words to which they have minimal exposure. In this work, we assess the syntactic generalization behavior of a contemporary neural network model  on two novel phenomena in English  and address the question of single-shot and few-shot learning, demonstrating that BERT makes robust grammatical generalizations after fine-tuning on minimal examples of a novel token.  We test BERT's few-shot learning capabilities on two phenomena at the syntax-semantics interface: English verbal alternations, and verb/object selectional preferences. In English, verbs can appear in multiple syntactic frames; which frame a verb appears in is governed by its argument structure properties. Often, frames are paired into alternation classes  such that when English speakers hear a novel verb in one frame they can be confident that it can be used in its alternation-class pair. Using the well-attested dative alternation as an example, if a listener hears the sentence ``I daxed the tennis racket to my friend"" they would expect that ``I daxed my friend the tennis racket"" is a grammatical English sentence, meaning approximately the same thing. They would not, however, have such an expectation for ``I daxed my friend for the tennis racket."" In addition, listeners may be attuned to semantic clustering of verbal arguments based on past experience. For instance, following the example above, English speakers may expect dax to take an animate indirect object, and would find examples such as ``I daxed the court the tennis racket"" to be surprising.   We take inspiration for our testing regime from a class of psycholinguistic experiments known as `novel word learning studies', which we adapt to the neural setting. In such experiments subjects are exposed to a novel word in context during a training phase, and assessed for what grammatical generalizations they have learned about the novel word during a later testing phase. Novel word learning experiments have been used to assess human grammatical generalization since , and have been deployed to assess semantic, as well as syntactic, generalizations . %For example, in , children were shown a novel creature and told it was a wug. At test time, they were shown a picture featuring two of the creatures and described them as wugs indicating that they had categorized the novel word as a noun, and applied the productive -s pluralization to it.  Children can also learn semantic properties of a word from a single exposure . %Bayesian models of word learning have shown successes in modeling novel word learning abilities , however it is not clear how well neural network models would exhibit these rapid generalizations. Recent work has shown that sequence-to-sequence neural architectures rarely generalize systematically in the way that would be required to match human syntactic generalization behavior , but it is an open question whether we might get different behavior depending on network architecture and training objective. In this work, we replicate the novel word learning paradigm in the neural setting by fine-tuning BERT on tightly-controlled sentences that contain novel verbs and objects, and assessing the model on carefully constructed test sets that reveal what grammatical generalizations it has learned. We find that BERT is able to make proper generalizations for both verbal alternations as well as semantic clustering for verbal arguments after just one or two exposures during training.    
","  Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT's  few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb/object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively. The code for our experiments is available at \url{https://github.com/TristanThrush/few-shot-lm-learning}.",230
"  When Natural Language Processing  systems are deployed in production, and interact with users , there are many potential ways of collecting feedback data or rich interaction logs. For example, one can ask for explicit user ratings, or collect user clicks, or elicit user revisions to get an estimate of how well the deployed system is doing. However, such user interaction logs are primarily used for an one-off assessment of the system, e.g., for spotting critical errors, detecting domain shifts, or identifying the most successful use cases of the system in production. This assessment can then be used to support the decision of keeping or replacing this system in production.   From a machine learning perspective, using interaction logs only for evaluation purposes are lost opportunities for offline reinforcement learning . Logs of user interactions are gold mines for off-policy learning, and they should be put to use, rather than being forgotten after a one-off evaluation purpose.  To move towards the goal of using user interaction logs for learning, we will discuss which challenges have hindered RL from being employed in real-world interaction with users of NLP systems so far.  Concretely, our focus is on sequence-to-sequence learning for NLP applications , such as machine translation, summarization, semantic parsing or dialogue generation for chatbots, since these applications provide the richest interaction with users. For example, many machine translation services provide the option for users to give feedback on the quality of the translation, e.g. by collecting post-edits. Similarly, industrial chatbots can easily collect vast amounts of interaction logs, which can be utilized with offline RL methods.  Recent work by has recognized that the poorly defined realities of real-world systems are hampering the progress of RL in production environments. They address, amongst others, issues such as off-line learning, limited exploration, high-dimensional action spaces, or unspecified reward functions. These challenges are important in RL for control systems or robots grounded in the physical world. However, they severely underestimate the human factor when collecting feedback in systems interacting with humans, e.g. through natural language. In the following, we will thus present challenges that are encountered in user-interactive RL for NLP systems. With this discussion, we aim to  encourage NLP practitioners to leverage their interaction logs through offline RL, and  inspire  RL researchers to steel their algorithms for the challenging applications in NLP.
"," Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning  setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions.",231
"     In addition to other challenges in multiword expression  processing that were addressed in previous work, such as non-compositionality , discontinuity , and syntactic variability , The PARSEME shared task edition 1.2 has focused on another prominent challenge in detecting MWEs, namely detection of unseen MWEs. The problem with unseen data is common for many NLP tasks. While rule-based and unsupervised ML approaches are less affected by unseen data, supervised ML techniques are often found to be prone to overfitting. In this respect, the introduction of language modelling objectives to be added to different NLP tasks and their effect on generalisation have shown promising results. Further improvements brought by pre-trained language models made them a popular approach to a multitude of NLP tasks. One particular advantage of such models is that they facilitate generalisation beyond task-specific annotations .  MWEs are inherent in all natural languages and distinguishable for their syntactic and semantic idiosyncracies . Since language models are good at capturing syntactic and semantic features, we believe they are a suitable approach for modelling MWEs.    In particular, our system relies on BERT pre-trained language models .  Additionally, we render the system semi-supervised by means of multi-task learning. The most promising feature to be jointly learned with MWEs is dependency parse information . Accordingly, we fine-tune BERT for two different objectives: MWE detection and dependency parsing. MWE learning is done via token classification using a linear layer on top of BERT, and dependency parse trees are learned using dependency tree CRF network .  Our experiments confirm that this joint learning architecture is effective for capturing MWEs in most languages represented in the shared task.~    
"," This paper describes a semi-supervised system that jointly learns verbal multiword expressions  and dependency parse trees as an auxiliary task. The model benefits from pre-trained multilingual BERT.  BERT hidden layers are shared among the two tasks and we introduce an additional linear layer to retrieve VMWE tags. The dependency parse tree prediction is modelled by a linear layer and a bilinear one plus a tree CRF on top of BERT. The system has participated in the open track of the PARSEME shared task 2020 and ranked first in terms of F1-score in identifying unseen VMWEs as well as VMWEs in general, averaged across all $14$ languages.",232
"  % \gn{Title candidate: ``Detecting Hallucinated Content ...'' . I wonder if you could also run your methods over extractive summarization outputs or the true references and see how many hallucinations they detect? Just an idea.} % However, recent studies on abstractive text summarization   % and neural machine translation~ have shown that conditional neural sequence models are prone to hallucinate content that is not faithful to the input text.  This risk of generating unfaithful content impedes the safe deployment of neural sequence generation models~. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for sequence evaluation, such as BLEU scores , ROUGE  and BERTScores , do not correlate well with the faithfulness of model outputs~. They also require reference output text, limiting their applicability to detecting halluciations in a deployed system at run-time. Very recent efforts~ have started to develop automatic metrics to measure the faithfulness of output sequences. These methods use external semantic models, e.g. the question-generation and question-answering systems~ or textual entailment inference models, to score faithfulness tailored for abstract text summarization.  However, these scores do not directly measure the number of hallucinated tokens %In addition, these metrics are often tailored for the evaluation of summaries in abstract text summarization  and only correlate weakly with human judgements.  % \gn{Big question: what is the difference from word-level quality estimation, which has been around for a very long time, since at least:  and has been covered in many WMT quality estimation shared tasks . This seems more related than the works cited below, and describing why we'd need to do something new over these works would probably be a big question in the minds of anyone familiar with the MT field. Also, would the proposed methods for detecting hallucination do better than SOTA word-level QE models?}  % \gn{Similar motivation: Moreover, they do not distinguish the types of errors in terms of fluency and adequacy: a substitution error referring to a simple morphological variation  is % considered in the same way as a content word substitution changing the meaning of the sentence.~.}  We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is a hallucinated or faithful to the source input.  This task does not use the reference output to assess faithfulness, which offers us the ability to apply it in the online generation scenario where references are not available. Similar to the spirit of our proposed task, word-level quality estimation~ in the machine translation community predicts if tokens are correctly translated based on human post-editing. However, they do not distinguish errors in terms of fluency and adequacy~.  % A substitution error referring to a simple morphological variation  is considered the same as a content word substitution changing the meaning of the sentence.~.  In contrast to estimating the amount of human post-editing work required to fix errors, we specifically focus only on hallucination  errors.  We measure hallucination for two conditional sequence generation tasks -- abstractive summarization and machine translation . For the former, we produce a benchmark dataset from recently released annotations ~. For MT, we carefully design the human assessment guideline and create high-quality annotations. We will also release our human annotated data for future research. To learn token-level hallucination prediction for general conditional sequence generations tasks, we propose a novel method that creates synthetic ``hallucinated"" data and finetunes a pretrained language model~ on it. Without any human annotated supervised training data, we achieve an average F1 of around 0.6 across all the benchmark datasets, setting initial performance levels for this new task. % } We also show that pretraining on MT can actually produce more faithful translations, confirming recent findings in abstractive summarization~.  Predicting hallucination labels at token-level provides a tool for diagnosing and interpreting model outputs, which allows us to flag potential risks at inference time for previously unseen inputs. On the other hand, the token-level labels also allow for fine-grained controls over the target sequence during learning full translation models.  We show how to use these token-level hallucination labels in two case studies to improve self-training and learning from noisy mined bitext in low-resource MT. In both cases, there can be noise in the target text, either produced by the self-training teacher or mining errors. However, most outputs are only partially hallucinated  and the rest of the output is still useful for training, as we show by introducing different token-level loss truncation schemes. %To further benefit self-training, we filter out the noisy part and also glean useful part of model predictions by applying token-level loss truncation or control of information flows to the target sequence at training time.  Our best methods outperform strong baselines by a large margin both in translation quality and hallucination reduction.            % 
"," Neural sequence models can generate highly fluent sentences but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input, which can cause a lack of trust in the model. To better assess the faithfulness of the machine outputs, we propose a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. We also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations.   Experiments on machine translation and abstract text summarization demonstrate the effectiveness of our proposed approach -- we obtain an average F1 of around 60 across all the benchmark datasets. Furthermore, we demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in the low-resource machine translation and achieve significant improvements over strong baseline methods. We will release our annotated data and code to support future research.",233
"  With rise in social media and e-commerce websites, there is a huge interest in analyzing these networks for tasks like link prediction, recommendation, community detection, etc. Traditionally, this is done by learning finite-dimensional vector embeddings/representations  for nodes in these networks and then used it for downstream tasks. One of the challenges is that the quality of these learned representation decreases if the network has many missing links. This affects its performance in downstream tasks. This can be addressed by using attribute similarity of nodes as connected usually have similar attributes. For example, in citation networks, papers on related works will cite each other, and in social media, people with similar interest follow each other. In real-world graphs, nodes of these networks themselves contain rich textual information  as attributes. So, we need techniques which can exploit this textual information while learning node embeddings. The representation learning of textual networks deals with this problem.    An adversarial technique for attributed network representation learning. Here, in addition to the supervision from training data, a discriminator using text embeddings is used to give supervision to structure embeddings.  A novel text embedding learning technique which uses both mutual and topological attention.  Extensive comparative study on downstream tasks of link prediction and node classification.  Experiments on link prediction on unseen nodes.    \iffalse We have evaluated our proposed method on three datasets Cora, Zhihu, and Hepth for link prediction. We observed that our model performs better than state-of-the-art methods in almost all settings in all three datasets. The performance of our model is especially high in low data regime. In Zhihu dataset, our model show a performance improvement of  over the previous state-of-the-art in the lowest supervision setting. A similar observation was made on the node classification task on Cora dataset, where our adversarial technique achieve state-of-the-art performance. As we mentioned earlier, the main advantage of this model is its ability to the care of representation learning in unseen nodes. We evaluated the quality of these embeddings in link prediction task for edges involving unseen nodes, and ACNE achieves state-of-the-art performance for all settings in all three datasets. On Zhihu dataset, it gave an impressive improvement of   improvement over previous methods in the low-data regime.  \fi  \iffalse  \fi  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," \label{section:abstract}  Representation learning of textual networks poses a significant challenge as it involves capturing amalgamated information from two modalities:  underlying network structure, and  node textual attributes. For this, most existing approaches learn embeddings of text and network structure by enforcing embeddings of connected nodes to be similar. Then for achieving a modality fusion they use the similarities between text embedding of a node with the structure embedding of its connected node and vice versa. %Then for achieving modality fusion they model intra-modal similarities involving networks structure and textual attributes of  nodes in an edge.  This implies that these approaches require edge information for learning embeddings and they cannot learn embeddings of unseen nodes. In this paper we propose an approach that achieves both modality fusion and the capability to learn embeddings of unseen nodes. The main feature of our model is that it uses an adversarial mechanism between text embedding based discriminator, and structure embedding based generator to learn efficient representations. Then for learning embeddings of unseen nodes, we use the supervision provided by the text embedding based discriminator. In addition this, we propose a novel architecture for learning text embedding that can combine both mutual attention and topological attention mechanism, which give more flexible text embeddings. Through extensive experiments on real-world datasets, we demonstrate that our model makes substantial gains over several state-of-the-art benchmarks. In comparison with previous state-of-the-art, it gives up to 7\% improvement in performance in predicting links among nodes seen in the training and up to 12\% improvement in performance in predicting links involving nodes not seen in training. Further, in the node classification task, it gives up to 2\% improvement in performance.",234
"  Streaming Automatic Speech Recognition  researches have made their way into our everyday products. Smart speakers can now transcribe utterances in a streaming fashion, allowing users and downstream applications to see instant output in terms of partial transcriptions. There is a growing interest in the community to develop end-to-end  streaming ASR models, because they can transcribe accurately and run compactly on edge devices. Amongst these streaming E2E models, Recurrent Neural Network Transducer  is a candidate for many applications. RNN-T is trained with a loss function that does not enforce on the temporal alignment of the training transcripts and audio. As a result, RNN-T suffers from token emission delays - time from when the token is spoken to when the transcript of the token is emitted. Delayed emissions of tokens adversely affects user experiences and downstream applications such as the end-pointer.   Some existing work tried to mitigate the token emission delays in streaming RNN-Ts. We introduce them in Section. Other works utilized semi-streaming or non-streaming models to predict better token emission time, at the cost of the overall latency of the transcripts. In this work, we propose a novel loss function for streaming RNN-T, and the resultant trained model is called Alignment Restricted RNN-T . It utilizes audio-text alignment information to guide the loss computation. In Section, we show that theoretically, Ar-RNN-T loss function is faster to compute and results in better audio-token alignment. In Section, we empirically compare our proposed method with existing works such as monotonic RNN-T training on two data set: LibriSpeech and voice command. In the results section, Section, we show improvement in training speed and that when used in tandem with an end-pointer, Ar-RNN-T provides an unprecedentedly refined control over the latency-WER trade-offs of RNN-T models.   
"," There is a growing interest in the speech community in developing Recurrent Neural Network Transducer  models for automatic speech recognition  applications. RNN-T is trained with a loss function that does not enforce temporal alignment of the training transcripts and audio. As a result, RNN-T models built with uni-directional long short term memory  encoders tend to wait for longer spans of input audio, before streaming already decoded ASR tokens. In this work, we propose a modification to the RNN-T loss function and develop Alignment Restricted RNN-T  models, which utilize audio-text alignment information to guide the loss computation. We compare the proposed method with existing works, such as monotonic RNN-T, on LibriSpeech and in-house datasets. We show that the Ar-RNN-T loss provides a refined control to navigate the trade-offs between the token emission delays and the Word Error Rate . The Ar-RNN-T models also improve downstream applications such as the ASR End-pointing by guaranteeing token emissions within any given range of latency. Moreover, the Ar-RNN-T loss allows for bigger batch sizes and 4 times higher throughput for our LSTM model architecture, enabling faster training and convergence on GPUs.",235
"  .     %      % % final paper: en-us version      %       % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. }     
","   Interpretability and explainability of deep neural networks are challenging due to their scale, complexity, and the agreeable notions on which the explaining process rests. Previous work, in particular, has focused on representing internal components of neural networks through human-friendly visuals and concepts. On the other hand, in real life, when making a decision, human tends to rely on similar situations and/or associations in the past. Hence arguably, a promising approach to make the model transparent is to design it in a way such that the model explicitly connects the current sample with the seen ones, and bases its decision on these samples.   Grounded on that principle, we propose in this paper an explainable, evidence-based memory network architecture, which learns to summarize the dataset and extract supporting evidences to make its decision. Our model achieves state-of-the-art performance on two popular question answering datasets . Via further analysis, we show that this model can reliably trace the errors it has made in the validation step to the training instances that might have caused these errors. We believe that this error-tracing capability provides significant benefit in improving dataset quality in many applications.",236
"  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  . }  Word segmentation is a fundamental NLP analysis problem for written languages with no space delimiters between words such as Chinese and Japanese.  In the age of digital communications, new URLs  and hashtags , which often include strings of concatenated words  are being added every day to a growing set of tokens that an NLP system may need to deal with, and they pose challenges for language and speech applications. For example, a Text-to-Speech  synthesis system will struggle to pronounce these concatenated tokens, since simply applying a grapheme-to-phoneme system out of the box to something like  will usually yield poor results. This suggests the need for a model that can split such tokens into the component words.  So-called ``end-to-end'' neural TTS systems , which learn to map directly from character sequences to speech might seem to hold out the hope of avoiding treating this problem separately. However, the fact that URLs occur relatively rarely in most TTS training data limits the promise of such models on this long-tail problem.   The problem of analyzing URLs does differ in one useful way from more general text normalization problems. For a token such as  in a text, one typically needs to know what context it occurs in in order to know how to read it: is it  or ; see , inter alia. In the case of URLs, these are largely  since the output segmentation is usually unaffected by the surrounding words. Hence the problem can be treated as a standalone one that does not require the system to be trained as part of broader text normalization training.  Our training data comes from  camel case URLs that naturally define the segment boundaries  along with manual corrections for non-trivial boundaries.  We release our training and evaluation data sets to promote research on this problem.  By drawing an analogy with Chinese word segmentation, we cast the URL segmentation problem as a sequence tagging problem. We propose a simple Recurrent Neural Network  based tagger with an encoder and a decoder.   The model trained on the data set has a decent full sequence accuracy  but fails to generalize to more rare words due to the size of the training data. Inspired by the success of pre-training in many NLP tasks , we propose a pre-training recipe for the segmenter. Based on the observation that URLs are often compound entity names and so are knowledge graph entities , we create a large synthetic training data set by concatenating the knowledge graph entity names. We observe 21\% absolute  improvement in sequence accuracy after applying pre-training followed by fine-tuning.  % % File acl2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \pdfoutput=1  \documentclass[11pt,a4paper]{article} \usepackage{coling2020} \usepackage{times} \usepackage{latexsym} \usepackage{graphicx} \usepackage{amsmath} {\ttfamily \usepackage{multirow} \usepackage{url}    % %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  [1]{{{#1}}}  [1]{{{#1}}}  \TeX} \title{Semi-supervised URL Segmentation with Recurrent Neural Networks Pre-trained on Knowledge Graph Entities} \author{Hao Zhang \and Jae Ro \and Richard Sproat \\         Google Research \\          @google.com}} \date{}     Breaking domain names such as  into component words  and  is important for applications like Text-to-Speech synthesis and web search. We link this problem to the classic problem of Chinese word segmentation and show the effectiveness of a tagging model based on Recurrent Neural Networks  using characters as input. To compensate for the lack of training data, we propose a pre-training method on concatenated entity names in a large knowledge database. Pre-training improves the model by 33\% and brings the sequence accuracy to 85\%.        
"," Breaking domain names such as openresearch into component words open and research is important for applications like Text-to-Speech synthesis and web search. We link this problem to the classic problem of Chinese word segmentation and show the effectiveness of a tagging model based on Recurrent Neural Networks  using characters as input. To compensate for the lack of training data, we propose a pre-training method on concatenated entity names in a large knowledge database. Pre-training improves the model by 33\% and brings the sequence accuracy to 85\%.",237
" .     %       % final paper: en-us version         % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. } Discourse parsing is an important upstream task within the area of Natural Language Processing   which has been an active field of research over the last decades. In this work, we focus on discourse representations for the English language, where most research %on the discourse analysis of English language  has been surrounding one of the two main theories behind discourse, the Rhetorical Structure Theory  proposed by  or interpreting discourse according to PDTB . While both theories have their strengths, the application of the RST theory, encoding documents into complete constituency discourse trees , has been shown to have many crucial implications on real world problems. A tree is defined on a set of EDUs , approximately aligning with clause-like sentence fragments, acting as the leaves of the tree. Adjacent EDUs or sub-trees are hierarchically aggregated to form larger  constituents, with internal nodes containing  a nuclearity label, defining the importance of the subtree  in the local context and  a relation label, defining the type of semantic connection between the two subtrees . In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization . More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular  BERT approach . Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis .  Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically inspired approaches for downstream applications has not been unleashed yet. The main open challenges of integrating discourse into more NLP downstream tasks and to deliver even greater benefits have been a combination of  discourse parsing being a difficult task itself, with an inherently high degree of ambiguity and uncertainty and  the lack of large-scale annotated datasets, rendering the initial problem more severe, as data-driven approaches cannot be applied to their full potential.  The combination of these two limitations has been one of the main reasons for the limited application of neural discourse parsing for more diverse downstream tasks. While there have been neural discourse parsers proposed , they still cannot consistently %strongly  outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches.  %due to the extra effort to integrate discourse trees into models as well as two major problems, the big breakthrough in the usage of discourse parsing has still not happened.   In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT ``silver-standard"" discourse treebank published by  containing over 250,000 discourse annotated documents from the Yelp'13 sentiment dataset , nearly three orders of magnitude larger than commonly used RST-style annotated discourse treebanks . Given this new dataset with a previously unseen number of full RST-style discourse trees, we revisit the task of neural discourse parsing, which has been previously attempted by   and others with rather limited success. We believe that one reason why previous neural models could not yet consistently outperform more traditional approaches, heavily relying on feature engineering , is the lack of generalisation when using deep learning approaches on the small RST-DT dataset, containing only 385 discourse annotated documents. This makes us believe that using a more advanced neural discourse parser in combination with a large training dataset can lead to significant performance gains. %, but also across datasets, capturing more general discourse phenomena and avoiding potential overfitting on the training corpus. Admittedly, even though MEGA-DT contains a huge number of datapoints to train on, it has been automatically annotated, potentially introducing noise and biases, which can negatively influence the performance of our newly proposed neural discourse parser when solely trained on this dataset. A natural and intuitive approach to make use of the neural discourse parser and both datasets  is to combine them during training, pretraining on the large-scale ``silver-standard"" corpus and subsequently fine-tuning on RST-DT or further human annotated datasets. This way, general discourse structures could be learned from the large-scale treebank and then enhanced with human-annotated trees. With the results shown in this paper strongly suggesting that our new discourse parser can encode discourse more effectively, we hope that our efforts will prompt researchers to develop more linguistically inspired applications based on our discourse parser. % for downstream models in the area of NLP.  Our contributions in this paper are: %     % We propose a novel neural discourse parsing architecture which combines multiple lines of previous work in a single framework.     % We combine %the two treebanks  a large-scale ``silver-standard"" treebank  with small, domain-specific gold-standard treebanks in a neural way during the training process, by initially pretraining on the large  dataset and subsequently fine-tuning on the dataset within the domain itself.    We apply the neural discourse parser on %the large-scale ``silver-standard"" discourse corpus as well as small-scale gold-standard treebanks two commonly used disocurse treebanks , showing large performance improvements of our model over previous state-of-the-art approaches.% to compare the performance individually for both datasets.  % %on how to train a neural discourse parser with large scale ``silver-standard"" discourse trees. With this new approach, we  drastically increase the amount of available training data available for discourse parsers is not sufficiently large to train modern, data-driven deep learning approaches for the task, hindering the application of new methodologies and  the shift in domain between the discourse parsers training data and the domain of application deminishes the applicability and performance of generated discourse trees for any domain outside of news , instructions  and a few other domains. 
"," RST-based discourse parsing is an important NLP task with numerous downstream applications, such as summarization, machine translation and opinion mining. In this paper, we demonstrate a simple, yet highly accurate discourse parser, incorporating recent contextual language models. Our parser establishes the new state-of-the-art  performance for predicting structure and nuclearity on two key RST datasets, RST-DT and Instr-DT. We further demonstrate that pretraining our parser on the recently available large-scale ``silver-standard"" discourse treebank MEGA-DT provides even larger performance benefits, suggesting a novel and promising research direction in the field of discourse analysis.",238
"  The last several years have seen a land rush in research on machine reading  comprehension and various dataset have been proposed such as SQuAD1.1, SQuAD2.0, NewsQA and CoQA . Different from the above which are extractive MRC, RACE is a multi-choice MRC dataset  proposed by . RACE was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets is that the answers in RACE often cannot be directly extracted from the passages, as illustrated by the two example questions  in Table . Thus, answering these questions needs inferences.  [t!]   }   \toprule   Passage: For the past two years, 8-year-old Harli Jordean from Stoke Newington, London, has been selling marbles . His successful marble company, Marble King, sells all things marble-related - from affordable tubs of the glass playthings to significantly expensive items like Duke of York solitaire tables - sourced, purchased and processed by the mini-CEO himself. ""I like having my own company. I like being the boss,"" Harli told the Mirror....Tina told The Daily Mail. ""At the moment he is annoying me by creating his own Marble King marbles - so that could well be the next step for him."" \\   \midrule   % \multirow{3}*{SDT-RPR-48L}   Q1: Harli's Marble Company became popular as soon as he launched it because \_\_\_ .     \\   A: it was run by ""the world's youngest CEO""      \\   B: it filled the gap of online marble trade      \\   C: Harli was fascinated with marble collection   \\   D: Harli met the growing demand of the customers \\   \midrule   Q2: How many mass media are mentioned in the passage?    \\   A: One      \\   B: Two     \\   C: Three   \\   D: Four \\         .}      % [t] % .} %  %   Recently, pretrained language models  such as BERT , RoBERTa , ALBERT  have achieved great success on MMRC tasks. Notably, Megatron-LM  which is a 48 layer BERT with 3.9 billion parameters yields the highest score on the RACE leaderboard in both single and ensemble settings. The key point to model MMRC is: first encode the context, question, options with BERT like LM, then add a matching network on top of BERT to score the options. Generally, the matching network can be various .  proposes an option comparison network  to compare options at word-level to better identify their correlations to help reasoning.  proposes a dual co-matching network  which models the relationship among passage, question and answer options bidirectionally. All these matching networks show promising improvements compared with pretrained language models. One point they have in common is that the answer together with the distractors are jointly considered which we name multi-choice models. We argue that the options can be concerned separately for two reasons, 1) when human works on MMRC problem, they always consider the options one by one and select the one with the highest confidence. 2) MMRC suffers from the data scarcity problem. Multi-choice models are inconvenient to take advantage of other MRC dataset.   In this paper, we propose a single-choice model for MMRC. Our model considers the options separately. The key component of our method is a binary classification network on top of pretrained language models. For each option of a given context and question, we calculate a confidence score. Then we select the one with the highest score as the final answer. In both training and decoding, the right answer and the distractors are modeled independently. Our proposed method gets rid of the multi-choice framework, and can leverage amount of other resources. Taking SQuAD as an example, we can take a context, one of its question and the corresponding answer as a positive instance for our classification with golden label 1. In this way many QA dataset can be used to enhance RACE. Experimental results show that single-choice model performs better than multi-choice models, in addition by transferring knowledge from other QA dataset, our single model achieves 90.7\% and ensemble model achieves 91.4\%, both are the best score on the leaderboard.     
"," Multi-choice Machine Reading Comprehension  aims to select the correct answer from a set of options based on a given passage and question. Due to task specific of MMRC, it is non-trivial to transfer knowledge from other MRC tasks such as SQuAD, Dream. In this paper, we simply reconstruct multi-choice to single-choice by training a binary classification to distinguish whether a certain answer is correct. Then select the option with the highest confidence score. We construct our model upon ALBERT-xxlarge model and estimate it on the RACE dataset. During training, We adopt AutoML strategy to tune better parameters. Experimental results show that the single-choice is better than multi-choice. In addition, by transferring knowledge from other kinds of MRC tasks, our model achieves a new state-of-the-art results in both single and ensemble settings.",239
"  % Images  are another important approach for expressing feelings and emotions in addition to using text in communication. In mobile messaging apps, these images can generally be classified into emojis and stickers. Emoji is a kind of small picture which is already stored in most of the keyboard of the mobile operational systems,  , for sticker response selection in multi-turn dialog. Specifically, SRS first learns representations of dialog context history using a self-attention mechanism and learns the sticker representation by a convolutional neural network .  % % % Next, SRS conducts deep matching between the sticker and each utterance and produces the interaction results for every utterance. % % Finally, SRS employs a fusion network which consists of a sub-network fusion RNN and fusion transformer to learn the short and long term dependency of the utterance interaction results. The final matching score is calculated by an interaction function. To evaluate the performance of our model, we propose a large number of multi-turn dialog dataset associated with stickers from one of the popular messaging apps.  Extensive experiments conducted on this dataset show that SRS significantly outperforms the state-of-the-art baseline methods in commonly-used metrics.  % However, the user's sticker selection does not only depend on the matching degree between dialog context and candidate sticker image, but also depends on the user's preference of using sticker. When users decide to use a sticker as their response in multi-turn dialog, they may choose their favorite one from all appropriate stickers as the final response.  % % % We assume that user tends to use the recently used sticker in their dialog history, and the recently-used-sticker can represent the user's preference of sticker selection. An example is shown in Figure. To verify this assumption, we retrieve 10 recently-used-stickers of each user and calculate the proportion of whether the currently used sticker appeared in these 10 stickers. The result shows that 54.09\% of the stickers exist in the 10 recently used sticker set. Hence, we reach to the conclusion that users have strong personal preference when selecting the sticker as their response for the current dialog context. However, in some cases, this also indicates a tendency to re-use stickers, but not necessarily a preference.  % Motivated by this observation, in this work, we take one step further and improve our previously proposed SRS framework with user preference modeling. Overall, we propose a novel sticker recommendation model which considers the user preference, namely  . Specifically, PESRS first employs a convolutional network to extract features from the candidate stickers. Then, we retrieve the recent user sticker selections then a user preference modeling module is employed to obtain a user preference representation. Next, we conduct the deep matching between the candidate sticker and each utterance as the same as SRS. Finally, we use a gated fusion method to combine the deep matching result and user preference into final sticker prediction.    % The key to the success of PESRS lies in how to design the user preference modeling module, which should not only identify the user's favorite sticker and but also consider the current dialog context. % Motivated by this, we first propose a recurrent neural network  based position-aware sticker modeling module which encodes the recently used stickers in chronological order. Then, we employ a key-value memory network to store these sticker representations as values and the corresponding dialog context as keys. Finally, we use the current dialog context to query the key-value memory and obtain the dynamic user preference of the current dialog context.  % We empirically compare PESRS and SRS on the public dataset\footnote{https://github.com/gsh199449/stickerchat} proposed by our early work. This is a large-scale real-world Chinese multi-turn dialog dataset, which dialog context is multiple text utterances and the response is a sticker image. Experimental results show that on this dataset, our newly proposed PESRS model can significantly outperform the existing methods.  Particularly, PESRS yields 4.8\% and 7.1\% percentage point improvement in terms of  and  compared with our early work SRS. % In addition to the comprehensive evaluation, we also evaluate our proposed user preference memory by a fine-grained analysis. The analysis reveals how the model leverages the user's recent sticker selection history and provides us insights on why they can achieve big improvement over state-of-the-art methods.  This work is a substantial extension of our previous work reported at WWW 2020.  The extension in this article includes the user preference modeling framework for the existing methods, a proposal of a new framework for sticker selection in the multi-turn dialog. Specifically, the contributions of this work include the following:         The rest of the paper is organized as follows: We summarize related work in \S.  \S introduces the data collection method and some statistics of our proposed multi-turn dialog sticker selection dataset. We then formulate our research problem in \S  and elaborate our approach in \S.  \S gives the details of our experimental setup and \S presents the experimental results.  Finally, \S concludes the paper.   % 
","   Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching the stickers image with previous utterances.   However, existing methods usually focus on measuring the matching degree between the dialog context and sticker image, which ignores the user preference of using stickers.   Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context and sticker using history of user.   Two main challenges are confronted in this task.   One is to model the sticker preference of user based on the previous sticker selection history.   Another challenge is to jointly fuse the user preference and the matching between dialog context and candidate sticker into final prediction making.   To tackle these challenges, we propose a   model.   Specifically, PESRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances.   Next, deep interaction network is proposed to conduct deep matching between the sticker and each utterance.   Then, we model the user preference by using the recently selected stickers as input, and use a key-value memory network to store the preference representation.   PESRS then learns the short-term and long-term dependency between all interaction results by a fusion network, and dynamically fuse the user preference representation into the final sticker selection prediction.   Extensive experiments conducted on a large-scale real-world dialog dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics.   Experiments also verify the effectiveness of each component of PESRS.   %",240
"  .}  Neural machine translation  has boosted machine translation significantly in recent years . However, it is still unclear how NMT models work due to the black-box nature of neural networks. Better understandings of NMT models could guide us in improving NMT systems. Currently most of the studies towards understanding NMT models only take into account subword-based  models.  Deeper character-based  models have been shown to perform better than BPE-based models . In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism.   Previous studies have tried to interpret and understand NMT models by interpreting attention weights , using gradients , applying layer-wise relevance propagation , probing classification tasks , and more intrinsic analysis .  However, only  have probed character-based representations.  have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in . We apply more composition methods to explore how CHAR models learn linguistic knowledge and how attention extracts features directly from characters.   Probing classification tasks  have emerged as a popular method to interpret the internal representations from neural networks. Given a probing classifier, the input is usually the representation of a word and the output is the corresponding linguistic tag.  CHAR models pose new challenges for interpretability, and we investigate whether we can probe CHAR models in a way similar to word-based models. In addition, can we extract word sense and morphological information about the full word from individual hidden states, or is this information distributed across multiple states? This has implications for interpreting neural CHAR models, but can also inform novel architectures, such as sparse attention mechanisms.  Thus we first investigate the ability of CHAR models to learn word senses and morphology in Section .  We apply different methods to compose information from characters and demonstrate that the word-level information is distributed over all the characters but characters at different positions play different roles in learning linguistic knowledge.  We also explore the effect of encoder depth to answer why CHAR models outperform BPE-based models only when they have the settings with deeper encoder. The probing results show that CHAR models need more layers to learn word senses.  Then in Section , we move on to explore the attention mechanism. The distribution pattern shows that separators attract much more attention compared to other characters.  To study the effect of enforcing characters to capture the full word-level information, we investigate a sparse attention mechanism, i.e. a model that only attends to separators, which can be viewed as a word-level attention. The BLEU score drops 1.2 points when we apply the word-level sparse attention. This implies that only attending to separators by a single attention head is workable but not enough to extract all the necessary information.   The main findings are summarized as follows:  [noitemsep,topsep=0pt]      
"," Recent work has shown that deeper character-based neural machine translation  models can outperform subword-based models. However, it is still unclear what makes deeper character-based models successful. In this paper, we conduct an investigation into pure character-based models in the case of translating Finnish into English, including exploring the ability to learn word senses and morphological inflections and the attention mechanism. We demonstrate that word-level information is distributed over the entire character sequence rather than over a single character, and characters at different positions play different roles in learning linguistic knowledge. In addition, character-based models need more layers to encode word senses which explains why only deeper models outperform subword-based models. The attention distribution pattern shows that separators attract a lot of attention and we explore a sparse word-level attention to enforce character hidden states to capture the full word-level information. Experimental results show that the word-level attention with a single head results in 1.2 BLEU points drop.",241
"  A prerequisite relation is a pedagogical relation that indicates the order in which concepts can be presented to learners. The relation can be used to guide the presentation sequence of topics and subjects during the design of academic programs, lectures, and curricula or instructional materials. %such as textbooks and study guides.   In this work, we present our systems to automatically detect prerequisite relations for Italian language in the context of the PRELEARN shared task  at EVALITA  2020 . %.  The evaluation of submissions considers:  in-domain and cross-domain scenarios defined by either the inclusion  or exclusion  of the target domain in the training set. The four domains are 'data mining' , 'geometry' , 'precalculus' , and 'physics' .  the type of resources  used to train the model -- raw text VS. structured information.  % four domains, namely 'data mining', 'geometry', 'precalculus' and 'physics'.    % PRELEARN participants had to submit their systems in a per-domain evaluation, considering in-domain and cross-domain scenarios,  % as well as discriminate between the kind of resources the models used, namely raw text such as distributional textual corpora, and structured information such as knowledge bases. % Additionally, the difference between in-domain and cross-domain lies in the inclusion or exclusion of the target domain in the training set.  The combination of these settings defined the four PRELEARN subtasks.  Formally, a prerequisite relation exists between two concepts if one has to be known beforehand in order to understand the other.  For the PRELEARN task, given a pair of concepts, the relation exists only if the latter concept is a prerequisite for the former.  Therefore, the task is a binary classification task.          We approach the problem from two perspectives: handcrafted features based on lexical complexity and pre-trained embeddings. We employed static embeddings from Wikipedia and Wikidata, and contextual embeddings from Italian-BERT model.  
",   English.   We present our systems and findings for the prerequisite relation learning task  at EVALITA 2020. The task aims to classify whether a pair of concepts hold a prerequisite relation or not. We model the problem using handcrafted features and embedding representations for in-domain and cross-domain scenarios.  Our submissions ranked first place in both scenarios with average F1 score of $0.887$ and $0.690$ respectively across domains on the test sets. We made our code freely available\footnote{\url{https://github.com/ajason08/EVALITA2020_PRELEARN}\label{code}}.,242
" Task-oriented dialog systems are commonplace in automated systems that interact with end users, including digital assistants, technical support agents, and various website navigation helpers. An essential part in any task-oriented dialog system is  , which consumes data, typically fed in the form of a , and converts it into natural language output to be served to the end user. The natural language response of the NLG component should 1) contain all essential information, 2) be contextualized around the user request, and 3) be natural sounding. Such a system requires consideration for content planning, correctness, grammaticality, and naturalness.  NLG systems employed in commercial settings are typically based on template-based text generation techniques . In these, humans author a minimal set of responses templates with placeholder slot values. These slots are later filled at runtime, with the dialog input. Although template-based NLG modules are appealing due to their deterministic nature,  inherent correctness, and low latency, they have major drawbacks: First, separate templates need to be authored for different response variations; this behavior is unfavorable for scaling. Second, templates authored for a particular domain are commonly not reusable.  Lastly, no matter the complexity of the language instilled into templates, they form a strictly discrete set of responses, and therefore are bound to be limited in their response naturalness.  More recently, advances in neural-network-based  language generation prompted a new direction in NLG research . The process is typically split into two steps:  serialization of input data into a flattened meaning representation , and  using the neural generation model to generate a natural language response conditioned on the MR. The models are trained on data that includes MR, response pairs, and therefore they are able to not only generate desired responses for MRs in their training data, but they are also expected to form coherent responses for novel MRs, owing to the generalization ability of their machine learning  backbone.  However, deploying neural NLG systems in an industry setting is quite challenging. First, it is not trivial to train a model that reliably presents its input data with the high fidelity required from a user-serving dialog system. Second, the models require much high-quality human-annotated data, which is resource intensive. Consequently, data annotation is a major limiting factor for scaling model-based NLG across domains and languages.  In this work, we detail our approach to production-level neural NLG, with a focus on scalability and data efficiency. Adopting the tree-structured MR framework introduced in Balakrishnan et al.~, which allows better control over generated responses, we train sequence-to-sequence RNN models  that can produce high-fidelity responses. We then employ a multitude of techniques for reducing the amount of  required data, primarily powered by eliminating the ``hidden'' redundancy by grouping data points with similar semantics into . We train models either on the reduced data, or after increasing the size of the dataset using a novel synthetic augmentation technique. We also employ large, pre-trained attention-based language models, fine-tuning them on the same datasets, and then using novel methods to distill their knowledge into smaller sequence-to-sequence models. Further, we train models on data from multiple domains, showing gains over models trained on individual domains when the domains are semantically close together. We conclude with a compiled list of best practices for production-level NLG model development based on our analyses, and we present it as a runbook.  
"," Natural language generation  is a critical component  in conversational systems, owing to its role of formulating a correct and natural text response. Traditionally, NLG components have been deployed using template-based solutions. Although neural network solutions recently developed in the research community have been shown to provide several benefits, deployment of such model-based solutions has been challenging due to high latency, correctness issues, and high data needs.  In this paper, we present approaches that have helped us deploy data-efficient neural solutions for NLG in conversational systems to production.  We describe a family of sampling and modeling techniques to attain production quality with light-weight neural network models using only a fraction of the data that would be necessary otherwise, and show a thorough comparison between each. Our results show that domain complexity dictates the appropriate approach to achieve high data efficiency. Finally, we distill the lessons from our experimental findings into a list of best practices for production-level NLG model development, and present them in a brief runbook. Importantly, the end products of all of the techniques are small sequence-to-sequence models  that we can reliably deploy in production.",243
"  Pre-training has been demonstrated as a highly effective method for boosting the performance of many natural language processing  tasks such as question answering, sentimental analysis, and so on. By training on massive unlabeled text data, pre-trained models are able to learn the contextual representations of input words, which are extremely helpful for accomplishing downstream tasks.  BERT , as one of the most widely used pre-trained models, is trained using two unsupervised tasks, namely, mask language modeling and next sentence prediction. By adding a few layers on top, BERT can be easily adapted into a task-specific model, which is then fine-tuned on the labeled data to achieve optimal performance. Such a practice has been exercised in various NLP scenarios and has achieved many state-of-the-art  results.  The study of integrating BERT into neural machine translation models, which is referred to as BERT-enhanced NMT, has received much research interest. However, exploiting BERT for NMT is not as straightforward as in other NLP tasks. The architecture of a typical NMT model consists of an encoder that transforms the source language words into a hidden representation, and a decoder that predicts the target language words based on the hidden representation. The challenge of exploiting BERT for NMT is twofold. Firstly, NMT models are mostly deep neural networks with a parameter size comparable to or even larger than that of BERT, which makes the combined model hard to optimize. Secondly, since existing NMT models are mostly trained with massive samples, the usual practice of fine-tuning BERT on the labeled corpus can lead to the problem of catastrophic forgetting .  [!t]              The recently proposed BERT-fused model  uses attention mechanisms to bridge between the NMT model and BERT. For example, they introduce an extra BERT-encoder attention module to fuse the encoder layer with the BERT representation. The outputs of the BERT-encoder attention module and the self-attention module are averaged.  Consider the case exemplified in  , it's more likely that the word  should be interpreted as  rather than other meanings in this context. However, if the training corpus doesn't contain similar expressions, the model can fail in this translation due to the ambiguity. When BERT representations are introduced, the contextual information learned by BERT can be helpful for the translation. Concretely, a BERT-encoder attention module can be used to capture the pre-trained knowledge embedded in the BERT representation that is absent in the self-attention module.   However, we find that averaging their outputs means regarding them as equally important, which can hurt performance under some circumstances. In the above example, only the BERT-encoder attention module provides useful information for interpreting the word , while the self-attention module offers faulty or noisy information. Combining their outputs directly can result in confusion during translation.  Hence we assert that it's essential to allow the model to decide which information to concentrate on. To this end, we propose to use a joint-attention module to integrate multiple representations that contain different contextual information. As shown in  , the learnable weights of the joint-attention module allow it to assign more attention to the BERT representation in this case. Compared with the BERT-fused model, our method is better at augmenting desired information and hence boosts performance.   Although existing BERT-enhanced NMT models mostly focus on leveraging BERT's last-layer representation, we find that the intermediate layers can contain semantic and contextual information that is absent in the last layer and might help improve translation performance. The dynamic fusion mechanism proposed by  allows the Transformer encoder to leverage BERT's intermediate representations. However, their method doesn't work for the decoder at the inference stage because it requires the ground truth as input. This motivates us to explore feasible techniques for generating composite BERT representations that can be used in both the encoder and the decoder.  In this paper, we introduce a BERT-enhanced NMT model called BERT-JAM, which stands for BERT-fused Joint-Attention Model. BERT-JAM is equipped with joint-attention modules that allow the encoder/decoder to selectively concentrate on the BERT representation or the encoder/decoder representation by attending to them simultaneously. Besides, we seek to improve upon the existing BERT-enhanced models by making better use of BERT's intermediate layers. Specifically, we allow each encoder/decoder layer to use a GLU module to transform BERT's intermediate representations into a composite representation used by the joint-attention module.   In order to achieve optimal performance, we train BERT-JAM following a three-phase optimization strategy which progressively unfreezes different components of the model during training.  We show that fine-tuning BERT is a crucial step to unearth the full potential of BERT-JAM, in contrast to the previous claim that fine-tuning BERT offers few gains  for NMT models. Moreover, we study how the BERT-enhanced NMT performance varies with the size of BERT by feeding different BERT models into BERT-JAM, ranging from the most compact BERT with 2 layers and embedding dimension 128 to the standard BERT-base model. This study can be beneficial because it can provide us with a guide on how to adjust the model with minimal performance loss when we have to resort to a smaller model size due to limited computation resources.    We summarize the contributions of this paper as follows:        The rest of this paper is organized as follows.  In , we introduce our approach to BERT-enhanced NMT where a detailed description of our model will be presented.  The experimental setups are described in .  In , several experiments are conducted and the results are discussed.  We give a review of related works in  and the conclusions are drawn in .   
"," BERT-enhanced neural machine translation  aims at leveraging BERT-encoded representations for translation tasks. A recently proposed approach uses attention mechanisms to fuse Transformer's encoder and decoder layers with BERT's last-layer representation and shows enhanced performance. However, their method doesn't allow for the flexible distribution of attention between the BERT representation and the encoder/decoder representation. In this work, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves upon existing models from two aspects: 1) BERT-JAM uses joint-attention modules to allow the encoder/decoder layers to dynamically allocate attention between different representations, and 2) BERT-JAM allows the encoder/decoder layers to make use of BERT's intermediate representations by composing them using a gated linear unit . We train BERT-JAM with a novel three-phase optimization strategy that progressively unfreezes different components of BERT-JAM. Our experiments show that BERT-JAM achieves SOTA BLEU scores on multiple translation tasks.",244
"   %Natural language is an abstract representation of thoughts and objects which have similar meaning through a community. We use raw sensory information to represent images. However, this is not possible for words, as they have no direct physical interpretation.  There have been many studies in natural language processing  to find suitable word representations  that carry information of a language. Even if finding these word representations can be computationally demanding, this can be advantageous since it is computed only once. These learned representations can be used for various downstream tasks.  Word2Vec  finds word embeddings by predicting a word given its neighborhood  or predicting its neighborhood given the word . Words that are used together have similar word embeddings due to the training strategy. However, these embeddings do not contain word order information and contextual information. ELMo  uses bidirectional LSTM   to predict a word given its context. Since BiLSTM is used for creating embeddings, both left-to-right and right-to-left contexts are implicitly encoded. Transformer  is shown to be more appropriate for training in large datasets due to its self-attention mechanism. OpenAI GPT  has the same objective as ELMo in the forward direction, except it uses transformer architecture. BERT  also uses transformer architecture with bidirectional pre-training tasks. Training objectives affect the information encoded in embeddings. Each objective and architecture presumes a different inductive bias.  In this work, we focused on BERT as it uses multiple training objectives. These objectives can create an inhibitory effect or a regulatory effect on each other. For this reason, we applied a hierarchical multitask learning approach to BERT by modifying its original structure. Our motivation is to create embeddings that encode the information from each task in a balanced way. Our contributions are as follows:      Our experimental results show that Lower NSP has a competitive performance when compared with the original BERT structure. We also evaluate the learned embeddings on probing tasks to provide useful insights into training strategies. Results on probing task experiments show that using bigram shift task for pre-training is useful for specific tasks. The remaining part of this paper is organized as follows. In Section , we mention related works. In Section , we explain our methods in detail. In Section , we report our experiment results. Lastly, we give a conclusion in Section .  
"," Recent works show that learning contextualized embeddings for words is beneficial for downstream tasks. BERT is one successful example of this approach. It learns embeddings by solving two tasks, which are masked language model  and the next sentence prediction . The pre-training of BERT can also be framed as a multitask learning problem. In this work, we adopt hierarchical multitask learning approaches for BERT pre-training. Pre-training tasks are solved at different layers instead of the last layer, and information from the NSP task is transferred to the masked LM task. Also, we propose a new pre-training task bigram shift to encode word order information. We choose two downstream tasks, one of which requires sentence-level embeddings , and the other requires contextualized embeddings of words . Due to computational restrictions, we use the downstream task data instead of a large dataset for the pre-training to see the performance of proposed models when given a restricted dataset. We test their performance on several probing tasks to analyze learned embeddings. Our results show that imposing a task hierarchy in pre-training improves the performance of embeddings.",245
" Definitions have a very important role in scientific literature because they define the major concepts with which an article operates. They are used in many automatic text analysis tasks, such as question answering, ontology matching and construction, formal concept analysis, and text summarization. Intuitively, definitions are basic building blocks of a scientific article that are used to help properly describe hypotheses, experiments, and analyses. It is often difficult to determine where a certain definition lies in the text because other sentences around it may have similar style.  Automatic definition extraction  is an important field in natural language processing  because it can be used to improve text analysis and search.  %Natasha: here adding formal definition of formal definitions Definitions play a key role in mathematics, but their creation and use differ from those of   definitions. A comprehensive study is given in a series of works by Edwards and Ward~, , , %, and    inspired by writings of Richard Robinson~ and lexicographer Sidney Landau~. %They distinguish between extracted definitions that report usage and have a truth value~, and stipulated  that create usage and create concepts but have no truth value. % Nat - fixed sentence %Moreover, in a stipulated definition a term has to be free from all the associations it acquired in its non-technical use.   %For example, ""Suppose that student is a person enrolled into an academic institution"" is a stipulated definition while    Mathematical definitions frequently have a history as they evolve over time. The definition we use for function, for instance, may not be the one that was used a hundred years ago. % Nat - fixed sentence The concept of connectivity has two definitions, one for path connectivity and another for set-theoretic connectivity. In mathematical texts the meaning of the defined concept is not determined by its context but it is declared and is expected to have no variance within that specific mathematical text~. % Nat - updated here  Mathematical definitions have many features, some critical and some optional but accepted within the  mathematical community. % Nat - added this %Van Dormolen and Zaslavsky~  describe a good mathematical definition as containing criteria of hierarchy% , existence% , equivalence, and axiomatization. Desired but not necessary criteria of a definition are minimality, elegance, and degenerations.  We give here short definitions of these concepts; detailed explanations with examples can be found in .  : any new concept must be described as a special case of a more general concept. : when one gives more than one formulation for the same concept, one must prove that they are equivalent. : the definition fits in and is part of a deductive system. : no more properties of the concept are mentioned in the definition than is required for its existence. : an elegant definition looks nicer, needs fewer words or less symbols, or uses more general basic concepts from which the newly defined concept is derived. : what occurs at instances when our intuitive idea of a concept does not conform to a definition. %Sometimes a definition allows instances that do not conform to our intuitive idea of the concept; these instances are called degenerations.   % end of formal definition of formal definitions Not every definition appearing in text is mathematical in the above sense. For example, Wikipedia articles contain definitions of different style. We see below that the Wikipedia definition of the Kane \& Abel musical group %shown in Figure   is not similar in style to the Wikipedia definition of an Abelian group.  % % %  Current methods for automatic DE view it as a binary classification task,  where a sentence is classified as a definition or a non-definition. A supervised learning process is usually employed for this task,  employing feature engineering for sentence representation.  The absolute majority of current methods study generic definitions and not mathematical definitions .  In this paper we describe a supervised learning method for automatic DE from mathematical texts. Our method  applies a Convolutional Neural Network , a Long Short-Term Memory network , and their combinations to the raw text data and sentence syntax structure, in order to detect definitions. Our method is evaluated on three different corpora; two are well-known corpora for generic DE and one is a new annotated corpus of mathematical definitions, introduced in this paper.     The main contributions of this paper are  analysis and introduction of the new annotated dataset of mathematical definitions,  evaluation of the state-of-the-art DE approaches on the new mathematical dataset,  introduction and evaluation of upgraded sentence representations adapted to mathematical domain with an adaptation of deep neural networks to new sentence representations,  extensive experiments with multiple network and input configurations  performed on different datasets in mathematical and non-mathematical domains,  experiments with   cross-domain and multi-domain learning in a DE task, and  introduction of the new parsed but non-annotated dataset composed of Wiki articles on  near-mathematics topics, used in an additional--extrinsic--evaluation scenario. These all contribute to showing that using specifically suited training data along with adapting sentence representation and classification models to the task of mathematical DE significantly improves extraction of mathematical definitions from surrounding text.   The paper is organized as follows. Section  contains a survey of up-to-date related work. Section  describes the sentence representations and the structure of neural networks used in our approach. Section  provides the description of the datasets, evaluation results, and their analysis. Section  contains our conclusions. Finally, Appendix contains some supplementary  materials -- annotation instructions, description of the Wikipedia experiment, and figures. 
"," Automatic definition extraction from texts is an important task that has numerous applications  in several natural language processing fields such as summarization, analysis of scientific texts, automatic taxonomy generation, ontology generation, concept identification, and question answering. For definitions that are contained within a single sentence, this problem can be viewed as a binary classification of sentences into definitions and non-definitions.  In this paper, we focus on automatic detection of one-sentence definitions in mathematical texts, which are difficult to separate from surrounding text.  We experiment with several data representations, which include sentence syntactic structure and word embeddings, and apply deep learning methods such as the  Convolutional Neural Network  and the Long Short-Term Memory network , in order to identify mathematical definitions.  Our experiments demonstrate the superiority of CNN and its combination with LSTM, when applied on the syntactically-enriched input representation.  % %We use data representation that includes sentence syntactic structure; to this we apply deep learning methods such as Convolutional Neural Network  and Recurrent Neural Network , in order to identify mathematical definitions.  We also present a new dataset for definition extraction from mathematical texts. %We demonstrate that the use of this dataset for training learning models improves the quality of definition extraction when these models are then used for other definition datasets.  We demonstrate that this dataset is beneficial for training supervised models aimed at extraction of mathematical definitions.  %Marina: added new sentence from the conclusions section Our experiments with different domains demonstrate that mathematical definitions require special treatment, and that using cross-domain learning is inefficient for that task.",246
"  As technology advances in the rapidly developing era, the exponentially increasing of text data makes analyzing and understanding textual files a tedious work . From the readers' perspective, capturing the salient information from overwhelming documents is a labor-intensive and time-consuming task. The voluminous documents are urgently required to be processed more efficiently and the abundance of text data calls for text summarization techniques. Text summarization is one of the important tasks of natural language processing that automatically convert a text or a collection of texts within the same topic into a concise summary that contains key semantic information. The length of summaries is usually significantly less than the original text . The research on automatic text summarization has been attractive in the field of natural language processing  which can be beneficial for many downstream applications such as creating news digests, search, and report generation .   According to the number of input documents, text summarization can be cast into single document summarization and multi-document summarization. Single document summarization aims to form a summary from only one document while multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. From the application perspective, single document summarization may not satisfy the requirement to produce comprehensive summaries, because it does not make good use of documents that are generated around the clock . For content to be summarized, it is more comprehensive and accurate to generate a summary from multiple documents written at different times, covering different perspectives. From the technical point of view, multi-document summarization is more complicated and difficult to tackle than single document summarization . This is because in the multi-document summarization task, there is more diverse and conflicting information among documents. The volume of documents is usually longer and the relations between documents are more complicated. In such large amount of documents, documents would inevitably be complement, overlapping and conflicting to each other .  In addition, excessively long input documents often lead to model degradation . It is challenging for models to retain the most critical contents of complex input sequences, while generating the coherent, non-redundant, non-factual error and grammatically readable summaries. Therefore, multi-document summarization requires models to have stronger capabilities for analyzing the corpora, identifying and merging consistent information. Furthermore, multi-document summarization task is more computation-hungry, due to the increasing sizes of current datasets and language model parameters.     Multi-document summarization task enjoys a wide range of real world applications, including summarization on news , scientific publications , emails , product reviews , lecture feedback , Wikipedia articles generation ,  medical documents  and software project activities . Recently, multi-document summarization technology has also received a great amount of attention in the industry. An intelligent multilingual news reporter bot named Xiaomingbot  was developed for news generation. This bot is able to summarize multiple news into one article and then translate it into multiple languages. Massive application requirements and rapidly growing online data promote the development of multi-document summarization. However, the majority of existing methods still generate summaries with manually crafted features , such as sentence position features , sentence length features , proper noun features , cue-phrase features , biased word features, sentence-to-sentence cohesion, sentence-to-centroid cohesion. The existing works using traditional algorithms can be divide into the following categories: term frequency-inverse document frequency  based methods , clustering based methods , graph based methods  and latent semantic analysis based methods .  Deep learning has gained enormous attention in recent years due to its success in various domains, for instance, computer vision , natural language processing  and multi-modal . Both industry and academia have been in a race to utilize deep learning to solve complex tasks due to its capability of capturing highly nonlinear relations of data. Recently, deep learning based models are applied in multi-document summarization , which prospers the development of text summarization and enables models to achieve better performance. Comparing to the conventional approaches, deep learning based models reduce dependence on manual feature extraction drastically. This task attracts increasing attention in the natural language processing community and enjoys steady expansion ever since. The number of research publications on deep learning based multi-document summarization is increasing rapidly over the last five years -- the statistics show the number of publications has 225\% increase from 2017 to 2019. It provides strong evidence for the inevitable pervasiveness of deep learning in multi-document summarization research.  The prosperity of deep learning for summarization in both academia and industry requires a comprehensive review of current publications for researchers to better understand the process and research progress. However, most of the existing summarization review articles are based on traditional algorithms instead of deep learning based methods . Therefore, we conduct this survey to embrace the knowledge of multi-document summarization. To the best of our knowledge, this is the first comprehensive survey in the direction of deep learning for multi-document summarization. This survey has been designed in a way such that it classifies the neural based multi-document summarization techniques into diverse categories thoroughly and systematically. We also conduct a detailed discussion on the categorization and progress of these approaches to establish a clearer concept standing in the shoes of readers. We hope this survey provides a panorama for researchers, practitioners and educators to quickly understand and step into the field of deep learning based multi-document summarization. The key contributions of this survey are three-folds:             In this article, we select, summarize, discuss, and analyze 30 representative works. We used Google Scholar as the main search engine to discover related works. The high-quality papers are selected from top NLP and AI journals and conferences, include  ACL\footnote{Annual Meeting of the Association for Computational Linguistics.}, EMNLP\footnote{Empirical Methods in Natural Language Processing.}, COLING\footnote{International Conference on Computational Linguistics}, NAACL\footnote{Annual Conference of the North American Chapter of the Association for Computational Linguistics.}, AAAI\footnote{AAAI Conference on Artificial Intelligence.}, ICML\footnote{International Conference on Machine Learning.}, ICLR\footnote{International Conference on Learning Representations} and IJCAI\footnote{International Joint Conference on Artificial Intelligence.}. The major keywords we used include  multi-documentation summarization, summarization, extractive summarization, abstractive summarization, deep learning and neural networks.         In the following sections, this survey will cover various aspects of recent advanced deep learning based works in multi-document summarization. Section  gives an overview of multi-document summarization. Section  highlights network design strategies and offers a comprehensive review of deep learning based multi-document summarization techniques. This survey also summarizes objective functions in the literature , evaluation metrics , and available multi-document datasets . Finally, section  discusses the future research directions for deep learning based multi-document summarization followed by the conclusion in Section .     
","  Multi-document summarization  is an effective tool for information aggregation which generates an informative and concise summary from a cluster of topic-related documents. Our survey structurally overviews the recent deep learning based multi-document summarization models via a proposed taxonomy and it is the first of its kind. Particularly, we propose a novel mechanism to summarize the design strategies of neural networks and conduct a comprehensive summary of the state-of-the-art. We highlight the differences among various objective functions which are rarely discussed in the existing literature. Finally, we propose several future directions pertaining to this new and exciting development of the field.",247
"  Computer-assisted cross-lingual conversation by automatic speech-to-speech translation has been one of the most challenging problems in spoken language technologies in decades . Recent remarkable advances in speech and language processing led by deep learning techniques benefit this challenge by real-time and accurate speech translation. %\memo{}  gogole multi-task model One crucial problem in automatic speech-to-speech translation is its delay. Spoken language processing tasks are usually handled in the utterance or sentence level. Their application to the speech-to-speech translation suffers from a long delay that is proportional to the input length, because the process starts after the observation of the end of an utterance. That is similar to consecutive interpretation and is not useful for long monologues such as lecture talks. On the other hand, in such situations, simultaneous interpretation is often used for an audience not proficient in the language of a talk. Simultaneous interpretation is a challenging task to listen to the talk and speak its interpretation in a different language.  In this work, we tackle the problem of automatic simultaneous speech-to-speech translation and develop a neural system to do that from English to Japanese. Here, we call our task simultaneous , not simultaneous . We think the task of simultaneous interpretation includes some additional efforts for summarization to make the output concise for small latency and better understanding for the audience. The problem requires real-time and incremental processing for the output generated simultaneously with the input. Previous attempts to incremental neural speech translation focused on speech-to-text translation . Our work aims to speech-to-speech translation for natural information delivery by speech without a need for visual attention on text-based subtitles. Our system is based on the cascade of three processing modules: incremental speech recognition , incremental machine translation , and text-to-speech synthesis , rather than recent end-to-end approaches %\memo{} due to the difficulty of applying them to the simultaneous translation.  We follow existing studies on incremental neural speech processing. For ASR, we choose an approach using a teacher-student training framework to train an incremental student model with the help of a non-incremental teacher model . For MT, we choose an approach called , which delays the start of the decoding process simply by  steps  . For TTS, we choose approach starting the segmental speech synthesis after observing the next accent phrase . These modules exchange their input/output symbols in the forms of subwords and work in a symbol-synchronous way, so they can be cascaded even if they have different waiting strategies.  We also conduct a system-level evaluation of our system in system-level latency and module-level performance on English-to-Japanese simultaneous translation on TED Talks. The system-level latency measures are:  processing delays for waiting and computation time, and  TTS speaking latency derived from overlaps of synthesized speech outputs. The module-level performance is measured by standard metrics in ASR, MT, and TTS. This work is the first attempt of system-level evaluation for a simultaneous speech-to-speech translation system and would be beneficial to future studies.  %The remainder of this paper is organized as follows. %In section , we review the problem of simultaneous speech-to-speech translation, mainly in its difficulty. %In section , we describe the details of the incremental processing modules for ASR, MT, and TTS. %In section , we present system-wise evaluation of our system, followed by some discussions in section . %We conclude this paper in section .   
"," This paper presents a newly developed, simultaneous neural speech-to-speech translation system and its evaluation. The system consists of three fully-incremental neural processing modules for automatic speech recognition , machine translation , and text-to-speech synthesis . We investigated its overall latency in the system's Ear-Voice Span and speaking latency along with module-level performance.",248
" The emergence of online collaboration platforms has dramatically changed the dynamics of human teamwork, creating a veritable army of virtual teams, composed of workers in different physical locations. Software engineering requires a tremendous amount of collaborative problem solving, making it an excellent domain for team cognition researchers who seek to understand the manifestation of cognition applied to team tasks.  Mining data from social coding platforms such as GitHub can yield insights about the thought processes of virtual teams.  Previous work on issue comments has focused on emotional aspects of team communication, such as sentiment and politeness.  Our aim is to map issue comments to states in team cognition such as information gathering, knowledge building and problem solving.  To do this we employ dialogue act  classification, in order to identify the intent of the speaker.  Dialogue act classification has a broad range of natural language processing applications, including machine translation, dialogue systems and speech recognition.  Semantic-based classification of human utterances is a challenging task, and the lack of a large annotated corpus that represents class variations makes the job even harder. Compared to the examples of human utterances available in standard datasets like the Switchboard  corpus and the CSI Meeting Recorder Dialogue Act  corpus, GitHub utterances are more complex.   The primary purpose of our study is the DA classification of GitHub issue comments by harnessing the strength of transfer learning, using word and sentence level embedding models fine-tuned on our dataset.  For word-level transfer learning, we have used GLoVe vectors, and Universal Sentence Encoders and BERT models were used for sentence-level transfer. This paper presents a comparison of the performance of various  architectures on GitHub dialogues in a limited resource scenario.  A second contribution is our publicly available dataset of annotated issue comments. The dataset is available at \url{https://drive.google.com/drive/folders/1kLZvzfE80VeEYA1tqua_aj6nSiT57f83?usp=sharing}. In the field of computational collective intelligence,  where people collaborate and work in teams to achieve goals, dialogue act classification can play a vital role in understanding human teamwork.   
"," Social coding platforms, such as GitHub, serve as  laboratories for studying collaborative problem solving in open source software development; a key feature is their ability to support issue reporting which is used by teams to discuss tasks and ideas.  Analyzing the dialogue between team members, as expressed in issue comments, can yield important insights about the performance of virtual teams.  This paper presents a transfer learning approach for performing dialogue act classification on issue comments.  Since no large labeled corpus of GitHub issue comments exists, employing transfer learning enables us to leverage standard dialogue act datasets in combination with our own GitHub comment dataset. We compare the performance of several word and sentence level encoding models including Global Vectors for Word Representations , Universal Sentence Encoder , and Bidirectional Encoder Representations from Transformers . Being able to map the issue comments to dialogue acts is a useful stepping stone towards understanding cognitive team processes.",249
" Large, densely-labeled datasets are a critical requirement for the creation of effective supervised learning models. The pressing need for high quantities of labeled data has led many researchers to collect data from social media platforms and online forums . Due to the presence of noise and the lack of structure that exist in these data sources, manual quality analysis  is necessary to extract structured labels, filter irrelevant examples, standardize language, and perform other preprocessing tasks before the data can be used. However, obtaining dataset annotations in this manner is a time-consuming and expensive process that is often prone to errors.   In this work, we develop automated data cleaning and verification mechanisms for extracting high-quality data from social media platforms\footnote{All code is available at \url{https://github.com/rachel-1/qa_plausibility}.}. We specifically focus on the creation of question-answer datasets, in which each data instance consists of a question about a topic and the corresponding answer. In order to filter noise and improve data quality,  we propose the task of question-answer  plausibility, which includes the following three steps:   Depending on the type of dataset being constructed, the question posed to respondents may be generated by a machine or a human. We determine the likelihood that the question is both relevant and answerable.   We predict whether the user's response contains a reasonable answer to the question.   If the response is deemed to be plausible, we identify and extract the segment of the response that directly answers the question.    Because we assume social media users generally answer questions in good faith , we can assume plausible answers are correct ones . Necessarily, if this property were not satisfied, then any adequate solutions would require the very domain knowledge of interest. Therefore, we look to apply this approach toward data with this property.  In this study, we demonstrate an application of QA plausibility in the context of visual question answering , a well-studied problem in the field of computer vision . We assemble a large VQA dataset with images collected from an image-sharing social network, machine-generated questions related to the content of the image, and responses from social media users. We then train a multitask BERT-based model and evaluate the ability of the model to perform the three subtasks associated with QA plausibility. The methods presented in this work hold potential for reducing the need for manual quality analysis of crowdsourced data as well as enabling the use of question-answer data from unstructured environments such as social media platforms.   
"," Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets from social media by proposing a novel task for automated quality analysis and data cleaning: question-answer  plausibility. Given a machine or user-generated question and a crowd-sourced response from a social media user, we determine if the question and response are valid; if so, we identify the answer within the free-form response.   We design BERT-based models to perform the QA plausibility task, and we evaluate the ability of our models to generate a clean, usable question-answer dataset. Our highest-performing approach consists of a single-task model which determines the plausibility of the question, followed by a multi-task model which evaluates the plausibility of the response as well as extracts answers .",250
"  In recent times, pre-trained neural language models  have become the preferred approach for language representation learning, pushing the state-of-the-art in multiple NLP tasks~. These approaches rely on a two-step training process: first, a self-supervised pre-training is performed on large-scale corpora; then, the model undergoes a supervised fine-tuning on downstream task labels using task-specific prediction heads. While this method was found to be effective in scenarios where a relatively large amount of labeled data are present, researchers highlighted that this is not the case in low-resource settings~.   Recently, pattern-exploiting training~(PET,  tackles the dependence of NLMs on labeled data by first reformulating tasks as cloze questions using task-related patterns and keywords, and then using language models trained on those to annotate large sets of unlabeled examples with soft labels. PET can be thought of as an offline version of knowledge distillation~, which is a well-established approach to transfer the knowledge across models of different size, or even between different versions of the same model as in self-training . While effective on classification tasks that can be easily reformulated as cloze questions, PET cannot be easily extended to regression settings since they cannot be adequately verbalized. Contemporary work by  showed how self-training and pre-training provide complementary information for natural language understanding tasks.  In this paper, I propose a simple self-supervised data augmentation approach that can be used to improve the generalization capabilities of NLMs on regression and classification tasks for modest-sized labeled corpora. In short, an ensemble of fine-tuned models is used to annotate a large corpus of unlabeled text, and new annotations are leveraged in a multi-task setting to obtain final predictions over the original test set. The method was tested on the AcCompl-it shared tasks of the EVALITA 2020 campaign~, where the objective was to predict respectively complexity and acceptability scores on a 1-7 Likert scale for each test sentence, alongside an estimation of its standard error. Results show considerable improvements over regular fine-tuning performances on COMPL and ACCEPT using the UmBERTo pre-trained model~, suggesting the validity of this approach for complexity/acceptability prediction and possibly other language processing tasks.  
","   English.  This work describes a self-supervised data augmentation approach used to improve learning models' performances when only a moderate amount of labeled data is available. Multiple copies of the original model are initially trained on the downstream task. Their predictions are then used to annotate a large set of unlabeled examples. Finally, multi-task training is performed on the parallel annotations of the resulting training set, and final scores are obtained by averaging annotator-specific head predictions. Neural language models are fine-tuned using this procedure in the context of the AcCompl-it shared task at EVALITA 2020, obtaining considerable improvements in prediction quality.",251
" Underresourced languages, from a natural language processing  perspective, are those lacking the resources  needed to support state-of-the-art performance on NLP problems like machine translation, automated speech recognition, or named entity recognition. Yet the vast majority of the world's languages---representing billions of native speakers worldwide---are underresourced. And the lack of available training data in such languages usually reflects a broader paucity of electronic information resources accessible to their speakers.  %The most prominent of these languages have millions of native speakers, who have previously been deprived of access to information on the web in their native language, due to missing translation tools.   For instance, there are over six million Wikipedia articles in English but fewer than sixty thousand in Swahili and fewer than seven hundred in Bambara, the vehicular and most widely-spoken native language of Mali that is the subject of this paper. Consequently, only 53\% of the worlds population have access to ``encyclopedic knowledge'' in their primary language, according to a 2014 study by Facebook. MT technologies could help bridge this gap, and there is enormous interest in such applications, ironically enough, from speakers of the languages on which MT has thus far had the least success. There is also great potential for humanitarian response applications .  Fueled by data, advances in hardware technology, and deep neural models, machine translation  has advanced rapidly over the last ten years.  %Yet underresourced languages have yet to benefit from these advances, because they lack the large volumes of translated texts needed to drive neural machine learning.  %Although neural models are generally considered to work best in domains where large amounts of training data exist Researchers are beginning to investigate the effectiveness of   low-resource languages, as in recent WMT 2019 and WMT 2020 tasks , and in underresourced African languages. %What has been done? Which challenges have been identified, which solutions have been found?  Most prominently, the   community, a grassroots initiative, has developed open-source NMT models for over 30 African languages on the base of the JW300 corpus~, a parallel corpus of religious texts.   Since African languages cover a wide spectrum of linguistic phenomena and language families , individual development of translations and resources for selected languages or language families are vital to drive the overall progress. Just within the last year, a number of dedicated studies have significantly improved the state of African NMT:  analyzed the depth of Transformers specifically for low-resource translation of South-African languages, based on prior studies by   on the Autshumato corpus~.  developed an MT model and compiled resources for translations between Fon and French,  modeled translations between English and Hausa,  for four languages of the Edoid language family, and  investigated supervised vs. unsupervised NMT for Nigerian Pidgin.   %Superficially, it might seem like this development simply grows the pool of ``standard MT'' evaluation tasks and data sets, with some data sets being smaller than others.    In this paper, we present the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara. We discuss challenges in working with low-resource languages and propose strategies to cope with data scarcity in low-resource MT.   We discuss the socio-cultural context of Bambara translation and its implications for model and data development. Finally, we analyze our best-performing neural models with a small-scale human evaluation study and give recommendations for future development. %These contributions a deeper understanding of the shortcomings of state-of-the-art methods for high-resource languages, and  We find that the translation quality on our in-domain data set is acceptable, which gives hope for other languages that have previously fallen under the radar of MT development.      % [Can model and data be made publicly available?] We released our models and data upon publication. Our evaluation setup may serve as benchmark for an extremely challenging translation task.  %
"," %Low-resource languages present unique challenges to machine translation.  %We discuss the case of Bambara, a Mande language where training data is scarce and requires significant amounts of pre-processing. Moreover, the socio-cultural context within which Bambara speakers live poses challenges for automated processing. We contribute the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara.  %New abstract?:  Low-resource languages present unique challenges to  machine translation. We discuss the case of Bambara, a Mande language for which training data is scarce and requires significant amounts of pre-processing. More than the linguistic situation of Bambara itself, the socio-cultural context within which Bambara speakers live poses challenges for automated processing of this language. In this paper, we present the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara. We discuss challenges in working with low-resource languages and propose strategies to cope with data scarcity in low-resource machine translation .",252
"  Language modelling is the task of transforming individual words into vector representations based on the context they appear in. Hence, distant term dependencies are an inherited issue within the task. Language models always seek for smart approaches towards incorporating context from longer distances as it allows for better representations compared to their limited context counterparts. Intuitively, imagine attempting to start reading a novel series from the second book onward, with no information about the first. The amount of information previously missed is something that cannot be acquired. However, this is the case with most language models. While an understanding of the words is present due to the contextual information at each word's occurrence, entity information that are in distant text are lost or not transferred.   Until recently, Recurrent Neural Networks , and specifically  Long Short-Term Memory  networks, have been the core of all the state-of-the-art approaches . Thanks to the Transformers architecture , through the use of attention mechanisms, models such as XLNet , GPT  and BERT  can account for even longer sequences. However, the computational limitations of the multi-head attention in the architecture make it hard to increase the contextual information in such models . As a result, research has been focused on introducing variations to the transformer architecture, with focus on the multi-head attention mechanism, in order to alleviate part of the computational cost and increase the contextual information available to models.   In this paper we present a novel approach, that makes use of coreference information during training a language model via our Entity-Transformer architecture, which extends the original Transformer block in Transformer-Based language models. To that end, we incorporate the important entity information that would otherwise be unreachable for the model. As a result, we effectively boost the representations of the entity mentions, where entity information is present, without hindering the performance of the language model where entities are not present.   In our experiments, we extend the GPT2 architecture to formulate our model, named GPT2E and train it on the CoNLL-2012 dataset  using the annotated coreference information. We evaluate the model's performance in terms of Perplexity on the ConLL 2012 and the LAMBADA  datasets and showcase the effects of such training on the word representations as well as on the downstream task of Named Entity Recognition  using the CoNLL 2012 dataset. To that end, we compare GPT2E's performance to a base model  when trained on the same data, to highlight the effects of coreference information when paird with our Entity-Transformer architecture.   
","     In the last decade, the field of Neural Language Modelling has witnessed enormous changes, with the development of novel models through the use of Transformer architectures. However, even these models struggle to model long sequences due to memory constraints and increasing computational complexity. Coreference annotations over the training data can provide context far beyond the modelling limitations of such language models. In this paper we present an extension over the Transformer-block architecture used in neural language models, specifically in GPT2, in order to incorporate entity annotations during training. Our model, GPT2E, extends the Transformer layers architecture of GPT2 to Entity-Transformers, an architecture designed to handle coreference information when present. To that end, we achieve richer representations for entity mentions, with insignificant training cost. We show the comparative model performance between GPT2 and GPT2E in terms of Perplexity on the CoNLL 2012 and LAMBADA datasets as well as the key differences in the entity representations and their effects in downstream tasks such as Named Entity Recognition. Furthermore, our approach can be adopted by the majority of Transformer-based language models.",253
"  . } To foster research on dialog policy learning for virtual digital assistants, several task-oriented dialog corpora have been introduced in recent years, such as SimDial, MultiWoZ, Taskmaster, and Schema Guided Dialog, to name a few.  Deep learning approaches, including mixture models hierarchical encoder/decoder, reinforcement learning, and pre-trained language models, have significantly advanced dialog policy research in the past few years, setting new state-of-the-art performance limits.  %More recently, SimpleTOD and SOLOIST have shown that pre-training dialog policy using large language models, e.g., GPT-2, can lead to significantly better performance on task-oriented neural dialog policy learning by using even larger neural models.  %\ab{somewhere we want to mention that data collection is expensive -- both in time; and other resources}  However, collecting annotated data for supervised dialog policy learning is an expensive and time-consuming process. Hence, it is desirable to explore approaches to train dialog policy with limited data and transfer an existing policy with few or even no additional training data to new domains.  This practical requirement has motivated the community to research resource-constrained dialog policy learning in the past few decades. Researchers have explored approaches including employing grammar constraints for dialog policy, transfer learning , or pre-trained language models. Few-shot domain adaptation has been researched since the 2000s on both end-to-end dialog systems  as well as dialog policy learning.  % We investigate one-shot policy learning and zero-shot domain transfer using \ab{I think up to 50 examples for original training is reasonable. Emphasizing that there is only ONE training sample available may not be reasonable. The reviewer may come back and say why 1? Why not 5? Why not 10? So showing that other methods need thousands of samples to match the performance of DILP would be a more convincing argument IMHO.} \ab{We  don't believe a single dialog is going to be representative of ALL of the conversational flows that may occur in a more complicated real-life dataset . Increasing the number of training samples beyond one may help with improving the policy on MultiWoZ, not necessarily in terms of inform and success but in terms of action F1 .} \ab{Finally, zero-shot transfer is an extremely desirable property. One can argue that they don't want to re-train and want their method to work on a new domain out of the box. Having said that, your argument would be even stronger if you could also show that the baseline methods need X shot transfer to match the performance of zero-shot DILP on a new domain.} differential inductive logic programming .   % Given an encoding of the common known knowledge and a set of examples represented as a logical set of facts, an inductive logic programming  system can be hypothesised  which conforms to all of the positive and none of the negative examples. % ILP has been extensively studied in the context of symbolic AI in the past few decades, and has been also adapted to dialog management. While ILP generalizes well from a noiseless/consistent set of rules, it is known to be prone to noisy samples , and hence would not be applicable to real-world problem scenarios, and particularly noisy dialog policy data. DILP addresses the noisy/inconsistent training data via a novel probabilistic treatment of the learned rules by relaxing them to have different probabilities of being true/false and solving the relaxed problem using recent advances in gradient-based optimization.   We present   We introduce \name in Section. We apply \name to the SimDial Dataset , and MultiWoZ Dataset , showing that on the task of one-shot dialog policy learning and zero-shot domain transfer, \name outperforms several other neural baselines. Finally, Section concludes this paper.  
","   Motivated by the needs of resource constrained dialog policy learning,   we introduce dialog policy via differentiable inductive logic . We explore the tasks of one-shot learning and zero-shot domain transfer with  representative dialog from the restaurant domain, we train \name on the SimDial dataset and obtain 99+\% in-domain test accuracy. We also show that the trained DILOG zero-shot transfers to all other domains with 99+\% accuracy, proving the suitability of \name to slot-filling dialogs. We further extend our study to the MultiWoZ dataset achieving 90+\% inform and success metrics. We also observe that these metrics are not capturing some of the shortcomings of DILOG in terms of false positives, prompting us to measure an auxiliary Action F1 score. We show that \name is 100x more data efficient than state-of-the-art neural approaches on MultiWoZ while achieving similar performance metrics. We conclude with a discussion on the strengths and weaknesses of \name.",254
"  Sequence labeling tasks are essential in web mining, such as named entity recognition , event extraction, and relation identification. For example, the NER models assign the predefined labels to tag tokens in the input sequences to indicate both the entity boundaries and types. In some web services, such as question answering, sequence labeling also plays a critical role, where it reads a passage in a Web page as the context and answers a given question by extracting a text span inside the given passage. This process is often called machine reading comprehension . MRC is also regarded as a sequence labeling task, since it predicts whether each token is the start, end, or none for the answer span.  There is a rich literature for sequence labeling. Classical methods include Hidden Markov models , maximum entropy Markov models , and conditional random field . Recently, combining neural networks as the representation layer with CRF models has further boosted the state-of-the-art performance. However, such statistical models require large amounts of training data. Consequently, they only show good performance in languages with rich training data, such as English. Sequence labeling on low-resource languages is still very challenging, mainly due to very limited training data available.  To tackle the challenge of sequence labeling in low-resource languages, some early works transfer the knowledge from rich-source languages to low-resource ones by information alignment through manually built bilingual parallel corpora,  or language-independent features. In recent years, multilingual pre-trained language models, such as Unicoder, mBERT, and XLM-Roberta , are developed for model transferring. For example, Wu et al.  fine-tune mBERT on a pseudo training set by a meta-learning method. To better leverage the unlabeled data in the target language, a teacher-student framework is proposed to distill knowledge from weighted teacher models. Inspired by back translation in neural machine translation , DualBERT is developed to learn source language and target language features simultaneously. Although these multilingual sequence labeling models can effectively locate target spans, they often fail to give the precise boundaries of the spans in the target languages.  %when predicting text spans in the target languages.  %that is, pairs of sentences with similar meanings but in different languages, %\jp{What is the conclusion we can draw from this paragraph?}  %The previous multilingual sequence labeling models can roughly identify the correct target spans, but often fail to give the precise boundaries when predicting text spans in the target languages.  We conduct an empirical study to quantitatively assess the challenge. In Figure , we categorize the mismatches between the predicted span and the ground truth span into four types:  the predicted answer is a { of the ground truth;  the predicted answer both miss some terms in the ground truth and add extra terms not in the ground truth , and   the predicted answer is adjacent to the ground truth but contains no common sub-span with it .  We further show in Table the statistics of the error cases in the cross-lingual NER task using the XLM-R model, where the boundary errors, including , , , and , contribute to a large portion of all error cases as shown in the last column. The other errors cases are mainly entity type detection errors. This observation motivates us to tackle the bottleneck of boundary detection in sequence labeling models.      [t]     $ baseline on the XGLUE-NER dataset. Detailed definitions of each error category are shown in Figure.}      \tiny     %      {l|cc|cccc|c}     \toprule      & \#Test & \#Error & \#Super & \#Sub & \#Drifted & \#Adjacent & Boundary error  \\     \midrule     en & 6,392  & 566   & 106 & 64  & 1 & 137 & 54.4 \\     es & 4,054  & 955   & 93  & 246 & 0 & 295 & 66.4 \\     de & 5,390  & 1,648 & 201 & 150 & 4 & 884 & 75.2 \\     nl & 6,884  & 949   & 148 & 96  & 0 & 42  & 30.1 \\          %         Accurately detecting answer boundaries becomes a bottleneck in sequence labeling.  To tackle the challenge, in this paper, we propose a separate model for boundary calibration based on the output of a base model. Intuitively, the base model captures the global context of the whole input sequence and roughly locates the region for answers. Then, the calibration model conducts finer search within the detected region and the neighborhood, and focuses on the local context to refine the boundary. This is analogous to the human perception and cognition process, which first locates the target, sets up the local context, and finally zooms into details.  Our design is novel for sequence labeling, and is orthogonal and complements to all existing approaches.  Using a second model to focus on detecting answer boundaries accurately is an intuitive and nice idea.  However, how to construct high-quality training data for the calibration model remains challenging. One straightforward method is to transform the original training data of sequence labeling task into a new training set for calibration model. However, the data collected in this way is still quite limited, especially for low-resource languages. To address this challenge, we strategically propose a novel phrase boundary recovery  task to pre-train the model on large-scale augmented datasets synthesized from Wikipedia documents in multiple languages. The new pre-training approach dramatically improves the capability of the calibration module to determine answer boundaries accurately.  % Besides the design of employing two models, we further equip the calibration model with a pre-training process by emphasizing on the capability of recovering meaningful phrases from noisy input.  Our approach is shown in Figure. CalibreNet consists of two modules, a base module and a calibration module. The base module can take any model of sequence labeling. The predicted answers by the base module are combined with the input sequence to form the input to the calibration module. The calibration module considers both the initial results by the base module and the whole passage to refine the span boundaries. In particular, the calibration module is pre-trained with the PBR task on large-scale multilingual synthesized data from Wikipedia-derived corpus.  We make the following technical contributions in this paper. First, we propose the CalibreNet framework for the task of cross-lingual sequence labeling to improve the accuracy of labeled answers. Second, we propose a novel phrase boundary recovery task and a weakly supervised pre-training method using Wikipedia data. This approach effectively enhances the model sensitivity to phrase boundaries. Last but not least, we conduct extensive experiments on zero-shot cross-lingual NER and improve the SOTA results. In addition, the experiments on the MRC tasks also show consistent improvement over strong baseline methods.  The rest of the paper is organized as follows. We first review the related work in Section. We then present our approach in Section. We report the extensive experimental results in Sections.  We conduct further analysis in Section, and conclude the paper in Section.  
","  \footnotetext[1]{Work done during the first author's internship at Microsoft STCA.} \footnotetext[2]{Daxin Jiang and Wanli Zuo are the corresponding authors.} % \footnotetext[3]{Jian Pei's research is supported in part by the NSERC Discovery Grant program. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.}   Lack of training data in low-resource languages presents huge challenges to sequence labeling tasks such as named entity recognition  and machine reading comprehension . One major obstacle is the errors on the boundary of predicted answers. To tackle this problem, we propose CalibreNet, which predicts answers in two steps. In the first step, any existing sequence labeling method can be adopted as a base model to generate an initial answer. In the second step, CalibreNet refines the boundary of the initial answer. To tackle the challenge of lack of training data in low-resource languages, we dedicatedly develop a novel unsupervised phrase boundary recovery pre-training task to enhance the multilingual boundary detection capability of CalibreNet. Experiments on two cross-lingual benchmark datasets show that the proposed approach achieves SOTA results on zero-shot cross-lingual NER and MRC tasks.",255
"  The Text-to-SQL task aims to translate natural language texts into SQL queries. Users who do not understand SQL grammars can benefit from this task and acquire information from databases by just inputting natural language texts. Previous works  focus on context-independent text-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task.   Recently, context-dependent text-to-SQL task has attracted more attention.  conduct experiments on ATIS dataset . Besides, two cross-domain context-dependent datasets SParC  and CoSQL  are released. Cross-domain means databases in test set differ from that in training set, which is more challenging.   EditSQL  is the previous state-of-the-art model on SParC and CoSQL datasets and it focuses on taking advantages of previous utterance texts and previously predicted query to predict the query for current turn. Table  shows the user inputs, ground truth queries and predicted queries of EditSQL for an interaction. In the second turn, EditSQL views ``Kacey"" as the name of a dog owner. However, since the context of the interaction is about dogs, ``Kacey"" should be the name of a dog. This example shows that a model using only historical information of user inputs may fail to keep context consistency and maintain thematic relations.   According to  and , to maintain thematic relations, users may change constraints, ask for different attributes for the same topic when they ask the next questions. Thus, database schema items  in current turn should have relation with items in previous turn. For example, in Table , the second question  adds a constraint of the name and asks for the age of a dog instead of the numbers of all dogs. The corresponding database schema items Dogs.age and Dogs.name in   belong to the same table as Dogs.* in previous query . Therefore, we propose to take historical information about database schema items into consideration.  %[tbp] %       In particular, we first construct a graph based on corresponding database, where graph nodes are database schema items and graph edges are primary-foreign keys and column affiliation. Short distance between graph nodes appearing in previous query and current query can reveal the context consistency since there is usually an edge between the different attributes of the same topic. We then propose a database schema interaction graph encoder to model database schema items together with historical items. Empirical results on two large cross-domain context-dependent text-to-SQL datasets - SParC and CoSQL show that our schema interaction graph encoder contributes to modeling context consistency and our proposed model with database schema interaction graph encoder substantially outperforms the state-of-the-art model.     [tbp]     ^1y^1x^2\Tilde{y}^2y^2x^3\Tilde{y}^3y^3x^4\Tilde{y}^4y^4 means that query is predicted by a model, which is EditSQL here.}        Our main contributions are summarized as follows:        .       
"," Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historical user inputs. In this work, in addition to using encoders to capture historical information of user inputs, we propose a database schema interaction graph encoder to utilize historicalal information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.",256
" Coherence refers to the properties of a text that indicate how meaningful sentential constituents are connected to convey document-level meaning. Different theories have been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text -- by constructing an entity-grid  representation, building on Centering Theory. Subsequent work has adapted and further extended Egrid representations.  Other research has focused on syntactic patterns that co-occur in text or semantic relatedness between sentences as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units or capturing topic shifts via Hidden Markov Models~. Other work has combined approaches to study whether they are complementary.  More recently, neural networks have been used to model coherence. Some models utilize structured representations of text~ and others operate on unstructured text, taking advantage of neural models' ability to learn useful representations for the task.  Coherence has typically been assessed by a model's ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document , and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order, evaluating on realistic data and focusing on open-domain models of coherence. However, less attention has been directed to investigating and analyzing the properties of coherence that current models can capture, nor what knowledge is encoded in their representations and how it might relate to aspects of coherence.   In this work, we systematically examine what properties of discourse coherence current coherence models can capture. We devise two datasets that exhibit various kinds of incoherence and analyze model ability to capture syntactic and semantic aspects of text implicated in discourse organisation.  We furthermore investigate a set of probing tasks to better understand the information that is encoded in their representations and how it might relate to aspects of coherence.  We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further.  Finally, we release our evaluation datasets as a resource for the community to use to test discourse coherence models.   
","  In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models.",257
"  Early detection of dementia is important for improving clinical outcomes and management of dementia, as well as for lifestyle, financial, and future planning for patients and their caregivers . Yet, dementia is not formally diagnosed or coded in claims for over 50\% of older adults living with probable dementia . Tools that screen medical records for warning signs and present the digested information to providers may prove to be an important step for early intervention.  In this study, we aim to use NLP to detect signs of cognitive dysfunction from clinician notes in electronic health records  by applying deep learning techniques that have not been hitherto applied to this problem. We present an attention-based transformer model that allows for long text sequences to reveal signs of cognitive concerns and compare its performance to baseline models.   
","   Dementia is under-recognized in the community, under-diagnosed by healthcare professionals, and under-coded in claims data. Information on cognitive dysfunction, however, is often found in unstructured clinician notes within medical records but manual review by experts is time consuming and often prone to errors. Automated mining of these notes presents a potential opportunity to label patients with cognitive concerns who could benefit from an evaluation or be referred to specialist care.  In order to identify patients with cognitive concerns in electronic medical records, we applied natural language processing  algorithms and compared model performance to a baseline model that used structured diagnosis codes and medication data only. An attention-based deep learning model outperformed the baseline model and other simpler models.",258
" A spelling corrector is an important and ubiquitous pre-processing tool in a wide range of applications, such as word processors, search engines and machine translation systems. %The popularity of mobile devices makes it increasingly crucial since typing on virtual keyboards is more error-prone. Having a surprisingly robust language processing system to denoise the scrambled spellings, humans can relatively easily solve spelling correction . %spelling correction is a relatively easy task for humans, who have a surprisingly robust language processing system  to denoise the scrambled spellings.  However, spelling correction is a challenging task for a machine, because words can be misspelled in various ways, and a machine has difficulties in fully utilizing the contextual information.   Misspellings can be categorized into , which is out-of-vocabulary, and the opposite,  misspellings . The dictionary look-up method can detect non-word misspellings, while real-word spelling errors are harder to detect, since these misspellings are in the vocabulary . In this work, we address the   spelling correction problem. It only corrects the spelling of each token without introducing new tokens or deleting tokens, so that the original information is maximally preserved for the down-stream tasks. %\textcolor{red}{[The last few sentences of this paragraph is not good, what are you trying to express?]}  We formulate the  spelling correction as a sequence labeling task and jointly detect and correct misspellings. Inspired by the human language processing system, we propose a novel solution on the following aspects:  We encode both spelling information and global context information in the neural network.  We enhance the real-word correction performance by initializing the model from a pre-trained language model .  We strengthen the model's robustness on unseen non-word misspellings by augmenting the training dataset with a synthetic character-level noise. As a result, our best model  outperforms the previous state-of-the-art result  by  absolute  score.  %As a result, we present a simple but powerful solution to the  spelling correction by simply fine-tuning a pre-trained LM to jointly detect and correct both non-word and real-word misspellings  as a sequence labeling task.  %We propose a novel solution by using transformer-encoders  to jointly perform detection and correction of misspellings. We extensively explore various training techniques. Our results show that a transformer-encoder-based architecture that encodes both local character-level and global word-level representations yields a strong performance. Specifically, both the combination of word embedding and character embedding or a subword embedding  produce strong models. We further obtain a state-of-the-art model by initializing the weight from a pre-trained language model   and training it on an augmented training dataset with a synthetic character-level noise. \textcolor{red}{[this paragraph need to rewrite. Please summarize your contribution in a coherent story. ]}  %We also explore additional training techniques such as leveraging a pre-trained language model  and adding more noise to the training corpora. Our results show that fine-tuning a pre-trained LM  with a subword embedding  yields a strong model. Furthermore, we obtain a state-of-the-art model by training it on a noisy corpus synthesized by randomly replacing correct words and characters with natural misspellings and random character. Finally, under the condition of no pre-training, we propose a strong model that outperforms the subword model by combining word and character embedding.      {[after reading introduction section, reviewers need to know roughy how you do? However, you just preset a lot what you do]}   % \fi    Formally, given a noisy input sentence , each noisy word  is drawn from a distribution of possible misspellings of the correct word $w_k{[as I said, this definition do not need on section]} \fi  
"," Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings.  On the contrary, humans can easily infer the corresponding correct words %\textcolor{red}{the semantics of unknown words:the corresponding correct words of misspellings}  from their misspellings and surrounding context. Inspired by this, we address the  spelling correction problem, which  %\textcolor{red}{[do not know which refers to what, confusing, please rewrite; at the same time, can you brief introduce your novel solution here?]}  only corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. We present a simple yet powerful solution that jointly detects and corrects misspellings as a sequence labeling task by fine-turning a pre-trained language model. Our solution outperform the previous state-of-the-art result by $12.8\%$ absolute $F_{0.5}$ score. %Furthermore, we obtain a state-of-the-art model by augmenting the training data with synthetic character-level noise. %We also provide three useful training techniques. Our results show that a transformer-based model that encodes both local character-level and global word-level representations yields a strong performance. Furthermore, a state-of-the-art model is obtained by leveraging pre-trained language model and augmenting the training corpus with synthetic character-level noises. %fine-tuning a pre-trained language model with a subword embedding yields a strong model. Furthermore, we obtain a state-of-the-art model by training it on a noisy corpus synthesized by randomly replacing correct words and characters with common misspellings and random characters. We also propose a strong architecture that combines character and word level encoder without pre-training.",259
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Conversational technologies offer a remarkable addition to the current approaches for providing mental healthcare. Communications with these conversational agents have been found to include discoverable psychological distress signs, such as the rate of filled pauses, speech rate, and various temporal and turn-related characteristics . In the human-human automatic analysis of patient-doctor conversations, it has also been found that different types of disfluency can indicate levels of adherence to medication . Markers of disfluency also hold predictive power for the identification of cognitive disorders .  Such devices are mainly used for processing content, which is then analyzed offline. There is much work on detecting disfluencies for offline analysis of transcripts with gold standard utterance segmentation within much of the current effort on disfluency detection on telephone conversations begun by . However, given that these models do not operate for live systems and rely on rich transcription data, including the pre-segmentation of dialogue acts, to facilitate more cost-effective study of other data, it would be easier to be able to perform directly and incrementally off the speech signal, or at least from automatic speech recognition  results as they arrive into the system. The incremental model must work with minimum latency as it receives word-by-word data and does so without modifying its initial assumptions and providing its best decisions as soon as possible in line with the principles set out in .  We combine incremental identification of disfluencies with three other essential tasks for active conversational models to provide a favorable inductive bias to disfluency detection and to study the way these tasks interact. We explore a multi-task learning  framework to enable the training of one universal model with four tasks of disfluency detection, language modelling, part-of-speech  tagging, and utterance segmentation, which in the data we use is also equivalent to dialogue act segmentation. Multi-task learning seeks to improve learning efficiency and predictive power by learning from a shared representation with multiple objectives. We investigate the entire power set of these tasks to investigate the interaction between them. We experiment with two different losses: a naive weighted sum of losses where the weights of loss are uniform and a loss function based on maximizing the Gaussian likelihood with task-dependent uncertainty.  We train and test a simple neural model for the different tasks, experiment with all the combinations of the tasks, different loss functions for each of the tasks, and experiment with different input representations .    
","  We present a multi-task learning framework to enable the training of one universal incremental dialogue processing model with four tasks of disfluency detection, language modelling, part-of-speech tagging, and utterance segmentation in a simple deep recurrent setting. We show that these tasks provide positive inductive biases to each other with the optimal contribution of each one relying on the severity of the noise from the task. Our live multi-task model outperforms similar individual tasks, delivers competitive performance, and is beneficial for future use in conversational agents in psychiatric treatment.",260
" We introduce \diagnnose, an open source library for analysing deep neural networks. The \diagnnose library allows researchers to gain better insights into the internal representations of such networks, providing a broad set of tools of state-of-the-art analysis techniques. The library supports a wide range of model types, with a main focus on NLP architectures based on LSTMs  and Transformers .  Open-source libraries have been quintessential in the progress and democratisation of NLP. Popular packages include HuggingFace's   -- allowing easy access to pre-trained Transformer models; % AllenNLP  -- providing useful abstractions over components in the NLP pipeline,   -- focusing on multitask and transfer learning within NLP;   -- providing a range of feature attribution methods; and   -- a platform for visualising and understanding model behaviour. We contribute to the open-source community by incorporating several \mbox{interpretability} techniques that have not been present in these packages.  Recent years have seen a considerable interest in improving the understanding of how deep neural networks operate . The high-dimensional nature of these models makes it notoriously challenging to untangle their inner dynamics. This has given rise to a novel subfield within AI that focuses on interpretability, providing us a peak inside the black box. \diagnnose aims to unify several of these techniques into one library, allowing interpretability research to be conducted in a more streamlined and accessible manner.  \diagnnose's main focus lies on techniques that aid in uncovering linguistic knowledge that is encoded within a model's representations. The library provides abstractions that allow recurrent models to be investigated in the same way as Transformer models, in a modular fashion. It contains an extensive activation extraction module that allows for the extraction of  model activations on a corpus. The analysis techniques that are currently implemented include: [leftmargin=.5cm]    , and control tasks .      that retrieve a feature's contribution to a model prediction .      Our implementation is model-agnostic, which means that any type of model architecture can be explained by it.    % <design principles> ?  In this paper we present both an overview of the library, as well as a case study on subject-verb agreement within language models. We first present a brief overview of interpretability within NLP and a background to the analysis techniques that are part of the library . We then provide an overview of \diagnnose and expand briefly on its individual modules . % Next, we provide a more extensive background on the feature attributions that are part of the library . We conclude with a case study on subject-verb agreement, demonstrating several of \diagnnose's features in an experimental setup .  
"," In this paper we introduce \diagnnose, an open source library for analysing the activations of deep neural networks. \diagnnose contains a wide array of interpretability techniques that provide fundamental insights into the inner workings of neural networks. We demonstrate the functionality of \diagnnose with a case study on subject-verb agreement within language models. \diagnnose is available at \url{https://github.com/i-machine-think/diagnnose}.",261
" % % % %   % % % % % \GW{Propaganda can be loosely defined as  {{www.britannica.com/topic/propaganda}}). }% % % % % % % % % % Various factors of propaganda have been studied in the humanities,  including emotionality of language, biased selection of information and deviation from facts, manipulation of cognition, and more . However, there is no consensus on the decisive factors that tell whether a given article or speech is propagandistic  or not. % % % % %  % In the modern digital world, the influence of propaganda on society has drastically increased.  % Hence, there is also a major increase in computer science, computational linguistics and computational sociology research on analyzing, characterizing and, ultimately, automatically detecting propaganda .  To a first degree, one may think of propaganda as a variation of fake news, and  some works investigate propaganda as a refined type of disinformation . % % % % % % % \GW{While false claims can be an element of propaganda, we think that fake news is merely the tip of the iceberg, and that the persuasive and manipulative nature of propagandistic contents requires deeper approaches.} Classifiers for propaganda detection need to better capture how propaganda is expressed in subtle ways by language style and rhetoric or even demagogic wording. This holds for news as well as social media posts and speeches. In all these cases, correct information may be presented in incomplete form or placed in distorted contexts, along with manipulative phrases, in order to mislead the audience.  % % % % Prior work has mostly looked into news articles  and tweets, and has typically focused on  strongly polarized topics like the 2016 US election and the related Russian Internet Research Agency  affair, the UK Brexit discussion, or political extremism. % % % % % % \LQ{All these approaches consider propaganda detection as a classification task assuming sufficient amounts of labeled in-domain training data.} \LQ{For example, in the ``Hack the News'' datathon challenge, a large number of news articles  and sentences  from such articles were annotated by distant supervision and human judgment, respectively, to train a variety of machine learning methods.} % % The resulting F1 scores on the leaderboard of this benchmark are amazingly high, around 90\%. This may give the impression that propaganda detection is a solved problem. However, most of the positively labeled samples are simple cases of ``loaded language'' with strong linguistic cues independent of the topic. Moreover, the learned classifiers % % benefit from ample training data, which is all but self-guaranteed in general.   % % % % %  % In this paper, we question these prior assumptions, hypothesizing  that propagandistic sources and speakers are sophisticated and creative and will find new forms of deception evading the trained classifiers. % % % \GW{The overall approach is still text classification; the novelty of our approach lies in {      [1]{}.~} [1]{\mbox{}} } [1]{}} [1]{{\mbox{#1}}} {\metric{P}} [1]{\mbox{\Pat@}} [1]{\mbox{\metric{R}@}} {``''} {``''} {``''} {``''}   
","  As news and social media exhibit an increasing amount of manipulative polarized content, detecting such propaganda has received attention as a new task for content analysis. Prior work has focused  % on supervised learning with training data from the same domain. However, as propaganda can be subtle and keeps evolving, manual identification and proper labeling are very demanding. As a consequence, training data is a major bottleneck.   In this paper, we tackle this bottleneck and present an approach to leverage cross-domain learning, based on labeled documents and sentences from news and tweets, as well as political speeches with a clear difference in their degrees of being propagandistic. We devise informative features and build various classifiers for propaganda labeling, using cross-domain learning.  % % % Our experiments demonstrate the usefulness of this approach, and identify difficulties and limitations in various configurations of sources and targets for the transfer step. We further analyze the influence of various features, and characterize salient indicators of propaganda. %",262
" \looseness=-1 Neural machine translation  architectures~ make it difficult for users to specify preferences that could be incorporated more easily in statistical MT models  and have been shown to be useful for interactive machine translation~ and domain adaptation~. Lexical constraints or preferences have previously been incorporated by re-training NMT models with constraints as inputs~ or with constrained beam search that drastically slows down decoding~.  \looseness=-1 In this work, we introduce a translation model that can seamlessly incorporate users' lexical choice preferences without increasing the time and computational cost at decoding time, while being trained on regular MT samples. We apply this model to MT tasks with soft lexical constraints. As illustrated in Figure, when decoding with soft lexical constraints, user preferences for lexical choice in the output language are provided as an additional input sequence of target words in any order. The goal is to let users encode terminology, domain or stylistic preferences in target word usage, without strictly enforcing hard constraints that might hamper NMT's ability to generate fluent outputs.    Our model is an Edit-Based TransfOrmer with Repositioning , which builds on recent progress on non-autoregressive sequence generation~. Specifically, the Levenshtein Transformer~ showed that iteratively refining output sequences via insertions and deletions yields a fast and flexible generation process for MT and automatic post-editing tasks. \modelname replaces the deletion operation with a novel reposition operation to disentangle lexical choice from reordering decisions. As a result, \modelname exploits lexical constraints more effectively and efficiently than the Levenshtein Transformer, as a single reposition operation can subsume a sequence of deletions and insertions. To train \modelname via imitation learning, the reposition operation is defined to preserve the ability to use the Levenshtein edit distance~ as an efficient oracle. We also introduce a dual-path roll-in policy which lets the reposition and deletion models learn to refine their respective outputs more effectively.  \looseness=-1 Experiments on Romanian-English, English-German, and English-Japanese MT show that \modelname achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer  on the standard MT tasks and exploit soft lexical constraints better: it achieves significantly better translation quality and matches more constraints with faster decoding speed than the Levenshtein Transformer. It also drastically speeds up decoding compared to lexically constrained decoding algorithms~. Furthermore, results highlight the benefits of soft constraints over hard ones \---\ \modelname with soft constraints achieves translation quality on par or better than both \modelname and Levenshtein Transformer with hard constraints~.      
"," We introduce an Edit-Based TransfOrmer with Repositioning , which makes sequence generation flexible by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation~, \modelname generates new sequences by iteratively editing hypotheses. It relies on a novel reposition operation designed to disentangle lexical choice from word positioning decisions, while enabling efficient oracles for imitation learning and parallel edits at decoding time. Empirically, \modelname uses soft lexical constraints more effectively than the Levenshtein Transformer~ while speeding up decoding dramatically compared to constrained beam search~. \modelname also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on standard Romanian-English, English-German, and English-Japanese machine translation tasks.",263
"  Word embeddings like Word2Vec or Glove can learn context-sensitive vector representations of words from very large corpora. These representations have proven useful for supervised tasks like language translation, entity recognition, sentiment analysis, or question answering.  The more general problem of tracking the semantic change of words through time has initially been addressed by a number of works, either by connecting several static embeddings through mapping transformations, or by initializing the training of each slice with the results from the previous one in the Word2Vec case . More recent works can deal with all the temporal slices simultaneously, as in Bamler and Mandt, Rudolph and Blei, and Yao et. al.  These works link the slices either by a diffusion process, a random walk, or a regularization term in the cost function.  The proposed approach does not assume any sequentiality in the slices .  By combining data from different sources, these embeddings can help to understand not only semantic drift, but also cross-cultural differences  or dialect variations .  In this work we consider that each corpus is divided into a set of segments called slices. All the slices are trained simultaneously, following the Word2Vec distributional hypothesis, with the addition that each word vector representation inside a slice is obtained by adding a central representation and a slice-dependent one. Thus, the different representations of one same word across different slices are tied by a common component. This constraint can be depicted as a star-like graph. Figure shows this representation for two cases:  a newspaper corpus through covering several years, and  a multi-source corpus combining two English-language newspapers.      The rest of the paper is organized as follows.  Section introduces the proposed model, giving its formal description, vocabulary selection and implementation details. Section describes the datasets used for this work: two corpora from The New York Times and The Guardian newspapers, and a curated multi-source corpus that combines both of them.   Section provides experimental work on the three datasets, and their corresponding quantitative and qualitative analysis. The related work is detailed in Section.  Finally, our conclusions and future work are discussed in Section.    
"," There is an increasing interest in the NLP community in capturing variations in the usage of language, either through time , across regions  or in different social contexts . Several successful dynamical embeddings have been proposed that can track semantic change through time.  Here we show that a model with a central word representation and a slice-dependent contribution can learn word embeddings from different corpora simultaneously. This model is based on a star-like representation of the slices. We apply it to The New York Times and The Guardian newspapers, and we show that it can capture both temporal dynamics in the yearly slices of each corpus, and language variations between US and UK English in a curated multi-source corpus. We provide an extensive evaluation of this methodology.",264
" The goal of relation extraction is to extract relationships between two entities from plain text. Supervised learning methods for relation extraction have been widely used to extract relations based on training labeled data. Distant supervision or crowdsourcing have been used to collect more examples with labels and train the model for relation extraction. However, these methods are limited by the quantity  and quality  of the training data because manually labeling the data is time-consuming and labor-intensive and data labeled by distant-supervision is noisy. To overcome the problem of insufficient high-quality data, few-shot learning have been designed to require only few labeled sentences for training. A lot of research has been done on few-shot learning for computer vision~, and some work also includes few-shot learning methods for relation extraction~. Although these works only require few instances for training, they still do not work in many scenarios in which no training instances are available.  Some work on open information extraction  discovers new relationships in open-domain corpora without labeling the data. OpenIE aims to extract relation phrases directly from text. However, this technique can not effectively select meaningful relation patterns and discard irrelevant information. In addition, this technique can not discover relations if the relation's name does not appear in the given sentence. For example, OpenIE can not identify the relation of the sentence shown in Figure.  To address the aforementioned limitations, we focus on relation extraction in the context of zero-shot learning. Zero-shot learning  is similar to the way humans learn and recognize new concepts. It is a novel learning technique that does not use any exemplars of the unseen categories during training. We propose a zero-shot learning model for relation extraction , which focuses on recognizing new relations that have no corresponding labeled data available for training. ZSLRE is modified on prototypical networks utilizing side  information.  We construct side information from labels and its synonyms, hypernyms of two name entities and keywords from training sentences. The ZSL-based model can recognize new relations based on the side information available for it instead of using a collection of labeled sentences. We incorporate side information to enable our model to extract relations that never appear in the training datasets. We also build an automatic hypernym extraction framework to help us acquire hypernyms of different entities directly from web. Details of side information construction are described in Section Side Information Extraction.     Figure shows an example of how side information can be used for extract relations. Different side information are given for different relations. The query sentence in the example has a relation of classmate\_of, but the word classmate never appears in the sentence. We first get the two name entities Nell Newman and Mayday Parker of the sentence and extract the hypernyms of the name entities person and person based on our proposed hypernym extraction module in Section Hypernyms Extraction. In this example, relationship capital\_of is eliminated because the hypernyms of capital\_of should be location and location. Then we extract the keywords course and school from the query sentence and compare the distance with the keywords in side information box.  In this way, relationship children\_of is eliminated.  To make relation extraction effective in real-world scenarios, we design our models with the ability that it can extract both relations with training instances and the relations without any training instances.  We modify the vanilla prototypical networks to deal with both scenarios and compare the distance between the query sentence and the prototype. If the exponential of the minus distance is above a threshold, we consider the query sentence has a new relation. For new relations extraction, we take the side information embedding from the query sentence and compare the distance of it with the side information embedding of new relations. We conduct different experiments on both a noisy and a clean dataset and adding different percentages of new relations to evaluate the effectiveness and robustness of our proposed model. Besides, we also evaluate our proposed model in supervised learning, few-shot learning and zero-shot learning scenarios and the results show that our proposed model outperforms other existing models in all three scenarios. The contributions of this paper can be summarized as follows:         The rest of this paper is organized as follows. Section Related Work reviews work on supervised relation extraction, open relation extraction and zero-shot learning.  Section Methodology describes the proposed ZSLRE model. Section Experiments presents the experiments and compares the performance of our model with other different models on two public datasets. Section Conclusion and Future Work includes a discussion of conclusion and promising future work.  
"," Most existing supervised and few-shot learning relation extraction methods have relied on labeled training data. However, in real-world scenarios, there exist many relations for which there is no available training data. We address this issue from the perspective of zero-shot learning  which is similar to the way humans learn and recognize new concepts with no prior knowledge. We propose a zero-shot learning relation extraction  framework, which focuses on recognizing novel relations that have no corresponding labeled data available for training. Our proposed ZSLRE model aims to recognize new relations based on prototypical networks that are modified to utilize side  information. The additional use of side information allows those modified prototype networks to recognize novel relations in addition to recognized previously known relations. We construct side information from labels and their synonyms, hypernyms of name entities, and keywords. We build an automatic hypernym extraction framework to help get hypernyms of various name entities directly from web. We demonstrate using extensive experiments on two public datasets  that our proposed model significantly outperforms state-of-the-art methods on supervised learning, few-shot learning and zero-shot learning tasks. Our experimental results also demonstrate the effectiveness and robustness of our proposed model in a combination scenario. Once accepted for publication, we will publish ZSLRE's source code and datasets to enable reproducibility and encourage further research.",265
"   Unlabeled data has been leveraged in many ways in natural language processing including  back-translation~, self-training~, or language model pre-training which led to improvements in many natural language tasks~. While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest~ with controlled studies showing a clear trend of diminishing returns as the amount of training data increases~.  In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text generation tasks for decades before the arrival of neural sequence to sequence models~. Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival~, it has been an important part in the winning entries of several high resource language pairs at WMT 2019~, improving over strong ensembles that used 500M back-translated sentences.  At the low resource WAT 2019 machine translation competition, noisy channel modeling was also a key factor for the winning entry~.  Noisy channel modeling turns text generation on the head: instead of modeling an output sequence given an input, Bayes' rule is applied to model the input given the output, via a backward sequence to sequence model which is combined with the prior probability of the output, typically a language model.  This enables the effective use of strong language models trained on large amounts of unlabeled data.  The role of the backward model, or the channel model, is to validate outputs preferred by the language model with respect to the input.  A straightforward way to use language models is to pair them with standard sequence to sequence models~. However, this does not address  under which modern neural sequence models still suffer~. As a consequence, models are susceptible to producing fluent outputs that are unrelated to the input~. The noisy channel approach explicitly addresses this via the channel model.   However, a major obstacle to efficient noisy channel modeling is that generating outputs is much slower than decoding from a standard sequence to sequence model. We address this issue by introducing several simple yet highly effective approximations which increase the speed of noisy channel modeling by an order of magnitude to make it practical. This includes smaller channel models as well as scoring only a subset of the channel model vocabulary.  Experiments on WMT English-Romanian translation show that noisy channel modeling can outperform recent pre-training results. Moreover, we show that noisy channel modeling benefits much more from larger beam sizes than strong pre-training methods.    
"," Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling.  The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, na\""{i}ve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives.  We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pre-training results by achieving a new state of the art on WMT Romanian-English translation.",266
"  % Sentiment analysis is a text classification technique that analyses a given text and returns the nature of the underlying opinion. Therefore, sentiment analysis is widely used for tasks such as brand monitoring, political research analysis, product analysis, workforce analysis and many more. Sentiment analysis techniques could be fundamentally sub divided into two categories as lexicon-based approach and machine learning based approach. Recently introduced deep learning based sentiment analysis techniques have outperformed the lexicon based approaches and traditional machine learning approaches.  With the development of deep learning techniques such as Convolutional Neural Networks , Recurrent Neural Networks  and language independent features, the domain of sentiment analysis has reported impressive results. Over the years, many of these variants and combinations of deep learning techniques and feature representations have been used for high resourced languages such as English. There also exist certain advancements in sentiment analysis for languages such as Chinese, Arabic, Spanish and some Indic languages.   Sinhala, which is a morphologically rich Indo-Aryan language, has not experienced these advancements due to its insular and under-resourced nature. One of the main challenges is not having large enough annotated corpora. The data set from~ is the only publicly  available annotated data set for sentiment analysis. However it includes only 5010 comments extracted from one news source, and contains only POSITIVE and NEGATIVE samples.  %Work of~ is an example of simple solutions for Sinhala sentiment analysis. Under these approaches, rule-based techniques, lexicon based techniques, supervised and semi-supervised machine learning techniques were employed with traditional language dependent features.   The 閾夸购st experiment on using deep learning techniques for Sinhala sentiment analysis was conducted by~. Under this research, basic deep learning techniques such as Long Short-Term Memory  network and CNN were used to categorize news comments as POSITIVE and NEGATIVE. %The LSTM trained with fastText embeddings outperformed traditional machine learning techniques such as Decision Tree, SVM, and Na\""ive Bayes. ~ conducted an experiment with the same data set using Sentence-State LSTM , which is a rather advanced technique where the analysis was further improved considering the n-gram features of text with word embeddings.  In this paper, we present a more comprehensive empirical study on the use of deep learning techniques for document-level sentiment analysis for Sinhala with respect to four sentiment categories as POSITIVE, NEGATIVE, NEUTRAL and CONFLICT. The experiments were conducted with the commonly used sequence models such as RNN, LSTM, Bi-LSTM, various improvements on these vanilla models such as stacking and regularization,  as well as more recent ones such as hierarchical attention hybrid neural networks and capsule networks. % for multi-class sentiment analysis using word embeddings as language independent features. These langauge independent features were able to outperform the usage of traditional language dependent features such as part of speech tagging and lexical resources.  ~Furthermore, we present a data set of 15059 comments, annotated with these four classes to be used for sentiment analysis, based on Sinhala news comments extracted from online newspapers namely GossipLanka and Lankadeepa. This is the only publicly available multi-class, multi-source dataset for Sinhala sentiment analysis.  Our code implementation, word embedding models, and annotated data set are publicly available.       % 
"," Due to the high impact of the fast-evolving fields of machine learning and deep learning, Natural Language Processing  tasks have further obtained comprehensive performances for highly resourced languages such as English and Chinese. However Sinhala, which is an under-resourced language with a rich morphology, has not experienced these advancements. For sentiment analysis, there exists only two previous research with deep learning approaches, which focused only on document-level sentiment analysis for the binary case. They experimented with only three types of deep learning models. In contrast, this paper presents a much comprehensive study on the use of standard sequence models such as RNN, LSTM, Bi-LSTM, as well as more recent state-of-the-art models such as  hierarchical attention hybrid neural networks, and capsule networks. Classification is done at document-level but with more granularity by considering POSITIVE, NEGATIVE, NEUTRAL, and CONFLICT classes. A data set of 15059 Sinhala news comments, annotated with these four classes and a corpus consists of 9.48 million tokens are publicly released. This is the largest sentiment annotated data set for Sinhala so far.   % In addition to that,  was extracted from both comments and articles of online newspapers. %Furthermore, the language-independent features such as Word2Vec and fastText were experimented for novel deep learning techniques which clearly indicate the importance of word embedding techniques for NLP tasks including sentiment analysis for Sinhala as a low resource language. % Due to the high impact of the fast-evolving field of machine learning and deep learning, the Natural Language Processing  tasks have further obtained comprehensive and prominent performances over the past few decades. Different variations and combinations of deep learning techniques have been employed for NLP tasks in general. These experiments illustrated highly improved performances with respect to the traditional rule-based and statistical machine learning techniques. These advancements were mainly impacted towards the development of popular languages such as English and Chinese. However, Sinhala which is an under-resourced language with rich morphology, have not experienced these advancements due to fewer resources for NLP tasks. For sentiment analysis, there exist only two previous research with deep learning approaches, which also conducted with less granularity while giving sub optimality with respect to recent advancements in deep learning techniques. In this paper, we present the use of state-of-the-art deep learning approaches such as RNN, LSTM, Bi-LSTM, Hierarchical Attention Hybrid Neural Networks, and capsule networks for multi-class sentiment analysis for Sinhala news comments while considering more granularity. Under this research, we present the multi-class annotated data set which consists of Sinhala news comments extracted from online newspapers. Furthermore, the language-independent features such as word2Vec and fastText were experimented for novel deep learning techniques which clearly indicates the importance of word embedding techniques for NLP tasks including sentiment analysis.",267
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. % % form to use if the first word consists of a single letter: % \IAENGPARstart{A}{demo} file is .... % % form to use if you need the single drop letter followed by % normal text : % \IAENGPARstart{A}{}demo file is .... % % Some journals put the first two words in caps: % \IAENGPARstart{T}{his demo} file is .... % % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word.  \IAENGPARstart{T}{he} Neural Machine Translation   has been used to model state-of-the-art translation systems for many high-resource languages . For many language pairs though, the amount and/or quality of parallel data is not enough to train an NMT model whose accuracy can reach an acceptable standard . This category of language pairs is known as low resource. Many works have explored how to use of the easier-to-get monolingual data to improve the quality of translation models in this category of languages -- and even high resource languages -- .  The back-translation has so far been one of the most successful methods , involving the use of the translations of the target language monolingual data to increase the amount of the training data . The additional parallel data consists of authentic sentences in the target language and their translations -- synthetic sentences in the source language -- generated using a reverse  model that is trained on the available parallel data -- see the procedure in Algorithm 1. The approach has proven to be successful at improving the quality of translations in high, middle and low resourced languages . Many studies have shown that the quality of the backward system influences the performance of the ultimate NMT model . In low resource conditions, the available parallel data may not be able to train a standard backward model and the quality of the additional data generated using this model may hurt the quality of the final model. Despite this, the aim of standard back-translation has always been to improve the performance of the target NMT model by providing sufficient training data.  Some previous works have proposed various methods to improve the performance of the backward model during training. These methods include iterative back-translation , transfer learning , self-training  and the training of a bi-directional translation model for both backward and forward translations . Others have tried to mask the deficiencies of the backward model either during inference through generating multiple translations of the same target sentence using sampling to average-out the errors in individual translations  and noising the output of beam search ; or reducing the effects of the errors in the synthetic data when training the forward model through methods such as tagged back-translation  and pre-training and fine-tuning .  We present a hybrid approach that utilizes the monolingual target data to improve both the forward and backward models in back-translation. In this approach, we used the synthetic data to enhance the backward model through self-learning and the standard back-translation for improving the forward model. The approach was preliminary investigated in  and it was shown to achieve positive results. Earlier use of stand-alone self-training in machine translation proposed extra methods of either using quality estimation  or freezing of the decoder weights  when training on the synthetic side of the training data. It was suggested that the mistakes in the synthetic data will hurt the performance of the self-trained model . Instead,  showed that self-training is capable of improving the quality of the backward model even without using either of the specialized approaches. It was shown that using all of the synthetic data generated by the backward model to help in re-training the backward model improved its performance. The work, though, did not show the benefits or otherwise of using any of the specialized approach in cleaning the data, especially in low resource languages. It also did not investigate if the model can continue to learn from its output through iterating the self-learning process.    \renewcommand{\arraystretch}{1.3} {rl} {\makecell[tl]{ALGORITHM I: STANDARD BACK-TRANSLATION }} \\ {l}{\makecell[tl]{Input: Parallel data \}, y^{})\}_{u=1}^U\) and Monolingual target \\ \quad \quad \quad \quad data \})\}_{v=1}^V\)}} \\ \);} \\ 3:& Train forward model \ on bilingual data \ and improved \ models}\\ {\textbullet}    The remainder of this paper is organized as follows: In Section , we reviewed the related works. We presented the proposed methods in Section . We reported the experiments and results in Section . We discussed the results and findings of the research work in Sections  and  respectively and, finally, the paper was concluded and directions for future work were proposed in Section .  
"," %\boldmath Many language pairs are low resource, meaning the amount and/or quality of available parallel data is not sufficient to train a neural machine translation  model which can reach an acceptable standard of accuracy. Many works have explored using the readily available monolingual data in either or both of the languages to improve the standard of translation models in low, and even high, resource languages. One of the most successful of such works is the back-translation that utilizes the translations of the target language monolingual data to increase the amount of the training data. The quality of the backward model which is trained on the available parallel data has been shown to determine the performance of the back-translation approach. Despite this, only the forward model is improved on the monolingual target data in standard back-translation. A previous study proposed an iterative back-translation approach for improving both models over several iterations. But unlike in the traditional back-translation, it relied on both the target and source monolingual data. This work, therefore, proposes a novel approach that enables both the backward and forward models to benefit from the monolingual target data through a hybrid of self-learning and back-translation respectively. Experimental results have shown the superiority of the proposed approach over the traditional back-translation method on English-German low resource neural machine translation. We also proposed an iterative self-learning approach that outperforms the iterative back-translation while also relying only on the monolingual target data and require the training of less models.",268
"   End-to-end techniques for automatic speech recognition , most notably sequence-to-sequence models with attention  and Recurrent Neural Network Transducer  , are becoming increasingly popular. Compared to the traditional hybrid system based on Hidden Markov Model and Deep Neural Network  with individually-trained components, all parts of an end-to-end model are optimized jointly, which often leads to better performance on recognition tasks with sufficient training data and low training-testing mismatch. End-to-end systems are simpler to train; they typically do not require pronunciation lexicons, decision trees, initial bootstrapping, nor forced alignment. End-to-end models are also more suitable for on-device use cases due to the lack of external language models  or decoding graphs, whose sizes can be prohibitively large in hybrid setups because of large vocabulary support, complex LMs, and context-dependent decision trees.  End-to-end systems do have limitations, however. Their end-to-end nature leads to a lack of composability, such as that between acoustic, language, and pronunciation models in hybrid setups. This lack of composability in turn leads to challenges in personalization, which traditionally involves on-the-fly modification of external LMs  to add, boost, and penalize certain words or phrases. Previous work in end-to-end ASR addressed this issue by incorporating external LMs during beam search , with special modifications to handle the model's spiky output . A fundamental limitation of shallow fusion is that it relies on late combination, hence the model needs to have the potential to produce the correct output in the first place without access to biasing information. Another class of method  adds an attention-based  or simple  biasing module over contextual phrases to provide additional signal to the decoder component of end-to-end models. While promising, these methods were shown to have problems scaling to large and highly confusable biasing lists.  A closely related challenge of ASR personalization is entity recognition, since in many cases biasing items are entity names. Rare name recognition presents significant challenges to end-to-end models because of two main reasons. First, the output units of end-to-end models are typically graphemes or WordPieces , both of which do not work well when the spelling of a word does not correspond to how it is pronounced . Second, rare names often decompose into target sequences that are not seen enough in training, making them difficult to recognize correctly. By contrast, both problems are alleviated in hybrid systems due to the use of phonetic lexicons and/or clustered context-dependent acoustic targets. Popular solutions to this problem include upsampling entity-heavy data or generating synthetic training data with names using text-to-speech  . While this method alleviates the data sparsity issue, it does not address the underlying problems of under-trained targets and unconventional spelling of rare names.   In this work, we propose several novel techniques to address both challenges and further improve RNN-T personalization. To alleviate the problem of under-trained targets and recognition of unconventional names, we adopt on-the-fly sub-word regularization  to increase WordPiece coverage during training, perform pre-training  and multi-task learning   to strengthen the encoder, and leverage grapheme-to-grapheme   to generate alternative graphemic pronunciations for names. To address the limitation of shallow fusion relying on late combination, we introduce deep personalized LM  fusion to influence the model's predictions earlier. We show that the combination of these techniques results in 15.4\%--34.5\% relative Word Error Rate  improvement on top of a strong RNN-T baseline which leverages shallow fusion and TTS augmentation. Our final model is also competitive with a hybrid system that has significantly larger disk and memory footprint.  
"," End-to-end models in general, and Recurrent Neural Network Transducer  in particular, have gained significant traction in the automatic speech recognition community in the last few years due to their simplicity, compactness, and excellent performance on generic transcription tasks. However, these models are more challenging to personalize compared to traditional hybrid systems due to the lack of external language models and difficulties in recognizing rare long-tail words, specifically entity names. In this work, we present novel techniques to improve RNN-T's ability to model rare WordPieces, infuse extra information into the encoder, enable the use of alternative graphemic pronunciations, and perform deep fusion with personalized language models for more robust biasing. We show that these combined techniques result in 15.4\%--34.5\% relative Word Error Rate improvement compared to a strong RNN-T baseline which uses shallow fusion and text-to-speech augmentation. Our work helps push the boundary of RNN-T personalization and close the gap with hybrid systems on use cases where biasing and entity recognition are crucial.",269
"  Our goal is to improve information extraction from business documents and contribute to the field of automated document processing. This work leads to a higher success metric and enables less manual work regarding data entry and/or annotation in the industry.  To put the work in context and define the terms closely let's briefly recall the definition of the task, the motivation and add more details.    The general problem of information extraction is not a new problem . A survey on information extraction methods  defines the task as: ``Information Extraction starts with a collection of texts, then transforms them into information that is more readily digested and analyzed. It isolates relevant text fragments, extracts relevant information from the fragments, and then pieces together the targeted information in a coherent framework''.  The relevant collection of texts for this study are the texts in business documents such as invoices, pro forma invoices and debit notes. The targeted information is a classification of the texts that helps in automating various business processes 閳 such as automated payment for invoices.    The typical user of our method would be any company medium-sized and bigger because, at some point, companies start to spend significant time on document processing. Details are harder to find in referenced and peer-reviewed works since the companies keep their spending information secret. Approximations from unofficial  sources as  and  lead to an estimate of how a success metric translates to company savings. A typical medium-sized company can have approximately  invoices per month and even just  improvement roughly translates to more than  dollars saving monthly and scales with the company size. Note that this is just a heuristics and thus we do not define the metric exactly.    As stated, we will focus on business documents. The explicit category of the documents varies. Existing works on information extraction  define these as ``visually rich documents'', ``structured'', or ``semi-structured''.   We will use the name ``structured documents'' throughout this work since the structure of the documents is clear and understandable to a human working in relevant fields, even though the specific structure varies. Moreover, the documents are machine-readable up to the detail of individual words and pictures  on a page, but for a machine, they are not ``understandable'' with respect to the goal of important information extraction.  It is important to classify all of the information that is needed in the financial/accounting industry, for the ``users'' of the documents. For example, the payment details, amount to be paid, issuer information etc. The input is a document's page and the goal is to identify and output all of the words and entities in the document that are considered important, along with their respective classifications.  One example of an input invoice and output extraction can be seen in \prettyref{fig:Example}. As you can see, the documents are not easily understandable inputs. An example of trivial inputs would be an XML document that has the desired target classes incorporated in a machine-readable way.  With this study, we aim to expand previous work , in which we have already shown that neural networks can succeed in the task of extracting important information and even identifying whole, highly specific tables.  As argued before, every improvement matters and so in this work, the focus is on improving the metrics by selecting relevant techniques from the deep learning field. A classical heuristic way to generally improve a target metric is to provide more relevant information to the network. Previously we have exhausted all the information present in a single invoice and so we will focus now on techniques related to ``similarity''. Existing works on similarity are presented in \prettyref{subsec:Inspiration} and our use and notion of similarity is defined here in \prettyref{subsec:The-learning-framework}. In short, we will present a similar annotated document as another input. More details on differences from the previous work are described in \prettyref{subsec:The-differences-to-prev}.  Since the idea of providing more information is fundamental even for simpler templating techniques , we need to stress that, due to the nature of our dataset , our problem cannot be solved by using templates. To prove this statement, a reasonable template-based baseline will be presented  and evaluated .  The research question will focus on a ``similarity'' based mechanism with various model implementations, and whether they can improve an existing solution . The hypothesis is that we are able to create at least one model that can significantly improve the results. Moreover, since the presented mechanism is theoretically applicable beyond the scope of document processing, this work can contribute to a broader audience.  Ultimately we will present a model and its source code  that outperforms the previous state-of-art results. An anonymized version of the dataset is also included as an open-source resource and should be a notable contribution since its size is greater than any other similar dataset known to date.        This subsection focuses on research on previous works and approaches in the relevant field of information extraction. The text in this subsection is heavily based on the text from .  The plethora of methods that have been used historically for general information extraction is hard to fully summarize or compare. Moreover, it would not be fair to compare methods developed for and evaluated on fundamentally different datasets.  However, we assessed that none of these methods is well-suited for working with structured documents , since they generally do not have any fixed layout, language, caption set, delimiters, fonts... For example, invoices vary in countries, companies and departments, and change in time. In order to retrieve any information from a structured document, you must understand it. Our criterion for considering a method to compare against is that no human-controlled preprocessing such as template specification or layout fixing is required because we aim for a fully automated and general solution. Therefore we will not be including any historical method as a baseline to compare against.  In recent works, a significant number does successfully use a graph representation of a document  and use graph neural networks. Also, the key idea close to the one-shot principle in information extraction is used and examined for example in  and . Both works use notions of finding similar documents and reusing their gold-standards . The latter  applies the principle in the form of template matching without the need for any learnable parameters.  Our approach can also be called ``word classification'' approach as written in , a work where an end-to-end architecture with a concept of memory is explored.  At this point, it is important to clarify the differences between other works and our stream of research .  The most important difference comes from the dataset that is at our disposal. The dataset explored here is far greater than the datasets used elsewhere, and allows for exploring deeper models as opposed to only using graph neural networks. Indeed in our previous paper, we have proven that graph neural networks work in synergy with additional convolution-over-sequence layers and even global self-attention. For clarity, the roles of said layers are described in \prettyref{subsec:Common-architecture}. Moreover, the dataset quality allowed us to discover  that information extraction and line-item table detection targets do boost each other.  As the research is focused on deeper models, we will not be using any of the other works as baselines and the commonly used graph neural networks will be incorporated only as one layer amidst many, with no special focus.  In the following pages, we will explore models that would be able to benefit from access to a known similar document's page. We hope that the model can exploit similarities between documents, even if they do not have similar templates.    A broader section on references is provided here since we are using a great variety of layers in the exploration of deep network architectures.    Presented in  is a model design concept that aims to improve models on new data without retraining of the network.   Typically, a classification model is trained to recognize a specific set of classes. In one-shot learning, we are usually able to correctly identify classes by comparing them with already known data. Unlike traditional multi-class classification, one-shot learning allows us to attain better scores even with surprisingly low numbers of samples . Sometimes it can work even for classes that are not present in the training set .  This concept can help in areas ranging from computer vision variants 閳 omniglot challenge   to object detection , finding similar images , face detection , autonomous vision , speech  and also the NLP area .  Among the methods that make one-shot learning able to work, the most fundamental one utilizes the concept of similarity. For similarity to work, we have two types of data 閳 ``unknown'' and ``known''. For the known data, its target values are known to the method and/or to the model. To classify any unknown input, the usual practice is to assign the same class to it as is the class of the most similar known input.  Technically speaking, the architecture  contains a 閳ユ笩iamese閳 part. In particular, both inputs  are passed to the same network architecture with tied weights. We will draw inspiration from this basic principle, and will leave other more advanced methods of one-shot learning  for further research.  Usually due to performance reasons the model is not asked to compare new inputs to every other known input 閳 only to a subset. Therefore, a prior pruning technique needs to be incorporated 閳 for example in the form of the nearest neighbor search in embedding space, as is done for example in the work . Another option would be to incorporate a memory concept  .  The loss used for similarity learning is called triplet loss because it is applied on a triplet of classes  for each data-point:   L=\min-f\right\Vert ^{2}-\\  & \left\Vert f-f\right\Vert ^{2}+\alpha,0)  Where  is a margin between positive and negative classes and  is the model function mapping inputs to embedding space .  Generally speaking, one-shot learning can be classified as a meta-learning technique. For more on meta-learning, we suggest a recent study, like  . Taking the concept one step further yields a concept called ``zero-shot learning'' .     It is now beneficial to mention other sources of inspiration that are also meaningfully close to one-shot learning. Since we ask ``what labels are similar in the new data'', a ``query answer'' approach should be considered. Recently, the attention principle  successfully helped to pave the way in language models . It is not uncommon to use attention in one-shot approaches   and also query answer problems in various problems domains .   The mentioned task of similarity can also be approached as pairwise classification, or even dissimilarity .    
"," The automation of document processing is gaining recent attention due to the great potential to reduce manual work through improved methods and hardware. Any improvement of information extraction systems or further reduction in their error rates has a significant impact in the real world for any company working with business documents as lowering the reliability on cost-heavy and error-prone human work significantly improves the revenue. In this area, neural networks have been applied before 闁 even though they have been trained only on relatively small datasets with hundreds of documents so far.  To successfully explore deep learning techniques and improve the information extraction results, a dataset with more than twenty-five thousand documents has been compiled, anonymized and is published as a part of this work. We will expand our previous work where we proved that convolutions, graph convolutions and self-attention can work together and exploit all the information present in a structured document. Taking the fully trainable method one step further, we will now design and examine various approaches to using siamese networks, concepts of similarity, one-shot learning and context/memory awareness. The aim is to improve micro $F_{1}$ of per-word classification on the huge real-world document dataset.  The results verify the hypothesis that trainable access to a similar  page together with its already known target information improves the information extraction. Furthermore, the experiments confirm that all proposed architecture parts  are all required to beat the previous results.  The best model improves the previous state-of-the-art results by an $8.25\,\%$ gain in $F_{1}$ score. Qualitative analysis is provided to verify that the new model performs better for all target classes. Additionally, multiple structural observations about the causes of the underperformance of some architectures are revealed.  All the source codes, parameters and implementation details are published together with the dataset in the hope to push the research boundaries since all the techniques used in this work are not problem-specific and can be generalized for other tasks and contexts.",270
"%Thanh     COLIEE is an annual competition to find automated solutions in  the field of law. This competition is challenging because legal documents are often complex and require a high level of comprehension. The problems in law are even tricky for law experts. COLIEE tasks cover two of the most popular legal systems in the world, Case law and Civil law. COLIEE provides real data from the Canadian judicial system and the Japanese legal system.     COLIEE organizes 4 tasks divided into 2 categories: retrieval and entailment. For retrieval tasks, the systems need to automatically find out the supporting cases of a given query case  or the relevant articles of a given bar question .     For the entailment tasks, the systems need to find the paragraphs in a relevant case that entail a given decision  or to conclude whether the statement of a given question is correct or incorrect .     These tasks can be solved with various text processing methods. On one hand, over the years, systems using only lexical similarity of texts yield inferior performance. On the other hand, deep learning approaches start gaining superior performance recently.       % 
"," We propose deep learning based methods for automatic systems of legal retrieval and legal question-answering in COLIEE 2020. These systems are all characterized by being pre-trained on large amounts of data before being finetuned for the specified tasks. This approach helps to overcome the data scarcity and achieve good performance, thus can be useful for tackling related problems in information retrieval, and decision support in the legal domain. Besides, the approach can be explored to deal with other domain specific problems.",271
"   The dominant paradigm in supervised NLP today is learning from examples, where machine learning algorithms are trained using a large set of task-specific input-output pairs. In contrast, humans learn to perform the same task by reading a description, after which they are able to perform the task in a zero-shot manner---indeed, this is how crowd-sourced NLP datasets are constructed. In this paper, we argue that learning from task descriptions in this way is a necessary attribute of a general purpose NLP system, and we propose it as a new paradigm to train and test NLP systems.    Recent work in NLP has shown significant progress in learning tasks from examples. Large pretrained language models have dramatically improved performance on standard benchmarks and have shown promising results in zero shot prediction by leveraging their language understanding capabilities.   Despite this progress, there are many serious issues that come with learning from examples.  There is an almost infinite number of tasks that a person might wish to solve with a general-purpose NLP system.  Learning to solve these tasks by reading a description instead of observing a collection of examples would solve the problem of having to create training sets for each language task.  Such a system would also be more accessible to practitioners and domain experts in other fields, who could describe their tasks and solve them, opening up new avenues of research where it is expensive or infeasible to gather training data.    Additionally, we find that current supervised learning techniques partly achieve their success due to memorizing uninteresting aspects of the training distribution.  Teaching a system to learn a task from the description alone would alleviate these biases, as new training data would not be needed to learn a novel task.  In this paper, we synthesize prior approaches to zero-shot learning in NLP and provide a formal framework for thinking about the zero-shot prediction problem.  We show that previous zero-shot approaches are limited in both scope of application and rigour of evaluation.  For example, while prior work has used zero-shot prediction for text classification, entity typing, and relation extraction, we push this to the more complex task of slot filling.  We instantiate our formalism in an English language dataset, \dataset , that is formatted similarly to reading comprehension datasets, in that we formulate task descriptions as questions and pair them with paragraphs of text.  We choose this format as it provides a natural way to crowdsource data.  This zero-shot dataset differs from typical reading comprehension datasets, however, in that each task description is paired with twenty different passages, and we evaluate a model's ability to solve the task, not just give the correct answer for a single  pair.  That is, given a question, a model produces some decision function , and it is this function which we comprehensively evaluate on many different inputs.  We also carefully select axes on which to evaluate the generalization of a model to different kinds of task descriptions, changing task descriptions in specific ways to systematically push the field towards more interesting and complex task descriptions.  We evaluate models based on recent state-of-the-art sequence to sequence architectures, which seem most suited to the task of zero shot prediction in this setting.  We find that our best model based on T5  achieves a score of only \finalscore\% on this data, leaving a significant gap to our human performance estimate of \humanestimate\%.  Zero shot learning from complex task descriptions remains a significant challenge for current NLP systems.    
"," Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this framework with a new English language dataset, \dataset, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of \finalscore\% on \dataset, leaving a significant challenge for NLP researchers.\footnote{Data, evaluation code, baseline models, and leaderboard at \url{https://allenai.org/data/zest}}",272
"  Because of the fact that obtaining   supervised training labels is costly and time-intensive,   and that   unlabeled data is relatively easy to obtain,   semi-supervised learning  , which  utilizes  in-domain  unlabeled data  to improve models trained on the labeled dataset , is of growing interest.  Under the context of large-scale of language model pretraining ,  where a language model is pretrained on an extremely large, open-domain dataset  affect performances regarding  of different sizes, and  of different sizes, etc.    In this paper, we conduct comprehensive studies on the behavior of semi-supervised learning in NLP  with the presence of large-scale language model pretraining.   We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks. Our work sheds important lights on the behavior of semi-supervised learning models:  we find that   with the presence of  in-domain pretraining LM on , open-domain LM pretraining   is unnecessary, and we are able to achieve better performance with pretraining on  the in-domain dataset ;    both the in-domain pretraining strategy and the pseudo-label based strategy  lead to significant performance boosts, with the former performing better with larger , the latter performing better with smaller , and the  combination   of both performing the best;  for pseudo-label based strategies,  self-training  yields better performances when  is small, while joint training on the combination of   and  yields better performances when  is large.   Using semi-supervised learning models, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6 with the full dataset.  More importantly, our work marks an initial step toward understanding the behavior of semi-supervised learning models in the context of large-scale pretraining.    The rest of this paper is organized as follows: related work is detailed in Section 2.  Different strategies for training semi-supervised models are shown in Section 3.  Experimental results and findings are shown in Section 4, followed by a brief conclusion in Section 5.   
"," The goal of semi-supervised learning is to utilize the unlabeled, in-domain dataset $U$ to improve models trained on the labeled dataset $D$.     Under the context of   large-scale language-model  pretraining,   how we  can make the best use of   $U$   is poorly understood:   Is semi-supervised learning still beneficial   with the presence of  large-scale pretraining?  Should $U$ be used for in-domain LM pretraining or pseudo-label generation? How should the pseudo-label based semi-supervised model    be actually implemented? How different semi-supervised strategies  affect performances regarding $D$ of different sizes, $U$ of different sizes, etc.   In this paper, we conduct comprehensive studies  on  semi-supervised learning in the  task of text classification   under the context of  large-scale LM pretraining. Our studies shed important  lights on the  behavior of semi-supervised learning methods.   We find that:    with the presence of  in-domain LM pretraining  on $U$, open-domain LM pretraining   is unnecessary, and we are able to achieve better performance with pretraining on  the in-domain dataset $U$;  both the in-domain pretraining strategy and the pseudo-label based strategy introduce  significant performance boosts,  with the former performing better with larger $U$,  the latter performing better with smaller $U$, and the combination leading to the largest performance gain;   vanilla self-training  yields better performances when $D$ is small, while joint training on the combination of  $D'$ and $D$ yields better performances when $D$ is large.   %We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks.  Using semi-supervised learning strategies, we are able to achieve a performance of around $93.8\%$ accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6$\%$ with the full  IMDB dataset.  Our work marks an initial step toward understanding the behavior of semi-supervised learning models under the context of large-scale pretraining.\footnote{Code, models and datasets  can be found at https://github.com/ShannonAI/Neural-Semi-Supervised-Learning-for-Text-Classification}",273
"  \todo{Completely rewrite - emphasize that many methods have been proposed for learning embeddings  learn representations of the entities in a knowledge base  typically based on the text of each entity's Wikipedia article or the surrounding local context for mentions of each entity . %   context surrounding mentions of each entity Recent advances in neural EL have involved methods for pretraining entity embeddings using the link graph of Wikipedia to learn related entities and words . Similar to word embeddings, past work has shown that these embeddings reside in a high-dimensional pseudo-semantic space, with entities that are close in the space being semantically similar . % \glarionov{""with entities close in the space being...} However, little work has been done to understand what information different entity embeddings capture about the underlying entities and how that information affects downstream performance.  Our goal in this work is to identify semantic information in entity representations and determine how that information is linked to performance on downstream EL tasks. For this, we develop a series of probing tasks, which have previously been used to examine lexical and syntactic properties of neural model layers such as sentence encoders and decoders for neural machine translation systems .  % \glarionov{I would group these two citations at the end for readability} % . We extract structured data about entities using DBpedia and context words from Wikipedia anchor links to create probing tasks designed to evaluate the knowledge-based and distributional semantic contents of different entity embedding models.  We compare five entity embedding methods, first by them on two downstream EL tasks. We then probe the learned embeddings to evaluate what semantic information is important for the downstream tasks and how it is represented by the different models. %  We find that pretrained entity embedding methods are generally more effective at representing distributional and knowledge-based semantic information than models that generate embeddings as a byproduct of training on an EL task. These improved representations lead to better performance on the EL tasks, with the best model showing high performance on both distributional and knowledge-based semantic tasks. We further find that entity embeddings trained to predict related words and entities in a skipgram-like model are able to learn fine-grained entity type information and specific relationship types between entities without explicitly providing this information.  Our primary contributions with this work are to:        % 1) describe methods for evaluating the semantic information learned by these methods and 2) to empirically demonstrate the importance of this information in creating models of entities for use in downstream tasks. %  Our hope is that this information can provide guidance in developing architectures that better combine explicit structured information with text to improve methods for representing entities that can be used in a variety of downstream tasks, similar to existing word embeddings. Our methods can additionally be used to potentially detect deficiencies in new representation methods and biases of learned attributes through other probing tasks. % and biases of current methods by probing .  
","  \todo{Complete rewrite} Pretrained entity embedding methods have shown strong results in entity linking  systems compared to methods that generate entity representations from text descriptions. Prior work has shown that these embeddings inhabit a pseudo-semantic space, but the semantic information they contain has not been thoroughly explored nor have  they been compared with other representations for differences in information.  We introduce methods for probing learned entity representations for information about their entity types, relationships, and context words using Wikipedia anchors and DBPedia structured data and use them to compare five entity embedding models. We show that improved representation of all types of semantic information is linked to improved performance on two downstream EL tasks. Our results provide potential directions for further research to better incorporate explicit semantic information into neural entity linking models.",274
" 	In this section, we mention different tokenization techniques for SLT and explain our perspective on the problem. We mentioned about the basics of SLT and NMT. From our research perspective, NMT methods can provide successful results if we have good tokens from SL vides. Therefore, tokenization is seen as the most crucial part of this research. Firstly, the visual properties are involved in the tokenization part. Secondly, there is no a generic approach to obtain strong tokens for all the SLs. In addition to that, it is not clear that discrete tokens should be obtained for better translation quality. For this reason, we extend the meaning of tokenization for NSLT and it covers the overall process to prepare the frames for the NMT module. 	\par For spoken to spoken languages, we generally use words as tokens to feed the NMT module. The current state-of-the-art method converts those tokens to continuous embeddings to reach a semantic representation. While learning translation, the word embedding is also trained to learn the relationship between words. Eventually, a meaningful embedding is obtained before the NMT module as seen in Figure . Based on this, it may be a good idea to learn a good representation of signs to replace with word embeddings to achieve the same advancements in NSLT as NMT has done. This representation is cross-lingual; but learning it is an open problem. Our research is mainly focused on this problem. Before introducing our approach, we discuss the existing three tokenization approaches in the following subsection.   	   \par  The first approach is using glosses as tokens. Glosses are intermediate word-like representations between signs and words in sentences. Therefore, they can be directly applicable to the NMT framework without any further effort. However, there are certain shortcomings in this method. Firstly, glosses rarely exist in real life. Gloss annotation requires a laborious process and special expertise. Secondly, glosses are unique to SLs. Therefore, each SL requires special effort to obtain glosses whereas sentences are commonly available. The last drawback is that a mistake in the gloss level can produce dramatic meaning differences in translation, since glosses are high level annotations, similar to words.  \par The second approach is the same as the first one in terms of tokens. On top of that, this approach learns to extract glosses from frames. In other words, this method uses glosses as explicit intermediate representations as seen in Figure . It eliminates the further search for tokenization, but it needs a special network for frame to gloss conversion. There are two main concerns. The first one is that a network for frame to gloss conversion is still dependent on gloss annotations. The second is that it is not clear that glosses are the upper bound for SLT as there is not sufficient evidence. The problem is immature and the result in  provides clues about whether glosses may restrict translation quality.    The third approach is called frame-level tokenization. This approach does not establish any explicit intermediate representation as seen in Figure . It aims to learn good sign embeddings to replace with word-embeddings. However, there is no golden way to represent signs with embeddings to feed into the NMT module. Furthermore, it is not clear what the length of the embedding should be. Embeddings can be obtained from each frame or extracted from inner short clips in the video. In addition to that, the representation can be learned with sentence-video pairs or trained outside the NSLT system. There are several ways for frame-level tokenization. However, the main difference from the gloss level tokenization is that discrete representation can be eliminated. If we find a proper one, there would be several advantages. The first one is that the resulting framework can be applied to any SL translation task without requiring annotation. The second advantage is the opportunity to inject additional supervision. The representations would be trained on different tasks and different datasets whereas gloss level tokenization cannot cover different SLs. The third one is that the token length can be adjusted. To boost translation speed, the number of tokens can be reduced to a pre-determined number.     
"," In this thesis, we propose a multitask learning based method to improve Neural Sign Language Translation  consisting of two parts, a tokenization layer and Neural Machine Translation . The tokenization part focuses on how Sign Language  videos should be represented to be fed into the other part. It has not been studied elaborately whereas NMT research has attracted several researchers contributing enormous advancements. Up to now, there are two main input tokenization levels, namely frame-level and gloss-level tokenization. Glosses are world-like intermediate presentation and unique to SLs. Therefore, we aim to develop a generic sign-level tokenization layer so that it is applicable to other domains without further effort. \par We begin with investigating current tokenization approaches and explain their weaknesses with several experiments. To provide a solution, we adapt Transfer Learning, Multitask Learning and Unsupervised Domain Adaptation into this research to leverage additional supervision. We succeed in enabling knowledge transfer between SLs and improve translation quality by 5 points in BLEU-4 and 8 points in ROUGE scores. Secondly, we show the effects of body parts by extensive experiments in all the tokenization approaches. Apart from these, we adopt  3D-CNNs to improve efficiency in terms of time and space. Lastly, we discuss the advantages of sign-level tokenization over gloss-level tokenization. To sum up, our proposed method eliminates the need for gloss level annotation to obtain higher scores by providing additional supervision by utilizing weak supervision sources.",275
" Transfer Learning  is a rapidly growing field of machine learning that aims to improve the learning of a data-deficient task by transferring knowledge from related data-sufficient tasks.  %Witness the success of deep learning, deep transfer learning has been widely studied and demonstrated remarkable performance over various applications, such as xxx.  Witnessing the great representation learning abilities of deep neural networks, neural architectures based TL methods, i.e., deep transfer learning, have gained increasing popularity and are shown to be effective for a wide variety of  applications. %%%%%  A few TL toolkits have also been developed to make it easy to apply TL algorithms. Notable projects include:        is a python based AI toolkit for training AI models and customizing them with users' own datasets. However, it mainly focuses on the computer vision field.      is an MXNet library which largely automates deep TL. It contains the ``ModelHandler'' component to extract features from pre-trained models and the ``Repurposer'' component to re-purpose models for target tasks.      is an integrated interface for 17 TL models written by python. It includes five types of models, namely ``feature-based'', ``concept-based'', ``parameter-based'', ``instance-based'' and ``deep-learning-based''.      specifically addresses model-finetuning, especially for BERT-like models. It is backended by PyTorch and Tensorflow and integrates 30+ pre-trained language models.     However, when it comes to industrial-scale real-world applications, the above mentioned toolkits might be less ideal. The reasons are threefold. i) Deep learning models are getting larger and larger, which makes it difficult to deploy those models in real-time applications. For example, pre-trained contextual representation encoders, such as BERT , RoBERTa  and GPT , have been widely adopted in a variety of Natural Language Processing  tasks . Despite their effectiveness, these models are built upon large-scale datasets and usually have parameters in the billion scale. To elaborate, the BERT-base and GPT-3 models are with M and B parameters respectively. This makes it difficult to train and deploy such models in real-time applications that have limited resources and require high inference speed. ii) There are a variety of TL algorithms proposed in literature, yet no comprehensive TL toolkit is available for users to examine different types of state-of-the-art TL algorithms. iii) A huge gap still exists between developing a fancy algorithm for a specific task and deploying the algorithm for online production. For many online applications, it is still a non-trivial task to provide a reliable service with high QPS~.   % The rest of this paper is organized as follows: Section  introduces the basic setting of deep transfer learning and background of inference attacks against deep learning models. Section  provides the general categorization of deep transfer learning and detailed privacy analysis. Section  shows the information leakage in deep transfer learning empirically. % Section briefly summarizes some % related works and Section draws the conclusion.  %% %% This is file `sample-sigconf.tex', %% generated with the docstrip utility. %% %% The original source files were: %% %% samples.dtx   %%  %% IMPORTANT NOTICE: %%  %% For the copyright see the source file. %%  %% Any modified versions of this file must be renamed %% with new filenames distinct from sample-sigconf.tex. %%  %% For distribution of the original source see the terms %% for copying and modification in the file samples.dtx. %%  %% This generated file may be distributed as long as the %% original source files, as listed above, are part of the %% same distribution.  %% %% The first command in your LaTeX source must be the \documentclass command. \documentclass[sigconf]{acmart} %%%% As of March 2017, [siggraph] is no longer used. Please use sigconf  for SIGGRAPH conferences.  %%%% Proceedings format for SIGPLAN conferences  % \documentclass[sigplan, anonymous, review]{acmart}  %%%% Proceedings format for SIGCHI conferences % \documentclass[sigchi, review]{acmart}  %%%% To use the SIGCHI extended abstract template, please visit % https://www.overleaf.com/read/zzzfqvkmrfzn  \usepackage{xcolor} \usepackage{soul} \usepackage{url} \usepackage{caption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{amsthm} \usepackage{booktabs} %\usepackage{algorithm} %\usepackage{algorithmic} \usepackage{multirow} \usepackage{array}  \usepackage{listings} \usepackage{color} \definecolor{mygreen}{rgb}{0,0.6,0} \definecolor{mygray}{rgb}{0.5,0.5,0.5} \definecolor{mymauve}{rgb}{0.58,0,0.82}  % \lstset{ % %   frame=tb,     %   backgroundcolor=,   % choose the background color %   basicstyle= %\orcid{1234-5678-9012} %\footnote{Corresponding author.} \affiliation{% 	 } %@alibaba-inc.com} %@alibaba-inc.com} %  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research. % \author{Ben Trovato} % % \authornote{Both authors contributed equally to this research.} %  % \affiliation{% %    %    %    % }  % \author{Lars Th{\o}rv{\""a}ld} % \affiliation{% %   rv{\""a}ld Group} %   rv{\""a}ld Circle} %    %    % } %   % \author{Valerie B\'eranger} % \affiliation{% %    %    %    % }  %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose.  % \renewcommand{  \renewcommand{ \renewcommand{\authors}{Qiu et al.}  %% %% The abstract is a short summary of the work to be presented in the %% article.  The literature has witnessed the success of applying deep Transfer Learning  algorithms to many NLP applications, yet it is not easy to build an easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the EasyTransfer platform is designed to make it easy to develop deep TL algorithms for NLP applications. It is built with rich API abstractions, a scalable architecture and comprehensive deep TL algorithms, to make the development of NLP applications easier. To be specific, the build-in data and model parallelism strategy shows to be 4x faster than the default distribution strategy of Tensorflow. EasyTransfer supports the mainstream pre-trained ModelZoo, including Pre-trained Language Models  and multi-modality models. It also integrates various SOTA models for mainstream NLP applications in AppZoo, and supports mainstream TL algorithms as well. The toolkit is convenient for users to quickly start model training, evaluation, offline prediction, and online deployment.  This system is currently deployed at Alibaba to support a variety of business scenarios, including item recommendation, personalized search, and conversational question answering. Extensive experiments on real-world datasets show that EasyTransfer is suitable for online production with cutting-edge performance. The source code of EasyTransfer is released at Github~\footnote{https://github.com/alibaba/EasyTransfer}.   %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %% %  % <ccs2012> % <concept> % <concept_id>10002978.10003022</concept_id> % <concept_desc>Security and privacy~Software and application security</concept_desc> % <concept_significance>500</concept_significance> % </concept> % <concept> % <concept_id>10010147.10010257.10010258.10010262.10010277</concept_id> % <concept_desc>Computing methodologies~Transfer learning</concept_desc> % <concept_significance>500</concept_significance> % </concept> % </ccs2012> %   %  %   %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.    %% This command processes the author and affiliation and title %% information and builds the first part of the formatted document.   % %    
"," The literature has witnessed the success of applying deep Transfer Learning  algorithms to many NLP applications, yet it is not easy to build an easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the EasyTransfer platform is designed to make it easy to develop deep TL algorithms for NLP applications. It is built with rich API abstractions, a scalable architecture and comprehensive deep TL algorithms, to make the development of NLP applications easier. To be specific, the build-in data and model parallelism strategy shows to be 4x faster than the default distribution strategy of Tensorflow. EasyTransfer supports the mainstream pre-trained ModelZoo, including Pre-trained Language Models  and multi-modality models. It also integrates various SOTA models for mainstream NLP applications in AppZoo, and supports mainstream TL algorithms as well. The toolkit is convenient for users to quickly start model training, evaluation, offline prediction, and online deployment.  This system is currently deployed at Alibaba to support a variety of business scenarios, including item recommendation, personalized search, and conversational question answering. Extensive experiments on real-world datasets show that EasyTransfer is suitable for online production with cutting-edge performance. The source code of EasyTransfer is released at Github.",276
" Sentiment polarity detection regarded as one of the significant research problems for opinion extraction in natural language processing . In recent years, the plenteous growth of the internet and the random access of e-devices facilitate the generation of voluminous reviews or opinions in textual form on social media or online platforms. Most of these reviews express the consumers feedback toward the products and services that they received. Several business companies, as well as online marketers, take advantage of these feedbacks to provide praiseworthy services to the consumers. In addition to that customer makes a perfect decision based on the previous reviews before receiving products or services.  Sentiment detection is a computational technique that attempts to uncover the viewpoint of a user towards a specific entity. It aims to identify the contextual polarity of the text contents  as the positive, neutral and negative . Sentiment analysis or detection has shown a remarkable impact in the business community, whereby taking into account the user opinions the communities can ensure the sustainability of their product or services. The restaurant is one such business, where customers opinions can be utilized to improve their quality of foods, environments, and services. Pompous lifestyle and assorted food habits led to a significant increase in the number of people in restaurants. To collect the excellence of services, customers instinct to look through the restaurant reviews before visit it. Therefore, reviewing a restaurant via the internet has become an ecumenical trend. Besides, an abundant amount of positive reviews can make a restaurant as a symbol of faith towards the customers. Also, it can assist a restaurant to reach the pinnacle of success. In contrast, without a sufficient amount of positive reviews, it becomes difficult to gain the attention of new customers by a restaurant. Sometimes, a restaurant with negative reviews loses the trustworthiness of the customers, which turned into reducing the profit.  Straightforwardly, users opinions on specific criteria such as food quality, ambience and service standards of a restaurant can have enough influence on the customers liking. However, it would not be wrong to say that customers inclination or reluctance towards a restaurant depends on the amount of positive and negative reviews. Therefore, the restaurants should appreciate the consents as well as the opinions of the customers. Nevertheless, scrutinizing every reviews one by one is a very time consuming as well as cumbersome task. Further, to govern such surveys, it requires plenty amount of investment in both money and human resources. Considering the fact of the explosive growth of the visitors as well as user preferences, it requires an automatic system that can comprehend the contextual polarity of reviewer opinions posted in different online platforms including Facebook, Twitter, company website, and blogs. Nevertheless, sentiment classification is a challenging research issue in a resource-poor language like Bengali. The inadequacy of benchmark dataset and the limited amount of e-textual contents or reviews in the Bengali language resulted in the sentiment classification task complicated. Deep learning algorithms are very effective to tackle such complications and classify the sentiments correctly . One main advantage of these algorithms are their ability to capture the semantic information in long texts. This paper proposed a deep learning-based sentiment classification technique to classify sentiment form reviews. By taking into consideration the current constraints of sentiment analysis in low resource languages, this paper contributions illustrate in the following:     
"," The amount of textual data generation has increased enor-mously due to the effortless access of the Internet and the evolution of various web 2.0 applications. These textual data productions resulted because of the people express their opinion, emotion or sentiment about any product or service in the form of tweets, Facebook post or status, blog write up, and reviews. Sentiment analysis deals with the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer闁炽儲鐛 attitude toward a particular topic is positive, negative, or neutral. The impact of customer review is significant to perceive the customer attitude towards a restaurant. Thus, the automatic detection of sentiment from reviews is advantageous for the restaurant owners, or service providers and customers to make their decisions or services more satisfactory. This paper proposes, a deep learning-based technique  to classify the reviews provided by the clients of the restaurant into positive and negative polarities. A corpus consists of 8435 reviews is constructed to evaluate the proposed technique. In addition, a comparative analysis of the proposed technique with other machine learning algorithms presented. The results of the evaluation on test dataset show that BiLSTM technique produced in the highest accuracy of 91.35\%.",277
"  %  %       Storytelling is a central part of human socialization and entertainment. Many of the popular forms of storytelling throughout history \---such as novels, plays, television, and movies\--- have passive audience experiences. However, gaming is an interesting medium because interactivity is a large part of the entertainment experience, and interactivity and storytelling can often be in conflict: too much player freedom means a storyline may never be explored, while on the other hand, too many restrictions on player freedom risks reducing gaming to a passive medium. Thus, interactivity in storytelling has been an important challenge for gaming, with much design effort put into striking a balance between entertaining gameplay and compelling storytelling.  As gaming technology advances, new opportunities for interactive storytelling present themselves. Better storage technology made telling longer, more intricate stories possible, and better graphical capabilities helped foster more immersive gaming experiences. Advances in artificial intelligence have lead to more challenging opponents, more realistic NPC behavior, and other benefits. Better procedural content generation algorithms help ensure unique gameplay experiences that stay fresh for longer. Finally, recent breakthroughs in language modeling present a new opportunity: language, and thus stories, can potentially be generated on demand.   In this paper, we introduce a novel game of { \---opening sentences meant to kick-start participants' storytelling creativity\--- and the human player responds by adding a line, which we refer to from here on out as a {.  Our primary contributions are as follows:       , where humans and AI agents work together to create a story.       
","   Storytelling plays a central role in human socializing and entertainment. However, much of the research on automatic storytelling generation assumes that stories will be generated by an agent without any human interaction. In this paper, we introduce the task of collaborative storytelling, where an artificial intelligence agent and a person collaborate to create a unique story by taking turns adding to it. We present a collaborative storytelling system which works with a human storyteller to create a story by generating new utterances based on the story so far. We constructed the storytelling system by tuning a publicly-available large scale language model on a dataset of writing prompts and their accompanying fictional works. We identify generating sufficiently human-like utterances to be an important technical issue and propose a sample-and-rank approach to improve utterance quality. Quantitative evaluation shows that our approach outperforms a baseline, and we present  qualitative evaluation of our system's capabilities.",278
"   The vast amounts of scientific literature can provide a significant source of information for biomedical research. Using this literature to identify relations between entities is an important task in various applications .  Existing approaches to biomedical relation extraction usually fall into one of two categories. Mention-level extraction aims to classify the relation between a pair of entities within a short span of text . In contrast, pair-level extraction aims to classify the relation between a pair of entities across an entire paragraph, document or corpus.  For both mention-level and pair-level relation extraction, recent work has been focused on representation learning. This is considered to be one of the major steps towards making progress in artificial intelligence . Representations of relations which understand their context are particularly important in biomedical research, where identifying fruitful targets is crucial due to the high costs of experimentation. Learning such representations is likely to require large amounts of unsupervised data due to the scarcity of labelled data in this domain.  Recent mention-level methods have been based on using large unsupervised models with Transformer networks  to learn representations of sentences containing pairs of entities. These representations are then used as the inputs to much smaller models, which perform supervised relation classification .  Recent pair-level methods have been based on encoding each mention of a pair of entities, and designing a mechanism to pool these encodings  into a single representation. This representation is then used to classify the relation between the entity pair .  However, representation learning methods for both mention-level and pair-level extraction typically use a point estimate for each representation. As a result, they may struggle to capture the nature of the true, potentially complex relations between each pair of entities. For example, Figure  shows sentences for two entity pairs which demonstrate that relation statements can be very different, typically depending on biological circumstances . Such nuanced relations can be difficult to capture with a single point estimate.  We hypothesise that there is a true underlying relation for each entity pair, and that this relation can be multimodal . The sentences containing each pair are textual observations of these underlying relations.  We therefore propose a probabilistic model which uses a continuous latent variable to represent the true relation between each entity pair. The distribution of a sentence containing that pair is then conditioned on this latent variable. In order to be able to model the complex relations between each entity pair, we use an infinite mixture distribution for the latent representation.  Our model provides a unified architecture for learning representations of relations between entity pairs both at mention and pair level. We show that  the posterior distribution of the latent variable can be used for mention-level relation classification. We also demonstrate that the prior distribution from the same model can be used for pair-level classification. On both tasks, we achieve results competitive with strong baselines with a model which has fewer parameters and is significantly faster to train.  The code is released at \url{ https://github.com/BenevolentAI/RELVM} %.    
","     Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence  or across an entire corpus . In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.",279
"   Human communication is inherently multi-modal in nature. Our expressions and tone of voice augment verbal communication.\ This can include vocal features like speaking rate, intonation and visual features like facial expressions . Non-verbal communication is important for tasks that involve higher level cognitive expressions like emotions , persuasiveness  and mental health analysis . We focus on a multi-modal approach to emotion recognition because humans fundamentally express emotions verbally using spoken words , as well as with acoustic signals  and visual expressions .  Getting large-scale labeled datasets for emotion recognition can be challenging.\ Our primary motivation for this paper is to study effective utilization of large unlabeled datasets to improve performance of multi-modal emotion recognition systems.\ The signals we consider are speech, visual information and spoken text.\ Our motivation stems from the popular use of pre-trained models in natural language, speech and visual understanding tasks to circumvent data limitations.\ BERT is a popular model for natural language understanding  that was trained using self-supervision.\ Devlin et al. use the masked language modeling  task on the Wikipedia corpus for pre-training.\ The model was successfully fine-tuned to improve performance on several tasks like question answering and the general language understanding evaluation benchmarks . Self-supervised learning has also been successfully applied to speech based applications.\ Schneider et al.\ in  use unsupervised pre-training on speech data by distinguishing an audio sample in the future from noise samples.\ Fine-tuning this model shows state of the art results on automatic speech recognition . Liu et al.\ show in  that a BERT-like pre-training approach can be applied to speech.\ By predicting masked frames instead of masked words, the performance on tasks like speaker recognition,\ sentiment recognition and phoneme classification can be improved. For emotion recognition, Tseng et al.\ show in  that text-based self-supervised training can outperform state of the art models. The authors use a language modeling task, that involves predicting a word given its context, to pre-train the model.\ Another area of work that has leveraged unlabeled data is detection and localization of visual objects and spoken words in multi-modal input.\ Harwath et al.\ in  train an audio-visual model on an image-audio retrieval task.\ The models are trained to learn a joint audio-visual representation in a shared embedding space.\ This model can learn to recognize word categories by sounds without explicit labels.\ Motivated by the success of these approaches, we study if similar methods can be applied to multi-modal emotion recognition.\ To the best of our knowledge, a joint self-supervised training approach using text, audio and visual inputs has not been well explored for emotion recognition.   Multi-modal emotion recognition models have been well studied in literature and typically outperform uni-modal systems .\ These models need to combine inputs with varying sequence lengths.\ In video, the sequence lengths for audio and visual frames differ from the length of text tokens by orders of magnitude.\ There has been considerable prior work in fusing multi-modal features. Liang et al.\ in  studied multiple fusion techniques for multi-modal emotion recognition and sentiment analysis.\ Their methods included early and late fusion of modalities, and a dynamic fusion graph based network.\ They showed that the graph fusion model outperforms other methods.\ Early fusion and graph fusion techniques both require alignment between various modalities.\ Late fusion can be performed without alignment, but does not allow interaction of features from different modalities at the frame level.\ To overcome this limitation,\ Tsai et al.\ introduce the cross-modal transformer in .\ It scales the features using cross-modal attention.\ In the process, the modalities are projected into sequences of equal lengths, eliminating the need for any alignment.\ This architecture has been successfully applied to problems like emotion recognition, sentiment analysis  and speech recognition .\ Recently, another transformer-based method to combine multi-modal inputs was introduced by Rahman et al. in , which uses a multi-modal adaptation gate.  In this paper, we propose using the same pre-training scheme as BERT, but extend it to a model that uses audio, visual and text inputs. We discuss the relevance of this approach in Section .\ The multi-modal representations learned in pre-training are fine-tuned for emotion recognition.\ We evaluate the efficacy of the pre-training approach.\ We also perform experiments to understand the importance of each modality on the CMU-MOSEI dataset and provide case-studies to interpret the results.   This paper is organized as follows.\ In Section  we describe our model architecture and the self-supervised approach for pre-training, along with further motivation for the self-supervised learning we choose.\ In Section , we discuss the training setup and data.\ We present our results and analysis in Section  and conclude in Section .  
","  Emotion recognition is a challenging task due to limited availability of in-the-wild labeled datasets.\ Self-supervised learning has shown improvements on tasks with limited labeled datasets in domains like speech and natural language.\ Models such as BERT learn to incorporate context in word embeddings, which translates to improved performance in downstream tasks like question answering.\ In this work, we extend self-supervised training to multi-modal applications.\ We learn multi-modal representations using a transformer trained on the masked language modeling task with audio, visual and text features.\ This model is fine-tuned on the downstream task of emotion recognition.\ Our results on the CMU-MOSEI dataset show that this pre-training technique can improve the emotion recognition performance by up to 3\% compared to the baseline.",280
" %  A long desired goal for AI systems is to play an important and collaborative role in our everyday lives.  Currently, the predominant approach to visual question answering  relies on encoding the image and question with a black-box transformer encoder.  These works carry out complex computation behind the scenes but only yield a single token as prediction output . Consequently, they struggle to provide an intuitive and human readable form of justification consistent with their predictions.  In addition, recent study has further demonstrated some unsettling behaviours of those models: they tend to ignore important question terms, look at wrong image regions, or undesirably adhere to superficial or even potentially misleading statistical associations.     To address this insufficiency, we reformulate VQA as a full answer generation task rather than a classification one, i.e. a single token answer. The reformulated VQA task requires the model to generate a full answer with natural language justification. We find that the state-of-the-art model answers a significant portion of the questions correctly for the wrong reasons.  To learn the correct problem solving process,  We propose \modelabbrevname{}  \underline{r}ead the question,  \underline{t}hink with multi-hop visual reasoning,      and finally  \underline{a}nswer the question.      %      Following this intuition, \modelabbrevname{} deploys four neural modules, each mimicking one problem solving step that humans would take:     %      A scene graph generation module first converts an image into a scene graph; A semantic parsing module parses each question into multiple reasoning instructions; A neural execution module  interprets reason instructions one at a time by traversing the scene graph in a recurrent manner and; A natural language generation module generates a full answer containing natural language explanations. The four modules are connected      through hidden states rather than explicit outputs.      Therefore, the whole framework can be trained end-to-end, from pixels to answers.     In addition, since \modelabbrevname{} also produces human-readable      output from individual modules during testing, we can easily     locate the error by checking the modular output.      %      %      Our experiments on GQA dataset show that      \modelabbrevname{} outperforms the state-of-the-art model by a large margin       on the full answer generation task.      Our perturbation analyses by removing relation linguistic cues from questions      confirm that      \modelabbrevname{} makes a step towards truly understanding the question rather than having a smart guess with superficial data correlations.      %      We discuss related work in Appendix A. To summarize, the main contributions of our paper are three-fold:              , an end-to-end trainable, modular VQA framework facilitating explainability and enhanced error analysis          % via plug-and-play          as compared to contemporary black-box approaches.                   % {{{https://github.com/Aishwarya-NR/LRTA\_Perturbed\_Dataset}}}                       %  
","   The predominant approach to visual question answering  relies on encoding the image and question with a ``black-box'' neural encoder and decoding a single token as the answer like ``yes'' or ``no''. Despite this approach's strong quantitative results, it struggles to come up with intuitive, human-readable forms of justification for the prediction process. To address this insufficiency, we reformulate VQA as a full answer generation task, which requires the model to justify its predictions in natural language. We propose LRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning framework for visual question answering that solves the problem step-by-step like humans and provides human-readable form of justification at each step. Specifically, LRTA learns to first convert an image into a scene graph and parse a question into multiple reasoning instructions. It then executes the reasoning instructions one at a time by traversing the scene graph using a recurrent neural-symbolic execution module. Finally, it generates a full answer to the given question with natural language justifications. Our experiments on GQA dataset show that LRTA outperforms the state-of-the-art model by a large margin  on the full answer generation task. We also create a perturbed GQA test set by removing linguistic cues  in the questions for analyzing whether a model is having a smart guess with superficial data correlations. We show that LRTA makes a step towards truly understanding the question while the state-of-the-art model tends to learn superficial correlations from the training data.",281
"  . Question pairs from both Source and Target domains are encoded by in a common representation space, which is  the result of unsupervised adaptation of BERT on Target data.  The top k similar items from Source are then aggregated based on their label and distance to provide a final confidence.  % The  diagram roughly parallels Figure 1 from  but adapted to the DQD and cross-domain setup.  } \figlabel{knnprocess}   Duplicate question detection  is an important application in information retrieval and NLP . It allows systems to recognize when two questions share an answer. This is significant for community forums, such as StackExchange\footnote{https://stackexchange.com/}   to increase their effectiveness in avoiding redundant questions and displaying relevant answers to search questions. It is also important for FAQ retrieval question answering systems .  To learn DQD models for , question pairs are usually annotated with duplication information that is extracted from community-provided meta-data. Such annotations are sparse for most domains, e.g., a new  forum providing support for a new product.  Therefore, leveraging other training signals either from unsupervised data or supervised data from other domains is important .  Pre-trained language models  like BERT  and RoBERTA  are  great unsupervised textual representations. Several recent efforts adapt PLMs for the domains of interest  by  self-supervised fine-tuning on unsupervised domain data, which has shown  to be promising in several scenarios  .  We follow that and tune BERT on  domains to obtain richer representations for the task of DQD.   Recently,  -nearest neighbors  is applied on the PLM representations for language modeling  and dialogue . We extend this line of study and apply  for  cross-domain generalization in DQD, where the models are trained on data from a  domain, and applied on data from a  domain.  To do so, we represent pairs from source and target in a common representation space and then score target pairs using nearest neighbors in the source pairs. \figref{knnprocess} shows an illustration of this procedure.   % The specific properties of  DQD % is important to make this approach effective.  Our study on AskUbuntu as target and source datasets of , which include several domains of  and also Quora and Sprint, reveals that  is more effective compared to cross-entropy classification if  the pair representation space from PLMs is rich for the target domain, i.e., adapted on the unsupervised data  from target or similar domains; or   source and target domains have large distributional shifts.   We make the following contributions:   We present the first study of combining strengths of  and      neural representations for cross-domain generalization in a sentence matching task, i.e., DQD.   Our experimental results  on cross-domain DQD demonstrate that  on      rich question-pair representations advances the results of      cross-entropy classification, especially when shifts in source to target domains is substantial.   
","  Duplicate question detection  is important to increase efficiency of  community and automatic question answering systems.  Unfortunately, gathering supervised data in a domain is time-consuming and expensive, and our ability to leverage annotations across domains is minimal.  In this work, we leverage neural representations and study nearest neighbors for  cross-domain generalization in DQD.   We first encode question pairs of the source and target domain in a rich representation space and then using a k-nearest neighbour retrieval-based method, we aggregate the neighbors' labels and distances to rank pairs. We observe robust performance of this method in different cross-domain scenarios of StackExchange, Spring and Quora datasets, outperforming cross-entropy classification in multiple cases. We will release our codes as part of the publication. % ervised adaptation to StackExchange domains by self-supervised finetuning of contextualized embedding models like BERt. %We show the effectiveness of this adaptation in scenarios when source domain comes from different types of distributions. %Our analysis also reveals that unsupervised domain adaptation on even small amounts of data boosts the performance significantly. %Further, we show how an approach based on nearest neighbors is effective  for this problem and outperforms training the full model using cross entropy.",282
"   Learning vocabulary is a major component of foreign language learning. In the school context, initially vocabulary learning is typically organized around the words introduced by the text book. In addition to the incrementally growing vocabulary lists, some textbooks also provide thematically organized word banks. When other texts are read, the publisher or the teacher often provides annotations for new vocabulary items that appear in the text.  A wide range of digital tools have been developed to support such vocabulary learning, from digital versions of file cards to digital text editions offering annotations.  While such applications serve the needs of the formal learning setting in the initial foreign language learning phase, where the texts that are read are primarily chosen to systematically introduce the language, later the selection of texts to be read can in principle follow the individual interests of the student or adult, which boosts the motivation to engage with the book. Linking language learning to a functional goal that someone actually wants to achieve using language is in line with the idea of Task-Based Language Teaching  as a prominent strand of foreign language education .  Naturally, not all authentic texts are accessible to every learner, but linguistically-aware search engines, such as FLAIR , make it possible to identify authentic texts that are at the right reading level and are rich in the language constructions next on the curriculum. Where the unknown vocabulary that the reader encounters in such a setting goes beyond the around 2\% of unknown words in a text that can be present without substantial loss of comprehension , many digital reading environments provide the option to look up a word in a dictionary. Yet, frequently looking up words in such a context is cumbersome and distracts the reader from the world of the book they are trying to engage with. Relatedly, one of the key criteria of TBLT is that learners should rely on their own resources to complete a task . But this naturally can require pre-task activities preparing the learner to be able to successfully tackle the task . But how can a learner systematically prepare for reading a text or book they are interested in reading?  In this paper, we explore how computational linguistic methods such as distributional semantics, morphological clustering, and exercise generation can be combined with graph-based learner models to answer this question both conceptually and in practice. On the practical side, we developed an application that supports vocabulary learning as a pre-task activity for reading a self-selected book. The conceptual goal is to automatically organize the lexical semantic space of any given English book in the form of a graph that makes it possible to sequence the vocabulary learning in a way efficiently exploring the space and to visualize this graph for the users as an open learner model  showing their growing mastery of the book's lexical space.  Lexical learning is fostered and monitored through automatically generated multi-gap activities  that support learning and revision of words in the contexts in which they occur in the book.  In section we discuss how a book or other text chosen by the learner is turned in to a graph encoding the lexical space that the learner needs to engage with to read the book, and how words that are morphologically related as word families  are automatically identified and compactly represented in the graph . In section we then turn to the use of the graph representation of the lexical semantic space of the book to determine the reader's learning path and represent their growing lexical knowledge as spreading activation in the graph. In section, the conceptual ideas are realized in an application. We discuss how the new learner cold-start problem is avoided using a very quick word recognition task we implemented, before discussing the content selection and activity generation for practice and testing activities. Section then provides a conceptual evaluation of the approach and compares it with related with, before wrapping up with a conclusion in section.  % learning of rare words of English  but what is the purpose? And % the relevance of learning entire frequency bands of words is unclear  % How about combining the goal of reading a book with systematic % learning of what is needed to do so? Problem: Individuals are % interested in different books, and individual differ in language % competence and vocabulary knowledge. So how about the vocabulary of % books organizing themselves individually adaptive organization  % Goal:  %  %   % Solution: %  %   
","   How can a learner systematically prepare for reading a book they are   interested in? In this paper, we explore how computational   linguistic methods such as distributional semantics, morphological   clustering, and exercise generation can be combined with graph-based   learner models to answer this question both conceptually and in   practice. Based on the highly structured learner model and concepts   from network analysis, the learner is guided to efficiently explore   the targeted lexical space. They practice using multi-gap learning   activities generated from the book focused on words that are central   to the targeted lexical space. As such the approach offers a unique   combination of computational linguistic methods with concepts from   network analysis and the tutoring system domain to support learners   in achieving their individual, reading task-based learning goals.",283
"  Recent decades have brought about an increase in the use of computer-based tools in practically every  field of human endeavor. The field of education is no exception. Such tools can be used to augment or  even completely replace traditional face-to-face teaching methods. The emergence of online learning platforms has necessitated the development of means to enable learning activities, such as  group discussions, to be performed through the use of technology. One such example of a learning  platform is the IMapBook software suite aimed at increasing the literacy and reading  comprehension skills of elementary school-aged children through the use of web-based eBooks,  embedded games related to their contents, as well as moderated group discussions. Keeping these discussions constructive and relevant can be difficult and usually requires a  discussion moderator to be present at all times. This can limit the opportunities for such discussions to take place. Leveraging the methods and insights  from the fields of artificial intelligence and machine learning, we can attempt to develop systems to automatically classify messages into  different categories and detect when the discussion has veered off course and necessitates intervention. Our research tackles this problem using a  compilation of discussions obtained during pilot studies testing the effectiveness of using the IMapBook software suite in 4th-grade classrooms.  The studies were performed in 8 different Slovene primary schools and, in total, included 342 students.  The discussions consist of 3541 messages along with annotations specifying their relevance to the  book discussion, type, category, and broad category. The ID of the book being discussed and the time  of posting are also included, as are the poster's school, cohort, user ID, and username.  Each message was also manually translated into English to aid non-Slovene-speaking researchers.  The use of the Slovene language presents unique challenges in applying standard language  processing methods, many of which are not as readily available as for other, more widely spoken languages.  Given a sequence of one or more newly observed messages, we want to estimate the relevance of  each message to the actual topic of discussion. Namely, we want to assign messages into two categories 閳 relevant to the book being discussed or not.  Additionally, we want to predict whether the message is a question, an answer, or a statement which we call the type of the message. Finally, we want to  assign a category label to each message where the possible labels can be either 'chatting', 'switching', 'discussion', 'moderating', or 'identity'.  Building a predictive model capable of performing such predictions with acceptable performance would allow us to experiment with including this new  level of automation in the IMapBook software suite as well as in any related products. The research insights are also applicable to areas such as  online user comments and content moderation.  
"," The increasing adoption of technology to augment or even replace traditional face-to-face learning has led to the development of a myriad of tools and platforms aimed at engaging the students and facilitating the teacher's ability to present new information. The IMapBook project aims at improving the literacy and  reading comprehension skills of elementary school-aged children by presenting them with interactive  e-books and letting them take part in moderated book discussions. This study aims to develop and  illustrate a machine learning-based approach to message classification that could be used to  automatically notify the discussion moderator of a possible need for an intervention and also to collect other useful information about the ongoing discussion. We aim to predict whether a message posted in the discussion is relevant to the discussed book, whether the message is a statement, a question, or an answer, and in which broad category it can be classified. We incrementally enrich our used feature subsets and compare them using standard classification algorithms as well as the novel Feature stacking method.  We use standard classification performance metrics as well as the Bayesian correlated t-test to show  that the use of described methods in discussion moderation is feasible. Moving forward, we seek to  attain better performance by focusing on extracting more of the significant information found in the  strong temporal interdependence of the messages.",284
" The   was proposed by  as a means to test whether a  machine has human-like intelligence. It is an alternative to the well known   and has been designed with the motivation of reducing certain problematic aspects that affect the TT. Specifically, while the TT is subjective in nature, the WSC provides a purely objective evaluation; and whereas passing the TT requires a machine to behave in a deceptive way, the WSC takes the form of a positive demonstration of intelligent capability.  The core problem of the WSC is to resolve the reference of pronouns occurring in natural language sentences.  To reduce the possibility that the task can be accomplished by procedures based on superficial or statistical characteristics, rather than `understanding' of the sentence, they specify that the test sentences used in the WSC, should be constructed in pairs, which have similar structure and differ only in some key word or phrase, and such that the correct referent of the pronoun is different in the two cases. This sentence pair, together with an indication of which pronoun is to be resolved and a pair of two possible candidates, is called a .   The following is an example of the Winograd schemas from the original WSC273 data set :     is too {.   the trophy / the suitcase, {    design Winograd schemas to require background knowledge to resolve a pronoun, which can be an evidence of thinking\/. Therefore, they exclude the sentences that can be resolved by a statistical association within a sentence.   In this paper, we introduce a keyword method to define domains in Winograd schemas. To our best knowledge, this is the first work to use keywords for defining domains in WSC and explore high-level patterns in them. To use the domain-specific high-level patterns, we also develop an advanced high-level knowledge-based reasoning method by modifying the method of . Furthermore, we suggest a simple ensemble method that combines knowledge-based reasoning and machine learning. By the experiments on the domain-specific data set, the ensemble method gives a better performance than each single method. Lastly, we also propose a `robust' accuracy  measure that is more objective by improving the switching method of .   
"," The   is a common sense reasoning task that requires background knowledge. In this paper, we contribute to tackling WSC in four ways. Firstly, we suggest a keyword method to define a restricted domain where distinctive high-level semantic patterns can be found. A  was defined by keywords, and the data set in this domain is used in our experiments. Secondly, we develop a high-level knowledge-based reasoning method using semantic roles which is based on the method of . Thirdly, we propose an ensemble method to combine knowledge-based reasoning and machine learning which shows the best performance in our experiments. As a machine learning method, we used Bidirectional Encoder Representations from Transformers  . Lastly, in terms of evaluation, we suggest a `robust' accuracy measurement by modifying that of . As with their switching method, we evaluate a model by considering its performance on trivial variants of each sentence in the test set.",285
"      The use of deep learning for processing natural language is becoming a standard, with excellent results in a diverse range of tasks. Two state-of-the-art  architectures for text-related modeling are long short-term memory  networks~ and transformers~. LSTMs are recurrent neural networks that process the text sequentially, meaning that they process text one token at a time, building up its internal representation in hidden states of the network. Due to the recurrent nature of LSTM, which degrades the efficiency of parallel processing, as well as demonstrated improvements in performance, models based on the transformer architecture are gradually replacing LSTMs across many tasks. Transformers can process the text in parallel, using self-attention and positional embeddings to model the sequential nature of the text.  A common trend in using transformers is to first pre-train them on large monolingual corpora with abstract, general-purpose objective, and then fine-tune them for a specific task, such as text classification.  For example, the BERT  architecture  uses transformers and is pretrained with masked language modelling and order of sentences prediction tasks to build a general language understanding model. During the fine-tuning for a specific downstream task, additional layers are added to the BERT model, and the model is trained on specific data to capture the specific knowledge required to perform the task.   Most of the research in the natural language processing  area focuses on English, ignoring the fact that English is specific in terms of the low amount of information expressed through morphology . In our work, we focus on adapting modern deep neural networks, namely LSTMs and BERT, for several morphologically rich languages, by explicitly including the morphological information. The languages we analyze contain rich information about grammatical relations in the morphology of words instead of in particles or relative positions of words . For comparison,  we also evaluate our models on English. Although previous research has shown that the state of the art methods such as BERT already captures some information contained in the morphology~, our experiments involve several languages with rich morphology where neural networks could benefit from explicit morphological features.  Specifically, we present methods which combine BERT with separately encoded morphological properties: universal part of speech tags  and universal features . We evaluate them on three downstream tasks: named-entity recognition , dependency parsing , and comment filtering . We perform similar experiments on LSTM networks and compare the results for both architectures. Besides English, we analyze eight more languages: Croatian, Estonian, Finnish, Latvian, Lithuanian, Russian, Slovene and Swedish. The choice of these languages reflects a mix of different language groups , for which we were able to obtain sufficient resources , due to their coverage in the EU EMBEDDIA project.   Our experiments show that the addition of morphological features has mixed effects depending on the task. Across the tasks where the added morphological features improve the performance, we show that  they benefit the LSTM-based models even if the features are noisy and  they benefit the BERT-based models only when the features are of high quality , suggesting that BERT models already capture the morphology of the language; however, there is a room for improvement either in designing pre-training objectives that can capture these properties or when high-quality features are available.   The remainder of this paper is structured as follows. In Section, we present different attempts to use morphological information in machine learning, in particular neural networks, as well as an overview of recent work in the three evaluation tasks. In Section, we describe the used datasets and their properties. In Section, we present the baseline models and models with additional morphological information, whose performance we discuss in Section. Finally, we summarize our work and present directions for further research in Section.  
"," Currently, deep learning approaches are superior in natural language processing due to their ability to extract informative features and patterns from languages. Two most successful neural architectures are LSTM and transformers, the latter mostly used in the form of large pretrained language models such as BERT.  While cross-lingual approaches are on the rise, a vast majority of current natural language processing techniques is designed and applied to English, and less-resourced languages are lagging behind. In morphologically rich languages, plenty of information is conveyed through changes in morphology, e.g., through different prefixes and suffixes modifying stems of words. The existing neural approaches do not explicitly use the information on word morphology. We analyze the effect of adding morphological features to LSTM and BERT models. As a testbed, we use three tasks available in many less-resourced languages: named entity recognition , dependency parsing , and comment filtering . We construct sensible baselines involving LSTM and BERT models, which we adjust by adding additional input in the form of part of speech  tags and universal features. We compare the obtained models across subsets of eight languages. Our results suggest that adding morphological features has mixed effects depending on the quality of features and the task. The features improve the performance of LSTM-based models on the NER and DP tasks, while they do not benefit the performance on the CF task. For BERT-based models, the added morphological features only improve the performance on DP when they are of high quality , while they do not show any practical improvement when they are predicted. As in NER and CF datasets manually checked features are not available, we only experiment with the predicted morphological features and find that they do not cause any practical improvement in performance.",286
" Past work has found that variability in speech signals is often poorly modeled, despite recent advances in speech representation learning using deep neural networks . An important source of acoustic variability comes from accent information embedded in the speech signals . Non-native accents are frequently observed when a second language is spoken, and are mainly caused by the first language background of non-native speakers. The accent strength of a non-native speaker is dependent on the amount of transfer from the native language, and is generally influenced by a variety of variables from which the age of second-language learning is one of the most valuable predictors . However, accent variability is often overlooked in modeling language, and consequently high-resource languages such as English are often treated as homogeneous . That this assumption is problematic is, for example, shown by comparing the number of native and non-native speakers of English, with the latter group being almost twice as large as the former group . It is therefore important to accurately model pronunciation variation using representations of speech that allow this variability to be incorporated.  Traditionally, pronunciations are often represented and evaluated by phonetically transcribing speech . However, transcribing speech using a phonetic alphabet is time consuming, labor intensive, and interference from transcriber variation might lead to inconsistencies . Additionally, fine-grained pronunciation differences that are relevant for studying accented speech may not be captured by using a set of discrete symbols .   therefore introduced an acoustic-only measure for comparing pronunciations.  In their method, they represented accented speech as 39-dimensional Mel-frequency cepstral coefficients , which were used to compute acoustic-based non-native-likeness ratings between non-native and native speakers of English.  They found a strong correlation of  between their automatically determined acoustic-based non-native-likeness ratings and native-likeness ratings provided by human raters .  This result was close to, but still not equal to the performance of a phonetic transcription-based approach .  also conducted several small-scale experiments to investigate whether more fine-grained characteristics of human speech were captured compared to the phonetic transcription-based pronunciation difference measure.  Their results showed that the acoustic-only measure captured segmental differences, intonational differences, and durational differences, but that the method was not invariant to characteristics of the recording device.  The quality of MFCC representations is known to be dependent on the presence of additive noise .  Recent work has shown that self-supervised representation learning models are less affected by noise, while being well-equipped to model complex non-linear relationships .  For example, these models can learn meaningful representations on the basis of read English speech without direct supervision. Fine-tuning these models using transcribed speech resulted in representations which resembled phonetic structure, and offered significant improvements in downstream speech recognition tasks .  Consequently, in this paper, we employ these self-supervised neural models to create an automatically determined acoustic-only pronunciation difference measure, and investigate whether this results in improved performance compared to the MFCC-based approach of  and the phonetic transcription-based approach of .  In the following, we compare and evaluate several neural models, namely , ,  ,  , and  . We evaluate the performance of these algorithms using two different datasets. The first is identical to the dataset used by  and . The second is a new dataset which only focuses on accented speech from a single group of  non-native speakers for which human native-likeness judgements are also available. For reproducibility, we provide our code via \url{https://github.com/Bartelds/neural-acoustic-distance}. The performance of our model is assessed by comparing the obtained neural acoustic-only pronunciation differences to phonetic transcription-based pronunciation distances, MFCC-based acoustic-only pronunciation distances, and human perception.  To understand which aspects of pronunciation variation the neural models can capture, we conduct several additional small-scale experiments, in line with those of .  
"," Variation in speech is often represented and investigated using phonetic transcriptions, but transcribing speech is time-consuming and error prone. To create reliable representations of speech independent from phonetic transcriptions, we investigate the extraction of acoustic embeddings from several self-supervised neural models.  We use these representations to compute word-based pronunciation differences between non-native and native speakers of English, and evaluate these differences by comparing them with human native-likeness judgments.  We show that Transformer-based speech representations lead to significant performance gains over the use of phonetic transcriptions, and find that feature-based use of Transformer models is most effective with one or more middle layers instead of the final layer.  We also demonstrate that these neural speech representations not only capture segmental differences, but also intonational and durational differences that cannot be represented by a set of discrete symbols used in phonetic transcriptions.",287
"  In this paper we focus on the problem of integrating syntactic features in a neural architecture for the Frame-Semantic parsing  process. Frame-semantic parsing is the task of extracting full semantic frame structures from text, as  defined by   Frame Semantics theory .  %Semantic frames are conceptual structures describing general situations, evoked in language by target words referred to as lexical units. Each frame is enriched by a set of semantic roles called frame elements, defining specific participants in the described situation.  %An example of a sentence annotated with Frame Semantics is shown in Figure .  From a theoretical perspective, Frame-Semantic parsing can be decomposed into three sub-tasks: 1) Target Identification  -- identifying target words acting as lexical units; 2)  Frame Identification  -- disambiguating each target into a possible frame; and 3) Semantic Role Labeling  -- extracting all the possible frame elements for a given frame.  Early neural approaches have focused in this regard on the integration of features extracted from dependency trees, both for the FI and SRL tasks , with positive results. Amongst all, SRL is the task that has received more attention when investigating methods for injecting syntax into neural models, mostly due to the strict correlation between syntax and argument structures .  Several solutions have been proposed, setting new baselines over general Frame-semantic parsing and specific SRL corpora. These include the use of dependency path embeddings , the application of Graph Convolutional Networks  to learn representations of the dependency graphs , or restricting the set of candidate arguments using pruning algorithms . Multi-task learning has been also applied, either directly supervising attention to learn dependency parsing  for both TI and SRL, or to implicitly bias learned encoded representations when jointly training a simplified syntactic dependency parser , or a semantic dependency parser for both FI and SRL . %   Although effective, these approaches have focused on exploiting syntactic dependencies rather than  constituency information, partly because dependencies are more suited to be encoded as features or learned through attention mechanisms. %, as they express relationships between words.  Semantic roles are technically provided over syntactic constituents, which directly cast argument boundaries over word sequences. This is demonstrated also by earlier work on SRL, which relied on constituency derived features . It follows that using constituency information should be beneficial, especially because reconstructing argument boundaries through dependencies would require an unbounded number of hops among words, making the problem hard to model in neural architectures . Following this idea, two recent approaches have attempted to rely on such constituency information to improve SRL performance.  use linearised representations of constituency trees in different learning settings, either by extracting salient features, by multi-task learning, or by combining both approaches in an auto-encoding fashion. , instead, train a  GCN with the SRL objective to learn constituent representations, which are then infused into words through the same GCN via the message-passing operation .  In this paper, we foster the same idea of relying on constituency information for every sub-task of Frame-semantic parsing, namely TI, FI, and SRL. We train a GCN to learn specific constituency representations, which are used in turn to compute syntactic paths between constituency nodes.  Our approach is similar to that of , although it significantly differs in: i) the initialisation and topology of the underlying graph; ii) the lower number of required parameters; and iii) the way that syntactic information is infused in every word representation, i.e. computing node-to-node syntactic paths. We show that our approach improves the state-of-the-art over the main Frame-semantic parsing benchmark, i.e.\ the FrameNet corpus , on the single TI and SRL tasks, and on FI in a joint-learning setting. Moreover, we demonstrate the generality of the approach by testing the same network on the CoNLL 2005 dataset .  
"," We study the problem of integrating syntactic information from constituency trees into a neural model in Frame-semantic parsing sub-tasks, namely Target Identification , Frame Identification , and Semantic Role Labeling . We use a Graph Convolutional Network to learn specific representations of constituents, such that each constituent is profiled as the production grammar rule it corresponds to. We leverage these representations to build syntactic features for each word in a sentence, computed as the sum of all the constituents on the path between a word and a task-specific node in the tree, e.g. the target predicate for SRL. Our approach improves state-of-the-art results on the TI and SRL of \texttildelow$1\%$ and \texttildelow3.5\% points, respectively , when tested on FrameNet 1.5, while yielding comparable results on the CoNLL05 dataset to other syntax-aware systems. %When testing the approach on the FrameNet 1.5 corpus, our system gives us strong insight on the role of syntax on TI, while improving the state-of-the-art on SRL of 3.5 points.  %While it yields comparable results on the CoNLL05 dataset.",288
" KR\&R systems work well for certain knowledge-rich domains that typically involve a  set of axioms or rules, use structured queries and datasets, and have a need for precise logical inference with explanations. Formal logic-based reasoning engines such as Cyc  and Ergo  have been successfully deployed in domains such as legal, healthcare and finance.  One of the main advantages of using such systems is transparency 閳 the underlying reasoning of the system is well-understood and can be justified to end-users.   However, there are several known drawbacks of logic-based approaches. For one, the inference procedures are highly brittle in that they require precise matching/unification of logical terms and formulae in order to construct a complete explanation. Secondly, traditional reasoners don閳ユ獩 deal with uncertainty well , whereas rules in real-world applications are often probabilistic and contextual. Thirdly, all such systems suffer from the knowledge acquisition problem . Often, the rules are hand-coded, an approach which doesn閳ユ獩 scale in general.  Our problem domain is Natural Language Understanding , an area where all the issues mentioned above come into play 閳 the need to acquire and use implicit background knowledge to understand text, the application of rules differently based on the context, and the use of imperfect/fuzzy alignment of concepts and relations when doing reasoning.   To address these issues, we devise a novel FOL-based reasoner, called Braid. Braid includes a backward and forward chainer, assumption based reasoner and a constraint solver. This paper only refers to the backward chaining component, which we refer to as Braid-BC.    Braid-BC supports rules with confidences, and uses the notion of  and  to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoning engines.  The custom-unifiers can be based on any statistical techniques, as long as they can propose and score mappings between the terms of two logical propositions . For example, we use neural matching functions as unifiers. Their purpose is to help the reasoner find proofs even when  goals, rule conditions and/or facts do not align perfectly.     The dynamic rule-generator  is given a target proposition  and a knowledge base  as input, and outputs a scored list of hypothesized rules that could be used to prove that proposition. The purpose of rule-generation is to connect the dots when the knowledge required for an inference is missing from the static KB. We describe two DRG implementations - one using a neural  rule generation model that was fine-tuned on a dataset of crowd-sourced causal rules, known as GLUCOSE , and the second that uses a rule-template based technique.     We describe the reasoning algorithms used in Braid-BC, and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query in a highly scalable manner. Our approach shares some similarities with the RETE framework  for matching production rules  but makes several novel extensions: we primarily do backward chaining via a heuristic best-first search , leverage a Master-Worker architecture where the Master builds the main proof graph while Workers make local inferential updates, and define general functions for Unifiers and Provers that lets us plug in various reasoning strategies combining standard reasoning  with statistical approaches .  
","  Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching  of logical terms, an inability to deal with uncertainty, and the need for a precompiled rule-base of knowledge . These issues are particularly severe for the Natural Language Understanding  task, where we often use implicit background knowledge to understand and reason about text, resort to fuzzy alignment of concepts and relations during reasoning, and constantly deal with ambiguity in representations.   To address these issues, we devise a novel FOL-based reasoner, called Braid, that supports probabilistic rules, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe the reasoning algorithms used in Braid-BC , and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query in a scalable manner. We use a simple QA example from a children闁炽儲鐛 story to motivate Braid-BC闁炽儲鐛 design and explain how the various components work together to produce a coherent logical explanation.",289
" Humans compose documents to record and preserve information. As information carrying vehicles, documents are written using different layouts to represent diverse sets of information for a variety of different consumers. In this work, we look at the problem of document understanding for documents written in English. Here, we take the term document understanding to mean the automated process of reading, interpreting, and extracting information from the written text and illustrated figures contained within a document's pages. From the perspective as practitioners of machine learning, this survey covers the methods by which we build models to automatically understand documents that were originally composed for human consumption. Document understanding models take in documents and segment pages of documents into useful parts , often using optical character recognition ~ with some level of document layout analysis. These models use this information to understand the contents of the document at large, e.g. that this region or bounding box corresponds to an address. In this survey, we focus on these aspects of document understanding at a more granular level and discuss popular methods for these tasks. Our goal is to summarize the approaches present in modern document understanding and highlight current trends and limitations.  In Section, we discuss some general themes in modern NLP and document understanding and provide a framework for building end-to-end automated document understanding systems. Next, in Section, we look at the best methods for OCR encompassing both text detection  and text transcription . We take a broader view of the document understanding problem in section, presenting multiple approaches to document layout analysis: the problem of locating relevant information on each page. Following this, we discuss popular approaches for information extraction .  
"," Documents are a core part of many businesses in many fields such as law, finance, and technology among others. Automatic understanding of documents such as invoices, contracts, and resumes is lucrative, opening up many new avenues of business. The fields of natural language processing and computer vision have seen tremendous progress through the development of deep learning such that these methods have started to become infused in contemporary document understanding systems. In this survey paper, we review different techniques for document understanding for documents written in English and consolidate methodologies present in literature to act as a jumping-off point for researchers exploring this area.",290
" Sequence labeling is one of the commonly used techniques for solving natural language understanding  tasks such as named-entity recognition  and slot filling. Furthermore, for these tasks, the state-of-the-art results are typically based on deep neural networks . However, the performance of these models is highly dependent on the availability of large amounts of annotated data. Moreover, compared with classification tasks, which require only one label for a sample, the sequence learning tasks require a series of token-level labels for an entire sequence, which makes them time-consuming and a costly annotation process.  \\ This problem can be mitigated using active learning , which achieves improved performance with fewer annotations by strategically selecting the examples to annotate .  There are two major strategies for active learning, namely, diversity-based sampling and uncertainty-based sampling . % 闉愵剠璧 闉氭尗鍎婇爟 Traditionally, uncertainty-based sampling is the most common pool-based AL approach. However, previous work pointed out that focusing only on the uncertainty leads to a sampling bias . It creates a pathological scenario where selected samples are highly similar to each other, which clearly indicates inefficiency. This may cause problems, especially in the case of noisy and redundant real-world datasets. Another approach is diversity-based sampling, wherein the model selects a diverse set such that it represent the input space without adding considerable redundancy . This approach can select samples while ensuring a maximum batch diversity. However, this approach might select points that provide little new information, thereby reducing the uncertainty of the model. Certain recent studies for classification tasks implemented an algorithm named Batch Active learning by Diverse Gradient Embeddings . This algorithm first computes embedding for each unlabeled sample based on induced gradients, and then geometrically picks the instances from the space to ensure their diversity .  Although it proves to be a robust improvement when performing an image classification task, its performance in sequence labeling tasks is yet unproven. \\ In this study, we investigated some practical active learning algorithms that consider uncertainty and diversity in sequence labeling tasks over different datasets and models. Moreover, we suggested a method to expand BADGE with weighted sampling based on the sequence length to ensure cost-effective labeling. This simple modification in it has a positive implication that it tends to select cost-effective samples.  The proposed model trades off between uncertainty and diversity by selecting diverse samples in the gradient space depending on the parameters in the final layer, for which, the currently available models focus only on uncertainty. To the best of our knowledge, our study is the first to apply diverse gradient embedding to a sequence labeling task. We experimented with the CoNLL 2003 English, ATIS, and Facebook Multilingual Task Oriented Dataset . Accordingly, it was empirically demonstrated that the proposed method consistently outperformed the baseline method including Bayesian AL by disagreement , which shows state-of-the-art performance in NER task, across the datasets, tasks and model architectures.   
","  Recently, several studies have investigated active learning  for natural language processing tasks to alleviate data dependency. However, for query selection, most of these studies mainly rely on uncertainty-based sampling, which generally does not exploit the structural information of the unlabeled data. This leads to a sampling bias in the batch active learning setting, which selects several samples at once. In this work, we demonstrate that the amount of labeled training data can be reduced using active learning when it incorporates both uncertainty and diversity in the sequence labeling task. We examined the effects of our sequence-based approach by selecting weighted diverse in the gradient embedding approach across multiple tasks, datasets, models, and consistently outperform classic uncertainty-based sampling and diversity-based sampling.",291
"}  {I}{n} the past decade, we have seen the emergence of various Knowledge Graphs , such as YAGO and DBPedia. They have achieved great success in both academic and industrial applications, ranging from recommendation to Question Answering. However, these KGs are far from complete, which limits the benefits of transferred knowledge. Relation Extraction  is a vital step to complete KGs by extracting the relations between entities from texts. It is nontrivial since the same relation type may have various textual expressions, and meanwhile, different types of relations can also be described with the same words. Such ambiguity between relations and texts challenges the supervision of RE models.  Due to the expensive human annotation cost, distant supervision is proposed to automatically annotate the mappings between sentences and relations. It assumes that if two entities participate in a relation, a.k.a., a triple  but express another relation . As shown in Figure, given the triple , we collect two sentences that include the entity pair . Clearly, the first sentence expresses a similar meaning with the given relation type, but the second one implies another type of relation city of, which brings in noise to the training corpora\footnote{As the term relation can refer to either relation type or relation instance , in the paper, we simplify the use of term relation for relation type unless otherwise stated.}. To highlight informative sentences, many existing works introduce the attention mechanism to assign sentences with different learning weights.  In terms of quantity, on the other hand, most of the training data collected by distant supervision concentrate mainly on a few relations, leading to the issue of the lack of sufficient annotations for the remaining relations. Take the widely used dataset, New York Times , as an example, we present the number of training instances of each relation in Figure. Unsurprisingly, the annotations are long-tail concerning different relations, and the tail relations suffer from insufficient training corpora. More specifically, each relation  refers to multiple entity pairs  is smaller than that between  should be more similar with respect to RE prediction distributions because of the more common textual contexts. Therefore, how to capture relation proximity in a more precise and general way remains challenging.  Another major challenge is to distinguish between different relations, in case the knowledge transfer introduces a bias towards the same prediction for proximate relations. For example, as mentioned above, both /location/us\_state/capital and /location/fr\_region/capital indicate the capital relation, and the only difference is that between two United States entities or French entities. DPEN incorporates entity type information to learn relation-specific classifier dynamically. However, entity type information is sparse in KGs , challenging the scalability.  To address the first issue, we propose to learn relation prototypes that capture the proximity relationship among relations from involved entity pairs. Inspired by Prototypical Networks, we represent each relation prototype with the centroid of its training data, and each data point is defined as the difference between the pair of entity embeddings, namely implicit mutual relation . Given any entity pair, we compute the implicit mutual relation and its distance to each relation prototype. These proximities suggest possible relations to the classifier, which further makes correct predictions by extracting discriminative signals from supportive sentences. Relation prototypes can also be enhanced by prior information , and be applied to arbitrary sentence encoder.  To address the second issue, we enhance entity embeddings with textual information for implicit mutual relation learning. In specific, we construct an entity co-occurrence graph from unlabeled texts and modeling both the first-order and second-order structural proximity. The massive textual contexts are helpful to infer entity types for distinguishment. Besides, long-tail entity pairs can also benefit from additional textual information. We summarize our main contributions as follows:  [leftmargin=*]       A preliminary version of this work has been published in the conference of ICDE 2020. We summarize the main changes as follows:  [leftmargin=*]     %The rest of the paper is organized as follows. In Section, we formulate the problem and overview the framework, and Section introduces our proposed method in detail. We report the promising experiment results on real-world datasets in Section. Section covers the related works. Finally, we conclude the paper in Section.  [htp]           
","   Relation Extraction  is a vital step to complete Knowledge Graph  by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information.      We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements . Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes will be released later.    %Relation Extraction  is a paramount step to complete Knowledge Graph by extracting entity relations from texts. However, it usually suffers from the long-tail issue, as the training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail RE by transferring knowledge from those with sufficient data. We learn prototypes as an implicit factor between entities, to reflect the meanings of relations and their proximities. Specifically, we construct an entity co-occurrence graph from texts, and capture structural proximities for embedding learning. Furthermore, we optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to many RE framework. We have conducted extensive experiments on two publicly available datasets. Compared with eight state-of-the-art baselines, our model achieves significant improvements . Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components and the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis.",292
"  % Understanding how BERT works is important. % the presence of blackbox nlp  is an indication that the research community values the ability to understand the internals of deep neural networks. Pre-trained transformer models such as BERT  are currently ubiquitous within natural language processing  research and have demonstrated improvements in topics from sentiment analysis to semantic parsing . The widespread development and use of such models has led to an increased effort to interpret such models' decisions . % * understanding models is important in society % * BERT is used all over, so important to understand BERT As defined in , model interpretability is ``the ability [of a model] to explain or present in understandable terms to a human''.  Intuitively, a more interpretable model is easier to understand, debug and improve.  % It's hard to understand BERT because  % * it's a neural model with many, many parameters % * pre-training + fine-tuning is newer than just training from scratch  -> read literature introductions/motivations Interpreting modern pre-trained transformer models is difficult. First, modern deep learning models have hundreds of millions of parameters, and scale only continues to increase . Understanding the impact of a single parameter is nearly impossible because these models are densely connected. Combined with the sheer number of parameters, manual analysis is infeasible. Secondly, while both pre-training and fine-tuning are required for state-of-the-art performance, effort has focused on alternative pre-training methods . % Understanding the impacts of fine-tuning is still not well understood. \todo{do I need a citation here?}   % Previous work attempted to use attention Previous work uses BERT's self-attention mechanism to interpret the model's predictions . However, a body of work  shows that models' attention mechanisms cannot be interpreted on single-sequence classification tasks.  % We apply bert to a sequence classification task We apply BERT and two BERT-based models  to an existing sentence classification task proposed in . We compare BERT-based models' performances with previous baselines and then use methods presented in  and  to evaluate BERT's interpretability in single-sequence classification tasks. We find that fine-tuning can teach BERT to recognize previously unknown patterns in natural language and that BERT is more interpretable than the attention-based models analyzed in  and . To summarize, the key contributions of this paper are:        % this is nice because  % * BERT hasn't been applied to it % * professional data set, we have a baseline, all human-annotated % * it has marked spans of edits before and after % To the best of our knowledge, BERT has not been applied to the Automatic Evaluation of Scientific Writing  task.  
"," Pre-trained transformer language models such as BERT are ubiquitous in NLP research, leading to work on understanding how and why these models work. Attention mechanisms have been proposed as a means of interpretability with varying conclusions. We propose applying BERT-based models to a sequence classification task and using the data set's labeling schema to measure each model's interpretability. We find that classification performance scores do not always correlate with interpretability. Despite this, BERT's attention weights are interpretable for over 70\% of examples.",293
"  .          % % final paper: en-us version           %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }    Recently, neural machine translation  has demonstrated impressive performance improvements and became the de-facto standard .    However, like other neural methods, NMT is data-hungry.   This makes it challenging when we train such a model in low-resource scenarios .   Researchers have developed promising approaches to low-resource NMT.   Among these are data augmentation , transfer learning , and pre-trained models .   But these approaches rely on external data other than bi-text.   To date, it is rare to see work on the effective use of bilingual data for low-resource NMT.    In general, the way of feeding samples plays an important role in training neural models.   A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems.   More systematic studies on this issue can be found in recent papers .   For example,  have pointed out that deep neural networks tend to prioritize learning ``easy'' samples first.   This agrees with the idea of curriculum learning  in that an easy-to-hard learning strategy can yield better convergence for training.    In NMT, curriculum learning is not new.   Several research groups have applied it to large-scale translation tasks although few of them discuss the issue in a low-resource setup .   The first question here is how to define the ``difficulty'' of a training sample.   Previous work resorts to functions that produce a difficulty score for each training sample.   This score is then used to reorder samples before training.   But the methods of this type enforce a static scoring strategy and somehow disagrees with the fact that the sample difficulty might be changing when the model is updated during training.   Another assumption behind curriculum learning is that the difficulty of a sample should fit the competence of the model we are training.   Researchers have implicitly modeled this issue by hand-crafted curriculum schedules  or simple functions , whereas there has no in-depth discussion on it yet.    In this paper, we continue the line of research on curriculum learning in low-resource NMT.   We propose a dynamic curriculum learning  method to address the problems discussed above.   The novelty of DCL is two-fold.   First, we define the difficulty of a sample to be the decline of loss .   In this way, we can measure how hard a sentence can be translated via the real objective used in training.   Apart from this, the DCL method explicitly estimates the model competence once the model is updated, so that one can select samples that the newly-updated model has enough competence to learn.      DCL is general and applicable to any NMT system.   In this work, we test it in a Transformer-based system on three low-resource MT benchmarks and different sized data selected from the WMT'16 En-De task.   Experimental results show that our system outperforms the strong baselines and several curriculum learning-based counterparts.    
","      Large amounts of data has made neural machine translation  a big success in recent years.    But it is still a challenge if we train these models on small-scale corpora.   In this case, the way of using data appears to be more important.    Here, we investigate the effective use of training data for low-resource NMT.   In particular, we propose a dynamic curriculum learning  method to reorder training samples in training.   Unlike previous work, we do not use a static scoring function for reordering.   Instead, the order of training samples is dynamically determined in two ways - loss decline and model competence.   This eases training by highlighting easy samples that the current model has enough competence to learn.    We test our DCL method in a Transformer-based system.   Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT'16 En-De.",294
" The World Health Organization  estimates that typical retinal diseases such as Age-related Macular Degeneration  and Diabetic Retinopathy  are expected to affect over 500 million people worldwide shortly . Besides, generally speaking, the traditional process of retinal disease diagnosis and creating a medical report for a patient takes time in practice. The above means that ophthalmologists will become busier and busier.   As we may know, the current state of the art in Artificial Intelligence  involves deep learning research, and we claim deep learning is one of the promising ways to help ophthalmologists and improve the traditional retinal disease treatment procedure. Deep learning based models such as convolutional neural networks  or recurrent neural networks  for computer vision or natural language processing tasks, respectively, have achieved, and, in some cases, even exceeded human-level performance. There is no better time than now to propose an AI-based medical diagnosis method to aid ophthalmologists.    In this paper, we propose an AI-based method for automatic medical report generation based on an input retinal image, as illustrated in Figure . The proposed method intends to improve the traditional retinal disease diagnosis procedure, referring to Figure , and help ophthalmologists increase diagnosis efficiency and accuracy. The main idea of this method is to exploit the deep learning based models, including an effective retinal disease identifier  and an effective clinical description generator , to automate part of the traditional treatment procedure. Then, the proposed method will make the diagnosis more efficient.   To train our deep learning models and validate the effectiveness of our RDI and CDG, we introduce a new large-scale retinal disease image dataset, called DeepEyeNet . Besides, as ground truth, we provide a retinal image dataset manually labeled by ophthalmologists to qualitatively show that the proposed AI-based model is effective. The dataset helps us show the activation maps of our deep models are aligned with image features that are clinically recognized by ophthalmologists as linked with the identified disease. Our experimental results show that the proposed AI-based method is effective and successfully improves the traditional retinal disease treatment procedure. Our main contributions are summarized as follows:                     
","     In this work, we propose an AI-based method that intends to improve the conventional retinal disease treatment procedure and help ophthalmologists increase diagnosis efficiency and accuracy. The proposed method is composed of a deep neural networks-based  module, including a retinal disease identifier and clinical description generator, and a DNN visual explanation module.     To train and validate the effectiveness of our DNN-based module, we propose a large-scale retinal disease image dataset. Also, as ground truth, we provide a retinal image dataset manually labeled by ophthalmologists to qualitatively show, the proposed AI-based method is effective. With our experimental results, we show that the proposed method is quantitatively and qualitatively effective. Our method is capable of creating meaningful retinal image descriptions and visual explanations that are clinically relevant.      {DeepOpht Github.}",295
"    One of the fundamental problems in Natural Language Processing  is learning a distributed encoding of sentences, as this is the stepping stone for many NLP tasks, such as sentence classification, sentiment analysis and natural language inference. The multitude of approaches addressing this problem can be categorised according to how a sentence is represented.  %In bag-of-words models, sentences are represented as words multisets and their encodings are generated by averaging word representations  The simpler sentence representation is bag-of-words, which depicts sentences as words multisets ignoring the word order. Despite the simple representation, it has been used to obtain meaningful sentence encodings .   Sequence representation overcomes this limitation considering the sentence as an ordered sequence of words. It allows building models which progressively constructs a sentence encoding, processing one word at the time. Recurrent Neural Network  and Long-Short Term Memory   are probably the most famous models which use this representation.% to produce sentence embeddings.  %This representation reflects how we read text, word after word. %One of the major drawbacks of bag-of-words models is that they are insensitive to word order. Sequence-based models overcome this limitation considering a sentence as a sequence of words .  A key aspect of sentences, which is missing in sequential processing, is compositionality. For example, the sentence """" is obtained by composing the two sub-phrases  """" and """" with the conjunction """". The intrinsic compositionality of sentences makes them suitable for a tree representation, where the whole sentence  is built in terms of sub-phrases  which in turn are defined in terms of smaller constituents; the base cases are words  since they are the atomic piece of information. This representation takes the name of . In Fig.\  we show the constituency tree of the sentence """": the leaves are the words while internal nodes represent syntactic categories which are the constituents of the whole sentence.  There are many models which compute a sentence encoding starting from its constituency tree. For our purposes, we restrict the discussion on bottom-up Recursive Neural Networks  . The parsing direction is constrained by the structure of constituency trees, having information  on leaf nodes. In this domain, we refer to the term  to indicate the state-transition function which computes the   of a tree node combining the representation of its  . Then, the hidden state of the root  is taken as sentence encoding.  The Matrix-Vector Recurrent Neural Network   and the Recursive Neural Tensor Network   apply the RecNN architecture to binary constituency trees using complex composition functions.  % apply the RecNN architecture to binary constituency trees. Moreover, they propose two new architectures which leverage more complex composition functions: the MV-RNN and the RNTN. In the MV-RNN , every word and sub-phrase is encoded as both a vector and a matrix. When two constituents are combined the matrix of one is multiplied with the vector of the other and vice versa, obtaining a composition function which is parameterised by constituents that participate in it. MV-RNN requires a huge number of parameters, since a composition matrix is attached to each word. RNTN  solves this limitation defining a tensor composition function. The tensor allows to obtain composition function parameters directly from the constituent that participate in it.   extends the well known Long-Short Term Memory  architecture to tree-structured data. They propose two different Tree-LSTMs : the -ary Tree-LSTM defines a composition function which considers constituent order while the child-sum Tree-LSTM ignores such an order. However, only the former model is applied to binary constituency trees. The latter is applied to dependency trees, which are another kind of tree representation for sentences, out of our scope.  In recent years, Tree-LSTM has been used as a building block to develop more sophisticated models. For example, , , ,   build new Tree-LSTM models which define dynamic composition functions depending on syntactic categories . Instead,  introduces a Bidirectional Tree-LSTM which takes advantage of both parsing directions: bottom-up and top-down. As we stated before, constituency trees are intrinsically bottom-up; to this end, the author introduces a first bottom-up pass, called , to propagate information from leaves to the root. All these models are applied only to binary constituency trees.    Thus far, we have shown that most of the models compute sentences encodings starting from binary constituency trees. This simplification solves one crucial problem of tree-structured data: the variable number of child nodes. However, the price to pay is the loss of structural information. For example, in Fig.\  and Fig.\  we report the constituency and the binary constituency tree of the sentence """". Comparing the two representation, we can observe that binary tree has one more node that breaks the ternary relation in the non-binary tree; in general, to break a node with  child nodes, we need to add  new nodes. All these new nodes create a chain which moves away the child nodes of the n-ary relation from their parent. The composition of them is obtained by considering one child at a time, as it happens in sequence representation. Hence, the binarisation removes the equality among child nodes, with the risk of weakening contribution of child nodes that are moved far away from their parent and strengthening the contribution of the ones that remain close.  As far as we know, the only work which builds a model suitable for non-binary constituency trees is the TreeNet . The idea is to consider all child nodes in a chain: the hidden state of a node depends on the hidden state of its left sibling and its rightmost child. Even if the model itself works with non-binary trees, the composition function expressed is binary since it always composes two elements. We discuss this observation in details in Sec. .  The definition of models for non-binary constituency trees requires to go beyond the standard definition of composition function. Standard RecNNs define learnable composition functions which are based on the summation of the contribution of each constituent.  proposed a generalisation of such sum-based composition functions leveraging more expressive multi-affine maps represented as tensors. The exponential number of parameters with respect to the tree out-degree  required by the full-tensorial approach can be controlled by applying tensor decomposition. The tensorial models outperform sum-based models, especially when the tree out-degree increases .  Within the scope of this paper, we unveil that non-binary constituency trees can be effectively exploited to improve predictive performance in NLP task, showing that more powerful composition functions are necessary to take advantages of such a rich representation. To this end, we introduce two new Tree-LSTM models which leverage canonical tensor decomposition: the former is suitable for binarised constituency trees, while the latter can process general non-binary constituency trees imposing weight sharing on the tensor decomposition factors. Finally, we test the quality of sentence encodings produced by our models on different NLP tasks, showing that the combination of a rich representation and a powerful composition function is able to outperform baseline models using the same number of parameters.  
"," Processing sentence constituency trees in binarised form is a common and popular approach in literature. However, constituency trees are non-binary by nature. The binarisation procedure changes deeply the structure, furthering constituents that instead are close. In this work, we introduce a new approach to deal with non-binary constituency trees which leverages tensor-based models. In particular, we show how a powerful composition function based on the canonical tensor decomposition can exploit such a rich structure. A key point of our approach is the weight sharing constraint imposed on the factor matrices, which allows limiting the number of model parameters. Finally, we introduce a Tree-LSTM model which takes advantage of this composition function and we experimentally assess its performance on different NLP tasks.",296
" Searching for code fragments is a very common activity in software development. The advent of large code repositories like GitHub\footnote{https://github.com/} and StackOverflow\footnote{https://stackoverflow.com/} has only increased the number of developers to rely on these repositories to search and reuse existing code . Traditional Information Retrieval techniques  do not work well for code search and retrieval tasks due to limited shared vocabulary between the source code and the natural language search text . Often, developers who are new to a programming language, search for code snippets in a context-free natural language. The choice of words used to search may not overlap with the code snippets leading to failure of traditional information retrieval systems. Therefore, there is a need to gain a deeper understanding of code and text in order to find semantically relevant code snippet.  Consider an example where a developer has a functional requirement to validate if age is always lesser than  and alert otherwise. The developer is tasked to enforce this check in Java. A naive Java developer who is not familiar with the language might make a query based on the requirement as: java check condition correctness. The top 10 results\footnote{As of December 9, 2019} in StackOverflow do not discuss the assert keyword. A more programming friendly query such as java boolean check or the assert keyword itself results in code snippets demonstrating the steps as the top result in StackOverflow.  Use of deep neural network models have shown tremendous improvements in many tasks across domains including language tasks . This success can be largely attributed, in part, to their ability to learn meaningful relationships among words in documents efficiently and represent them in a way such that semantically equivalent words tend to have similar representations . One such family of models that are popular for determining text similarity are Siamese networks. First introduced by , a typical Siamese network consists of two identical sub networks that share weights. They work in tandem on different inputs and the output of both the networks are evaluated by a distance measure that also acts as a scoring function. This has been successfully applied in many similarity tasks in image domain  and recently in text domain as well . Another useful property of these models is their capability to learn from fewer data examples . Since code can be treated as a special kind of text data, one possible way to approach the problem of Semantic Code Search  is to treat it as a similarity task where the objective is to bring semantically equivalent code snippets and their natural language descriptions closer. Therefore, we study the application of Siamese networks to code and corresponding text descriptions for semantic code search.  We apply multiple variations of the base Siamese network model on two different datasets for semantic code search and study its efficacy.  We further take the state of the art baselines -  and  on these datasets and observe that Siamese networks can improve over the baseline results invariably . Finally, we present our analysis on the  performance of different Siamese network architectures explored and identify the conditions for improved performance.  The rest of the paper is organized as follows. We introduce some relevant prior art in section . Next, in section , we provide some background on Siamese networks and semantic code search and introduce terminology. In section , we describe our approach and the different architectures investigated. In section , we describe our experiments and present the results. Finally in section , we perform a detailed analysis of our observations, followed by conclusions in section .  % \tikz \draw[]  rectangle  node[pos=.2]{Answer Here:};  
"," % Availability of large code repositories and discussion forums, has enabled code search as a common activity among developers. They tend to express their intent as a query in natural language to find examples of related code. However performance of such systems are restricted due to 1) limited shared vocabulary across code and user query and 2) lack of semantic understanding of the user query.   % In this work, we evaluate Siamese network for the task of code retrieval. Building on two sub network, our siamese model can jointly learn between code and its description and represent them based on their semantic distance. We evaluate the performance of applying siamese networks 1) as a stand-alone model directly feeding code and its description 2) as a model stacked on existing state of the art models. We experiment on 2 datasets and 3 baseline models, and conclude that applying siamese networking on top of base models yield better embedding and improves the performance of the code sesearch taks significantly.  With the increase in the number of open repositories and discussion forums, the use of natural language for semantic code search has become increasingly common. The accuracy of the results returned by such systems, however, can be low due to 1) limited shared vocabulary between code and user query and 2) inadequate semantic understanding of user query and its relation to code syntax. Siamese networks are well suited to learning such joint relations between data, but have not been explored in the context of code search. In this work, we evaluate Siamese networks for this task by exploring multiple extraction network architectures. These networks independently process code and text descriptions before passing them to a Siamese network to learn embeddings in a common space. We experiment on two different datasets and discover that Siamese networks can act as strong regularizers on networks that extract rich information from code and text, which in turn helps achieve impressive performance on code search beating previous baselines on $2$ programming languages. We also analyze the embedding space of these networks and provide directions to fully leverage the power of Siamese networks for semantic code search.",297
"  We are motivated by the problem of labelling a dataset for word sense disambiguation, where we want to use a limited budget to collect annotations for a reasonable number of examples of each sense for each word.  This task can be thought of as an active learning problem , but with two nonstandard challenges. First, for any given word we can get a set of candidate labels from a knowledge base such as WordNet . However, this label set is not necessarily representative of what occurs in the data: there may exist labels in the knowledge base that do not occur in the corpus because the sense is rare in modern English;  conversely, there may also exist true labels that do not exist in our knowledge base. For example, consider the word ``bass.'' It is frequently used as a noun or modifier, e.g., ``the  and alto are good singers'', or ``I play the  guitar''. It is also commonly used to refer to a type of fish, but because music is so widely discussed online, the fish sense of the word is orders of magnitude less common than the low-frequency sound sense in internet text. The Oxford dictionary  also notes that bass  once referred to a fibrous material used in matting or chords, but that sense is not common in modern English. We want a method that collects balanced labels for the common senses, `` frequencies'' and `` fish'', and ignores sufficiently rare senses, such as ``fibrous material''. Second, the empirical distribution of the true labels may exhibit extreme skew: word sense usage is often power-law distributed  with frequent senses occurring orders of magnitudes more often than rare senses.    When considered individually, neither of these constraints is incompatible with existing active learning approaches:  incomplete label sets do not pose a problem for any method that relies on classifier uncertainty for exploration ; and extreme skew in label distributions has been studied under the guided learning framework wherein annotators are asked to explicitly search for examples of rare classes rather than simply label examples presented by the system .  But taken together, these constraints make standard approaches impractical. Search-based ideas from guided learning are far more sample efficient with a skewed label distribution, but they require both a mechanism through which annotators can search for examples and a correct label set because it is undesirable to ask annotators to find examples that do not actually occur in a corpus.    Our approach is as follows. We introduce a frequency threshold, , below which a sense will be deemed to be ``sufficiently rare'' % to be ignored  = p_y < \thresholdp_y_y$ by using importance-weighted samples. Once we have found examples of common classes, we switch to more standard active learning methods to find additional examples to reduce classifier uncertainty.  Overall, this paper makes two key contributions. First, we present an Exemplar Guided Active Learning  algorithm that offers strong empirical performance under extremely skewed label distributions by leveraging exemplar embeddings. Second, we identify a stopping rule that makes EGAL robust to misspecified label sets and prove that this robustness only imposes a logarithmic cost over a hypothetical approach that knows the correct label set.  Beyond these key contributions, we also present a new Reddit word sense disambiguation dataset, which is designed to evaluate active learning methods for highly skewed label distributions.  
"," We consider the problem of wisely using a limited budget to label a small subset of a large unlabeled dataset. We are motivated by the NLP problem of word sense disambiguation. For any word, we have a set of candidate labels from a knowledge base, but the label set is not necessarily representative of what occurs in the data: there may exist labels in the knowledge base that very rarely occur in the corpus because the sense is rare in modern English; and conversely there may exist true labels that do not exist in our knowledge base. Our aim is to obtain a classifier that performs as well as possible on examples of each 闁炽儲绔穙mmon class闁 that occurs with frequency above a given threshold in the unlabeled set while annotating as few examples as possible from 闁炽儲绗re classes闁 whose labels occur with less than this frequency. The challenge is that we are not informed which labels are common and which are rare, and the true label distribution may exhibit extreme skew. We describe an active learning approach that  explicitly searches for rare classes by leveraging the contextual embedding spaces provided by modern language models, and  incorporates a stopping rule that ignores classes once we prove that they occur below our target threshold with high probability. We prove that our algorithm only costs logarithmically more than a hypothetical approach that knows all true label frequencies and show experimentally that incorporating automated search can significantly reduce the number of samples needed to reach target accuracy levels.",298
" Argumentation is a paramount process in society, and debating on socially relevant topics requires high-quality and relevant arguments. In this work, we deal with the problem of , which is also known as . The goal is to develop an  which organizes arguments, previously extracted from various sources  , in an accessible form. Users then formulate a query to access relevant arguments retrieved by the .  The query can be defined as a , e.g. Energy in which case the  retrieves all possible arguments without further specification. Our work deals with a more advanced case, where a query is formulated in the form of a , and the user expects  attacking or supporting this query claim.  An example of a claim related to the topic Energy could be ``We should abandon Nuclear Energy"" and a supporting premise, e.g., ``Accidents caused by Nuclear Energy have longstanding negative impacts"". % A popular search methodology to find relevant premises is a similarity search, where the representations of the retrieved premises are similar to the representation of the  query claim. However, as noted by, the relevance of a premise does not necessarily coincide with pure text similarity.  Therefore, the authors of  advocate to utilize the similarity between the query claim and other claims in an  database and retrieve the premises assigned to the most similar claims. However, such  requires ground truth information about the premise to claim assignments and therefore has limited applicability: Either the information sources are restricted to those sources where such information is already available or can automatically be inferred, or expensive human annotations are required. To mitigate this problem and keep the original system's advantages, we propose to use a machine learning model  the relevance between premises and claims. Using this model, we can omit the  claim-claim matching step and evaluate the importance of  candidate premises directly for the query claim. Since the relevance is defined on the semantic level, we have to design an appropriate training task to enable the model to learn semantic differences between relevant and non-relevant premises. Furthermore, an essential subtask for an  is to ensure that the retrieved premises do not repeat the same ideas.  Previous approaches employ clustering to eliminate duplicates.  However, clustering approaches often group data instances by other criteria than expected by the users, as also observed in \gls{argument-mining} applications.  For our method, we propose an alternative to clustering based on the idea of , where the goal is to cover the space of relevant premises as well as possible. % This is samplepaper.tex, a sample chapter demonstrating the % LLNCS macro package for Springer Computer Science proceedings; % Version 2.20 of 2017/10/04 % \documentclass[runningheads]{llncs} % \usepackage{graphicx} \usepackage{xcolor} \usepackage{amsmath} \usepackage{amssymb} %\usepackage{ulem} \usepackage{multirow} \usepackage{booktabs} \usepackage{footnote} \makesavenoteenv{tabular} \makesavenoteenv{table} \usepackage{cite} \usepackage[ruled,vlined]{algorithm2e} \usepackage{float}  \renewcommand\UrlFont{\rmfamily}  % for equal contribution \makeatletter [1]{%   \textsuperscript{\@fnsymbol{#1}}% } \makeatother  % \title{Diversity Aware Relevance Learning for Argument Search} % %\titlerunning{Abbreviated paper title} % If the paper title is too long for the running head, you can set % an abbreviated paper title here %  \author{ Michael Fromm\thanks{equal contribution} %\orcidID{0000-0002-7244-4191} \and Max Berrendorf\printfnsymbol{1}  %\orcidID{0000-0001-9724-4009} \and Sandra Obermeier   \and Thomas Seidl  %\orcidID{0000-0002-4861-1412} \and Evgeniy Faerman  }   \authorrunning{Fromm et al.} % First names are abbreviated in the running head.  % If there are more than two authors, 'et al.' is used.   }   [1]{\textcolor{red}{#1}}  % Acronyms \usepackage[acronym]{glossaries} %\makeglossaries  % Example % {ACR}{Acronym for Clustering Representations} %  -> ACR %  -> Acronym for Clustering Representations %  -> Acronym for Clustering Representations  {AM}{Argument Mining} {ARS}{Argument Retrieval System} {BERT}{BERT} {CLAIM-SIM}{CLAIM-SIM} {relevance-model}{relevance model}  % methods {first512}{Dumani first512} {sentences}{Dumani sentences} {sliding}{Dumani sliding} {BERT Zero-Shot}{BERT Zero-Shot} {Learned Similarity}{Learned Similarity} {Biased Coreset}{Biased Coreset}  {BERT Zero-Shot + Cluster}{}   \DeclareMathOperator*{\argmax}{argmax}  %{relevance model }  % {Dumani first512 } % {Dumani sentences }  % {Dumani sliding }  % {Premise Similarity } % {Clustered Premise Similarity } % {Premise Importance }  % {Bert-Negatives } % {Simple-Negatives } % {Same-Topic-Negatives }  % disable hyperref for glossaries  \glsdisablehyper                   % typeset the header of the contribution   In this work, we focus on retrieving relevant arguments for a query claim covering diverse aspects. State-of-the-art methods rely on explicit mappings between claims and premises and thus cannot utilize extensive available collections of premises without laborious and costly manual annotation. Their diversity approach relies on removing duplicates via clustering, which does not directly ensure that the selected premises cover all aspects. This work introduces a new multi-step approach for the argument retrieval problem. Rather than relying on ground-truth assignments, our approach employs a machine learning model to capture semantic relationships between arguments. Beyond that, it aims to cover diverse facets of the query instead of explicitly identifying duplicates.  Our empirical evaluation demonstrates that our approach leads to a significant improvement in the argument retrieval task, even though it requires fewer data than prior methods. Our code is available at \url{https://github.com/fromm-m/ecir2021-am-search}.         
"," In this work, we focus on retrieving relevant arguments for a query claim covering diverse aspects. State-of-the-art methods rely on explicit mappings between claims and premises and thus cannot utilize extensive available collections of premises without laborious and costly manual annotation. Their diversity approach relies on removing duplicates via clustering, which does not directly ensure that the selected premises cover all aspects. This work introduces a new multi-step approach for the argument retrieval problem. Rather than relying on ground-truth assignments, our approach employs a machine learning model to capture semantic relationships between arguments. Beyond that, it aims to cover diverse facets of the query instead of explicitly identifying duplicates.  Our empirical evaluation demonstrates that our approach leads to a significant improvement in the argument retrieval task, even though it requires fewer data than prior methods. Our code is available at \url{https://github.com/fromm-m/ecir2021-am-search}.",299
"  Speaker diarization is the process of partitioning an audio stream into homogeneous segments according to speaker identities. Thus, diarization determines ``who spoke when'' in a multi-speaker environment, with a variety of applications to conversations involving multiple speakers, such as meetings, television shows, medical consultations, or call center conversations. In particular, the speaker boundaries produced by a diarization system can be used to map transcripts generated by a multi-speaker automatic speech recognition  system into speaker-attributed transcripts . Moreover, speaker embeddings inferred by diarization can help the ASR system adapt to, or focus on the speech of a targeted speaker .   Conventional speaker diarization systems are based on clustering of speaker embeddings. In this approach, several components are integrated into a single system: speech segments are determined by voice activity detection ; these speech segments are further divided into smaller chunks of fixed size; speaker embeddings are then extracted by speaker embedding extractors for each chunk; finally, those speaker embeddings are clustered to map each segment to a speaker identity . For embeddings, i-vectors , x-vectors , or d-vectors  are commonly used. Clustering methods typically used for speaker diarization are agglomerative hierarchical clustering  , k-means clustering , and spectral clustering . Recently, neural network-based clustering has been explored . Clustering-based speaker diarization achieves good performance but has several shortcomings. First, it relies on multiple modules  that are trained separately. Therefore, clustering-based systems require careful joint calibration in the building process. Second, systems are not jointly optimized to minimize diarization errors; clustering in particular is an unsupervised process. Finally, clustering does not accommodate overlapping speech naturally, even though recent work has proposed ways to handle regions with simultaneously active speakers in clustering .  End-to-end neural diarization  with self-attention  is one of the approaches that aim to model the joint speech activity of multiple speakers. It integrates voice activity and overlap detection with speaker tracking in end-to-end fashion.  Moreover, it directly minimizes diarization errors and has demonstrated excellent diarization accuracy on two-speaker telephone conversations. However, EEND as originally formulated is limited to a fixed number of speakers because the output dimension of the neural network needs to be prespecified. Several methods have been proposed recently to overcome the limitations of EEND. One approach uses a speaker-wise chain rule to decode a speaker-specific speech activity iteratively conditioned on previously estimated speech activities . Another approach proposes an encoder/decoder-based attractor calculation . The embeddings of multiple speakers are accumulated over the time course of the audio input, and then disentangled one-by-one, for speaker identity assignment by speech frame.  However, all these state-of-the-art EEND methods only work in an offline manner, which means that the complete recording must be available before diarization output is generated. This makes their application impractical for settings where potentially long multi-speaker recordings need to be processed incrementally .   In this study, we propose a novel method to perform EEND in a blockwise online fashion so that speaker identities are tracked with low latency soon after new audio arrives, without much degradation in accuracy compared to the offline system. We utilize the incremental Transformer encoder, where we attend to only its left contexts and ignore its right contexts, thus enabling blockwise online processing. Furthermore, the incremental Transformer encoder uses block-level recurrence in the hidden states to carry over information block by block, reducing computation time while attending to previous blocks. To our knowledge, ours is the first method that uses the incremental Transformer encoder with block-level recurrence to enable online speaker diarization.   
"," We present a novel online end-to-end neural diarization system, BW-EDA-EEND, that processes data incrementally for a variable number of speakers. The system is based on the Encoder-Decoder-Attractor  architecture of Horiguchi et al., but utilizes the incremental Transformer encoder, attending only to its left contexts and using block-level recurrence in the hidden states to carry information from block to block, making the algorithm complexity linear in time. We propose two variants: For unlimited-latency BW-EDA-EEND, which processes inputs in linear time, we show only moderate degradation for up to two speakers using a context size of 10 seconds compared to offline EDA-EEND. With more than two speakers, the accuracy gap between online and offline grows, but the algorithm still outperforms a baseline offline clustering diarization system for one to four speakers with unlimited context size, and shows comparable accuracy with context size of 10 seconds. For limited-latency BW-EDA-EEND, which produces diarization outputs block-by-block as audio arrives, we show accuracy comparable to the offline clustering-based system.",300
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. %  % form to use if the first word consists of a single letter: % {A}{demo} file is .... %  % form to use if you need the single drop letter followed by % normal text : % {A}{}demo file is .... %  % Some journals put the first two words in caps: % {T}{his demo} file is .... %  % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word.  {T}{here} are many methods for automatic speech recognition  systems, such as GMM-HMM and deep neural network  based acoustic models . Recently, end-to-end speech recognition methods  have made significantly breakthroughs. Although these ASR methods have made a lot of progresses on clean speech signals, the performance could be dramatically degraded in the noisy and reverberation environments. In realistic environments, recorded speech signals are always interfered by various background noises and reverberations. Therefore, improving the robustness of ASR is very important. This paper focuses on boosting the noise robustness of end-to-end speech recognition.  %In realistic environments, recorded speech signals are always interfered by various background noises and reverberations. However, these interferences can dramatically degrade the performance of automatic speech recognition  systems.  In order to boost the noise robustness of ASR, there are three mainstream methods. The first mainstream method is adding the speech enhancement component at the front-end of ASR. Speech enhancement methods include spectral subtraction , Wiener filtering  and deep neural network  based speech enhancement methods . However, speech enhancement optimizes their models to estimate the target speech, which is different from the speech recognition part. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, the enhanced speech by these speech enhancement methods usually generates over-smoothed speech, which is the reason of speech distortion after speech enhancement. The speech distortion can degrade the performance of ASR . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . %In addition, speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end .  The second mainstream method uses the multi-condition training  to boost the noise robustness of ASR. MCT uses different kinds of data  to train the speech recognition model. However, the complexity and computing costs of MCT are increased. In addition, it gives unimpressive performance on the unmatched conditions  and the performance is also affected by the speech distortion . In order to alleviate speech distortion problem, the enhancement front-end enhances both training and test set first, and ASR model is trained with the enhanced data. It can improve the ASR performance in some degree, but it still highly depends on the performance of the enhancement front-end. Different from the MCT method, the SpecAugment  directly applies the  data augmentation to the input features of neural networks . The SpecAugment is used only during the training, which consists of three spectrogram deformations:  time warping,  time and frequency masking. Although the SpecAugment can improve the performance of end-to-end ASR, it needs to be improved on the noisy condition.  %%MCT not only uses the clean data for training but also the noisy data. Therefore, MCT can learn different distributions from the clean and noisy data so that the speech recognition model boosts the noise robustness. However, the complexity and computing costs of MCT are increased. In addition, it gives unimpressive performance in unmatched conditions  and the performance is also affected by the speech distortion . In order to alleviate speech distortion problem, the enhancement front-end enhances both training and test set first, and ASR model is trained on the enhanced training set. It can improve the ASR performance in some degree, but it still highly dependent on the performance of the enhancement front-end. Different from the MCT method, the SpecAugment  directly applies the  data augmentation to the input features of neural networks . The SpecAugment is used only during the training, which consists of three spectrogram deformations:  time warping,  time and frequency masking. Although the SpecAugment can improve the performance of end-to-end ASR, it is still affected by the speech distortion problem.  %Then it applies the enhanced data for test. Although this method can boost the robustness of ASR in some degree, the complexity and computing costs are increased. In addition, the performance of MCT is also affected by the speech distortion .  %In order to boost the noise robustness of ASR, the mainstream method is adding the speech enhancement component at the front-end of ASR. However, speech enhancement aims to estimate the target speech , which is different from the speech recognition. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, speech enhancement usually leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . In order to alleviate this issue, the multi-condition training   method is proposed. MCT not only uses the clean data for training but also the noisy and enhanced data. Although this method can boost the robustness of ASR in some degree, the complexity and computing costs are increased. In addition, the performance of MCT is also affected by the speech distortion.  %speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end .  %In order to boost the noise robustness of ASR, the mainstream method is adding the speech enhancement component at the front-end of ASR. Speech enhancement methods include spectral subtraction , Wiener filtering  and deep neural network  based speech enhancement methods . However, speech enhancement part optimizes their models to estimate the target speech, which is different from the speech recognition part. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . %speech enhancement methods usually lead to speech distortion. %However, applying these speech enhancement methods has two shortcomings. Firstly,  %Firstly, these speech enhancement methods will increase the computation and has more complex pipelines, which limit the applications of speech recognition. Secondly,   The third mainstream method is the joint training methods . These methods apply the joint training framework to optimize the speech enhancement and recognition, simultaneously. The reason is that speech enhancement and speech recognition are not two independent tasks and they can clearly benefit from each other. In order to boost noise robustness of end-to-end ASR, in , authors propose a joint adversarial enhancement training method. They utilize the joint training framework to optimize the mask based enhancement network and attention based encoder-decoder speech recognition network. However, this method only uses the enhanced feature as the input of speech recognition, which is still affected by the speech distortion problem. In addition, in the noisy AISHELL-1  dataset, the character error rate  of this method is still more than 50\%, which needs to be improved. As for the end-to-end speech recognition, speech transformer  models have shown impressive performance and acquired state-of-the-art results. Self-attention network  is one of the key components of speech transformer and it is more powerful to model long-term dependencies than recurrent neural networks  based sequence to sequence models. Therefore, applying the joint training of enhancement and speech transformer can further improve the performance of robust end-to-end ASR. %One of the key components of speech transformer is self-attention network , which is more powerful to model long-term dependencies than Recurrent neural networks -based sequence to sequence models. Therefore, the performance of robust end-to-end ASR can be further improved by using the joint training of enhancement and speech transformer. %To address the speech distortion problem and acquire an optimal performance, the joint training method of speech enhancement and speech recognition is proposed for robust ASR . This is because that speech enhancement and speech recognition are not two independent tasks and they can clearly benefit from each other. In , a joint adversarial enhancement training method is proposed to boost noise robustness of end-to-end ASR systems. It applies the joint training of mask based-enhancement network and attention-based encoder-decoder speech recognition network. However, this method only uses the enhanced features as the input of speech recognition, which is still affected by the speech distortion problem. And the character error rate of this method is still more than 50\% in the noisy AISHELL-1  dataset. Speech transformer models have shown impressive performance in end-to-end speech recognition  and acquire state-of-the-art performances. One of the key components of speech transformer is self-attention network , which is more powerful to model long-term dependencies than Recurrent neural networks -based sequence to sequence models. Therefore, the performance of robust end-to-end ASR can be further improved by using the joint training of enhancement and speech transformer. %Liu et al propose a joint adversarial enhancement training to boost noise robustness of end-to-end ASR systems . They use the joint training of mask based-enhancement network and attention-based encoder-decoder speech recognition network   %In this paper, we propose a joint training method of enhancement and speech transformer for robust end-to-end ASR, which uses the deep attention fusion representations of noisy and enhanced features. To be our best knowledge, it is the first time to apply the speech transformer and enhancement joint training for robust end-to-end ASR. Specifically, the proposed joint training method includes two parts:   In , a one-pass robust speech recognition method is proposed. It combines the noisy and enhanced features by a gating mechanism. Although it can improve the robust of ASR, the enhancement and speech recognition are trained separately instead of the joint training algorithm. In addition, the simple gate mechanism can not make full use of the sequence information so that it can not fuse the noisy and enhanced features very well. %The speech enhancement and speech recognition are   Fig. illustrates the spectrogram example of a test speech sample. From Fig. we can find that the spectrogram of the enhanced speech by the enhancement network has significant leaks  by block boxes), which leads to the speech distortion. There are significant leaks in these black boxes. This is because that the noise is dominant in these T-F bins, which drowns the target speech. Therefore, the enhancement network deals with these T-F bins as the noise signals and removes most of the information. These leaks lose so much very important speech information, for example: formants. Although the enhancement network can remove noise signals in some degree, these leaks are unknown for the speech recognition system and lose so much speech information. These are the reasons why speech distortion damages the performance of speech recognition.   In this paper, we propose a gated recurrent fusion  with joint training framework for robust end-to-end ASR. In order to address the speech distortion problem, motivated by , the GRF is utilized to dynamically combine the noisy and enhanced features. Therefore, the GRF can offset these leaks from the noisy features. In addition, GRF can reduce the noise from the enhanced features. So the GRF aims to learn to adaptively select and fuse the relevant information from noisy and enhanced features by making full use of the gate and memory modules. The GRF can extract more appropriate and robust speech features. In addition, we apply the joint training algorithm to optimize the enhancement and speech recognition. The state-of-the-art end-to-end ASR method speech transformer with self-attention method is used as the speech recognition component. Specifically, the proposed joint training method includes three parts: speech enhancement, gated recurrent fusion and speech recognition. With the joint optimization of enhancement and recognition, the proposed model is expected to learn more robust representations suitable for the recognition task automatically.  %In this paper, we propose a joint training method of enhancement and speech transformer for robust end-to-end ASR, which uses the deep attention fusion  representations of noisy and enhanced features. We apply the state-of-the-art end-to-end ASR method speech transformer with self-attention as the speech recognition component. In addition, to further alleviate speech distortion problem, the deep attention fusion component is utilized to combine the noisy and enhanced features, which can dynamically fuse these features in a deep way so that can extract more appropriate and robust speech features. Therefore, these GRF representations can learn the raw fine structures from the noisy features to make up the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features. Specifically, the proposed joint training method includes three parts: speech enhancement, deep attention fusion and speech recognition. With the joint optimization of enhancement and recognition, the proposed model is expected to learn more robust representations suitable for the recognition task automatically.  %dynamically select appropriate speech features. As for the enhancement component, we apply the mask-based enhancement network to estimate the clean speech. As for the speech recognition component, the speech transformer with self-attention is used for ASR.   To summarize, the main contribution of this paper is two-fold. Firstly, to address the speech distortion problem, the gated recurrent fusion algorithm is utilized to dynamically fuse the noisy and enhanced features. Secondly, to the best of our knowledge, it is the first time to apply the speech transformer and single channel speech enhancement for the joint training framework. Our experiments are conducted on AISHELL-1 Mandarin dataset. Experimental results show that the proposed method achieves the relative CER reduction of 10.02\% over the conventional joint enhancement and transformer method using the enhanced features only. Especially for the low signal-to-noise ratios, our proposed method can achieve better performance.  %The rest of this paper is organized as follows. Section 2 presents the conventional joint training method for robust ASR. Our proposed method is stated in section 3. Section 4 shows detailed experiments and results. Section 5 draws conclusions. The rest of this paper is organized as follows. Section \ presents the conventional joint training method for robust ASR. Section \ introduces our proposed joint training method with gated recurrent fusion algorithm. The experimental setup is stated in section \. Section \ shows experimental results. Section \ shows the discussions. Section \ draws conclusions.     %The rest of this paper is organized as follows. Section \ presents discriminative learning for monaural speech separation using deep embedding features. Section \ introduces the proposed end-to-end post-filter speech separation method. The experimental setup is stated in section \. Section \ shows experimental results. Section \ shows the discussions. Section \ draws conclusions.    
"," %The joint training of speech enhancement and speech recognition methods have acquired good performances for robust end-to-end automatic speech recognition . However, they only use the enhanced features as the input of speech recognition component, which is still affected by the speech distortion problem. In this paper, we propose a deep attention fusion  of noisy and enhanced features with joint enhancement and speech transformer training method for robust end-to-end ASR. We apply the state-of-the-art end-to-end ASR method speech transformer as our speech recognition component. To address the speech distortion problem and extract more robust features for ASR, we propose the deep attention fusion algorithm to combine the noisy and enhanced features deeply. Therefore, these GRF representations can learn the raw fine structures from the noisy features to alleviate the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features. Systematic experiments on AISHELL-1 show that the proposed method achieves the relative character error rate  reduction of 8.32\% over the conventional joint enhancement and transformer method using the enhanced features only. Especially for the low signal-to-noise ratios, our proposed method can achieves better performances. %The joint training framework for speech enhancement and recognition methods have obtained quite good performances for robust end-to-end automatic speech recognition . However, these methods only utilize the enhanced feature as the input of speech recognition component, which are affected by the speech distortion problem. In order to address this problem, in this paper, we propose a gated recurrent fusion  method with joint training framework for robust end-to-end ASR. The proposed method consists of speech enhancement, GRF and speech recognition. Firstly, the mask based speech enhancement network is applied to enhance the input speech. Secondly, to address the speech distortion problem and extract more robust features for end-to-end ASR, the GRF algorithm is used to dynamically combine the noisy and enhanced features. Therefore, the GRF can not only remove the noise signals from the enhanced features, but also learn the raw fine structures from the noisy features so that it can alleviate the speech distortion. Thirdly, to improve the performance of ASR, the state-of-the-art end-to-end speech recognition method speech transformer with self-attention algorithm is used as the speech recognition component. Finally, the joint training framework is utilized to optimize these three components, simultaneously. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% reduction, which suggests the potential of our proposed method. The joint training framework for speech enhancement and recognition methods have obtained quite good performances for robust end-to-end automatic speech recognition . However, these methods only utilize the enhanced feature as the input of the speech recognition component, which are affected by the speech distortion problem. In order to address this problem, this paper proposes a gated recurrent fusion  method with joint training framework for robust end-to-end ASR. The GRF algorithm is used to dynamically combine the noisy and enhanced features. Therefore, the GRF can not only remove the noise signals from the enhanced features, but also learn the raw fine structures from the noisy features so that it can alleviate the speech distortion. The proposed method consists of speech enhancement, GRF and speech recognition. Firstly, the mask based speech enhancement network is applied to enhance the input speech. Secondly, the GRF is applied to address the speech distortion problem. Thirdly, to improve the performance of ASR, the state-of-the-art speech transformer algorithm is used as the speech recognition component. Finally, the joint training framework is utilized to optimize these three components, simultaneously. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% CER reduction, which suggests the potential of our proposed method. %The joint training of speech enhancement and speech recognition methods have acquired good performances for robust end-to-end automatic speech recognition . However, they only use the enhanced features as the input of speech recognition component, which is still affected by the speech distortion problem. In this paper, we propose a gated recurrent fusion  of noisy and enhanced features with joint enhancement and speech transformer training method for robust end-to-end ASR. We apply the state-of-the-art end-to-end ASR method speech transformer with self-attention algorithm as our speech recognition component. To address the speech distortion problem and extract more robust features for end-to-end ASR, we apply the GRF algorithm to dynamically combine the noisy and enhanced features. Therefore, these GRF representations can learn the raw fine structures from the noisy features so that they can make up the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features to improve the robustness of end-to-end speech recognition. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% reduction, which suggests the potential of our proposed method.",301
"  {M}{usic} composition is a human creative process that requires a wide range of strong musical knowledge and expertise to create soothing music which continues to remain in our heart forever. Given the vast majority of music lovers and the limited availability of professional music composers, there is a strong need for machines to assist human creativity. Recent advancement in the software based music creation technology helped the professional and amateur music creators to produce music with great joy and ease of production in masses to be consumed by the music consumers with personal computers and hand-held devices. %The software applications such as Ableton Live, FL Studio, Logic Pro X, Garageband are the few examples which changed the way the music is produced in the past.  Though there exists a plenty of machine assistance to create high quality music with relative ease of production, the process of songwriting that is automatically generating lyrics, composing melody corresponding to the generated lyrics and synthesizing singing voice corresponding to the generated melody and lyrics remained as mutually exclusive tasks. Till date, the construction of novel/original songs is limited to the individuals who possess the following skills: the ability to create lyrics, compose melody and combine lyrics and melody to create a rational, relevant and soothing final complete songs. %Though by remixing technology, we can create new music to some extent which satisfies some music lovers, there is a need for creating truly novel songs under multiple constraints on remaking existing works.                       % -------------------------------------------------------------------------------------------------------------  [htbp]                      In literature, we can find considerable amount of research work published on automatic music generation . Early machine assisted music generation is mostly based on music theory and expert domain knowledge to create novel works. With the advent of data driven approaches and exploded public music collections in the internet, data driven methods such as Hidden Markov models, graphic models and deep learning models showed a potential for music creation. Though there exists substantial amount of research on unconditional music generation, there exists considerably less amount of work done so far on generating melody from lyrics given in the form of text, which we call conditional melody/song generation from lyrics. The primary reasons for substantially less research on conditional melody generation can be attributed to i) the non-availability of the direct source for lyrics-melody pair dataset to train the data driven models, ii) a lyrics composition can have multiple melodic representations, which makes it hard to learn the correlation between the lyrics and melodies, and iii) it is hard to evaluate the generated melodies by objective measures.  This paper focuses on the most challenging aspect of algorithmic songwriting process which enables the human community to discover original lyrics, and  melodies suitable for the generated lyrics. To the best our knowledge, the proposed AutoNLMC is the first attempt to make the whole process of songwriting automatic using artificial neural networks. We also present the lyrics to vector model which is trained on a large dataset of popular English songs to obtain the dense representation of lyrics at syllables, words and sentence levels. The proposed AutoNLMC is an attention based encoder-decoder sequential recurrent neural network model consists of a lyric generator, lyric encoder and melody decoders trained end-to-end. We train several encoder-decoder models on various dense representations of the lyric tokens to learn the correlation between lyrics and corresponding melodies. Further, we prove the importance of dense representation of lyrics by various qualitative and quantitative measures. AutoNLMC is designed in such a way that it can generate both lyrics and corresponding melodies automatically for an amateur or a person without music knowledge by accepting a small piece of initial seed lyrics as input. It can also take lyrics from professional lyrics writer to generate the matching meaningful melodies.  
"," In this paper, we propose a technique to address the most challenging aspect of algorithmic songwriting process, which enables the human community to discover original lyrics, and melodies suitable for the generated lyrics. The proposed songwriting system, Automatic Neural Lyrics and Melody Composition  is an attempt to make the whole process of songwriting automatic using artificial neural networks. Our lyric to vector  model trained on a large set of lyric-melody pairs dataset parsed at syllable, word and sentence levels are large scale embedding models enable us to train data driven model such as recurrent neural networks for popular English songs. AutoNLMC is a encoder-decoder sequential recurrent neural network model consisting of a lyric generator, a lyric encoder and melody decoder trained end-to-end. AutoNLMC is designed to generate both lyrics and corresponding melody automatically for an amateur or a person without music knowledge. It can also take lyrics from professional lyric writer to generate matching melodies. The qualitative and quantitative evaluation measures revealed that the proposed method is indeed capable of generating original lyrics and corresponding melody for composing new songs.",302
"  Machine learning systems struggle to learn predictors that are robust to distribution shift. When tested on i.i.d data drawn from the training distribution these systems can achieve nearly perfect accuracy, even when regularized to prevent over-fitting. However, performance can degrade to below-chance accuracy when the testing and training distributions are even slightly different . The field of Domain Generalization   addresses this challenge by proposing robust methods that ensure good test performance on distributions that are different from but systematically related to the training distribution . Invariant Risk Minimization   is a one of several recently successful approaches to Multi-Source Domain Generalization  which encourages models to learn predictors with invariant performance across different ``domains'', or ``environments'' . Given  different training environments, these models extract a set of predictors from the feature space such that the conditional distribution of the outcomes given the predictors is invariant  across all training environments. These predictors can consequently generalize well to all test out-of-distribution  environments which share this same invariance. Building on work in philosophy which characterizes causation as invariance , existing invariance-based DG methods have been interpreted as a weak form of causal discovery whose returned predictors are the causal factors underlying the phenomena we wish to predict.   Fairness can be often characterized by robustness to changes in ``sensitive attributes'' , especially in the context of toxicity classification.      Consider an automated moderation system used by an online news platform to determine which comments on a news article are toxic to online discourse and should be censored. The performance of a fair system should not be affected by characteristics such as whether the comment is about issues related to race, gender or other politically sensitive topics. Alternative definitions of distributive fairness differ in how the system's predictions should be invariant to changes in the sensitive attribute. For instance, statistical definitions such as Demographic Parity require that some conditional distribution of predictions given the sensitive attribute are invariant to the sensitive attribute , and causal definitions such as counterfactual fairness  require that every individual's prediction is invariant to counterfactual changes in that individual's sensitive attribute. There are a number of ethical and legal criticisms to be levied against systems that predict based on sensitive group membership . Moreover, over-reliance on sensitive information could decrease robustness when the predictive performance of this information spuriously depends on the environmental context in which it is employed. Discussion about non-caucasian racial identities, for example, may be highly predictive of comment toxicity on white supremacist internet forums whose members routinely make discriminatory remarks about ethnic minorities. However, on other internet forums that are more welcoming of diversity the association between racial identity mention and toxicity would likely be far weaker. This brittleness of sensitive information has been identified as a key challenge that Perspective API, a Google-backed internet comment toxicity classifier, faced during implementing in real-world contexts , and has also been observed to cause bias in sentiment analysis  and facial detection  tasks. Fair models, then, can perhaps be constructed by learning predictors whose performance remains invariant across a variety of different environments.   In this work, we empirically demonstrate that Domain Generalization can used to build fair machine learning systems by constructing models that are invariant to spurious correlations involving the sensitive attribute. Specifically, we assess  the performance of IRM on a fair internet comment toxicity classification task derived from the Civil Comments Dataset. In this task, the model must generalize from biased training environments exhibiting a strong but spurious correlation between mention of a particular demographic identity and toxicity to a test environment in which this correlation is reversed.    Our contributions are as follows:          
","     Robustness is of central importance in machine learning and has given rise to the fields of domain generalization and invariant learning, which are concerned with improving performance on a test distribution distinct from but related to the training distribution. In light of recent work suggesting an intimate connection between fairness and robustness, we investigate whether algorithms from robust ML can be used to improve the fairness of classifiers that are trained on biased data and tested on unbiased data. We apply Invariant Risk Minimization , a domain generalization algorithm that employs a causal discovery inspired method to find robust predictors, to the task of fairly predicting the toxicity of internet comments. We show that IRM achieves better out-of-distribution accuracy and fairness than Empirical Risk Minimization  methods, and analyze both the difficulties that arise when applying IRM in practice and the conditions under which IRM will likely be effective in this scenario. We hope that this work will inspire further studies of how robust machine learning methods relate to algorithmic fairness.",303
"   Deep Neural Networks  are the current state-of-the-art models in many speech related tasks. From a computational neuroscience perspective, DNNs can be seen as rate coding based models, in the sense that if a neuron is responsive to a given stimulus, then if we augment the stimulus intensity, the neuron output intensity will also increase. Temporal coding based models try to also take into account information carried by the temporal structure of the stimulus. In the case of Spiking Neural Networks , spike timing and delays between spikes is important in order to retrieve patterns in the spike sequences given as input to a model. %https://en.wikipedia.org/wiki/Neural_coding  There is a growing interest for SNNs applied to speech recognition tasks, from isolated word and phone recognition,to large-vocabulary automatic speech recognition  very recently. Reasons are that the audio speech signal is particularly suited to event-driven models such as SNNs, SNNs are also more biologically realistic than DNNs, hardware friendly and energy efficient models, if implemented on dedicated energy-efficient neuromorphic chips. Furthermore, it has been shown recently that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. This new approach allows to train SNNs as one would do for DNNs.  In this work, we propose to use supervised SNNs for speech command  recognition. We explore the Leaky Integrate-and-Fire  neuron model for this task, and show that convolutional SNNs can reach an accuracy very close to the one obtained with state-of-the-art DNNs, for this task. Our main contributions are the following: i) we propose to use dilated convolution spiking layers, ii) we define a new regularization term to penalize the averaged number of spikes to keep the spiking neuron activity as sparse as possible, iii) we show that the leaky variant of the neuron model outperforms the non-leaky one , used in.  In order to facilitate reproducibility, our code using PyTorch is available online\footnote{https://github.com/romainzimmer/s2net}.  
"," Deep Neural Networks  are the current state-of-the-art models in many speech related tasks. There is a growing interest, though, for more biologically realistic, hardware friendly and energy efficient models, named Spiking Neural Networks . Recently, it has been shown that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. In this work, we report speech command  recognition experiments using supervised SNNs. We explored the Leaky-Integrate-Fire  neuron model for this task, and show that a model comprised of stacked dilated convolution spiking layers can reach an error rate very close to standard DNNs on the Google SC v1 dataset: \ER{94.5}\%, while keeping a very sparse spiking activity, below 5\%, thank to a new regularization term. We also show that modeling the leakage of the neuron membrane potential is useful, since the LIF model outperformed its non-leaky model counterpart significantly.",304
" Books have been the one of the most important mediums for recording information and imparting knowledge in human history. Books can be classified into different categories based on their physical formats, contents, languages, and so on. In this paper, we focus on the task of book classification by its genre using the information provided just by the cover. Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Figure  presents some sample book covers. The information provided by a cover includes visual and textual information . For instance, in Figure 1, the background picture contains different food items and cookware which give the readers a visual impression about the book, while the texts shown on the cover states that it is a book about the ``authentic recipes from Malaysia"". Both the visual and textual information are shown in the cover and they together indicate that its genre is ``Cookbooks, Food \& Wine"". It is worth to mention that having only the visual information often makes the task extremely hard without textual information. For instance, in Figure 1 , without reading the texts on the cover, someone may classify the book as ``Cookbooks, Food \& Wine"" as well solely based on the visual information we get from the cover that includes food items on a table in a dining room setting. Therefore, it is sometimes essential to consider both visual information and textual information extracted from the cover when we conduct book genre classification. The automatic classification of books based on only covers without human intervention would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task.     The challenges of this task are the following. First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, varies in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc . To overcome these difficulties, we present a deep learning framework involving two moralities: one for visual information and the other for textual information extracted from the covers.   Recently, deep learning approaches have reached high performances across a wide variety of problems . In particular, some deep convolutional neural networks can achieve a satisfactory level of performance on many visual recognition and categorization tasks, exceeding human performances. One of the most attractive qualities of these techniques is that they can perform well without any external hand-designed resources or task-specific feature engineering.  The theoretical foundations of deep learning are well rooted in the classical neural network  literature. It involves many hidden neurons and layers as an architectural advantage in addition to the input and output layers . A deep convolutional neural network is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough .  The main contributions of this paper are fourfold: []   The rest of the paper is structured as follows. Section 2 presents related works about book cover classification. Section 3 elaborates on the details of the proposed multi-modal architectures. In section 4, we discuss the experimental results. The last section concludes the paper and discusses future work.  
"," Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Book genre classification based on its cover would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task. At the same time, it is also an extremely challenging task due to the following reasons: First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, vary in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc. With the growing competitiveness in the book industry, the book cover designers and typographers push the cover designs to its limit in the hope of attracting sales. The cover-based book classification systems become a particularly exciting research topic in recent years. In this paper, we propose a multi-modal deep learning framework to solve this problem. The contribution of this paper is four-fold. First, our method adds an extra modality by extracting texts automatically from the book covers. Second, image-based and text-based, state-of-the-art models are evaluated thoroughly for the task of book cover classification. Third, we develop an efficient  and salable multi-modal framework based on the images and texts shown on the covers only.  Fourth, a thorough analysis of the experimental results is given and future works to improve the performance is suggested. The results show that the multi-modal framework significantly outperforms the current state-of-the-art image-based models. However, more efforts and resources are needed for this classification task in order to reach a satisfactory level.",305
"  \vskip 0.15in  Despite recent developments of activation functions for Machine Learning -based classifiers, such as the m-arcsinh~ for shallow Multi-Layer Perceptron ~, usable, repeatable and reproducible functions for both shallow and deep neural networks, e.g., the Convolutional Neural Network ~, have remained very limited and confined to three activation functions regarded as 'gold standard'. These include the Rectified Linear Unit , the sigmoid function and its modified version, hyperbolic tangent sigmoid or 'tanh'~, which extends its range from [0, +1] to [-1, +1]. The sigmoid and tanh have well-known vanishing gradient issues; thus, the ReLU function was devised to be more scalable for deep neural networks, despite its 'dying ReLU' problem, which has recently been solved by~. These have been made freely accessible in the open source Python library named 'Keras'~ for Deep Learning. The availability of these functions in the public domain has enabled not-for-profit and for-profit organisations to leverage them for several intelligence-based applications, from academic to industrial applications~~. \\  Nevertheless, considering the above-mentioned challenges in the Computer Science and ML communities, such activation functions lack robustness with classification tasks of varying degrees of complexity, e.g., slow or lack of convergence~ ~, caused by trapping at local ~. Moreover, amongst the three above-mentioned activation functions, only the ReLU is applicable from shallow to deep neural networks, with its novel quantum variations  found more scalable than its traditional version only recently~. \\  On the other side, in sciences dealing with the study of human behaviour, in the last 20 years, considerable progress has been made towards the prevention of mental health disorders~~. Specifically, professionals working in the field of counselling psychology have slightly enhanced their ability of grasping relational issues in their subjects via novel ML-based tele-monitoring technologies~. Nevertheless, these technologies have not yet changed the traditional counselling psychology practice, which is still based on a structured methodology that is adopted to help individuals to become more self-aware, more conscious of their own needs and moods~. The main goal counsellors pursue is guiding individuals to get to know themselves at a deeper level and to help them discover and resurface their own resources to better manage their emotions in their daily life. This process first requires a tailored dialogue between the counsellor and the individual and, subsequently, leveraging practical tools to aid the individual in their experience to understand their inner self more deeply~. Moreover, there are still limitations within the counselling setting. For instance, individuals, out of fear, may not reveal fundamental aspects of their  that would help counsellors guide them better in getting to know themselves. Furthermore, in many cases, subjects may express a verbal language opposite to their non-verbal one. Counsellors often hardly understand the dynamic patterns observed in the behaviours of their subjects, thus being unable to provide the required help and support to them. \\  In counselling, neural network algorithms, both shallow and deep depending on the amount of good-quality data and hardware available, have the potential to support counsellors in image and text classification tasks to understand and guide their subjects by helping them infer subtle dynamic changes in their behaviours. Via a careful and effective observation of images, micro- and macro- body movements, and facial expressions~~, it is possible to better interpret and understand the subjects' non-verbal language. Even the emotions underlying the written content from subjects may reveal inner aspects of their  that are fundamental for counsellors to help resurface to increase the subjects' self-awareness and related capability of 'self-healing'~. \\  Therefore, from both theoretical and practical standpoints, there is an increasing need for accurate and reliable open source activation functions, which reach convergence faster, avoiding trapping at local , are more stable and can also be used and scale across both shallow and deep neural network algorithms for image and text classification. Entirely written in Python and made freely available in TensorFlow~ and Keras~, the proposed hyperbolic function is demonstrated as a competitive function with respect to gold standard functions, which suits both shallow and deep neural networks, thus being accurate and reliable for pattern recognition to aid image and text classification tasks.  Thanks to its liberal license, it has been widely distributed as a part of the free software Python libraries TensorFlow~ and Keras~, and it is available for use for both academic research and commercial purposes.\\  %%%%%%%%%%%%%%% Methods section %%%%%%%%%%%%%%%%%%%%%  \vskip 0.3in  
","%   <- trailing '%' for backward compatibility of .sty file This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation function suitable for Deep Learning -based algorithms for supervised learning, such as Convolutional Neural Networks . hyper-sinh, developed in the open source Python libraries  and , is thus described and validated as an accurate and reliable activation function for both shallow and deep neural networks.  Improvements in accuracy and reliability in image and text classification tasks on five  benchmark data sets available from Keras are discussed.  Experimental results demonstrate the overall competitive classification performance of both shallow and deep neural networks, obtained via this novel function.  This function is evaluated with respect to gold standard activation functions, demonstrating its overall competitive accuracy and reliability for both image and text classification.",306
"  In grounded language theory, the semantics of language are given by how symbols connect to the underlying real world---the so-called ``symbol grounding problem''. For example, we want a robotic system that sees an eggplant  to ground the recognition object to a canonical symbol for `eggplant.' When a user asks ""Please grab me the eggplant,"" the robot should ground the natural language word ""eggplant"" to the same symbol that denotes the relevant visual percepts. Once both language and vision successfully ground to the same symbol, it becomes feasible for the robot to complete the task. We learn this connection by using physical sensors in conjunction with language learning: paired language and perceptual data are used to train a joint model of how linguistic constructs apply to the perceivable world.   Machine learning of grounded language often demands large-scale natural language annotations of things in the world, which can be expensive and impractical to obtain. It is not feasible to build a dataset that encompasses every object and possible linguistic description. Novel environments will require symbol grounding to occur in real time, based on inputs from a human interactor. Learning the meanings of language from unstructured communication with people is an attractive approach, but requires fast, accurate learning of new concepts, as people are unlikely to spend hours manually annotating even a few hundred samples, let alone the thousands or millions commonly required for machine learning.  % Active learning, in which a system queries for specific training data, has the potential to improve learning efficiency and reduce the number of labels required to learn a grounded language model.  In this work we study active learning, in which a system deliberately seeks information that will lead to improved understanding with less data, to minimize the number of samples/human interactions required. The field of active learning typically assumes that a pool of unlabeled samples is available, and the model can request specific example that it would like to obtain a label for. By having the model select the most informative data points for labeling, the number of samples that need to be labeled is reduced. This maps to the goal of human-robot learning with minimum training data provided by the human. Furthermore, active learning can be part of a pipeline with other few-shot learning methods.   However, active learning is not a magic bullet. When not carefully applied, it does not outperform sequential or random sampling baselines. Thoughtful selection of suitable approaches for problems is required. While active learning has been used for language grounding %, , to the best of our knowledge, we present the first broad exploration of the best methods for active learning for grounding vision-language pairs. %  In this paper, our focus is on developing guidelines by which active learning methods might be appropriately selected and applied to vision-language grounding problems. We test different active learning approaches on grounded language problems of varying linguistic and sensory complexity, and use our results to drive a discussion of how to select active learning methods for different grounded language data acquisition problems in an informed way.  We consider the grounded language task of learning novel language about previously unseen object types and characteristics. Our emphasis is on determining what methods can reduce the amount of training data needed to achieve performance consistent with human evaluation. Primarily, we address five relevant questions concerning characteristic-based grounded language learning: []  % We make conclusions with respect to these questions in . % In addition to addressing the above research questions, we verify how generalizable these learning techniques are beyond characteristic-based grounding.    We find that a right ordering of training data makes it possible to learn successfully from significantly fewer descriptions in most cases, but also that the active learning methodology chosen is specific to the nature of the learning problem. Our main contribution is a principled analysis of using active learning methods as unsupervised data sampling techniques in language grounding with a discussion of what aspects of those problems are relevant to approach selection. While our contributions are primarily analytic rather than algorithmic, we argue they address a critical need within grounded language understanding, an active research area in which questions of efficiency and data collection are widespread, and have the potential to support additional algorithmic developments.  
"," % In grounded language acquisition, a physical agent uses language combined with high-frequency sensor data to learn a model of how language refers to the physical world. This approach, while powerful, often requires extensive data annotation, which can be difficult to obtain. This work  % Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora. We present an exploration of active learning approaches applied to three grounded language problems of varying complexity in order to analyze what methods are suitable for improving data efficiency in learning. We present a method for analyzing the complexity of data in this joint problem space, and report on how characteristics of the underlying task, along with design decisions such as feature selection and classification model, drive the results. We observe that representativeness, along with diversity, is crucial in selecting data samples.",307
" Deep neural networks are powerful and have been widely applied in natural language processing. However, recent studies demonstrate that these models are vulnerable to adversarial examples, which are malicious inputs intentionally crafted to fool the models. % The introduction of the adversarial example ushered in a new era to understand and improve neural the network-based models.  % Adversarial attacks and defenses against these attacks have drawn significant attention in recent years . Although generating adversarial examples for texts has proven to be a more challenging task than for images due to their discrete nature, a number of methods have been proposed to generate adversarial text examples and reveal the vulnerability of deep neural networks in natural language processing  tasks including reading comprehension , text classification , machine translation , dialogue systems , and dependency parsing . These methods attack text examples by replacing, scrambling, and erasing characters or words or other language units.  To settle the susceptible attack direction, they require a large number of queries to the target model for the predictions of given inputs. Thus the adversarial examples are typically generated for a specific model.  This motivates the main questions we aim to answer in this paper: { %are there universal adversarial examples that can transfer to any neural network-based models?  It is well known that adversarial examples exhibit black-box transferability, meaning that adversarial examples generated for one model can fool another model .  Transfer attackers launch white-box attacks on local models to find candidate adversarial examples that may transfer to the target model. % In the white-box setting, an adversary can access the model's architecture, parameters and input feature representations while not in the black-box one. % However, adversarial examples are typically overfitted to the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models.  However, which factors most affect the transferability of adversarial examples is still unclear, especially for NLP models. In this study, we quantitatively investigate how adversarial transferability is impacted by several critical factors, including the network architecture, input form, word embedding type, and model capacity.  Based on the understanding of transferability among various neural models, we study whether it is possible to craft universal, model-agnostic text adversarial examples for almost all existing models.  Universal adversarial examples have at least two advantages. First, the adversaries do not need white-box access to the target models. They launch the attacks by their own models trained on similar data, which can transfer across models .  Second, universal adversarial examples are a useful analysis tool because, unlike typical attacks, they are model-agnostic.  Thus, they highlight general input-output patterns learned by a model. We can leverage this to study the influence of dataset biases and to identify those biases that are learned by models.   [t] {1.0mm} {c|l}  {Senate Panel Gives NASA Extra Money  AP - NASA would get  \#36;16.4 billion next year under a \\ bill a Senate committee approved Tuesday, reversing a decision by House lawmakers to \textcolor{blue}{} \textcolor{red}{contract} \\ the space agency's budget below this year's levels.} \\  {Deal in Congress to \textcolor{blue}{} \textcolor{red}{preserve} tax cuts, Widening Deficit Republican and Democratic leaders agreed \\ to extend \$5 billion worth of tax cuts sought by \textcolor{blue}{} \textcolor{red}{Chairman} Bush without trying to pay for them.} \\  {Nortel Downsizes Again Aug. 23, 2004  Problem-plagued Nortel \textcolor{blue}{} \textcolor{red}{Web} Corp. \\ announced plans Thursday, Aug. 19, to eliminate an additional 3,500 jobs and fire seven more senior \\ \textcolor{blue}{} \textcolor{red}{administrators} as the company labors to reinvent.} \\  \hline \hline        In this study, we first systematically investigated a few critical factors of neural models, including network architectures , input forms , embedding types , and model capacities  and how they impact the transferability of text adversarial examples through extensive experiments on two datasets of text classification.  We vary one factor at a time while fixing all others to see which factor is more significant, and found that the input form has the greatest influence on the adversarial transferability, following by network architecture, embedding type, and model capacity. Then, we propose a genetic algorithm to find an optimal ensemble with minimum number of members on the basis of our understanding of the adversarial transferability among neural models.  The adversarial examples generated by attacking the ensemble found by our algorithm strongly transfer to other models, and for some models, they exhibit better transferability than those generated by attacking models with different random initialization. Finally, we generalize the adversarial examples constructed by the ensemble method into universal semantics-preserving word replacement rules that can induce adversaries on any text input strongly transferring to any neural network-based NLP model . Since those rules are model-agnostic, they provide an analysis of global model behavior, and help us to identify dataset biases and to diagnose heuristics learned by the models.   
"," Deep neural network models are vulnerable to adversarial attacks. In many cases, malicious inputs intentionally crafted for one model can fool another model in the black-box attack setting. However, there is a lack of systematic studies on the transferability of adversarial examples and how to generate universal adversarial examples.  In this paper, we systematically study the transferability of adversarial attacks for text classification models.  In particular, we conduct extensive experiments to investigate how various factors, such as network architecture, input format, word embedding, and model capacity, affect the transferability of adversarial attacks.  Based on these studies, we then propose universal black-box attack algorithms that can induce adversarial examples to attack almost all existing models. These universal adversarial examples reflect the defects of the learning process and the bias in the training dataset.  Finally, we generalize these adversarial examples into universal word replacement rules that can be used for model diagnostics.     \if0 It has been known that adversarial examples exhibit black box transfer, i.e. malicious inputs intentionally crafted for one model can also cause another model to make mistakes. However, which factors affect the most and how they impact the transferability of adversarial examples are still unclear, especially for NLP models.  Through extensive experiments, we systematically investigate how adversarial transferability is impacted with a few critical, model-specific factors, including the network architecture, input form, pre-trained word embedding, and model capacity. Based on the understanding of the adversarial transferability among neural models, we propose a population-based algorithm to find an optimal ensemble with minimum number of models, which can be used to generate adversarial examples that strongly transfer across other neural models.  We also generalize the adversarial examples generated by the ensemble method into universal word replacement rules that can induce adversaries on any text input to fool almost all the existing models with a much higher success rate. Those rules also help us to identify dataset biases and diagnose heuristics improperly learned by the models. \fi",308
" Emotional analysis has been an active research area for a few decades, especially in recognition domains of text and speech emotions. Even if text and speech emotions are closely relevant, both kinds of emotions have different challenges. One of the challenges in text emotion recognition is ambiguous words, resulting from omitted words . On the other hand, one of the challenges in speech emotion recognition is creating an efficient model. However, this paper focuses on only the recognition of speech emotions. In this area, two types of information, linguistic and paralinguistic, were mainly considered in speech emotion recognition. The linguistic information refers to the meaning or context of speech. The paralinguistic information implies the implicit message meaning, like the emotion in speech . Speech characteristics can interpret the meaning of speech; therefore, behavioral expression was investigated in most of the speech emotion recognition works  .   In recent works, local feature learning block  , one of the efficient methods, has been used in integrating local and global speech emotion features, which provide better results in recognition. Inside LFLB, convolution neural network  was used for extracting local features, and then long short-term memory  was applied for extracting contextual dependencies from those local features to learn in a time-related relationship. However, vanishing gradient problems may occur with CNN . Therefore, residual deep learning was applied to the CNN by using skip-connection to reduce unnecessary learning and add feature details that may be lost in between layers.  Furthermore, the accuracy of speech recognition does not only rely on the efficiency of a model, but also of a speech feature selection . In terms of speech characteristics, there are many distinctive acoustic features that usually used in recognizing the speech emotion, such as continuous features, qualitative features, and spectral features . Many of them have been investigated to recognize speech emotions. Some researchers compared the pros and cons of each feature, but no one can identify which feature was the best one until now .  As previously mentioned, we proposed a method to improve the efficiency of LFLB  for deeper learning. The proposed method, deep residual local feature learning block , was inspired by the concept of human brain learning; that is, 閳ユΜepeated reading makes learning more effective,閳 as the same way that Sari  and Shanahan  were used. Responding to our inspired concept, we implemented a learning method for speech emotion recognition with three parts: Part 1 is for general learning, like human reading for the first time, Part 2 is for further learning, like additional readings, and the last part is for associating parts learned to decide types of emotions. Besides, the feature selection is compared with two types of distinctive features to find the most effective feature in our work: the normal and specific distinctive features are log-mel spectrogram , which is fully filtered sound elements, and %log-mel spectrogram,  MFCC deltas, delta-deltas, and chromagram  are more clearly identify speech characteristics extracted based on %according to  the human mood.  Our main contributions of this paper are as follows:   Deep residual local feature learning block  was proposed. DeepResLFLB was arranged its internal network as LFLB, batch normalization , activation function, normalization-activation-CNN , and deep layers.   Learning sequences of DeepResLFLB were imitated from human re-reads.   Speech emotion features, %according to  based on human mood determination factors such as LMS and LMSDDC, were applied and compared their performances.   
"," Speech Emotion Recognition  is becoming a key role in global business today to improve service efficiency, like call center services. Recent SERs were based on a deep learning approach. However, the efficiency of deep learning depends on the number of layers, i.e., the deeper layers, the higher efficiency. On the other hand, the deeper layers are causes of a vanishing gradient problem, a low learning rate, and high time-consuming. Therefore, this paper proposed a redesign of existing local feature learning block . The new design is called a deep residual local feature learning block . DeepResLFLB consists of three cascade blocks: LFLB, residual local feature learning block , and multilayer perceptron . LFLB is built for learning local correlations along with extracting hierarchical correlations; DeepResLFLB can take advantage of repeatedly learning to explain more detail in deeper layers using residual learning for solving vanishing gradient and reducing overfitting; and MLP is adopted to find the relationship of learning and discover probability for predicted speech emotions and gender types. Based on two available published datasets: EMODB鐠虹棏nd RAVDESS, the proposed DeepResLFLB can significantly improve performance when evaluated by standard metrics: accuracy, precision, recall, and F1-score.",309
"  We are interested in the problem of algorithmically learning a description for a formal language  when presented successively all and only the elements of that language; this is called , a branch of  learning theory. For example, a learner  might be presented more and more even numbers. After each new number,  outputs a description for a language as its conjecture. The learner  might decide to output a program for the set of all multiples of , as long as all numbers presented are divisible by~. Later, when  sees an even number not divisible by , it might change this guess to a program for the set of all multiples of~.  Many criteria for determining whether a learner  is  on a language~ have been proposed in the literature. , in his seminal paper, gave a first, simple learning criterion, \footnote{ stands for learning from a  of positive examples;  for Gold, indicating full-information learning;  stands for .}, where a learner is  if and only if, on every  for   it eventually stops changing its conjectures, and its final conjecture is a correct description for the input language.  Trivially, each single, describable language  has a suitable constant function as a -learner . Thus, we are interested in analyzing for which   is there a   learning  member of . This framework is also known as  and has been studied extensively, using a wide range of learning criteria similar to -learning .  In this paper we  put the focus on the possible descriptions for languages. Any computably enumerable language  has as possible descriptions any program enumerating all and only the elements of , called a -index . This system has various drawbacks; most importantly, the function which decides, given  and , whether \tau\Ex_CC\CalR\Cind\Ex_C\Cind\Ex_C\tau\Ex_C\CalL_{i . For a learner  learning according to  we have that  gives an indexing of a family of languages, and  learns some subset thereof. We are specifically interested in the area between this setting and learning with -indices .   The criteria we analyze naturally interpolate between these two settings. We show that we have the following hierarchy:  allows for learning strictly fewer classes of languages than , which allow for learning the  classes as , which again are fewer than learnable by , which in turn renders fewer classes learnable than .  All these results hold for learning with full information. In order to study the dependence on the mode of information presentation, we also consider  learners , which only get the set of data presented so far and the iteration number as input;  learners , which get only the set of data presented so far;  learners , which only get the new datum and its current hypothesis and, finally,  learners , which only get the current data. Note that transductive learners are mostly of interest as a proper restriction to all other modes of information presentation.  We show that full-information learners can be turned into partially set-driven learners without loss of learning power. Furthermore, iterative learning is strictly less powerful than set-driven learning, in all settings. Altogether we analyze 25 different criteria and show how each pair relates. All these results are summarized in Figure as one big map stating all pairwise relations of the learning criteria mentioned, giving 300 pairwise relations in one diagram, proven with 13 theorems in Section. Note that the results comparing learning criteria with -indices were previously known, and some proofs could be extended to also cover learning with -indices.     We derive a similar map considering a possible relaxation on -learning: while  requires syntactic convergence to one single correct -index, we consider  learning,  for short, where the learner only has to semantically converge to correct -indices . We again consider the different modes of data presentation and determine all pairwise relations in Figure.     Before getting to our results in detail, we continue with some  preliminaries in Section.         
"," 	In , the most common type of hypothesis     is to give an enumerator for a language. This so-called $W$-index allows for     naming arbitrary computably enumerable languages, with the drawback that     even the membership problem is undecidable. In this paper we use a different     system which allows for naming arbitrary decidable languages, namely      . These     indices have the drawback that it is now not decidable whether a given     hypothesis is even a legal $C$-index. 	 	In this first analysis of learning with $C$-indices, we give a structured     account of the learning power of various restrictions employing $C$-indices,     also when compared with $W$-indices. We establish a hierarchy of learning     power depending on whether $C$-indices are required  on all outputs;      only on outputs relevant for the class to be learned and  only in the     limit as final, correct hypotheses. Furthermore, all these settings are     weaker than learning with $W$-indices . We analyze all these questions also in relation to     the mode of data presentation. 	 	Finally, we also ask about the relation of semantic versus syntactic     convergence and derive the map of pairwise relations for these two kinds of     convergence coupled with various forms of data presentation.",310
" Classification is an important task of knowledge discovery in databases and data mining. It is a task of learning a discriminative function from the given data that classifies previously unseen data to the correct classes. Current research trends in natural language processing focus on developing deep neural network models such as BERT  that have been pre-trained with a large text corpus and thus show immense improvement in different text classification tasks. Despite the success of large pre-trained models, DNNs still suffer from generalizing to a balanced testing criterion in cases of data imbalance . In realistic settings, it is rarely the case where the discrete distribution of the data acquired is perfectly balanced across all classes. Realistic settings are prone to be skewed to specific classes while such classes are often the class of interest. Some situations may be binary, as in detecting spams in forums . The majority of the contents posted from users are not spams and is in accordance with the intended goal. As a result, the number of spam samples is sparse in comparison to non-spam samples. Imbalanced data may also occur in a multi-classification setting such as classifying articles into different categories .       Text classification can be used for numerous application purposes. In this paper, we address the problem of detecting sexual harassment and toxicity in comments from news articles. In the name of anonymity, online discussion platforms have become a place where people undermine, harass, humiliate, threaten, and bully others  based on their superficial characteristics such as gender, sexual orientation, and age . Each toxic comment can further be classified into classes based on their degree of toxicity . Figure  shows the overall procedure of detecting sexual harassment and performing sentimental analysis on comment data in the wild. When collecting and annotating comments, data skewness occurs naturally since users do not consider data imbalance levels when writing toxic or non-toxic comments. Classifiers trained in imbalanced settings tend to become biased toward the class with more samples in the training data. This is because standard deep learning architectures  do not take the data imbalance level into consideration. In order to develop intelligent classifiers, methods to temper the classifier from biasing towards certain classes are of great importance.   Previous methods addressing data imbalance in the text can be divided into data-level and algorithm-level methods. Data-level methods  apply manipulation on the data by undersampling majority classes or oversampling minority classes. However, most of the methods require an effective numerical representation algorithm since methods are applied directly to the representation instead of on the actual text. Algorithm-level methods modify the underlying learner or its output to reduce bias towards the majority group. However, these methods are task-sensitive and somewhat heuristic since it requires the researchers to modify the classifier considering the innate properties of the task. This property leads to the inefficiency in training the learner since heuristic approaches are often time-consuming and arbitrary. Since only traditional oversampling and undersampling methods, which simply duplicate or sample data instances, are independent of these two limitations, methods addressing data imbalance in the text without the utilization of feature spaces or task-dependent is needed.  We propose a novel training architecture, Sequential Targeting , that handles the data imbalance problem by forcing an incremental learning setting. ST divides the entire training data set into mutually exclusive partitions, target-adaptively balancing the data distribution. Target distribution is a predetermined distributional setting that enables the learner to exert maximum performance when trained with. In an imbalanced distributional setting, the target distribution is idealistic to follow a uniform distribution where all the classes hold equal importance. Optimal class distribution may differ by innate property of the data but research shows that a balanced class distribution has an overall better performance compared to other distributions~. The remaining partitions are then sorted in the magnitude of similarity with the target distribution which is measured by KL-divergence. The first partition of the split data is imbalanced while the last partition is arbitrarily modeled to be uniform across classes and all the partitions are utilized to train the learner sequentially.   We handle the issue of catastrophic forgetting~, which is an inevitable phenomenon when transfer learning, by utilizing Elastic Weight Consolidation~ to stabilize the knowledge attained from the previous tasks. This allows the discriminative model to learn from the incoming data while not forgetting the previously inferred parameters from previous tasks.   Our proposed method is both independent of the numerical representation method and the task at hand. We validate our method on simulated datasets with varying imbalance levels and apply our method to a real-world application. We annotated and construct three datasets consisting of comments made by users from different social platforms of NAVER\footnote{NAVER is the Korean No.1 web search portal where around 16 million users visit every day. www.naver.com}: two for detecting sexual harassment and one for multiple sentimental analysis. Annotations on the datasets were improved iteratively by in-lab annotations and crowdsourcing. Experimental results show that ST outperforms traditional approaches, with a notable gap, especially in extremely imbalanced cases. Lastly, ST proves to be compatible with previous approaches.  Our contribution in this paper is three-folds:      The rest of the paper is organized as follows. Section  summarizes related works. Section  provides the details of the proposed method. Section    presents dataset descriptions, experiment setups, and qualitative experimental results on various datasets. Finally, Section  concludes the paper.  
"," %% Text of abstract Classification tasks require a balanced distribution of data to ensure the learner to be trained to generalize over all classes. In real-world datasets, however, the number of instances vary substantially among classes. This typically leads to a learner that promotes bias towards the majority group due to its dominating property. Therefore, methods to handle imbalanced datasets are crucial for alleviating distributional skews and fully utilizing the under-represented data, especially in text classification. While addressing the imbalance in text data, most methods utilize sampling methods on the numerical representation of the data, which limits its efficiency on how effective the representation is. We propose a novel training method, Sequential Targeting, independent of the effectiveness of the representation method, which enforces an incremental learning setting by splitting the data into mutually exclusive subsets and training the learner adaptively. To address problems that arise within incremental learning, we apply elastic weight consolidation. We demonstrate the effectiveness of our method through experiments on simulated benchmark datasets  and data collected from NAVER.",311
"  As the growth of robots interacting with humans, different levels of environment understanding is required by the robot. A robot acting in an environment has to deal with many open questions, thus needs different levels of reasoning to do a task. Usually, robots rely on their initial knowledge, perception and their cognitive abilities to be able to understand and do reasoning in their situated environment. A recently hooked topic to a better Knowledge-Based cognition is dialogic interaction between a human and a robot, where the robot captures fresh information about the environment from a user through Natural Language. Information comes from Natural Language together with visually perceived information, and a Knowledge Base  lets a cognitive agent reach different levels of understanding in the environment.   The first level of understanding can be seen as classification and detection on sensory inputs, e.g. detection of objects in visual perception, or role tagging of lexical in a sentence. The second level of understanding concerns finding relations between different sensory inputs, e.g. finding common attributes in language and vision. Some famous problems such as symbol grounding  and anchoring  concern finding correspondences in different sensory input modalities. A higher and abstract level of understanding can be thought to find relations between the entities in an environment. e.g. in a scene with a desk and a book on top, some of the relationships between these are their relative physical position and their semantics that shows how entities  are similar.   Understanding relationships between physical entities can also be extended to the attributes of entities. Indeed the same definition of the relationship between entities can be found for the attributes. For example, when a user declares freshness attribute of 'apple-1' is 'spoiled', as well as 'apple-2' and 'apple-3', but 'orange-1' and 'banana-1' are 'fresh', a relation between the values of freshness attribute exists which connects semantic of entities; In this example, is that all apples are 'spoiled', and the rest of fruits are 'fresh', with closed world assumption.   Relation and rules for attributes of entities can help a robot that is interacting with a human in many applications. For example when a user utters ""bring me a fruit"", using the rules obtained for freshness attribute, the robot notices which fruits are spoiled and which are fresh to eat. Such logical rules between attributes let the robot realize that apples are spoiled, apples should be thrown out, and added to the shopping list. Moreover, the obtained rule for attributes can be used in a robot's low-level sensory input processing. Consider an utterance where the user of our example is declaring that a physical entity is spoiled, but the robot's visual perception has doubt whether the perceived object is apple or pear. As the robot already found that all apples are spoiled and other fruits are fresh, so the perceptual detection refines the recognized object as the apple.  %Different attributes can represent characteristics of an entity, where some are computed from visual perception and some from Natural Language through interaction with a user. In this work, we deal with nine different attributes, as a category, color, label, functionality, owner, size, weight, restriction, and location of entities in a scene; where the first two are computed from visual perception and the rest are obtained from Natural Language.  %%It is worth emphasis on the importance of attributes that come from Natural Language. Such information is almost impossible to obtain from visual perception, e.g. the information that a user can give about owner of an entity, cannot be obtained from the camera. Also, an initial knowledge base only gives information about the category of an entity, and not about a particular entity , and some of the assignments might be temporary. On the other hand, information about size, weight, and location, may be used for refinement of knowledge base and camera, or just a shortcut to obtaining such information from the user.   In this work, we propose a framework for learning logical rules that represent relations between attributes in a semantic model of the robot's environment. Such logical rules help the robot to find which attributes  entail a specific attribute. A distinctive novelty of our work is to generalize rules from a semantic model built via Human-Robot Interaction , through the integration of visual and linguistic cues. Our framework goes all the way from sensory input data to abstract First-Order Logic formulas that describe abstract relationship between attributes of entities in a scene. %Our approach differs from other works as our system is able to capture more attributes from Natural Language in addition to attributes from computer vision.  %Our proposed framework compute First-Order Logic formulas, which is useful for general reasoning upon entities that have common attributes.  We focus on , which the robot can capture implicitly when a human describes objects to the robot. In other words, we do not require the user to give rules explicitly to the robot, but rather we let the robot find rules and do further reasoning based on self-computed rules for improving its interaction with the user.    This paper continues with the review of related work, and then in Section  the proposed framework is described, followed by an implementation to demonstrate the viability of the proposed framework in Section . In Section  results of a test scenario are given, followed by the discussion about the applicability of the framework. In the end, conclusions of this work are drawn.      
"," Humans have a rich representation of the entities in their environment. Entities are described by their attributes, and entities that share attributes are often semantically related.  For example, if two books have ``Natural Language Processing'' as value of their `title' attribute, we can expect that their `topic' attribute will also be equal, namely, ``NLP''.  Humans tend to generalize such observations, and infer sufficient conditions under which the `topic' attribute of any entity is ``NLP''.  If robots need to interact successfully with humans, they need to represent entities, attributes, and generalizations in a similar way. This ends in a contextualized cognitive agent that can adapt its understanding, where context provides sufficient conditions for a correct understanding. In this work, we address the problem of how to obtain these representations through human-robot interaction.  We integrate visual perception and natural language input to incrementally build a semantic model of the world, and then use inductive reasoning to infer logical rules that capture generic semantic relations, true in this model.  These relations can be used to enrich the human-robot interaction, to populate a knowledge base with inferred facts, or to remove uncertainty in the robot's sensory inputs.",312
" In recent years, science, engineering and mathematics education has emphasized supporting students閳 disciplinary ""practices"" of inquiry. These practices閳ユ敃uch as of formulating questions, designing investigations, or arguing from evidence閳ユ攣re more difficult to identify and assess than the traditional objectives of particular correct content knowledge. In order to study students閳 practices, researchers rely mainly on qualitative analyses of naturalistic data. These studies have advanced the field閳ユ獨 understanding of practices.\\  These studies have been limited in scope, however, because they are extremely labor-intensive: Analysis of naturalistic data requires significant and extensive effort by trained researchers, from transcribing to coding to the construction of meaning. It has been time and cost-prohibitive to conduct qualitative studies with large samples of data. Our purpose in this project is to develop computational tools that can support qualitative research at large scales on students' inquiry practices in science. In this paper we report on initial progress towards applying natural language processing  techniques to research on students' written arguments in college biology laboratory reports. \\  In this work, we report on our success in designing NLP that approached the reliability of trained, human coders. Specifically, we show that contrastive learning in the Wasserstein space is able to achieve a high level of agreement  on average. \\  The rest of the paper is organized as follows. In section  first overview current state-of-art in automating assessment of writing in science using machine learning for natural language processing. Following that we outline the writing assessment setting particular to our case in section .  In section  we briefly survey the relevant literature in machine learning for NLP and introduce our novel approach for automatic scoring. In section  we evaluate the performance of the proposed approach and discuss the results in detail.\\  
"," Qualitative analysis of verbal data is of central importance in the learning sciences. It is labor-intensive and time-consuming, however, which limits the amount of data researchers can include in studies. This work is a step towards building a statistical machine learning  method for achieving an automated support for qualitative analyses of students闁 writing, here specifically in score laboratory reports in introductory biology for sophistication of argumentation and reasoning. We start with a set of lab reports from an undergraduate biology course, scored by a four-level scheme that considers the complexity of argument structure, the scope of evidence, and the care and nuance of conclusions. Using this set of labeled data, we show that a popular natural language modeling processing pipeline, namely vector representation of words, a.k.a word embeddings, followed by Long Short Term Memory  model for capturing language generation as a state-space model, is able to quantitatively capture the scoring, with a high Quadratic Weighted Kappa  prediction score, when trained in via a novel contrastive learning set-up. We show that the ML algorithm approached the inter-rater reliability of human analysis. Ultimately, we conclude, that machine learning  for natural language processing  holds promise for assisting learning sciences researchers in conducting qualitative studies at much larger scales than is currently possible.",313
"   Mental illnesses are a common problem of our modern world. More than one in ten people was living with mental health disorders in 2017 , with women being the most affected. These disorders affect people's way of thinking, mood, emotions, behaviour and their relationships with others. Most mental illnesses remain undiagnosed because of the social stigma around them.  Depression is one of the main causes of disability globally , it affects people of all ages. Prevention is used to reduce depression and to save the lives of people at risk of suicide, but prevention is only limited to raising awareness and programs to cultivate positive thinking in case of depression and monitoring people who attempted suicide or self-harm.  With the rise in social media use, more computational efforts are made to detect mental illnesses such as depression  and PTSD , but also to detect misogyny , irony and sarcasm  from users' texts.  People tend to talk more about their emotions and mental health problems online and to seek support. The sources of mental health cues used for detection are Twitter, Facebook, Reddit and forums . Reddit is a social media site very similar to forums. It is organized in subreddits with specific topics, some dedicated to mental health problems. The use of throwaway accounts to maintain anonymity promotes disclosure, and users are more likely to share problems they have not discussed with anyone before. The use of these accounts makes it difficult for users to receive more social support because the majority of them are used only for one post .  In this work, we choose to tackle the problem of detecting early onset of depression from users' posts on social media, specifically from Reddit. As such, we explore the eRisk 2018 dataset through topic analysis by means of Latent Semantic Indexing  and learned out-of-distribution confidence scores . Due to the nature of the dataset, we repurpose the learned confidence score to make a decision on whether to label the user as depressed or non-depressed or to wait for more data, as test chunks were progressively released every week.   
","   English.  Computational research on mental health disorders from written texts covers an interdisciplinary area between natural language processing and psychology. A crucial aspect of this problem is prevention and early diagnosis, as suicide resulted  from  depression being the second leading cause of death for young adults. In this work, we focus on methods for detecting the early onset of depression from social media texts, in particular from Reddit. To that end, we explore the eRisk 2018 dataset and achieve good results with regard to the state of the art by leveraging topic analysis and learned confidence scores to guide the decision process. \footnote{Copyright \copyright2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International .} %   Our analysis paves way to more in depth exploration of detection of mental illnesses from social media interactions.",314
" 	 		Neural text-to-speech  techniques have significantly improved the naturalness of speech produced by TTS systems. We refer to NTTS systems as a subset of TTS systems that use neural networks to predict mel-spectrograms from phonemes, followed by the use of neural vocoder to generate audio from mel-spectrograms. 			 		In order to improve the prosody\footnote{We use the subtractive definition of prosody from .} of speech obtained from NTTS systems, there has been considerable work in learning prosodic latent representations from ground truth speech. These methods use the target mel-spectrograms as input to an encoder which learns latent prosodic representations. These representations are used by the decoder in addition to the input phonemes, to generate mel-spectrograms. The latent representations obtained by encoding a target mel-spectrogram at the sentence level will have information that is not directly available from the phonemes, and by the subtractive definition of prosody, we may claim that these representations capture prosodic information. Several variational and non-variational methods have been proposed for learning prosodic latent representations. While these methods improve the prosody of synthesised speech, they need an input mel-spectrogram which is not available while running inference on unseen text. This gives rise to the problem of sampling from the learnt prosodic space. Sampling at random from the prior may result in the synthesised speech not having contextually appropriate prosody, as it has no relationship with the text being synthesised. In order to improve the contextual appropriateness of prosody in synthesised speech, there has been work on using textual features like contextual word embeddings and other grammatical information to directly condition NTTS systems. These methods require the NTTS model to learn an implicit correlation between the given textual features and the prosody of the sentence. One work also poses this sampling problem as a selection problem and uses both syntactic distance and BERT embeddings to select a latent prosodic representation from the ones seen at training time.  		Bringing both the aforementioned ideas of using ground truth speech to learn prosodic latent representations and using textual information, we build Kathaka, a model trained using a two-stage training process to generate speech with contextually-appropriate prosody. In Stage~\Romannum{1}, we learn the distribution of sentence-level prosodic representations from ground truth speech using a VAE. In Stage~\Romannum{2}, we learn to sample from the learnt distribution using text. In this work, we introduce the BERT+Graph sampler, a novel sampling mechanism which uses both contextual word-piece embeddings from BERT and the syntactic structure of constituency parse trees through graph attention networks. We then compare Kathaka against a strong baseline and show that it obtains a relative improvement of  in naturalness. 		 	 	
"," 		In this paper, we introduce Kathaka, a model trained with a novel two-stage training process for neural speech synthesis with contextually appropriate prosody. In Stage, we learn a prosodic distribution at the sentence level from mel-spectrograms available during training. In Stage, we propose a novel method to sample from this learnt prosodic distribution using the contextual information available in text. To do this, we use BERT on text, and graph-attention networks on parse trees extracted from text. We show a statistically significant relative improvement of $13.2\%$ in naturalness over a strong baseline when compared to recordings. We also conduct an ablation study on variations of our sampling technique, and show a statistically significant improvement over the baseline in each case.",315
" Due to the growing presence of AI-powered systems in our lives, affective computing has become an important part of human-computer interaction. Emotion plays a role in our thoughts and actions and is an integral part of the way we communicate . The ability to leverage context to understand emotions communicated both verbally and non-verbally is trivial for humans but remains difficult for machines . Emotional responses depend on both our psyche and physiology and are governed by our perception of situations, people and objects. They also depend on our mental state   . The way we exhibit and perceive emotion may also differ based on our age, gender, race, culture and accent . In addition to all of this, unlike targets in other classification tasks, the emotions we experience are rarely distinct: they often coexist without clear temporal boundaries, adding considerable complexity to the task .  Despite these difficulties, automated emotion recognition has social and commercial applications that make it worth pursuing. In the medical domain, it has exciting potential: to identify and diagnose depression and stress in individuals , to monitor and help people with bipolar disorder  and to assist the general public in maintaining mental health. Commercial applications include call center customer management, advertising through neuro-marketing and social media engagement . As intelligent chatbots and virtual assistants have become more widely used, emotion detection has become a vital component in the design, development and deployment of these conversational agents .  Early research in emotion detection focused on binary classification in a single modality, whether in text, speech , or images . Text-based classifiers used the n-gram vocabulary of sentences to predict their polarity and speech models modeled the vocal dynamics that characterize these emotions. These approaches are inherently limited: a binary granularity and cues from a single modality are far removed from the actual human process they're meant to model. As a result, joint approaches which leverage all available modalities  are promising.  While existing multi-modal emotion corpora like IEMOCAP  and Crem-D  have been critical for the progress in affective computing to date, they suffer from three issues that are the focus of our work. First, these corpora tend to be small due to the high costs of annotating for emotion. This precludes the use of deep neural models with high model complexity as they require many training samples to generalize well. This also compounds the second difficulty inherent to many emotion datasets: while there are usually many neutral, happy and sad training examples, there are often very few examples of rarer emotions like disgust making them difficult to classify. This issue is not easily solved by combining different corpora due to the third issue, their lack of mutual compatibility -- they differ in the emotions identified, the types of dialogue and number of speakers represented and the naturalness of the recordings . This severely restricts the generalizability of models trained on a single corpus.  Contemporary literature has dealt with these problems by dropping labels . Hard and scarce emotions like disgust are dropped from the corpus and the models are trained and evaluated on the trimmed corpus. This allows evaluating models on different corpora by using utterances exhibiting only the most common emotions. While this is a reasonable, the resulting performance is not a complete reflection of how these models perform once deployed to production. When emotion models are used in real-world applications, we can expect them to encounter utterances corresponding to dropped labels. For such cases, these models are likely to exhibit degraded performance by predicting one of the known, but incorrect labels.  In this work, we address the problem of data sparsity by transfer learning via the pretrain-then-finetune paradigm. Deep complex models can be trained on large datasets for an auxiliary but related task to learn network parameters that reflect abstract notions related to the target task. As the expression of emotions is highly dependent on the individual, we train a multilayer TDNN  on the task of speaker identification using the VoxCeleb corpus  and then fine-tune its final few layers on the task of emotion identification using the Crema-D corpus . Using this network, we extract speech embeddings for Crema-D from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a fine-tuned BERT model  and then train an LDA - pLDA  model on the resulting dense representations.  pLDA allows our model to more easily adapt to previously unseen classes and domains, a requirement for both evaluating against a different emotion corpus with an incompatible label set and performing well in the wild.  To understand the merits of each component, we exhaustively evaluate the predictive power of every permutation: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof. Our best variant, trained on only VoxCeleb and Crema-D and evaluated on IEMOCAP, achieves an Equal Error Rate  of \%. Including a portion of IEMOCAP during training produces a 5-fold averaged EER of \%.   
"," Automated emotion detection in speech is a challenging task due to the complex interdependence between words and the manner in which they are spoken. It is made more difficult by the available datasets; their small size and incompatible labeling idiosyncrasies make it hard to build generalizable emotion detection systems. To address these two challenges, we present a multi-modal approach that first transfers learning from related tasks in speech and text to produce robust neural embeddings and then uses these embeddings to train a pLDA classifier that is able to adapt to previously unseen emotions and domains. We begin by training a multilayer TDNN on the task of speaker identification with the VoxCeleb corpora and then fine-tune it on the task of emotion identification with the Crema-D corpus.  Using this network, we extract speech embeddings for Crema-D from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a fine-tuned BERT model and then train an LDA - pLDA classifier on the resulting dense representations. We exhaustively evaluate the predictive power of every component: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof.  Our best variant, trained on only VoxCeleb and Crema-D and evaluated on IEMOCAP, achieves an EER of $38.05$\%. Including a portion of IEMOCAP during training produces a 5-fold averaged EER of $25.72$\% .",316
"   Vocoders were originally used for speech compression in the field of communication. Recently, vocoders have been utilized in various fields such as text-to-speech and voice conversion or speech-to-speech translation. Neural vocoders generate human-like voices using neural networks, instead of using traditional methods that contain audible artifacts .  Recently, it has been demonstrated that vocoders exhibit superior performances in generation speed and audio fidelity when trained with single speaker utterances. However, some models face difficulty when generating natural sounds in multiple domains such as speakers, language, or expressive utterances. The ability of these models can be evaluated by the sound quality when the model is trained on data of multiple speakers and the sound quality of the unseen domain . A vocoder that can generate high-fidelity audio in various domains,  regardless of whether the input has been encountered during training or has come from an out-of-domain source, is usually called a universal vocoder.  MelGAN is a vocoder based on generative adversarial networks . It is a lightweight and robust model for unseen speakers but yields lower fidelity than popularly employed models . MelGAN alleviates the metallic sound that occurs mainly in unvoiced and breathy speech segments through multi-scale discriminators that receive different scale waveforms as inputs. However, it has not been implemented efficiently for learning with multiple speakers for a universal vocoder.  In this study, we propose Universal MelGAN. The generated waveform of the original MelGAN with audible artifacts appears as an over-smoothing problem with a non-sharp spectrogram. We added multi-resolution spectrogram discriminators to the model to address this problem in the frequency domain. Our multi-scale discriminators enable fine-grained spectrogram prediction by discriminating waveforms and spectrograms. In particular, they alleviate the over-smoothing problem in the high frequency band of the large footprint model, enabling the generation of realistic multi-speaker waveforms.  To evaluate the performance of the proposed model, we compare with full-band MelGAN  as a baseline and two other vocoders: WaveGlow and WaveRNN. We designed experiments in both Korean and English for language independency. For evaluation, we prepared multiple speaker utterances that included unseen domain scenarios, such as new speakers, emotions, and languages.  The evaluation results indicate that the proposed model achieved the best mean opinion score  in most scenarios and efficiently preserved the fidelity in unseen speakers. In addition, the evaluations show that the model efficiently preserves the original speech, even in challenging domains such as expressive utterances and unseen languages. In multi-speaker text-to-speech scenarios, our model can generate high-fidelity waveforms with high MOS, and the model outperforms compared vocoders. This results without any external domain information suggest the possibility of the proposed model as a universal vocoder.  
"," We propose Universal MelGAN, a vocoder that synthesizes high-fidelity speech in multiple domains. To preserve sound quality when the MelGAN-based structure is trained with a dataset of hundreds of speakers, we added multi-resolution spectrogram discriminators to sharpen the spectral resolution of the generated waveforms. This enables the model to generate realistic waveforms of multi-speakers, by alleviating the over-smoothing problem in the high frequency band of the large footprint model. Our structure generates signals close to ground-truth data without reducing the inference speed, by discriminating the waveform and spectrogram during training. The model achieved the best mean opinion score  in most scenarios using ground-truth mel-spectrogram as an input. Especially, it showed superior performance in unseen domains with regard of speaker, emotion, and language. Moreover, in a multi-speaker text-to-speech scenario using mel-spectrogram generated by a transformer model, it synthesized high-fidelity speech of 4.22 MOS. These results, achieved without external domain information, highlight the potential of the proposed model as a universal vocoder.",317
" %What is spoken term detection  Unsupervised speech modeling is the task of discovering and modeling speech units at various levels from audio recording without using any prior linguistic information. It is an interesting, challenging and impactful research problem as phonetic, lexical and even semantic information could be acquired without the process of transcribing and understanding the given speech data. The relevant technology is particularly important to facilitate data preparation especially in the scenarios where: 1) a large  amount of audio data are readily available online but they are untranscribed; 2) a large amount of audio recording is available for an unpopular language about which no structured linguistic knowledge or documentation can be found.  Spoken term discovery is a representative task of unsupervised speech modeling. It aims to discover repetitively occurred words and/or phrases from untranscribed audio.  The problem is commonly tackled with a two-stage approach. In the first stage, a set of subword units are automatically discovered from untranscribed speech data and these units in turn can be used to represent the speech data as a symbol sequence. In the second stage, variable-length sequence matching and clustering are performed on the subword sequence representations. One major drawback of this is that the subword decoding errors in the first stage would propagate to deteriorate the outcome of spoken term discovery in the second stage. The present study investigates the use of Siamese and Triplet networks in spoken term discovery. Siamese network has been commonly applied to pattern classification or matching problems when only weak labels are available. We propose to train a Siamese/Triplet network with a small dataset of matched and mismatched sequence pairs obtained and use the trained network to generate feature representations for unseen subword sequences. The training dataset is constructed based on hypothesized spoken term clusters from an baseline spoken term discovery system developed in our previous study. With the new feature representations learned by the Siamese/Triplet network, re-clustering of subword sequences is carried out to generate an improved set of discovered spoken terms.   
"," Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery.  Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method.",318
" Evidence-based medicine  is a medical practice that aims to find all the evidence to support medical decisions. This evidence nowadays is obtained from biomedical journals, usually accessible through online databases like PubMed and EMBASE, which provide free access to articles' abstracts and in some cases, to full articles. In the context of the COVID-19 pandemic, EBM is critical to making decisions at the individual level and public health since research articles address topics like treatments, adverse cases, and effects of public policies in medicine. The EBM foundation Epistemonikos has made essential contributions by curating and publishing updated guides of what treatments are working and not against COVID-19~\footnote{http://epistemonikos.org/}. Epistemonikos addresses EBM by a combination of software tools for data collection, storage, filtering , and retrieval, as well as by the vital labor of volunteer physicians who curate and label research articles based on quality , type  and PICO labels . However, this workflow has been challenged during 2020 by increasing growth and rapidly evolving evidence of COVID-19 articles published in the latest months. Moreover, to ensure the rapid collection of the latest evidence published, pre-print repositories such as medRXiv and bioRXiv have been added to the traditional online databases. % In order to support Epistemonikos' effort to filter and curate the flood of articles related to COVID-19, we present the results of an applied AI project where we implement and evaluate a text classification system to filter and categorize research articles related to COVID-19. The current model, based on Random Forests, has an acceptable performance classifying systematic reviews  but fails on classifying other document categories. In this article, we show how using BioBERT yields marginal improvements, while XLNET results in significant progress with the best performance. These results save a considerable amount of time from volunteer physicians by pre-filtering the articles worth of manual curation and labeling for EBM. In average, a physician takes two minutes in reviewing one article, while the system we present in this article can review up to  within one hour.   %With the help of volunteer physicians, they classify emergent literature for the COVID-19 virus in systematic reviews, broad syntheses, or primary studies, which is the first step for finding relevant clinical evidence. Until now, they produced a Random Forest model for classifying documents into different categories. However, in this paper, we show how the use of Transformers-based Language Models  helped this foundation save significant effort to their collaborators.   %
","  The COVID-19 has brought about a significant challenge to the whole of humanity, but with a special burden upon the medical community. Clinicians must keep updated continuously about symptoms, diagnoses, and effectiveness of emergent treatments under a never-ending flood of scientific literature. In this context, the role of evidence-based medicine  for curating the most substantial evidence to support public health and clinical practice turns essential but is being challenged as never before due to the high volume of research articles published and pre-prints posted daily. Artificial Intelligence can have a crucial role in this situation. In this article, we report the results of an applied research project to classify scientific articles to support Epistemonikos, one of the most active foundations worldwide conducting EBM. We test several methods, and the best one, based on the XLNet neural language model, improves the current approach by 93\% on average F1-score, saving valuable time from physicians who volunteer to curate COVID-19 research articles manually.",319
"  The natural language processing community has made tremendous progress  in using pre-trained language models to improve predictive accuracy . Models have now surpassed human performance on language understanding benchmarks such as SuperGLUE . However, studies have shown that these results are partially driven by these models detecting superficial cues that correlate well with labels but which may not be useful for the intended underlying task . This brittleness leads to overestimating model performance on the artificially constructed tasks and poor performance in out-of-distribution or adversarial examples.  A well-studied example of this phenomenon is the natural language inference dataset MNLI . The generation of this dataset led to spurious surface patterns that correlate noticeably with the labels.  highlight that negation words  are often associated with the contradiction label.  show that a model trained solely on the hypothesis, completely ignoring the intended signal, reaches strong performance. We refer to these surface patterns as dataset biases since the conditional distribution of the labels given such biased features is likely to change in examples outside the training data distribution .  A major challenge in representation learning for NLP is to produce models that are robust to these dataset biases. Previous work  has targeted removing dataset biases by explicitly factoring them out of models. These works explicitly construct a biased model, for instance, a hypothesis-only model for NLI experiments, and use it to improve the robustness of the main model. The core idea is to encourage the main model to find a different explanation where the biased model is wrong. During training, products-of-experts ensembling  is used to factor out the biased model.   While these works show promising results, the assumption of knowledge of the underlying dataset bias is quite restrictive. Finding dataset biases in established datasets is a costly and time-consuming process, and may require access to private details about the annotation procedure, while actively reducing surface correlations in the collection process of new datasets is challenging given the number of potential biases .  In this work, we explore methods for learning from biased datasets which do not require such an explicit formulation of the dataset biases. We first show how a model with limited capacity, which we call a weak learner, trained with a standard cross-entropy loss learns to exploit biases in the dataset. We then investigate the biases on which this weak learner relies and show that they match several previously manually identified biases. Based on this observation, we leverage such limited capacity models in a product of experts ensemble to train a more robust model and evaluate our approach in various settings ranging from toy datasets up to large crowd-sourced benchmarks: controlled synthetic bias setup , natural language inference  and extractive question answering .  Our contributions are the following:  we show that weak learners are prone to relying on shallow heuristics and highlight how they rediscover previously human-identified dataset biases;  we demonstrate that we do not need to explicitly know or model dataset biases to train more robust models that generalize better to out-of-distribution examples;  we discuss the design choices for weak learners and show trade-offs between higher out-of-distribution performance at the expense of the in-distribution performance.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," State-of-the-art natural language processing  models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations.  Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.",320
" Topic models  have been popularly used to extract abstract topics which occur commonly across documents in a corpus in the field of Natural Language Processing. Each topic is a group of semantically coherent words that represent a common concept. In addition to gaining insights from unstructured texts, topic models have been used in several tasks of practical importance such as learning text representations for document classification , keyphrase extraction , review understanding for recommendations in e-commerce domain , semantic similarity detection between texts  etc.  % in order to make topic sampling distribution converge to the desired posterior distribution  Early popular works on topic discovery include statistical methods such as Latent Dirichlet Allocation   which approximates each topic as a probability distribution over word vocabulary and performs approximate inference over document-topic and topic-word distributions through Variational Bayes . This was followed by a modified inference algorithm - Collapsed Gibbs sampling  that follows Markov Chain Monte Carlo  . However, these methods require an expensive iterative inference step which has to be performed for each document. This was circumvented through introduction of deep neural networks  and emergence of Variational Autoencoders   in particular, where variational inference can be performed in single forward pass.  % while estimating the posterior distribution. % Laplace approximation of  % The re-parameterisation trick of VAEs allows to perform variational inference in a differentiable manner while training the neural network.  Such neural variational inference based topic models  outperformed the traditional probabilistic sampling methods. Broadly, they model a document as Bag-of-Words  determined on the basis of frequency count of each vocabulary token in the given document. The BoW input is processed through an MLP followed by variational inference  which samples a latent document-topic vector. A decoder network then reconstructs original BoW using latent document-topic vector which allows it to capture relationship between document-topic and topic-word distributions. VAE family of neural topic models can be categorised on the basis of prior enforced on latent document-topic distribution. Methods such as NVDM , NTM-R , NVDM-GSM  use Gaussian prior. NVLDA and ProdLDA  use Dirichlet prior approximation which enables model to capture that a document stems from sparse set of topics.  % and perform better by providing more coherent topics compared to Gaussian prior.  % in order to capture latent document-topic distribution,  % The context vector obtained as a result of attention is used to perform variational inference %  and capture semantics effectively  % which can further help in inferring latent document-topic vector % as carried  in usual VAE based topic models  % using the final LSTM state and the outputs corresponding  While the main focus of previous neural topic models has been to enforce suitable priors, little effort has been spent on explicitly improving the document encoding framework in order to capture document semantics better. In this work, we build upon VAE based topic model using laplace approximation to Dirichlet prior and propose a novel framework where we model the input document as a sequence of tokens. The sequence is processed through an LSTM  that allows it to encode the sequential order which does not remain preserved in BoW. To allow the model to focus on specific parts in the document, we use an attention mechanism  to attend at different document tokens. We hypothesise that topic-word distribution being learned by the model can be factored in the attention mechanism to enable the model to attend on tokens which convey topic related information and cues. We validate our hypothesis and propose TAN-NTM: Topic Attention Networks for Neural Topic Modeling which performs attention efficiently in a topic guided manner. We perform separate attention for each topic using its corresponding word probability distribution and obtain topic-wise context vectors. The context vectors are then composed using topic weights which represent proportion of each topic present in a given document. These topic weights are obtained using the learned token embedding and topic-word distribution. The final composed context vector is then used to perform variational inference followed by BoW decoding. We perform extensive ablations to compare different ways of composing topic-wise context vectors.  % and averages the coherence score over the topics % generated by the model   In order to evaluate our approach, we estimate commonly used NPMI coherence  which measures the extent to which most probable words in a topic are semantically related to each other. Using this metric, we compare our TAN-NTM model with several previous state-of-the-art topic models  outperforming them significantly over 4 benchmark datasets of varying scale and complexity - 20NewsGroup  , Yelp Review Polarity, DBpedia  and AGNews . We demonstrate the efficacy of our model in learning better document feature representations and latent document-topic vectors by achieving higher document classification accuracy over baseline topics models. Furthermore, topic models have previously been used to improve supervised keyphrase generation . We show that our proposed framework can be adapted to modify the topic model and further improve keyphrase generation achieving SOTA performance on StackExchange and Weibo datasets. Our contributions can be summarised as:            % 
"," Topic models have been widely used to learn representations from text and gain insight into document corpora. To perform topic discovery, existing neural models use document bag-of-words  representation as input followed by variational inference and learn topic-word distribution through reconstructing BoW. Such methods have mainly focused on analysing the effect of enforcing suitable priors on document distribution. However, little importance has been given to encoding improved document features for capturing document semantics better. In this work, we propose a novel framework: TAN-NTM which models document as a sequence of tokens instead of BoW at the input layer and processes it through an LSTM whose output is used to perform variational inference followed by BoW decoding. We apply attention on LSTM outputs to empower the model to attend on relevant words which convey topic related cues. We hypothesise that attention can be performed effectively if done in a topic guided manner and establish this empirically through ablations. We factor in topic-word distribution to perform topic aware attention achieving state-of-the-art results with $\sim$ $9$ - $15$ percentage improvement over score of existing SOTA topic models in NPMI coherence metric on four benchmark datasets - 20NewsGroup, Yelp, AGNews, DBpedia. TAN-NTM also obtains better document classification accuracy owing to learning improved document-topic features. We qualitatively discuss that attention mechanism enables unsupervised discovery of keywords. Motivated by this, we further show that our proposed framework achieves state-of-the-art performance on topic aware supervised generation of keyphrases on StackExchange and Weibo datasets.",321
" Popular static word representations such as word2vec~ lie in Euclidean space and are evaluated against symmetric judgments. Such a measure does not expose the geometry of word relations, e.g., asymmetry. For example, ``ellipses are like circles'' is much more natural than ``circles are like ellipses''. An acceptable representation may exhibit such a property.  ~ proposed a similarity measure that encodes asymmetry. %Similarity sim  of two words A, B can be evaluated by  and  can be obtained from such data to obtain an asymmetry ratio~ that resonates with the theory of~ and ~.  Large scale evocation datasets had been created to study the psychological aspects of language. We are interested in three of them; the Edinburgh Association Thesaurus %  ~, Florida Association Norms % ~ and Small World of Words %    %.  Those three datasets have thousands of cue words each and all publicly available. We use them to derive the human asymmetry judgments and see how well embedding-derived asymmetry measure aligns with this data.  Evocation data was rarely explored in the Computational Linguistics community, except that~ derived from the Florida Association Norms an asymmetry ratio for a pair of words to measure the directionality of word relations in topic models, and~ used it for word embedding. In this paper, we conduct a larger scale study using three datasets, on both static embedding ~, GloVe ~, fasttext~) and contextual embedding such as BERT~. We hope the study could help us better understand the geometry of word representations and inspire us to improve text representation learning.  To obtain  for static embedding, we leverage vector space geometry with projection and soft-max similar to ~; For contextual embedding such as BERT we can not use this method because the embedding varies by context. Thus, we use a Bayesian method to estimate word conditional distribution from thousands of contexts using BERT as a language model. In so doing, we can probe the word relatedness in the dynamic embedding space in a principled way.  Comparing an asymmetry measure to the popular cosine measure, we observe that similarity judgment fails to correctly measure BERT's lexical semantic space, while asymmetry judgment shows an intuitive correlation with human data. In the final part of this paper, we briefly discuss the result and what it means to representation learning. This paper makes the following contributions:         
"," Human judgments of word similarity have been a popular method of evaluating the quality of word embedding. But it fails to measure the geometry properties such as asymmetry. For example, it is more natural to say ``Ellipses are like Circles'' than ``Circles are like Ellipses''. Such asymmetry has been observed from a psychoanalysis test called word evocation experiment, where one word is used to recall another. Although useful, such experimental data have been significantly understudied for measuring embedding quality. In this paper, we use three well-known evocation datasets to gain insights into asymmetry encoding of embedding. We study both static embedding as well as contextual embedding, such as BERT. Evaluating asymmetry for BERT is generally hard due to the dynamic nature of embedding. Thus, we probe BERT's conditional probabilities  using a large number of Wikipedia contexts to derive a theoretically justifiable Bayesian asymmetry score. The result shows that contextual embedding shows randomness than static embedding on similarity judgments while performing well on asymmetry judgment, which aligns with its strong performance on ``extrinsic evaluations'' such as text classification. The asymmetry judgment and the Bayesian approach provides a new perspective to evaluate contextual embedding on intrinsic evaluation, and its comparison to similarity evaluation concludes our work with a discussion on the current state and the future of representation learning.",322
" Humor plays an important role in social communications. Unlike many objective classification tasks, the task of humor recognition is constrained by its subjectivity. The perception of the same joke can differ among people due to individual differences in their cognitive processes responsible for humor processing , which is as illustrated in Figure . This makes it challenging for humor recognition models to generalize to a wide range of users, as the training data may reflect the subjectivity of the annotators  or experiment participants . To achieve personalized humor recognition, it is necessary to consider the diversity of user preferences.    Previous research on automated humor recognition casts the task as a binary classification problem . These methods mainly focus on how to design humor-related linguistic features as input to a classifier to obtain high classification performance. With well-established computational humor theories , they can curate many heuristics to extract informative features. The key of heuristic rules is to design effective approaches to capture linguistic patterns  or n-gram statistics  that can distinguish humorous text from plain text. These methods are able to characterize intra-sentence and inter-sentence dependencies that are unique to humor, and thus do not rely a lot on the complexity of classifiers. Nevertheless, the feature generation process requires significant efforts and many have difficulties to cope with newly encountered terms .    Deep learning shifts the focus of AI research from feature engineering to automatic feature extraction. Convolutional Neural Networks  and Transformer-based language models  have been used for end-to-end humor recognition.  Most of previous studies are conducted on curated and explicitly balanced datasets  with the underlying assumption that people more or less agree on the distinction between humorous and non-humorous text. This assumption could limit the model's ability to generalize in practice.  Federated Learning, a technique that trains a deep neural network based on iterative averaging of decentralized local updates, has been proved to be good at handling unbalanced and non-IID data distributions . Inspired by recent progress of federated learning in diversity  and personalization , we propose to improve the ability of humor recognition models to generalize to diverse user preferences with the help of federated learning. We name the model . Specifically, we adopt the Federated Averaging  algorithm  in the fine-tuning of a pretrained Transformer-based language model on our task, and employ a diversification strategy  to handle disparate user preferences.   The main idea of our solution is to force the humor recognition model to learn from a diverse range of user preferences, thereby enhancing the adaptability to new users. For this purpose, there are two important issues to consider. First, as users are increasingly aware of privacy issues and reluctant to provide personal information , it is imperative that we preserve users' privacy and avoid direct harvesting of explicit user preference from their personal devices. To address this, we propose an approximation strategy to generate implicit user feedback  on given humorous text and we diversify the label distributions to represent diverse user preferences. Second, marginal distributions of user preferences  often lead to salient class imbalance issue which requires us to select a more suitable evaluation metric rather than widely adopted accuracy. As such, we use F1 score to evaluate and select best models.   To the best of our knowledge, FedHumor is the first federated learning-based humor recognition model. Extensive results show that our approach is able to increase the generalization bounds of the humor recognition model compared to 9 other state-of-the-art approaches. It is a promising approach to help future AI applications recommend suitable humorous texts to users under tightened data privacy protection regulations , thereby enabling innovative and emerging forms of human-AI interaction.  
"," Understanding humor is critical to creative language modeling with many applications in human-AI interaction. However, due to differences in the cognitive systems of the audience, the perception of humor can be highly subjective. Thus, a given passage can be regarded as funny to different degrees by different readers. This makes training humorous text recognition models that can adapt to diverse humor preferences highly challenging. In this paper, we propose the FedHumor approach to recognize humorous text contents in a personalized manner through federated learning . It is a federated BERT model capable of jointly considering the overall distribution of humor scores with humor labels by individuals for given texts. Extensive experiments demonstrate significant advantages of FedHumor in recognizing humor contents accurately for people with diverse humor preferences compared to 9 state-of-the-art humor recognition approaches.",323
"  .     %      % % final paper: en-us version      %       % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } Rhetorical Structure Theory   is one of the most influential theories of discourse analysis, under which a document is represented by a hierarchical discourse tree. As shown in Figure a, the leaf nodes of an RST tree are text spans named Elementary Discourse Units , and the EDUs are connected by rhetorical relations  to form larger text spans until the entire document is included.  The rhetorical relations are further categorized to Nucleus  and Satellite  based on their relative importance. Thus, document-level discourse parsing consists of three sub-tasks: tree construction, nuclearity determination and relation classification. Moreover, downstream natural language processing tasks can benefit from RST-based structure-aware document analysis, such as summarization  and machine comprehension .  By utilizing various linguistic characteristics , statistical approaches have obtained substantial improvement on the English RST-DT benchmark . Recently, neural networks have been making inroads into discourse analysis frameworks, such as attention-based hierarchical encoding  and integrating neural-based syntactic features into a transition-based parser . Lin et al.  and their follow-up work  successfully explored encoder-decoder neural architectures on sentence-level discourse analysis, with a top-down parsing procedure.  Although discourse parsing has received much research attention and progress, the models are mainly optimized and evaluated in English. The main challenge is the shortage of annotated data, since manual annotation under the RST framework is labor-intensive and requires specialized linguistic knowledge. For instance, the most popular benchmark English RST-DT corpus  only contains 385 samples, which is much smaller than those of other natural language processing tasks. The treebank size of other languages such as German , Dutch  and Basque  are even more limited. Such limitations make it difficult to achieve acceptable performance on these languages required to fully support downstream tasks, and also lead to poor generalization ability of the computational approaches.  Since the treebanks of different languages share the same underlying linguistic theory, data-driven approaches can benefit from joint learning on multilingual RST resources . Therefore, in this paper, we investigate two methods to build a cross-lingual neural discourse parser:  From the embedding perspective: with the cross-lingual contextualized language models, we can train a parser on the shared semantic space from multilingual sources without employing a language indicator;  From the text perspective: since each EDU is a semantically-cohesive unit, we can unify the target language space by EDU-level translation, while preserving the original EDU segmentation and the discourse tree structures . To this end, we adapted and enhanced an end-to-end neural discourse parser, and investigated the two proposed approaches on 6 different languages. While the RST data for training is still in a small scale, we achieved the state-of-the-art performance on all fronts, significantly surpassing previous models, and even approaching the upper bound of human performance. Moreover, we conducted a topic modeling analysis on the collected multilingual treebanks to evaluate the model generality across various domains.     
"," Text discourse parsing plays an important role in understanding information flow and argumentative structure in natural language. Previous research under the Rhetorical Structure Theory  has mostly focused on inducing and evaluating models from the English treebank. However, the parsing tasks for other languages such as German, Dutch, and Portuguese are still challenging due to the shortage of annotated data. In this work, we investigate two approaches to establish a neural, cross-lingual discourse parser via:  utilizing multilingual vector representations; and  adopting segment-level translation of the source content. Experiment results show that both methods are effective even with limited training data, and achieve state-of-the-art performance on cross-lingual, document-level discourse parsing on all sub-tasks. \newline",324
" In recent years, smart devices with built-in personal assistants like Google Assistant and Siri are becoming omnipresent. Behind these intelligent systems, a key question is how to identify the underlying intent of a user utterance, which has triggered a large amount of work on intent detection . Most existing intent detection systems are built on deep learning models trained on large-scale annotated data. However, as user demands and the functions of smart devices continue to grow, collecting supervised data for every new intent becomes time-consuming and labor-intensive.  To address this issue, some studies tackle intent detection in the zero-shot learning  manner, attempting to utilize the learned knowledge of seen classes to help detect unseen classes. The recent methods of zero-shot intent detection  can be roughly divided into two categories: The first category , referred to as the transformation-based methods, utilizes word embeddings of label names to establish a similarity matrix, which is then used to transfer the prediction space of seen intents to unseen intents. Another line of work is based on the compatibility-based methods , which aims to encode the label names and utterances into representations in the same semantic space and then calculate their similarity. In both kinds of methods, a critical problem is learning intent representations. However, most existing ZSID methods are class-inductive, which relies entirely on labeled data from seen intents in the training stage. Consequently, the representations of unseen intents cannot be learned, resulting in two limitations.  First, the ZSID methods are not good at modeling the relationship between seen and unseen intents. For the transformation-based methods, when the label names are given in the form of raw phrases or sentences, word embeddings of label names are inadequate to associate the connections between seen and unseen intents. For example, 閳ユ窂ookRestaurant閳 is similar to 閳ユ珐ateBook閳 when measured by word embeddings, as they share the word 閳ユ窂ook閳 . However, the meaning of these two intents are not that relevant. % As a result, the computed similarity matrix is inadequate in associating the connections between seen and unseen intents .  For the compatibility-based methods, they minimize the similarity between seen intent samples and seen label names in a shared semantic space, and directly transfer it to detect unseen intents. Since the unseen intent representations are not learned, they might be entangled with the representations of seen intents. This can severely hurt the accuracy of the predicted label-utterance similarity, especially when the expressions of utterances are diverse. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Second, the vanilla ZSL methods are not applicable to generalized intent detection . Compared with the ZSL setting , which assumes that the models are only presented with utterances from unseen classes at test time, GZSID requires the model to detect both seen and unseen intents. In GZSID, existing ZSL models usually suffer from the dubbed domain shift  problem, in which utterances from unseen intents are almost always mistakenly classified into seen intents.   Unlike the class-inductive methods, class-transductive ZSL uses semantic information about the unseen classes for model training . In the context of intent detection, the label name provides a proper sketch of the intent meaning. Motivated by this, we propose to utilize label names of the unseen intents to learn disentangled intent representations . Specifically, we include the unseen intents into the prediction space during training, with the label names serving as the pseudo utterances. This allows the model to learn the boundary of each seen and unseen class in the semantic space. Under this framework, we introduce an assistant task that forces the model to find the distinction between seen and unseen intents, thereby alleviating the domain-shift problem. On this basis, we refine the word embedding based similarity matrix by averaging the representations of all corresponding  utterances and  label names. As a result, we can better capture the intent meanings and the similarity matrix reflects more accurate intent connections.   In summary, our contribution is three-fold: %{\topsep}{0pt} % %{-1pt} {0pt} {0pt} %        We believe that the potential of class-transductive ZSL  in intent detection is still not fully exploited, to encourage more related studies in the future, we will release our codes and data.    
"," Zero-shot intent detection  aims to deal with the continuously emerging intents without annotated training data. However, existing ZSID systems suffer from two limitations: 1) They are not good at modeling the relationship between seen and unseen intents, when the label names are given in the form of raw phrases or sentences. 2) They cannot effectively recognize unseen intents under the generalized intent detection  setting. A critical factor behind these limitations is the representations of unseen intents, which cannot be learned in the training stage. To address this problem, we propose a class-transductive framework that utilizes unseen class labels to learn Disentangled Intent Representations . Specifically, we allow the model to predict unseen intents in the training stage, with the corresponding label names serving as input utterances. Under this framework, we introduce a multi-task learning objective, which encourages the model to learn the distinctions among intents, and a similarity scorer, which estimates the connections among intents more accurately based on the learned intent representations. % Moreover, we present a novel approach to calculate the inter-intent similarities, on the basis of the learned intent representations, which estimates the connections among intents more accurately.  Since the purpose of DIR is to provide better intent representations, it can be easily integrated with existing ZSID and GZSID methods. Experiments on two real-world datasets show that the proposed framework brings consistent improvement to the baseline systems, regardless of the model architectures or zero-shot learning strategies.",325
" Multi-turn open-domain dialogue modeling is an active research topic in the field of natural language processing.  However, generating a coherent and informative response for a given dialogue context remains a challenge. % However, it is still challenging for dialogue models to generate a coherent and informative response for a given dialogue context. %Research in this domain mainly addresses the following two questions: 1) How can we learn to represent the context? 2) In the presence of context representation, how can we infer the distribution of the response?  A critical challenge is the learning of rich and robust context representations of dialogue utterances~, namely the challenge of encoding a dialogue context into a vector that adequately captures the semantics . % A major challenge in this domain is to learn rich and robust context representations of dialogue utterances~, namely the challenge of encoding a dialogue context into a vector that adequately captures the semantics .  Large-scale pre-training language models using Transformer-based architectures have recently achieved remarkable successes in a variety of NLP tasks~. % Recently, large-scale pre-training language models using Transformer-based architectures have achieved remarkable successes in a variety of NLP tasks~.  As such, there are increasingly work that aims to use pre-training language models for conversation modeling~. For example, DialoGPT~ extends the GPT-2~ to generate conversation responses on large-scale dialogue corpus. Meena~ trains a sequence-to-sequence model~ with the Evolved Transformer~ on large-scale multi-turn conversations.  Blender, developed by Facebook, provides recipes for building open-domain chatbots that perform well in human evaluations~.  However, existing pre-training conversation models usually view the dialogue context as a linear sequence of tokens and learns to generate the next word through token-level self-attention.  One issue of this approach is that the high-level relationships between utterances are harder to capture using word-level semantics. % One issue of this approach is that the relationships between utterances are scattered into individual words, hindering the capturing of discourse-level coherence.  For example, the discourse-level relationship between the utterances  and   is apparent, but word-level comparisons, such as ,  and , , obscures the high-level relationship. % For example, the utterance  and  in Figure have a strong certain relationship, by contrast, pairs of individual words in these two utterances such as ,  and ,  have obscure correlations. Furthermore, this full pairwise attention is inefficient since it requires each word in the context and the decoder to interact with all other words regardless of their distances and semantic units. % Furthermore, the full pairwise attention is inefficient since it requires each word in the context and the decoder to interact with all other words regardless of their distances and semantic units.  To alleviate the issues above, we present DialogBERT, a novel conversational response generation model.  % To alleviate the aforementioned issues, we present DialogBERT, a novel conversational response generation model.  DialogBERT employs a hierarchical Transformer architecture to represent the dialogue context.  It first encodes dialogue utterances through a Transformer encoder and then encodes the resulting utterance vectors using a discourse-level Transformer to obtain a representation of the entire dialogue context.  To efficiently capture discourse-level coherence among utterances, we propose two training objectives in analogy to the original BERT training: 1) masked context regression, which masks a randomly-selected utterance and predicts the encoding vector for the masked utterance directly; and 2) distributed utterance order ranking, which %reconstructs the order of utterances that belong to the same dialog context   organizes randomly shuffled utterances of a conversation into a coherent dialogue context  through a ~ neural network.  We evaluate DialogBERT on popular multi-turn conversation datasets, namely Weibo, MultiWOZ and DailyDialog.  Results show that DialogBERT outperforms baselines in terms of perplexity, BLEU, and NIST. Human evaluation supports the superiority of our approach in capturing discourse-level semantics and generating more plausible dialogue responses.  %Our contributions can be summarized as follows: % %     %    . %     %      %    for encouraging the model to be more aware of relationships among dialog utterances and, ultimately, discourse coherence. %     %       
"," Recent advances in pre-trained language models have significantly improved neural response generation.  However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention.  Such token-level encoding hinders the exploration of discourse-level coherence among utterances.  This paper presents DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. DialogBERT employs a hierarchical Transformer architecture.  To efficiently capture the discourse-level coherence among utterances, we propose two training objectives, including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training.  Experiments on three multi-turn conversation datasets show that our approach remarkably outperforms the baselines, such as BART and DialoGPT, in terms of quantitative evaluation.  The human evaluation suggests that DialogBERT generates more coherent, informative, and human-like responses than the baselines with significant margins. % Pre-trained language models  have been successfully adapted to neural response generation.  % However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention. % Such token-level encoding hinders the exploration of discourse-level coherence among utterances. % In this paper, we present DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. % %In order to model the utterance-level interactions, % Instead of a flat encoding of linear tokens, DialogBERT employs a hierarchical Transformer architecture.  % %DialogBERT consists of an utterance encoder for encoding utterances and a context encoder for learning to contextualize given utterances' representations. % To efficiently capture the discourse-level coherence among utterances, we propose two new training objectives including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training.  % Experiments on three multi-turn conversation datasets show that  % our approach remarkably outperforms three baselines such as BART and DialoGPT in terms of quantitative evaluation.  % Human evaluation  % suggests  % %\jw{supports}  % that DialogBERT generates more coherent, informative and human-like responses than the baselines with significant margins.",326
" Event Detection ,  the task of which involves identifying the boundaries of event triggers and classifying them into the corresponding event types, aims to seek recognize events of specific types from given texts. As a fundamental task of information extraction, many high-level NLP tasks, such as information retrieval and question answering, need an event detector as one of their essential components.    % 娑旂喕顩﹂崝鐘茬穿閻 Recent studies show that English ED models have achieved great performance by treating the problem as a word-by-word sequence labeling task.  Different from English ED, many East Asian languages, including Chinese, are written without explicit word boundary, resulting a much tricky ED task. An intuitive solution is to apply Chinese Word Segmentation  tools first to get word boundaries, and then use a word-level sequence labeling model similar to the English ED models.  However, word boundary is ambiguous in Chinese thus word-trigger mismatch problem exists in Chinese ED, where an event trigger may not exactly match with a word, but is likely to be part of a word or cross multiple words as Figure demonstrates. Meanwhile, character-level sequence tagging is able to alleviate this problem, but Chinese character embedding can only carry limited information due to the lack of word and word-sequence information, resulting to ambiguous semantics. {UTF8}{gbsn} 	For example in Figure, the character ``閹''  in lexicon word ``閹舵洝绁'' and ``閹舵洘骞'' has entirely different meanings, triggering the event of ``Transaction:TransferMoney'' and ``Conflict:Attack'', respectively.  % Therefore, how to better integrate segmentation-related information and character-level semantics is a key feature in Chinese ED models.  Several recent works have demonstrated that considering the lexicon word information could provide more exact information to discriminate semantics of characters. ~ designed NPN, a CNN-like network to model character compositional structure of trigger words and introduced a gate mechanism to fuse information from characters and words. ~~ proposed TLNN, a trigger-aware Lattice LSTM architecture, exploiting semantics from matched lexicon words to improve Chinese ED.    Although these methods have achieved great success, they continue to have difficulty in fully exploiting the interaction between characters and lexicon words. Specifically, for each character, NPN exploits a gate mechanism to fuse its information with one corresponding word. This means that each character could only be incorporated with one matched word, but actually one character is likely to match with several words, leading to information loss. For TLNN, it constructs cut paths to link the start and end character for each matched word, but semantic information of the matched lexicon word fails to flow into all the characters it covers except the last one, due to the inherently unidirectional sequential nature of Lattice LSTM. %For characters without matched words, no extra information is provided enhance its representation.  Besides, previous ED works usually ignore semantic information maintained by the event types. We observe that event types are usually semantically related to the corresponding event triggers.  {UTF8}{gbsn} 	For example, some common event triggers of type ``Conflict:Attack'', such as ``hit'', ``strike'' and ``invade'', are specific behaviors of ``Attack''.  Such an observation shows that considering the semantic information of event labels may provide fine-grained semantic signals to guide the detection of event triggers, and accordingly benefit ED performance.  In this paper, we propose a novel neural architecture, named Label Enhanced Heterogeneous Graph Attention Networks , for Chinese ED. To promote better information interaction between words and characters, we transform each sentence into a graph.  We first connect lexicon words with all the characters it covers. And then neighboring characters are also linked with each other to provide local context information to enhance character representations, especially for those without matched lexicon word. To capture different granularity of semantic information from words and characters, we formulate words and characters as two types of nodes, thus a heterogeneous graph attention networks is utilized to enable rich information propagation over the graph. Additionally, we design a matcher module to leverage the semantic information of event labels. Specifically, we transform event labels into an event-trigger-prototype based embedding matrix by summarizing the trigger representations belonging to each event label. Based on the generated event label representation, a margin loss is further exploited to enhance the ability to discriminate confusing event labels. Comparing with previous works, our contributions are as follows:  	  
"," Event Detection  aims to recognize instances of specified types of event triggers in text. Different from English ED, Chinese ED suffers from the problem of word-trigger mismatch due to the uncertain word boundaries.  Existing approaches injecting word information into character-level models have achieved promising progress  to alleviate this problem, but they are limited by two issues. First, the interaction between characters and lexicon words is not fully exploited. Second, they ignore the semantic information provided by event labels.  We thus propose a novel architecture named Label enhanced Heterogeneous Graph Attention Networks .  Specifically, we transform each sentence into a graph, where character nodes and word nodes are connected with different types of edges, so that the interaction between words and characters is fully reserved. A heterogeneous graph attention networks is then introduced to propagate relational message and enrich information interaction. Furthermore, we convert each label into a trigger-prototype-based embedding, and design a margin loss to guide the model distinguish confusing event labels. Experiments on two benchmark datasets show that our model achieves significant improvement over a range of competitive baseline methods.",327
" % \rev{@Ileana: this is an example on how to indicate changes in the text, based on the revision.} % \todo[inline]{we need to add color bars on figures, as promised to the reviewers}    Given enough computational power, the scalability of the attention mechanism~ will allow for building ever larger Natural Language Processing  models with billions of parameters . While impressive, these advances also pose a responsibility to the NLP community to interpret the behavior of the hundreds of attention heads in a single model, and potentially to reduce the number of computations. Responding to this challenge, previous work has taken pioneering steps to discover and to explain the sparseness in the attention patters. Here, we argue that as the number of heads grows in the range of thousands, automatic measures would be needed to discover and to impose sparseness to such models.  We introduce a simple task-agnostic data-informed pruning method for attention mechanisms: Attention Pruning. We train Transformer-based models and we analyze  observed attention patterns, averaged over all input sequences in the train set, in order to identify and to remove weak connections between the input tokens. Following , we then retrain these models, enforcing sparseness through masking, and we demonstrate that attention mechanisms incorporate extraneous connections between the input tokens: we obtain comparable  % \question{or even marginally better performance} performance while using sparse attention patterns for NLP tasks such as language and sequence-to-sequence  modelling, as well as %Natural Language  Inference . \rev{prediction on GLUE tasks. Figure summarizes the impact of using our pruning method on standard NLP tasks.}     These global sparseness patterns could help improve both interpretability and inference-time computational efficiency for widely-used attention models. Our contributions are as follows:    % show theoretical computation gains     % The rest of the paper is organized as follows: In Section, we present related work. In Section, we introduce the details behind our attention pruning method. In Section, we apply AP to experiments with language modelling. In Section, we apply AP for seq2seq modelling on machine translation tasks. In Section, we extend our machine translation experiments to demonstrate that AP is compatible with -entmax regularization~, which is another promising sparseness technique. In Section, we study the effect of AP with BERT on the GLUE benchmark. % % Section. In Section we discuss theoretically how our pruned Transformers could yield speedups in terms of MACs.  % In Section, we discuss the hardware efficiency of AP and its promise for speeding up modelling for really long sequences. In Section, we conclude and we point to promising directions for future work.  
"," The attention mechanism is a key component of the neural revolution in Natural Language Processing . As the size of attention-based models has been scaling with the available computational resources, a number of pruning techniques have been developed to detect and to exploit sparseness in such models in order to make them more efficient. The majority of such efforts have focused on looking for attention patterns and then hard-coding them to achieve sparseness, or pruning the weights of the attention mechanisms based on statistical information from the training data. In this paper, we marry these two lines of research by proposing  : a novel pruning framework that collects observations about the attention patterns in a fixed dataset and then induces a global sparseness mask for the model. Through attention pruning, we find that about 90\% of the attention computation can be reduced for language modelling and about 50\% for machine translation and %natural language inference \rev{prediction with BERT on GLUE tasks}, while maintaining the quality of  the results. Additionally, using our method, we discovered important distinctions between self- and cross-attention patterns, which could guide future NLP research in attention-based modelling. Our approach could help develop better models for existing or for new NLP applications, and generally for any model that relies on attention mechanisms. Our implementation and instructions to reproduce the experiments are available at \url{https://github.com/irugina/AP}.",328
" DEEP learning  is a modern machine learning technique based on artificial neural networks. The field of natural language processing  has significantly benefited from the use of deep learning techniques in recent years . There are three prevalent deep learning architectures concerned with  NLP tasks: long-short term memory   %networks , transformer networks  and convolutional neural networks  . LSTMs exhibit relatively slow inference speeds and are less performant than transformers and CNNs with regards to text classification accuracy . Transformers are a recent innovation and have shown significant successes in many NLP tasks . Their massive complexity with trainable parameters in the order of hundreds of millions presents critical  challenges to researchers. State-of-the-art transformers are difficult to reproduce in lab conditions as they have a high training cost in monetary terms. There are only a limited number of pre-trained transformer models available for different languages. \par CNNs have demonstrated excellent success in text classification tasks . There are two paradigms available when using CNNs for text classification tasks, namely: world-level   and character-level CNNs . \par Word-level approaches are dependant on a word-model to represent the text. The reliance on a pre-trained word-model poses the potential problem of not having one available for a particular language. Training new word models is computationally time-consuming and costly. There is also the technical challenges of dealing with misspellings and words that may not exist in the word-model. The other paradigm is char-CNNs. No pre-trained language or word models are required. They also do not require a costly pre-processing step of the text data. In general, char-CNNs are not as accurate as word-level CNNs or transformers. Adding depth has not given the benefit of improved classification accuracy, as seen in image classification tasks. There is an open question in the research literature of what is the optimal architecture for char-CNNs. Little research has been performed to address these limitations. Deep learning is an iterative process requiring the tuning of many hyper-parameters and repeated experiments to test the efficacy of any potential architecture. It is a time consuming, costly and a tedious process that requires expert skills and domain knowledge. The task of finding optimal char-CNNs is an NP-hard problem. \par Evolutionary computation   is a collection of search algorithms inspired by the principals of biological evolution, in particular the concept of . EC methods use a population of individuals  to conduct a simultaneous search during a limited time frame to improve the optimisation of a specified objective function via the exchange of information between individuals in the population. The exchange of information is one of the key motivating factors of selecting EC methods for evolving char-CNNs in this work. There is the potential that this information exchange may reveal the essential characteristics of what makes a non-performant char-CNN into a performant one. EC methods are concerned with locating near-optimal solutions to NP-hard problems. \par Evolutionary deep learning  is the technique of using EC methods to search for candidate CNN architectures combined with the backpropagation algorithm to train any potential candidate network architecture. EDL has demonstrated success when searching for performant CNN architectures on image classification tasks . EDL has not been used to search for performant char-CNN architectures. \par Motivated by the success of applying EDL techniques in the image classification domain, we propose a novel surrogate-based EDL algorithm appropriate for searching the landscape of char-CNN architectures for the text classification domain. The proposed algorithm is based on genetic programming  and an indirect encoding that is capable of representing novel char-CNN  architectures. The algorithm employs the use of surrogate models to significantly reduce the training time of the candidate char-CNNs during the evolutionary process.  In summary, the contributions of the proposed algorithm and work are:     %------------------------------------------------------------------------------ 
"," Character-level convolutional neural networks  require no knowledge of the semantic or syntactic structure of the language they classify. This property simplifies its implementation but reduces its classification accuracy. Increasing the depth of char-CNN architectures does not result in breakthrough accuracy improvements. Research has not established which char-CNN architectures are optimal for text classification tasks. Manually designing and training char-CNNs is an iterative and time-consuming process that requires expert domain knowledge. Evolutionary deep learning  techniques, including surrogate-based versions, have demonstrated success in automatically searching for performant CNN architectures for image analysis tasks. Researchers have not applied EDL techniques to search the architecture space of char-CNNs for text classification tasks. This article demonstrates the first work in evolving char-CNN architectures using a novel EDL algorithm based on genetic programming, an indirect encoding and surrogate models, to search for performant char-CNN architectures automatically. The algorithm is evaluated on eight text classification datasets and benchmarked against five manually designed CNN architectures and one long short-term memory  architecture. Experiment results indicate that the algorithm can evolve architectures that outperform the LSTM in terms of classification accuracy and five of the manually designed CNN architectures in terms of classification accuracy and parameter count.",329
" .     %     % % final paper: en-us version     %     %   % space normally used by the marker     % This work is licensed under a Creative Commons     % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } Pre-trained language models have received great interest in the natural language processing  community in the last recent years . These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence . Then, transfer learning  can be used to leverage the learned knowledge for a down-stream task, such as text-classification .   introduced the  ``Bidirectional Encoder Representations from Transformers'' , a pre-trained language model based on the Transformer architecture . BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context . The fact is, BERT has achieved state of the art results on the ``General Language Understanding Evaluation''  benchmark  by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis , relation extraction  and word sense disambiguation , as well as its adaptability to languages other than English . However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios .  In this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques , i.e. reducing the parameter space, impact model training convergence with fewer data points?  To answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty  for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model. To the best of our knowledge, the work presented in this paper is the first demonstration of combining modern transfer learning using pre-trained Transformer-based language model such as the BERT model with active learning to improve performance in low-resource scenarios. Furthermore, we explore the effect of trainable parameters reduction on model performance and training stability by analyzing the layer-wise change of model parameters to reason about the selection of layers excluded from training. %Furthermore, we explore whether a more sophisticated decoder architecture, i.e. convolutional neural networks  can improve the overall performance or if the added complexity hinders a fast model adaption with such little training data.  The main findings of our work are summarized as follows: a) we found that the model's classification uncertainty on unseen data can be approximated by using Bayesian approximations and therefore, used to efficiently select data for manual labeling in an active learning setting; b) by analyzing layer-wise change of model parameters, we found that the active learning strategy specifically selects data points that train the first and thus more general natural language understanding layers of the BERT model rather than the later and thus more task-specific layers.  
","     Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT - a pre-trained Transformer based language model - by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings.",330
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %       % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. }  Multilingual relation extraction is an important problem in NLP, facilitating a diverse set of downstream tasks from the autopopulation of knowledge graphs  to question answering . While early efforts in relation extraction used supervised methods that rely on a fixed set of predetermined relations, research has since shifted to the identification of arbitrary unseen relations in any language. In this paper, we present a method for extracting high quality relation training examples from date-marked news articles. This technique leverages the predictable distributional structure of such articles to build a corpus that is denoised . We use this corpus to learn general purpose relation representations and evaluate their quality on few-shot and standard relation extraction benchmarks in English and Spanish with little to no task-specific fine-tuning, achieving comparable results to a significantly more data-intensive approach that is the current state-of-the-art.  The current state-of-the-art model, ``Matching the Blanks"" or MTB, is a distant supervision technique that provides large gains on many relation extraction benchmarks and builds on Harris' distributional hypothesis and its extensions. ~ assume that the informational redundancy of very large text corpora  results in sentences that contain the same pair of entities generally expressing the same relation. Thus, an encoder trained to collocate such sentences can be used to identify the relation between entities in any sentence  by finding the labeled relation example whose embedding is closest to . While~ achieve state-of-the-art on FewRel and SemEval 2010 Task 8, their approach relies on a huge amount of data, making it difficult to retrain in English or any other language with standard computational resources: they fine-tune BERT large, which has mil parameters, on mil+ relation pair statements with a batch size of  for mil steps. In contrast our method, with only  relations statements and a language-model one-third the size, achieves comparable performance when fine-tuned on little to no task-specific data.   Our main contribution is a distant supervision approach in which we assume that sections of news corpora exhibit even more informational redundancy than Wikipedia. Specifically, news in the days following an event  frequently re-summarizes the event before adding new details. As a result, news exhibits a strong form of local consistency over short rolling time windows where otherwise fluid relations between entities remain fixed. For example, the relation between Italy and France as expressed in a random piece of text is dynamic and context-dependent, spanning a wide range of possibilities that include ``enemies"", ``neighbors"" and ``allies"".  But, in the news coverage following the 2006 World Cup, it is static -- they are sporting competitors. Therefore, by considering only sentences around specific events, we extract groups of statements that express the same relation and are relatively free of noise .    Training multilingual BERT  on our denoised corpus yields relation representations that adapt well to resource-constrained downstream tasks: we evaluate their quality on FewRel and SemEval 2010 Task 8, producing near state-of-the-art results when finetuned on little to no task-specific data. In addition to the strong performance of our approach in English, it is easily generalizable to other languages, requiring only news corpora and event descriptions from Wikipedia to build a high-quality training corpus. We evaluate this in Spanish and find our method outperforms mBERT on the TAC KBP 2016 relation corpus. We share our code to allow other researchers to apply our approach to news corpora of their own.    
"," General purpose relation extraction has recently seen considerable gains in part due to a massively data-intensive distant supervision technique from~ that produces state-of-the-art results across many benchmarks. In this work, we present a methodology for collecting high quality training data for relation extraction from unlabeled text that achieves a near-recreation of their zero-shot and few-shot results at a fraction of the training cost. Our approach exploits the predictable distributional structure of date-marked news articles to build a denoised corpus -- the extraction process filters out low quality examples. We show that a smaller multilingual encoder trained on this corpus performs comparably to the current state-of-the-art  on few-shot and standard relation benchmarks in English and Spanish despite using many fewer examples .",331
" Domain shift is common in language applications. One is more likely to find ""internet"" or ""PC"" in reviews on electronics than those on books, while he or she is more likely to find ""writing"" or ""B.C."" in reviews on books than those on electronics. This proposes a fundamental challenge to NLP in that many computational models fail to maintain comparable level of performance across domains. Formally, a distribution shift happens when a model is trained on data from one distribution , but the goal is to make good predictions on some other distribution  that shares the label space with the source.   We study unsupervised domain adaptation in this work, where we have fully-labeled data on source domain but no labeled data on target domain. The most prevailing methods in this field aim to learn domain-invariant feature by aligning the source and target domains in the feature space. The pioneering works in this field try to bridge domain gap with discrepancy-based approach.  first introduce MMD to measure domain discrepancy in feature space and use its variant MK-MMD as an objective to minimize domain shift. Another line of work introduces a domain classifier and adversarial training to induce domain invariant feature, followed by works using generative models to enhance adversarial training. However, note that both MMD-based approach and adversarial training formulates with a minimax optimization procedure that is widely known as hard to converge to a satisfactory local optimum. Moreover, some recent works have discovered that both of them don't guarantee good adaptation and will introduce inevitable error on target domain under label distribution shift because they may render incorrect distribution matching. For example, thinking of a binary classification task, the source domain has 50\% of positive samples and 50\% of negative samples while the target domain has 30\% postive and 70\% negative. Successfully aligning these distributions in representation space requires the classifier to predict the same fraction of positive and negative on source and target. If one achieves 100\% accuracy on the source, then target accuracy will be at most 80\%, that is 20\% error at best.   % Self-supervised learning is prominent in feature representation learning. Recent works have approached unsupervised domain adaptation for computer vision with SSL[][]. [] adopted rotation prediction, flip prediction and patch location prediction to induce domain-invarint feature and find that some auxiliary tasks involving fine-grained semantics like pixel reconstruction may force the model to focus on domain-specific feature, further widening the domain gap.   Self-supervised representation learning could be a good workaround for this problem because it enforces predictive behaviour matching instead of distribution matching. The main idea is to learn discriminative representation that is able to genenralize across domains.  use sentiment-indicating pivot prediction as their auxiliary task for cross-domain sentiment analysis. The method proposed in this paper adopts contrastive learning to extract generalizable discriminative feature. Contrastive learning is a subclass of self-supervised learning that is gaining popularity thanks to recent progress. It utilizes positive and negative samples to form contrast against the queried sample on pretext tasks in order to learn meaningful representations. However, the pretext tasks must be carefully chosen. shows with experiments on computer vision tasks that the transfer performance will suffer under improper pretext tasks like pixel reconstruction.  % Recent developments in contrastive learning obtained promising results both on representation learning benchmarks for CV[][][] and for NLP[][][]. % Like with self-supervised learning[], joint learning of pretext tasks in contrastive learning is able to align domain in the feature space, as illustrated in figure.  There are a group of works adopting it for domain adaptation for CV[][][]. However, these method cannot be easily adopted to NLP due to the inherent signal difference between the to domains. .   Therefore, in this paper we explore two classic data augmentation methods in natural language processing閳ユ敃ynonym substitution and back translation to define our pretext task. Experiments on two cross-domain sentiment classification benchmarks show the efficacy of the proposed method. We also examine whether in-domain contrastive learning and entropy minimization helps cross-domain sentiment classification under varied label distribution settings. Our main contributions in this work  are summarized as follows:         
","   Contrastive learning  has been successful as a powerful representation learning method. In this paper, we propose a contrastive learning framework for cross-domain sentiment classification. We aim to induce domain invariant optimal classifiers rather than distribution matching. To this end, we introduce in-domain contrastive learning and entropy minimization. Also, we find through ablation studies that these two techniques behaviour differently in case of large label distribution shift and conclude that the best practice is to choose one of them adaptively according to label distribution shift. The new state-of-the-art results our model achieves on standard benchmarks show the efficacy of the proposed method.",332
"  Data augmentation is a widely-used technique in classification tasks. In the field of computer vision , data is augmented by flipping, cropping, tilting, and altering RGB channels of the original images~; however, similar intuitive and simple strategies do not obtain equal success in NLP tasks. Existing methods tend to produce augmentation with low readability or unsatisfying semantic consistency~.   [tp!] {!}{% {@{}ll@{}} \toprule Original                                                                        & So Cute! The baby is very lovely!                                                    \\ \midrule [c]{@{}l@{}}Naive Aug.\\ Delete + Swap              & So Cute! is The baby very!                                                           \\ \midrule [c]{@{}l@{}}Word2Vec Aug.\\ Insert + Replace & [c]{@{}l@{}}So Cute \underline{adorable}!\\ The baby is very \underline{fabulous}! \\ \midrule [c]{@{}l@{}}Back Translate Aug.\\ Eng.  Fr.  Eng. & \underline{Cute}! The baby is very \underline{cute}!                                                         \\ \midrule Data Boost                                                             & [c]{@{}l@{}}	extbf{Look at this adorable baby!}\\ 	extbf{He is so cute!} \\ % }  sentiment label.}     Table shows some output samples of popular text augmentation methods. Naive methods imitate pixel manipulation in CV, augmenting sentences by adding spelling errors~, or randomly deleting and swapping tokens~. The output of such augmentation methods are often illegible since the word order is disrupted ; even worse, crucial feature words  could be mistakenly removed through random deletion. A more advanced method is synonym insertion or replacement~, which uses Word2Vec~ to replace words with their synonyms. Such a method respects the original sentence structure but fails to consider the context. It sometimes replaces words with synonyms that are awkward in the full context of the sentence. For example, replacing lovely with fabulous to get the sentence  ``The baby is fabulous!"". Recent work leans towards translation-based methods for augmentation~. In particular,  proposed a back-translation method that first translates the text to French and then translates back to English, using the noisy output as the augmentation data. Although back-translation is intuitive and valid, its generation skews towards high frequency words , which not only causes repetition but also leads to lexical shrinkage in the augmented data. In a nutshell, existing techniques  are still far from perfect, partially due to the strong interdependency of syntactic and semantic features in text data.   In recent years, we have witnessed extensive progress in language models . Large-scale LMs such as BERT~, XLNet~, and GPT-2~, are commonly trained on large amounts of text data . One of the most interesting usages of these models is utilizing them as text generators~. In this paper, we explore whether we can leverage the generation ability of the state-of-the-art LMs, to generate augmented samples for a given target class.   Augmentation samples should exhibit features of the target class. Off-the-shelf LMs cannot be directly used to augment data; since they are not trained for specific contexts, their generation is undirected and random. Conditional LMs can generate text directed by certain condition , but they require training a LM from scratch with data covering all the conditions. , for instance, trained a 1.6 billion-parameter LM conditioned to a variety of control codes. The training is rather costly; however, collecting sufficient data for the training is also tedious, especially in low-resource tasks~.  We thus present Data Boost: a  reinforcement learning guided text data augmentation framework built on off-the-shelf LM . Data Boost requires neither collecting extra data nor training a task-specific LM from scratch. We convert GPT-2 into a conditional generator, and for a given task, we guide the generator towards specific class labels during its decoding stage through reinforcement learning. The generated samples can then serve as augmentation data which are similar to the original data in terms of semantics and readability.    The advantages of Data Boost are three-fold: First, Data Boost is powerful. We achieve significant advances in three tasks on five different classifiers compared with six related works. Second, Data Boost generates sentence-level augmentation. Unlike prior methods that do word-level or phrase-level replacement~, our augmented data is of much greater variety in terms of vocabulary and sentence structure. Human evaluations also verify the high readability and label consistency of our augmentation. Third, Data Boost is easy to deploy. It does not require external datasets or training separate systems . Instead, we take the off-the-shelf GPT-2 language model and modify its decoding stage without changing its architecture.    
"," Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7\% on average when given only 10\% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations , we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.",333
" } is ``Red is the color at the end of the visible spectrum of light, next to orange and opposite violet.'' The aim is to identify the word ``color'' as the hypernym of ``red'' from all the nouns in the definition. Intuitively, this task can be solved by general resources such as WordNet dictionary  or Wikipedia. But given a word's different meanings in different contexts, these resources can not sufficiently complete this task. As an example, the term ``LDA'' in Wikipedia denotes ``Linear Discriminant Analysis'' in machine learning, ``Low dose allergens'' in medicine, and ``Landing distance available'' in aviation. The combination of general resources and context identification would also fail in some domain-specific applications where the general resources do not cover the special or technical terms in that area. Moreover, existing technical approaches also demonstrate certain limitations in the task of hypernym extraction from definitions, which we summarize as follows: [1)]   To briefly illustrate the difficulty, let us consider a definition from the Stack-Overflow with an irregular format: ``fetch-api: the fetch API is an improved replacement for XHR''. The term ``fetch-api'' is not included in any common dictionary. While the definition has the ``is an'' pattern, it does not connect to the hypernym. The definition is very short and every distinct word in this definition appears just once, which makes it difficult to accurately learn the word representation. Overall, it is challenging to find a method that would accurately identify ``API'' as the correct hypernym.   The definition of a word represents a certain type of knowledge extracted and collected from disordered data. Indeed, there are tools capable of extracting definitions from the corpora with good accuracy . Nevertheless, tools to extract hypernym from definitions remain limited.  % To cope with this issue, we propose a recurrent network method using syntactic features. Because the definition directly points to a noun, the hyponym is already given. Therefore, the hypernym extraction is to identify the correct hypernym from all words in the definition sentence. This task can be considered as a binary classification, in which the classifier judges if a candidate noun is a hypernym or not. In order to better learn the syntactic feature, we transfer the definition sentence into the part of speech  sequence after labeling the PoS of each word by a standard tool . The syntactic structure surrounding the candidate is learned by a bidirectional gated recurrent units  based model. To further fine tune the results, we use a set of features including the centrality of the word in the hypernym co-occurrence network. We use two corpora to evaluate our method. One is Wikipedia, featuring definitions with canonical syntax structure and intensively used by previous studies. The other is from Stack-Overflow, whose definition is domain-specific and usually with the irregular format. Our method is compared with several existing ones. Overall, it outperforms all others in both corpora, which demonstrates the advantage of combing both the tool of RNN and the PoS information in the task of hypernym extraction.    This paper is organized as follows. We review related works in Section  and introduce details of the method in Section . Experiments and evaluations of the proposed model are presented in Section . After that, we draw a conclusion about this research in Section .   
"," % The abstract should briefly summarize the contents of the paper in % 150--250 words. The hyponym-hypernym relation is an essential element in the semantic network. Identifying the hypernym from a definition is an important task in natural language processing and semantic analysis. While a public dictionary such as WordNet works for common words, its application in domain-specific scenarios is limited. Existing tools for hypernym extraction either rely on specific semantic patterns or focus on the word representation, which all demonstrate certain limitations. Here we propose a method by combining both the syntactic structure in definitions given by the word闁炽儲鐛 part of speech, and the bidirectional gated recurrent unit network as the learning kernel. The output can be further tuned by including other features such as a word闁炽儲鐛 centrality in the hypernym co-occurrence network. The method is tested in the corpus from Wikipedia featuring definition with high regularity, and the corpus from Stack-Overflow whose definition is usually irregular. It shows enhanced performance compared with other tools in both corpora. Taken together, our work not only provides a useful tool for hypernym extraction but also gives an example of utilizing syntactic structures to learn semantic relationships \footnote{Source code and data available at \url{https://github.com/Res-Tan/Hypernym-Extraction}}.",334
"  Although neural machine translation  has achieved great success on sentence-level translation tasks, many studies pointed out that  translation mistakes become more noticeable at the document-level. They proved that these mistakes can be alleviated by feeding the inter-sentential contexts into context-agnostic NMT models.  Previous works have explored various methods to integrate context information into NMT models. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks  or extra context encoders . Different from representation-based approaches, ~~ and ~~ propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is usually updated when new translations are generated. Therefore, long-distance contexts would likely to be erased.  How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence  and using memory and hierarchical structures , are proposed to take global contexts into consideration. However, ~ point out that not all the words in a document are beneficial to context integration, suggesting that it is essential for each word to focus on its own relevant context.    \footnotetext{Dependency and coreference relations are from Stanford CoreNLP .}  To address this problem, we suppose to build a document graph for a document, where each word is connected to those words which have a  direct influence on its translation. Figure  shows an example of a document graph. Explicitly, a document graph %for a document  is defined as a directed graph where:  each node represents a word in the document;  each edge represents one of the following relations between words:  adjacency;  syntactic dependency;  lexical consistency; or  coreference.   We apply a Graph Convolutional Network  on the document graph to obtain a document-level contextual representation for each word,  fed to the conventional Transformer model  by additional attention and gating mechanisms. We evaluate our model on four translation benchmarks, IWSLT English--French  and Chinese--English , Opensubtitle English--Russian , and WMT English--German . Experimental results demonstrate that our approach is consistently superior to previous works  on all the language pairs.   The contributions of this work are summarized as:    % model via attention and gating mechanisms.    
","     Previous works have shown that contextual information can improve the performance of neural machine translation . However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph     that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English--French, Chinese-English, WMT English--German and Opensubtitle English--Russian, demonstrate that using document graphs can significantly improve the translation quality.",335
"  Automatic summarization is a fundamental task in natural language generation and computational linguistics. It is crucial to help the user quickly read and understand daily events, and has been continuously studied for decades. . In this paper, we focus on meeting summarization, which is an extensively studied task in the field of automatic summarization. Given multiple speakers and corresponding utterances in text, the task calls for generating a shorter transcript, covering salient information of the entire meeting. An example is shown in Figure , which includes 3 speakers and their utterances , and , as well as a human-written summary.  Meeting summarization is typically regarded as a kind of abstractive summarization problem in the literature. The majority of existing studies build summarization systems based on the sequence-to-sequence model, which adopts a sequence modeling strategy for encoding utterances . Despite the effectiveness of these approaches, they typically only use sequential text information while ignoring the important influences of dialogue structure. We claim that dialogue-specific structural information is important for meeting summarization. For example, dialogue discourse is an effective structural feature. As shown in Figure , ``Contrast閳, ``Question-Answer閳 and ``Continuation閳 are three dialogue discourse relations, which can provide more precise semantic relationships between each utterance. Specifically, we can see that the existing sequence modeling method is unable to generate correct summary results ), which can be attributed to the system not knowing the  and  are opposed to the 閳ユ獨 proposal. Differently, the dialogue discourse can provide this key information via labeling the 閳ユ窅ontrast閳 relationship, as shown in Figure . Accordingly, how to effectively integrate the discourse relationship into the existing summarization model become a crucial step in meeting summarization.  In this paper, we propose Dialogue Discourse-Aware Graph Convolutional Networks  to address this problem. In detail, we first convert the entire meeting with dialogue discourse labeling into a discourse graph, which represents both utterances and discourse relationships as vertices. Afterwards, we additionally design six types of directed edges and one global vertex in the discourse graph to facilitate information flow. Finally, we employ a graph convolutional network  to encode the graph and pass the semantic representation to the RNN decoder. Besides, we further use the question-answer discourse relationship to construct a pseudo-summarization corpus for pre-training DDA-GCN. In a conversation, a question often sparks a discussion, so naturally, the question can be used as a pseudo-summary for subsequent discussions.  We conduct experiments on the widely used AMI benchmark . Our approach outperforms various baselines. Moreover, we analyze the effectiveness of dialogue discourse and pseudo-summarization corpus. In the end, we give a brief summary of our contributions:  To the best of our knowledge, we are the first to apply dialogue discourse to model the structure of a meeting for meeting summarization;  We design a discourse-aware graph model to encode the entire meeting;  Our model achieves a new SOTA on the AMI dataset.    
"," Sequence-to-sequence methods have achieved promising results for textual abstractive meeting summarization. Different from documents like news and scientific papers, a meeting is naturally full of dialogue-specific structural information. However, previous works model a meeting in a sequential manner, while ignoring the rich structural information. In this paper, we develop a Dialogue Discourse-Aware Graph Convolutional Networks  for meeting summarization by utilizing dialogue discourse, which is a dialogue-specific structure that can provide pre-defined semantic relationships between each utterance. We first transform the entire meeting text with dialogue discourse relations into a discourse graph and then use DDA-GCN to encode the semantic representation of the graph. Finally, we employ a Recurrent Neural Network to generate the summary. In addition, we utilize the question-answer discourse relation to construct a pseudo-summarization corpus, which can be used to pre-train our model. Experimental results on the AMI dataset show that our model outperforms various baselines and can achieve state-of-the-art performance.",336
"  Pre-trained language models such as BERT or RoBERTa learn contextualized word representations on large-scale text corpus through self-supervised learning, and obtain new state-of-the-art results on many downstream NLP tasks . Recently, researchers have observed that pre-trained language models can internalize real-word knowledge into their model parameters. For example, pre-trained language models are able to answer the questions such as ``the sky is }'' or ``Beethoven was born in }'' with moderate accuracy. To further explore their potential, researchers have proposed various approaches to guide the pre-training of the language models by injecting different forms of knowledge into them, such as structured knowledge graph or linguistic knowledge  .     [t] 	{p{5cm}XX} 		{l}{Knowledge Injection Approaches} & Pre-training data \\ 		Model & Generative tasks & Discriminative tasks\\ 		)  \\ 		%	KnowBERT & entity linking     \\ 		SenseBERT~ & supersense prediction & -  \\ 		BERT\_CS~ & - & multi-choice question answering \\ 		LIBERT~ & - & lexical relation prediction \\ 		%	MTB & relation statement comparison   \\ 		LIMIT-BERT~  & semantic/syntactic phrase masking & -  \\ 		KEPLER~ & - & knowledge representation learning  \\ 		SpanBERT~ & span masking & -  \\ 		WKLM~ &  - &  entity replacement checking \\ 		K-Adapter~ & - & relation classification, dependency relation prediction \\ 		T5+SSM~ & salient span masking & - \\ 		TEK~ & span masking on TEK-augmented text & - \\ 		CN-ADAPT~ & MLM training on synthetic knowledge corpus & - \\ 		 tasks and discriminative tasks. Generative tasks are often formulated as predicting the masked tokens given the context. By particularly masking out the words that contain certain types of knowledge  in generative pre-training, the model can be more adept in memorizing and completing such knowledge. While discriminative tasks are often formulated as a classification problem with respect to the sentence or the tokens. By training on the positive and negative examples constructed according to the external knowledge, the discriminator can be more capable of verifying the true or false knowledge in natural language. Existing research has demonstrated that generative and discriminative training have their advantages: the former has a large negative sample space so that the model can learn fine-grained knowledge, while the latter avoids the ``'' tokens in pre-training, and is therefore more consistent with fine-tuning. On the other hand, generative and discriminative capture the different aspects of data distribution and could be complementary to each other in knowledge consolidation. However, to the best of our knowledge, there is not previous work in combining the two approaches in a systematic way. Inspired by the recent success on the generative-discriminative pre-trained model named ELECTRA, we propose to learn the generator and discriminator jointly in the knowledge-guided pre-training, which we call the KgPLM model.  In this paper, we design masked span prediction as the generative knowledge completion task, and span replacement checking as the discriminative knowledge verification task. Hybrid knowledge, including link structure of Wikipedia and structured knowledge graph in Wikidata, is used to guide the both tasks. The spans covering the factual knowledge are more likely to be selected for masking or replacement, and the choices of their replacements are also related to the proximity to the original span in the knowledge space. Figure shows an example of the span masking and replacement tasks. To further explore effective ways to the joint training of the two tasks, we design two learning schemes, which we called two-tower scheme and pipeline scheme. Basically, the generator and discriminator are trained in parallel with the shared parameters in the two-tower scheme. While in the pipeline scheme, the output of generator is input to the successive discriminative training. The generator and discriminator in our KgPLM model are both pre-trained based on RoBERTa. They have some additional benefits: 1) the model can be readily extended to much larger pre-training corpus, which keeps some potential room for further improvement; 2) the model retains the same amount of parameters as RoBERTa, and does not require any modifications in fine-tuning for the downstream tasks.  We evaluate the model performance on LAMA~, which consists of several zero-shot knowledge completion tasks, and MRQA shared tasks~, which include several benchmark question answering datasets. The experiments show the proposed KgPLM, especially that trained with the pipeline scheme, achieves the state-of-the-art performance, and significantly outperform several strong baselines  on some of the tasks. The results indicate that the knowledge-guided generative and discriminative pre-training provides an effective way to incorporate external knowledge and achieve competitive performance on the knowledge intensive NLP tasks.  
"," Recent studies on pre-trained language models have demonstrated their ability to capture factual knowledge and applications in knowledge-aware downstream tasks. In this work, we present a language model pre-training framework guided by factual knowledge completion and verification, and use the generative and discriminative approaches cooperatively to learn the model. Particularly, we investigate two learning schemes, named two-tower scheme and pipeline scheme, in training the generator and discriminator with shared parameter. Experimental results on LAMA, a set of zero-shot cloze-style question answering tasks, show that our model contains richer factual knowledge than the conventional pre-trained language models. Furthermore, when fine-tuned and evaluated on the MRQA shared tasks which consists of several machine reading comprehension datasets, our model achieves the state-of-the-art performance, and gains large improvements on NewsQA  and TriviaQA  over RoBERTa.",337
" 	 	Knowledge graphs , such as WordNet , Freebase  and Wikidata , aggregate a large amount of human knowledge and express in a structured way. 	% are representative of existing KGs, in which knowledge is formalized as triples. 	%, such as  and . 	Unreliable relation paths are common in knowledge graphs, and   found that it is necessary to select reliable relation paths for knowledge representation learning. 	%In this work, a path-constraint resource allocation algorithm is proposed to measure the weights of inference patterns. 	They learn inference patterns between relations and paths to utilize knowledge contained in relation paths. 	%Despite its success, the modeling objects are more limited to the inference patterns between relations and paths. 	%Recently,   propose a method to model the contextual nature of triples and relation paths, and they explore the benefits of graph contextual information for link prediction tasks on two specific datasets. 	%However, simply adding graph contextual information  into the training pool is not always effective, and this operation may reduce the performance of the original model. 	Instead of relying on inference patterns, we propose PPKE, a path-based pre-training approach that integrates  graph contextual information contained in relation paths into the model parameters. 	We think this is a more general way to develop the unexploited graph contextual information. 	During the path-based pre-training procedure,  two-step relation paths are extracted from the knowledge graph and fed into the pre-training module with original triples. 	Then, the pre-trained model can be finetuned for downstream KGC tasks, such as link prediction and relation prediction. 	Our contributions are as follows: 	 		  
"," 		Entities may have complex interactions in a knowledge graph , such as multi-step relationships, which can be viewed as graph contextual information of the entities. 		Traditional knowledge representation learning  methods usually treat a single triple as a training unit, and neglect most of the graph contextual information exists in the topological structure of KGs. 		In this study, we propose a Path-based Pre-training model to learn Knowledge Embeddings, called PPKE, which aims to integrate more graph contextual information between entities into the KRL model. 		Experiments demonstrate that our model achieves state-of-the-art results on several benchmark datasets for link prediction and relation prediction tasks, indicating that our model provides a feasible way to take advantage of graph contextual information in KGs.",338
"  Machine reading comprehension  is a challenging natural language understanding task which lets the machine predict appropriate answer to the question according to a given passage or document .  According to answer styles, MRC tasks can be roughly divided into generative , extractive  and multi-choice  tasks . The multi-choice task is the focus of this work.  Recently, various datasets and tasks have been proposed, promoting a rapid improvement of MRC techniques . Early MRC datasets usually provide passages whose contents are extracted from articles . Recently, conversational reading comprehension has aroused great interests whose passages are derived from multi-turn dialogue segments , making the task be more challenging.    The popular practice to solve MRC problems is adopting pre-trained language models  as encoder module . Instead of better exploiting pre-trained LMs, this paper is motivated by human reading strategies to decouples MRC into sketchy reading by extracting the critical spans from the passage, and extensive reading by seeking external knowledge.  As a result, we propose a knowledge enhancement model based on extracted critical information called ference Knowledgeable Network)}. In detail, the proposed  refines the fine-grained critical information by a span extraction model and defines it as , then quotes relevant external knowledge in the form of quadruples by the co-occurrence information of  and answer options. An example process of  is shown in Figure .   In summary, our main contributions are follows:\\ 1) We propose a novel reference-based knowledge enhancement model , which makes the first attempt to obtain fine-grained evidence for inference and knowledge retrieving on MRC tasks.\\ 2)  uses novel knowledge quadruples to quote relevant and credible knowledge.\\ 3)  is applied to two multi-choice MRC benchmarks, RACE  and DREAM  and improves the performance of baseline models by 1.0\% and 1.1\% respectively, which both pass the significance test of MRC tasks.  
"," Multi-choice Machine Reading Comprehension  is a major and challenging form of MRC tasks that requires model to select the most appropriate answer from a set of candidates given passage and question. Most of the existing researches focus on the modeling of the task datasets without explicitly referring to external fine-grained commonsense sources, which is a well-known challenge in multi-choice tasks. Thus we propose a novel reference-based knowledge enhancement model based on span extraction called ference Knowledgeable Network }, which simulates human reading strategy to refine critical information from the passage and quote external knowledge in necessity. In detail,  refines fine-grained critical information and defines it as , then quotes external knowledge quadruples by the co-occurrence information of  and answer options. Our proposed method is evaluated on two multi-choice MRC benchmarks: RACE and DREAM, which shows remarkable performance improvement with observable statistical significance level over strong baselines.",339
"   Data collection is an essential part of the field of spoken dialogue systems and conversational AI. %, and requires developers to make difficult decisions and budget accordingly.   In particular, designing a dialogue system for a completely new domain is still a very challenging task.  Data collection options include running lab-based experiments, crowd-sourced tasks  or gathering data from social media platforms, such as Reddit or Twitter. Ambitious large scale data collections across multiple domains have resulted in widely used datasets, such as MultiWOZ . % and collected from various platforms .% to create representations of dialogues in the vector space.   However, starting off in a new domain from scratch still has its challenges. Difficult and costly decisions have to be made as to how and where to collect the data.    A large majority of recent dialogue corpora has been collected using crowd-sourcing either by pairing workers and letting them chat, often about a given topic , or by asking them to add the next utterance to the dialogue given a set of conditions . Other studies have recruited subjects to play the role of the system, i.e., to act as a wizard or user . Each of these approaches has its own advantages and disadvantages, depending on if the dialogue is task-oriented or not. By letting users type in an unrestricted way, the richness of the dialogue increases, which is a positive feature for chit-chat. On the other hand, too much variability could be a problem for a high stakes, task-oriented dialogues, such as in the medical domain. Letting multiple users contribute with one utterance per dialogue , speeds up the data collection, however, dialogues may lack coherence and severely diverge from real dialogues. On the other hand, hiring and training subjects to chat or perform the wizard role results in a more controlled data collection but dramatically increases the cost of the data collection and makes it less scalable.     The quality of such datasets has been often assessed according to the degree of variability  observed  or the lexical complexity of the utterances collected . %, however to the best of our knowledge, there is no work assessing the impact of the different methods directly on training dialogue models.   %This paper aims at addressing this issue by investigating the impact of two different data collection methods on the performance of the model. Furthermore, most of the above-mentioned datasets focus on increasing the size of the dataset available for dialogue research, rather than investigating the impact of the data collection strategies on the performance of the models trained. The work presented in this paper aims at highlighting the pros and cons, using a methodology to quickly leverage a robust dialogue system, minimising the cost and effort involved in the data collection process. Analyses comparing different strategies for the data collection process across various platforms have been done in the past , but we are not aware of a similar study for dialogue data.  The data used in this study was collected in the scope of an emergency response system to be used on an off-shore energy platform as part of the EPSRC ORCA Hub programme . One of the collections was done using crowd-sourcing  and the second one was done in a lab using a Wizard-of-Oz setting, where participants were interacting either with a social robot or a smart speaker. Both datasets were used to train a dialogue model using an implementation of a Hybrid Code Network  and here we compare the results achieved by models trained on data collected by either method. To validate the use of crowd-sourced data to bootstrap a dialogue system for situated interaction, we ran experiments where we train the model on the crowd-sourced data and test it on the lab data, in order to verify if it %This will result in an estimate of the number of dialogues needed to  %varied the amount of crowd-sourced dialogues during training to estimate the necessary amount of crowd-sourced data needed to  achieves comparable performances with the models trained only with the lab data.   The contributions of this paper are as follows: 1) a comparison of models trained with two datasets collected in different ways but on the same task, 2) evidence that suggests that specialised dialogue tasks, such as our emergency response task, are not well covered by current pre-trained dialogue models, and 3) a set of recommendations regarding the data collection for dialogue research.\footnote{Please find code and data in: {}.}  The paper is organised as follows. Section  will cover previous work related to this problem. Our experimental set-up will be introduced in Section , followed by the results in Section . The paper concludes with the discussion in Section  and future work and conclusions in Section .       
","  Challenges around collecting and processing quality data have hampered progress in data-driven dialogue models. %, particularly data-hungry neural and hybrid models.   Previous approaches are moving away from costly, resource-intensive lab settings, where collection is slow but where the data is deemed of high quality. The advent of crowd-sourcing platforms, such as Amazon Mechanical Turk, has provided researchers with an alternative cost-effective and rapid way to collect data.   %However, these platforms are sometimes notorious for data anomalies due to the rapid nature of which data is collected.   However, the collection of fluid, natural spoken or textual interaction can be challenging, particularly between two crowd-sourced workers. In this study, we compare the performance of dialogue models for the same interaction task but collected in two different settings: in the lab vs. crowd-sourced. We find that fewer lab dialogues are needed to reach similar accuracy, less than half the amount of lab data as crowd-sourced data.. We discuss the advantages and disadvantages of each data collection method. %, which is of interest to the community in terms of platform choice and how much data will be needed to be collected.",340
" .     %     % % final paper: en-us version     %    % space normally used by the marker  This work is licensed under a Creative Commons  Attribution 4.0 International License. \\  License details:  \url{http://creativecommons.org/licenses/by/4.0/}. } The recent surge in popularity of voice assistants, such as Google Home, Apple閳ユ獨 Siri, or Amazon閳ユ獨 Alexa resulted in interest in scaling these products to more regions and languages. This means that all the components supporting Spoken Language Understanding  in these devices, such as Automatic Speech Recognition , Natural Language Understanding , and Entity Resolution  are facing the challenges of scaling the development and maintenance processes for multiple languages and dialects.  When a voice assistant is launched in a new locale, its underlying speech processing components are often developed specifically for the targeted country, marketplace, and the main language variant of that country. Many people assume that if a device ``understands'' and ``speaks'' in a specific language, for example English, it should be able to work equally well for any English-speaking country, but this is a misunderstanding. For instance, if a speaker of UK English asks a device trained on data collected in the United States ``tell me a famous football player'', it is highly unlikely that this device will provide the user's desired answer, since football means different things in the US and UK cultures. As a result, developers need to take into account not only the language or dialectal differences, but also local culture, to provide the right information in the right language setup. An increase in the number of target marketplaces often means a linear increase in effort needed to develop and maintain such locale-specific models.  NLU models, which classify the user閳ユ獨 intent and extract any significant entities from the user閳ユ獨 utterance, face the same challenge of maintaining high accuracy while being able to accommodate multiple dialects or language content. The major tasks in NLU are intent classification and slot filling. Intent classification is a task to predict what action the user intends the voice assistant to take. Slot filling is a task to identify the specific semantic arguments for the intention. For example, if the user閳ユ獨 request is to ``play Poker Face by Lady Gaga'', the user閳ユ獨 intention will be ``play music'', while in order to fulfill this command with specified details, the system needs to capture the slots for \{song name = Poker Face\}, and \{artist name = Lady Gaga\}. These tasks are called intent classification  and named entity recognition , respectively.  One common approach is to use a max-entropy  classification model for the IC task and a conditional random fields  model for the NER task. Following the advent of deep learning techniques in related fields, such as computer vision and natural language processing, deep learning is becoming more popular in NLU as well. Some of the recent multilingual approaches to NLU include, for example, the Convolutional Neural Network  model for sentence classification , or the Long Short-Term Memory  model for NER prediction . In the deep neural network architecture, the aforementioned NLU tasks can be combined into a single multi-task classification model. An increasing number of experiments also focus on multilingual setups, especially in the field of machine translation, where the task is to translate input from one language to another .  One recent thread of multilingual research centers around learning multilingual word representation. Multilingual word embeddings in the shared cross-lingual vector space have one main property: words from different languages but with similar meaning must be geometrically close. This property allows for transfer learning from one language to another in various multilingual tasks, such as dependency parsing  or classification and NER . A number of model architectures have been proposed to pre-train multilingual word representations, such as leveraging large-scaled LSTM networks trained on monolingual corpora and adversarial setup for space alignment , or transformers trained on multilingual corpora as a single language model .  Although some of these models can be used to solve IC and NER tasks by appending corresponding decoders to generate final predictions, it is not straightforward to use them in production environments due to latency and memory constrains. A different way of benefitting from larger models could be to use them for transfer learning to smaller-size models to improve their performance by initializing some parts of the model with close-to-optimal rather than random weights. In this paper, we extend the multi-task approach studied in  to a general multilingual model for IC and NER tasks, based on deep learning techniques, such as a bidirectional Long Short-Term Memory  CRF sequence labeling model for NER along with a multilayer perceptron  for IC.  We also explore multilingual transfer learning and its benefits to our setup. Transfer learning is widely adapted for zero-shot or few-shot setups, and was explored in some multilingual NLP studies , and also has been used in multi-task IC-NER models ,  yet to the best of our knowledge, there is no study applying transfer learning for data-rich target languages in a multilingual setup. In our experiment, we apply few-shot transfer learning from data-rich languages to a language with a smaller amout of training data. In additon, we also apply  transfer learning to mimic the situation of expanding the model ability to same-level-resource language with known context from another high-resource language, such that the new multilingual model will ``inherit'' context information from its ancestors. We investigate these approaches to transfer learning and their effects on model performance. We show that transfer learning can improve NLU model performance even in data-rich conditions.  
"," 	With the recent explosion in popularity of voice assistant devices, there is a growing interest in making them available to user populations in additional countries and languages. However, to provide the highest accuracy and best performance for specific user populations, most existing voice assistant models are developed individually for each region or language, which requires linear investment of effort. In this paper, we propose a general multilingual model framework for Natural Language Understanding  models, which can help bootstrap new language models faster and reduce the amount of effort required to develop each language separately. We explore how different deep learning architectures affect multilingual NLU model performance. Our experimental results show that these multilingual models can reach same or better performance compared to monolingual models across language-specific test data while require less effort in creating features and model maintenance.",341
"  . 	%  	% % final paper: en-us version  	% 	%	   % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/}. }  The widespread dissemination of fake news has lead to a significant influence on personal fame, public trust, and security. For example, spreading misinformation, such as ``Asians are more vulnerable to novel coronavirus''~\footnote{https://www.thestar.com.my/news/regional/2020/03/11/myth-busters-10-common-rumours-about-covid-19} about COVID-19 has very serious repercussions, making people ignore the harmfulness of the virus and directly affecting public health. Research has shown that misinformation spreads faster, farther, deeper, and more widely than true information. Therefore, fake news detection on social media has attracted tremendous attention recently in both research and industrial fields.   Early research on fake news detection mainly focused on the design of effective features from various sources, including textual content, user profiling data, and news diffusion patterns. Linguistic features, such as writing styles and sensational headlines, lexical and syntactic analysis, have been explored to separate fake news from true news. Apart from linguistic features, some studies also proposed a series of user-based features, and temporal features about the news diffusion. However, these feature-based methods are very time-consuming, biased, and require a lot of labor to design. Besides, these features are easily manipulated by users.   To solve the above problems, many recent studies apply various neural networks to automatically learn high-level representations for fake news detection. For example, recurrent neural network , convolutional neural network , matrix factorization and graph neural network are applied to learn the representation of content and diffusion graph of news. These methods only apply more types of information for fake news detection, but paying little attention to early detection. Moreover, these models can only detect fake news in consideration of all or a fixed proportion of repost information, while in practice they cannot detect fake news in the early stage of news propagation. Some studies explore to detect fake news early by relying on a minimum number of posts. The main limitation of these methods is that they ignore the importance of publishers' and users' credibility for the early detection of fake news.   When we humans see a piece of breaking news, we firstly may use common sense to judge whether there are factual errors in it. At the same time, we will also consider the reputation of the publishers and reposted users. People tend to believe the news from a trusted and authoritative source or the news shared by lots of users with a good reputation. If the publisher is reliable, we tend to believe this news. On the other hand, if the news is reposted by many low-reputation users in a short period, it may be that some spammers tried to heat up on the news, resulting in lower credibility of the news.   Inspired by the above observation, we explicitly take the credibility of publishers and users as supervised information, and model fake news detection as a multi-task classification task. We can annotate a small part of publishers and users by their historical publishing and reposting behaviors. Although the credibility of publishers and users does not always provide correct information, they are necessary complementary supervised information for fake news detection. To make the credibility information generalized to other unannotated users, we construct a heterogeneous graph to build the connections of publishers, news, and users. Through a graph-based encoding algorithm, every node in the graph will be influenced by the credibility of publishers and users.    In this paper, we address the following challenges:  How to fully encode the heterogeneous graph structure and news content; and  How to explicitly utilize the credibility of publishers and users for facilitating early detection of fake news. To tackle the above challenges, we propose a novel structure-aware multi-head attention network for the early detection of fake news. Firstly, we design a structure-aware multi-head attention module to learn the structure of the publishing graph and produce the publisher representations for the credibility prediction of publishers. Then, we apply the structure-aware multi-head attention module to encode the diffusion graph of the news among users and generate user representations for the credibility prediction of users. Finally, we apply a convolutional neural network to map the news text from word embedding to semantic space and utilize the fusion attention module to combine the news, publisher, and user representations for early fake news detection.   The contributions of this paper can be summarized as follows:  	     
"," The\let\thefootnote\relax\footnotetext{* Corresponding author.} dissemination of fake news significantly affects personal reputation and public trust. Recently, fake news detection has attracted tremendous attention, and previous studies mainly focused on finding clues from news content or diffusion path. However, the required features of previous models are often unavailable or insufficient in early detection scenarios, resulting in poor performance. Thus, early fake news detection remains a tough challenge. Intuitively, the news from trusted and authoritative sources or shared by many users with a good reputation is more reliable than other news. Using the credibility of publishers and users as prior weakly supervised information, we can quickly locate fake news in massive news and detect them in the early stages of dissemination.  In this paper, we propose a novel Structure-aware Multi-head Attention Network , which combines the news content, publishing, and reposting relations of publishers and users, to jointly optimize the fake news detection and credibility prediction tasks. In this way, we can explicitly exploit the credibility of publishers and users for early fake news detection. We conducted experiments on three real-world datasets, and the results show that SMAN can detect fake news in 4 hours with an accuracy of over 91\%, which is much faster than the state-of-the-art models. The source code and dataset can be available at https://github.com/chunyuanY/FakeNewsDetection.",342
"  Real-world events such as sports games or elections involve competing teams, each with capabilities and tactics, aiming to win . The performance of such teams is typically not only dependent on the teams' abilities but also on the environment within which they operate. For example, a political party may have the best orators and policies but their opponents may be better at getting votes in key areas. Similarly, a top football team may be playing the worst team in a league but the fact that the latter may be facing relegation  may provide them with extra motivation to win the game. Given this, in many cases, the performance of such teams may not be easily predictable.    % In particular, in sporting events many human factors impact how a team performs in given games. There are often situations that would be very hard to represent in numbers and statistics alone. For example, sporting rivalries often affect human emotions and team performance and teams fighting to avoid relegation from a league often obtain unexpected results.   Traditional AI and machine learning techniques to predict the outcome of real-world events tend to focus on the use of statistical machine learning using historical data about the individual teams .  However, as per the examples above, historical performance may not be useful when team performance may be dependent on dynamic factors such as human performance  or environmental variables . In turn, humans can be better judges than algorithms when faced with previously unseen situations. Journalists, online communities, and experienced analysts may be better at evaluating human and environmental elements to forecast an outcome. For example, one approach of looking at more than just statistics in sports have been through sentiment analysis on social media platforms. Schumaker, Jarmoszko and  Labedz  use this approach to predict English Premier League  results and achieve an accuracy of 50\% and  show use of similar analysis being performed for American Football results in the National Football League  predicting the winner 63.8\% of the time. However, these approaches focus on opinion aggregation rather than trying to extract the potential indicators of performance for individual human teams from human experts.  Against this background, we set new baselines for results when predicting real-world sporting events involving humans based on the combination of Natural Language Processing  and statistical machine learning techniques. In more detail, we focus specifically on football games in the EPL using match previews from the media alongside statistical machine learning  techniques. The prediction of football match outcomes is a challenging computational problem due to the range of parameters that can influence match results. To date, probabilistic methods devised since the seminal work of Maher  have generated fairly limited results and appear to have reached a glass ceiling in terms of accuracy. By using media previews we can improve on the accuracy of current approaches for match outcome prediction. By so doing, we show that by incorporating human factors into our model, rather than just basic performance statistics, we can improve accuracy . Thus, the contributions of this paper are as follows:     In the next section we discuss the match outcome prediction problem for football and the new feature set we explore.  %The rest of this paper is organised as follows. Section  discusses the problem that we are aiming to solve, Section  outlines how we model human opinion and use this to predicting real-world football games. Section  provides the detail of how we test our models and set the baseline for the prediction accuracy. Finally, Section  concludes.  %
"," In this paper, we present a new application-focused benchmark dataset and results from a set of baseline Natural Language Processing and Machine Learning models for prediction of match outcomes for games of football . By doing so we give a baseline for the prediction accuracy that can be achieved exploiting both statistical match data and contextual articles from human sports journalists. Our dataset is focuses on a representative time-period over 6 seasons of the English Premier League, and includes newspaper match previews from The Guardian. The models presented in this paper achieve an accuracy of 63.18\% showing a 6.9\% boost on the traditional statistical methods.",343
"  Deep neural networks are successful at various morphological tasks as exemplified in the yearly SIGMORPHON Shared Task. However these neural networks operate with continuous representations and weights which is in stark contrast with traditional, and hugely successful, rule-based morphology. There have been attempts to add rule-based and discrete elements to these models through various inductive biases.   In this paper we tackle two morphological tasks and the copy task as a control with an interpretable model,  refers to the fact that the patterns are intended to learn abstract representations that may have multiple surface representations, which , while the abstract patterns,  throughout the paper.  An important upside of \footnote{also called encoder-decoder model} model, and add an LSTM decoder.  We initialize the decoder's hidden state with the final scores of each \sopa pattern and we also apply Luong's attention on the intermediate outputs generated by \sopa. We call this model \sopaseq.  We compare each setup to a sequence-to-sequence with a bidirectional LSTM encoder, unidirectional LSTM decoder and Luong's attention.  We show that \sopaseq is often competitive with the LSTM baseline while also interpretable by design. \sopaseq is especially good at \morphana, often surpassing the LSTM baseline, which confirm our linguistic intuition namely that subword patterns are useful for extracting morphological information.  We also compare these models using a generalized form of Jaccard-similarity and we find that some trends coincide with linguistic intuition.  
","  We examine the role of character patterns in three tasks: morphological analysis, lemmatization and copy. We use a modified version of the standard sequence-to-sequence model, where the encoder is a pattern matching network. Each pattern scores all possible N character long subwords  on the source side, and the highest scoring subword's score is used to initialize the decoder as well as the input to the attention mechanism.  This method allows learning which subwords of the input are important for generating the output. By training the models on the same source but different target, we can compare what subwords are important for different tasks and how they relate to each other. We define a similarity metric, a generalized form of the Jaccard similarity, and assign a similarity score to each pair of the three tasks that work on the same source but may differ in target. We examine how these three tasks are related to each other in \goodlangno languages. Our code is publicly available.\footnote{https://github.com/juditacs/deep-morphology}",344
"  Infusing emotions into conversation systems can substantially improve its usability and promote customers' satisfaction. Moreover, perceiving emotions sufficiently is the core premise of expressing emotions. In real-life scenarios, humans can instinctively perceive complex or subtle emotions from multiple aspects, including the emotion flow of dialogue history, facial expressions and personalities of speakers, and then express suitable emotions for feedback. Figure shows the organization of multi-source information in a dialogue graph and the relationship between them.     
"," The success of emotional conversation systems depends on sufficient perception and appropriate expression of emotions. In a real-world conversation, we firstly instinctively perceive emotions from multi-source information, including the emotion flow of dialogue history, facial expressions, and personalities of speakers, and then express suitable emotions according to our personalities, but these multiple types of information are insufficiently exploited in emotional conversation fields. To address this issue, we propose a heterogeneous graph-based model for emotional conversation generation. Specifically, we design a  to represent the conversation content  with a heterogeneous graph neural network, and then predict suitable emotions for feedback. After that, we employ an  to generate a response not only relevant to the conversation context but also with appropriate emotions, by taking the encoded graph representations, the predicted emotions from the encoder and the personality of the current speaker as inputs. Experimental results show that our model can effectively perceive emotions from multi-source knowledge and generate a satisfactory response, which significantly outperforms previous state-of-the-art models.",345
" Text classification is one of the fundamental tasks in natural language processing  with wide applications such as sentiment analysis, news filtering, spam detection and intent recognition. Plenty of algorithms, especially deep learning-based methods, have been applied successfully in text classification, including recurrent neural networks ,  convolutional networks   . More recently, large pre-training language models such as ELMO , BERT , Xlnet  and so on have also shown their outstanding performance in all kinds of NLP tasks, including text classification.   Although numerous deep learning models have shown their success in text classification problems, they all share the same learning paradigm: a deep model for text representation, a simple classifier to predict the label distribution and a cross-entropy loss between the predicted probability distribution and the one-hot label vector. However, this learning paradigm have at least two problems:  In general text classification tasks, one-hot label representation is based on the assumption that all categories are independent with each other. But in real scenarios, labels are often not completely independent and instances may relate to multiple labels, especially for the confused datasets that have similar labels. As a result, simply representing the true label by a one-hot vector fails to take the relations between instances and labels into account, which further limits the learning ability of current deep learning models.  The success of deep learning models heavily relies on large annotated data, noisy data with labeling errors will severely diminish the classification performance, but it is inevitable in human-annotated datasets. Training with one-hot label representation is particularly vulnerable to mislabeled samples as full probability is assigned to a wrong category. In brief, the limitation of current learning paradigm will lead to  confusion in prediction that the model is hard to distinguish some labels, which we refer as label confusion problem . A label smoothing  method is proposed to remedy the inefficiency of one-hot vector labeling , however, it still fails to capture the realistic relation among labels, therefore not enough the solve the problem.      In this work, we propose a novel Label Confusion Model  as an enhancement component to current deep learning text classification models and make the model stronger to cope with label confusion problem. In particular, LCM learns the representations of labels and calculates their semantic similarity with input text representations to estimate their dependency, which is then transferred to a label confusion distribution . After that, the original one-hot label vector is added to the LCD  with a controlling parameter and normalized by a softmax function to generate a simulated label distribution . We use the obtained SLD to replace the one-hot label vector and supervise the training of model training. With the help of LCM, a deep model not only capture s the relations between instances and labels, but also learns the overlaps among different labels, thus, performs better in text classification tasks. We conclude our contributions as follows:    
"," Representing a true label as a one-hot vector is a common practice in training text classification models. However, the one-hot representation may not adequately reflect the relation between the instances and labels, as labels are often not completely independent and instances may relate to multiple labels in practice. The inadequate one-hot representations tend to train the model to be over-confident, which may result in arbitrary prediction and model overfitting, especially for confused datasets  or noisy datasets . While training models with label smoothing  can ease this problem in some degree, it still fails to capture the realistic relation among labels. In this paper, we propose a novel Label Confusion Model  as an enhancement component to current popular text classification models. LCM can learn label confusion to capture semantic overlap among labels by calculating the similarity between instances and labels during training and generate a better label distribution to replace the original one-hot label vector, thus improving the final classification performance. Extensive experiments on five text classification benchmark datasets reveal the effectiveness of LCM for several widely used deep learning classification models. Further experiments also verify that LCM is especially helpful for confused or noisy datasets and superior to the label smoothing method.",346
" Over recent years, various task-oriented conversational agents, such as Amazon Alexa, Apple閳ユ獨 Siri, Google Assistant, and Microsoft閳ユ獨 Cortana, have become more popular in people閳ユ獨 everyday life and are expected to be highly intelligent. For the NLU component, this means that we expect models to perform recognition of the actions and entities within a user閳ユ獨 request with high accuracy. When first training an NLU model on a new language , there is a strong requirement for high quality annotated data that would support the most common user requests across a range of domains. As the modeling space expands to support new features and additional languages, NLU models are regularly re-trained on updated data sets to ensure support for these new functions. The major bottleneck in both of these processes is the labor and cost associated with collecting and annotating new training utterances for every new feature or language.   Recent advances in machine learning methods, including the use of techniques such as transfer learning~ and active learning, can lead to more efficient data usage by NLU models and therefore decrease the need for annotated training data. Additionally, data augmentation models are being widely explored. The advantage of data augmentation is that once synthetic data is generated, it can be ingested into subsequent models without additional effort, allowing for faster experimentation.   NLU models in dialog systems can perform a variety of tasks. In this study, we will focus on three of them: Domain classification  -- identify the domain that the user request belongs to , Intent classification  -- extract actions requested by users , and Named Entity Recognition  -- identify and extract entities  from user requests.  For each utterance we expect our NLU model to output a domain, intent, and set of extracted entities with corresponding tags. For example, if a user requests ``play Bohemian Rhapsody by Queen'', we expect the NLU model to return \{domain: music, intent: play\_song, named\_entities: [, ]\}. We call this output annotation, and the utterance along with annotation is called an annotated utterance. Named entities with corresponding labels are called slots.  For our NLU model to perform well on real-time user requests, we need to train it on a large dataset of diverse annotated utterances. However, there could be some areas of functionality where large datasets for training are not available. To boost model performance in situations where training data is limited, we use synthetic data generated from a small set of unique utterances that cover the basic functionality of the user experience, called Golden utterances. We leverage a Sequence Generative Adversarial Networks  introduced by~ to generate new utterances from this ``seed'' set, and use these generated utterances to augment training data and evaluate the performance of the classification and recognition tasks. We also investigate how the metrics that we use to evaluate the quality of the generated synthetic data links to the performance boost in the underlying tasks.   
"," Data sparsity is one of the key challenges associated with model development in Natural Language Understanding  for conversational agents. The challenge is made more complex by the demand for high quality annotated utterances commonly required for supervised learning, usually resulting in weeks of manual labor and high cost. In this paper, we present our results on boosting NLU model performance through training data augmentation using a sequential generative adversarial network . We explore data generation in the context of two tasks, the bootstrapping of a new language and the handling of low resource features. For both tasks we explore three sequential GAN architectures, one with a token-level reward function, another with our own implementation of a token-level Monte Carlo rollout reward, and a third with sentence-level reward. We evaluate the performance of these feedback models across several sampling methodologies and compare our results to upsampling the original data to the same scale. We further improve the GAN model performance through the transfer learning of the pre-trained embeddings. Our experiments reveal synthetic data generated using the sequential generative adversarial network provides significant performance boosts across multiple metrics and can be a major benefit to the NLU tasks.",347
"  	Encoder-decoder architecture~ has been extensively used in neural machine translation ~. Given a source sentence, an encoder firstly converts it into hidden representations, which are then conditioned by a decoder to generate the target sentence. Attention mechanism~ is very effective in learning the alignment between a source sentence and a target sentence. Hence, attention mechanism is usually used in the architecture to improve its capability, such as capturing long-distance dependencies.  	Similar to traditional machine learning efforts~, some recent approaches in deep learning attempt to improve encoder-decoder architecture with multiple passes of decoding~. NMT refers this to polish mechanism~. Under this scheme, more than one translations are generated for a source sentence and, except for the first translation, each of them is based on the translation from the previous decoding pass. While these methods have achieved promising results, they lack a proper termination policy to the multi-turn process.  adopt a fixed number of decoding passes that can be inflexible in deciding the optimal number of decoding passes.  use reinforcement learning ~ to automatically decide the optimal number of decoding passes. However, RL is unstable due to its high variance of gradient estimation and objective instability~. Since these methods may have premature termination or over translation, their potential can be limited.  	 	 To address this problem, we propose a novel framework, Rewriter-Evaluator, in this paper. It consists of a rewriter and an evaluator. The translation process involves multiple passes. Given a source sentence, at every pass, the rewriter generates a new target sequence aiming at improving the translation from prior passes, and the evaluator measures the translation quality to determine whether to terminate the rewriting process. We also propose a prioritized gradient descent  method that facilitates training the rewriter and the evaluator jointly. The essential idea is using a priority queue to improve sampling efficiency by collecting the translation cases that yield low scores from the evaluator for next-pass rewriting. The size of the queue is a few times larger than the batch size. Although Rewriter-Evaluator involves multiple decoding passes, training time using PGD method is comparable to that of training an encoder-decoder~ that doesn't have multiple decoding passes.  	  	 We apply Rewriter-Evaluator to improve the widely used NMT models,  RNNSearch~ and Transformer~. Extensive experiments have been conducted on two translation tasks, Chinese-English and English-German, to verify the proposed method. The results demonstrate that the proposed framework notably improves the performance of NMT models and significantly outperforms prior methods.  
"," 	 	Encoder-decoder architecture has been widely used in neural machine translation . A few methods have been proposed to improve it with multiple passes of decoding. However, their full potential is limited by a lack of appropriate termination policy. To address this issue, we present a novel framework, Rewriter-Evaluator. It consists of a rewriter and an evaluator. Translating a source sentence involves multiple passes. At every pass, the rewriter produces a new translation to improve the past translation and the evaluator estimates the translation quality to decide whether to terminate the rewriting process. We also propose a prioritized gradient descent  method that facilitates training the rewriter and the evaluator jointly. Though incurring multiple passes of decoding, Rewriter-Evaluator with the proposed PGD method can be trained with similar time to that of training encoder-decoder models. We apply the proposed framework to improve the general NMT models . We conduct extensive experiments on two translation tasks, Chinese-English and English-German, and show that the proposed framework notably improves the performances of NMT models and significantly outperforms previous baselines.",348
"    In this article we are proposing to add coinduction\footnote{Throughout this article we use the term `coinduction' in its most generic meaning, encompassing also coalgebra, corecursion, and bisimulation. This terminology will be explained.} to the computational apparatus of natural language semantics. This, we argue, will provide a basis for a more realistic, computationally sound, and scalable model of natural language understanding. Given that the bottom up, inductively\footnote{We use the terms `induction' and `inductive' in their logical and mathematical sense, e.g. as in 	`definition by induction' or `proof by induction,' and not in the philosophical sense of deriving general knowledge from specific cases, as in `inductive reasoning.'}  constructed, semantic structures are brittle, and seemingly incapable of correctly representing the meanings of longer sentences or realistic dialogues, semantics is in the need of a new foundation. Coinduction, which uses top down constraints, has been successfully used in the design of operating systems and programming languages. Moreover,  implicitly it has been present in text mining, machine translation, and in some attempts to model intensionality and modalities. So, there is scattered evidence it works. Since coinduction and induction can coexist, they can provide a common language and a conceptual model for research in natural language  understanding.   We elaborate on this proposal in several ways. We motivate it by discussing the accuracy and conceptual gaps between inductive and coinductive views of NL semantics. We introduce the coinduction, coalgebras and related concepts focusing on intuitions and referring the reader to other works for in depth treatments. We show the natural match between coinduction and several natural language processing  tasks such as modeling dialogue and text mining.  And we show examples of how induction and coinduction can jointly improve the process of assigning representations to text.    We argue for the joint use of deep learning and deep semantics in natural language understanding. Just as the tensor product allows us to jointly explore and use two different but related algebras or vector spaces, we imagine induction and coinduction as jointly providing a better foundation for NL understanding. Although in the remainder of this article we try to convey some intuitions about their joint use, the mathematical and computational requirements for their optimal joint use are not at this point clear to us.      Our motivation to pursue this topic comes from two sources, which we elaborate below. The first one is the difference in concepts used in deep learning vs. traditional semantics. The second one has to do with the limitations of processing of long sentences using the traditional semantic representation vs. the relatively successful assignment of much shallower structures using deep learning. Our proposal to think coinductively about the latter allows us to incorporate both methodologies within a single conceptual framework.      Intuitively there is a gap between using deep neural networks for natural language processing  and using deep semantic analysis for natural language understanding . If we dig deeper into this gap we might observe that their  conceptual apparatus is different.   Reading the textbooks. This can be perhaps most clearly seen in the new version of a leading NLP textbook. Looking at Chapter 16 ``Logical Representations of Sentence Meaning""\footnote{We are using here the manuscript from  \url{https://web.stanford.edu/~jurafsky/slp3/}, version from  October 16, 2019} we notice it not sharing the vocabulary of the encoder-decoder and embedding models introduced earlier in the book. This is not a criticism of the book: first, this is work on progress; second, a currently missing section might create a bridge. Our point is that a bridge is needed.  To reverse the perspective, logical representations do not appear in deep learning focused NLP books such as  and , and NLP doesn't appear as topic in .  A similar gap can be seen in , where lambda calculus and discourse representation is avoided in the sections mentioning the applications of logistic regression and Naive Bayes to NLP, and vice versa.   Even much earlier the problem of bridging the two views of language, one governed by rules and the other by observations was discussed at length ,  but arguably with little impact on the field. Somewhat similar sentiment has been expressed more recently in , commenting on capabilities of deep learning:  ``really dramatic gains may only have been possible on true signal processing tasks.""    This article takes the position that such bridge should be formed by creating an abstraction of both approaches, and not by an ad hoc combination. The value of this abstraction could lie in informing the theory, i.e. models of meaning, but it could also be in guiding the process of creation of better tools for human-computer interaction and natural language understanding.   The historical analogy we might keep in mind is the creation of modern computer architectures and operating systems , which introduced new layers of abstraction  and new disciplines . \\   There was no research article on Google Scholar, as of early June 2020, mentioning ``mathematics of deep learning"" and ``logical inference,"" although aspects of both are covered in experimental research -- ``deep learning"" + ``logical inference"" produces about 800 hits. Thus,   combining logical and neural model is an active area of research. For example,   presents a data set for question answering using both scene graphs modeling of elements present in images and challenging questions about them. In context of a different problem,   discuss ensuring factual correctness of summaries, using two models, one logical  and one neural . In our third example,  show that neural attention based models such as BERT  can be retrained to master aspects of natural language inference. On the other hand, the examples we present in Section  show that deep neural networks still seem incapable of deeper reasoning without special purpose architectures, and even modeling elementary arithmetical operations is a challenge.        There is a gap in the accuracy between deep neural networks and deep semantic analysis, irrespective of the fact that they try to address different aspects of natural language understanding.  %illustrated in Table  and Figure   [] %\resizebox{\textwidth}{!}{%  "" & ``Structures"" \\ , shows in the left column intuitively `successful' NLP applications; and in the right the areas where in our view we have seen limited progress in the last 30 years. Obviously, metrics used by the applications mentioned in the two columns are different. For example, one can argue that computational pragmatics did not exist 30 years ago, and only recently we have started to see computational, probabilistic models of pragmatics    ,\footnote{\url{https://michael-franke.github.io/probLang/} \url{http://www.mit.edu/~tessler/short-courses/2017-computational-pragmatics/} last retrieved on May 13 2020}, which suggests a big jump.  Nevertheless, the areas on the right do not scale with sentence length. And later, in Sections   and  , we will argue, from a more abstract perspective, that the differences between the columns can be attributed to the differences in their respective computational models.  \FloatBarrier  Let us discuss long sentences. Prior research in this area shows how parsing accuracy decreases with the length of the sentence. For example,   observe fast drop  in precision and recall of dependency parsing with the increase of the dependency length, the distance to the root, and length of sentences. Similar results appear in Fig.4 of .  Actually the situation might be worse than these sources suggests. In an analysis of parsing of sentences up to the length of 156, 	   	   entertain a possibility that 閳 parsing of long sentences would be intractable."" Clearly, deep neural networks improved the accuracy of parsing. However, even with the attention-based models,  reports a $    Adding coinduction to semantics can provide a foundation  for a more realistic, computationally sound and scalable model of natural language understanding. \\   We are proposing adding coinduction to the computational apparatus of semantics. This we argue, will provide a basis for a more realistic, computationally sound and scalable model of natural language understanding. Given that the bottom up, inductively constructed semantic structures are brittle, and seemingly incapable of representing longer sentences or realistic dialogues, semantics is in the need of a new foundation. Coinduction, which uses top down constraints, has been successful in the design of operating systems and programming languages. Moreover, one can argue that implicitly it has been present in text mining, machine translation and in some attempts to model intensionality .  In , which is a good introductory textbook, it is used to describe self reference, paradoxes and modal logics.    So, there is scattered evidence coinduction works. Since coinduction and induction can coexist, they can provide a common language and conceptual model for research in NL understanding.   We should mention that one of the first theoretical proposals to look at agent interaction as coinduction appeared in , and it included explicit mention of NL dialogue and question answering. Within the following twenty years, as argued in the present article, the focus of NLP shifted towards coinductive  methods\footnote{ This shift occurred without ever mentioning the concept itself. There are literally 12 entries mentioning ""deep learning"" and ""coinduction"", mostly accidentally, although     is an exception. .},  namely deep learning, with the theoretical justification coming from the universal approximation properties of neural networks.  This article summarizes some of these developments and argues for an explicit introduction of the term `coinduction' to the vocabulary of NLP. \\   To make the argument, we will focus on the following questions:         
","  This article contains a proposal to add coinduction to the computational apparatus of natural language understanding. This, we argue, will provide a basis for more realistic, computationally sound, and scalable models of natural language dialogue, syntax and semantics. Given that the bottom up, inductively constructed, semantic and syntactic structures are brittle, and seemingly incapable of adequately representing the meaning of longer sentences or realistic dialogues, natural language understanding is in need of a new foundation. Coinduction, which uses top down constraints, has been successfully used in the design of operating systems and programming languages. Moreover,  implicitly it has been present in text mining, machine translation, and in some attempts to model intensionality and modalities, which provides evidence that it works.  This article shows high level formalizations of some of such uses.   Since coinduction and induction can coexist, they can provide a common language and a conceptual model for research in natural language understanding. In particular, such an opportunity seems to be emerging in research on compositionality. This article shows several examples of the joint appearance of induction and coinduction in natural language processing. We argue that the known individual limitations of induction and coinduction can be overcome in empirical settings by a combination of the the two methods. We see an open problem in providing a theory of their joint use.",349
" The problem of predicting citation counts of papers has been a long-standing research problem. Predicting citation counts allows us to better understand the relationship between a paper %and its citation count and gives us insight into what affects a paper's impact. and its impact. However, prior research has viewed this as a static prediction problem, i.e. only predicting a single citation count at a static point in time. %With the natural development of new papers being published,  %However,  %Viewing this as a static problem  This ignores the natural development of the data as new papers are being published. Here, we propose to view the problem as a sequence prediction task, with models then having the ability to capture the evolving nature of citations.  %By extending the problem to a sequence prediction problem,  This, in turn, requires a dataset to contain the papers' citation counts over a period of time, which adds a temporal element to the data, which can then be encoded by sequential machine learning models, such as Long short-term memory models . Additionally, scholarly documents exhibit a natural graph-like structure in their citation networks. Given recent developments in modeling such data and prior research showing that modeling input as graphs can be beneficial, we hypothesize that modeling a paper's citation network is useful for predicting citation counts over time.   In this paper, we consider citation networks, a dynamic graph which evolves over time as new citations and papers are added to the network. Leveraging the structured data in the graph allows us to discover complex relationships between papers. We want to tap into that knowledge and treat the citation data as a network, such that we can further exploit topological information and not just temporal information. By doing so, we investigate the hypothesis of paper citation counts being correlated with features such as authors, venue, and topics.  We use the well-established Semantic Scholar dataset to construct our citation network. Its meta-data allows us to construct a dynamic citation network which covers a  year time-line, with an updated graph for each year. The Semantic Scholar dataset's meta-data also contains information about each paper's authors, venue, and topics, allowing us to study the correlation between these features and the citation count of a paper when considering the evolving nature of the citation network. The correlation between these features and citation counts is well known and studied by prior work. Prior studies show that citations are correlated and there is a strong correlation between features such as authors, but are limited by only predicting a single citation, and not predicting the natural evolution of a papers growth.   We propose to use the constructed dynamic citation network  to predict the trajectory of the number of citations papers will receive over time, a new sequence prediction task introduced in this work. Furthermore, we propose an encoder-decoder model to solve the proposed task, which uses graph convolutional layers to exploit the graphs' topological features and an LSTM to model the temporal component of the graphs. We compare our model against a standard GCN and standard LSTM, which individually incorporate either the topological information or the temporal information, but not both.  Our contributions are as follows: 1) A dynamic citation network based on the Semantic Scholar dataset. The dynamic citation network contains  time-steps, with an updated graph at each time-step, based on yearly information. 2) We introduce the task of sequence citation count prediction. 3) A novel encoder-decoder model based on a GCN and LSTM to extract the dynamic graph's topological and temporal components. 4) A thorough study of the correlation between citation counts and temporal components.   are as follows:        \fi  
"," %Citation count prediction is the task of predicting the number of citations, which a paper has gained after a given period. Prior work view this as a static prediction task, but due to papers and their citations develops over time, predicting the sequence of citations will also capture the papers' development. We further employ the recent development in graph structured data and view the papers as a citation network, linked by papers citations. Viewing the papers as a citation network allows us to exploit the topological information of the citation network. While no prior citation network allow for predicting the development of a paper's citations over time. Therefore, we use the well known Semantic Scholar dataset to construct a dynamic citation network, i.e., a graph which evolves over time. This dynamic citation network spans over $42$. Using the constructed dynamic citation network, we introduce the task of sequence citation count prediction. To solve the introduced task, we propose a model which exploits topological and temporal information. We compare the proposed model against baseline models and analyze the performance. Furthermore, we study the importance of topological and temporal features for predicting a paper's citation count. \andreas{findings TBD} % revised Citation count prediction is the task of predicting the number of citations a paper has gained after a period of time. Prior work viewed this as a static prediction task. As papers and their citations evolve over time, considering the dynamics of the number of citations a paper will receive would seem logical. Here, we introduce the task of sequence citation prediction, where the goal is to accurately predict the trajectory of the number of citations a scholarly work receives over time. We propose to view papers as a structured network of citations, allowing us to use topological information as a learning signal. Additionally, we learn how this dynamic citation network changes over time and the impact of paper meta-data such as authors, venues and abstracts. To approach the introduced task, we derive a dynamic citation network from Semantic Scholar which spans over $42$ years. We present a model which exploits topological and temporal information using graph convolution networks paired with sequence prediction, and compare it against multiple baselines, %where we will use GCN and LSTM as standalone, to  testing the importance of topological and temporal information and analyzing model performance.  Our experiments show that leveraging both the temporal and topological information greatly increases the performance of predicting citation counts over time.",350
"   Recent advances in open domain question answering  have mostly revolved around machine reading comprehension   where the task is to read and comprehend a given text and then answer questions based on it. However, most recent work in MRC has only been in English ), which is trained on Wikipedia articles from 104 languages and equipped with a 120k shared wordpiece vocabulary, has encouraged a lot of progress on cross-lingual tasks  XNLI , NER  and QA  by performing zero-shot training: train on one language and test on unseen target languages.  In this work, we focus on multilingual QA and, in particular, on two recent large-scale datasets: MLQA and TyDiQA\footnote{All uses of TyDiQA in our paper refer to the Gold Passage task.}. Both datasets contain English QA pairs but also examples from 13 other diverse languages.  Some examples are shown in Figure . MLQA evaluates two challenging scenarios: 1) Cross-Lingual Transfer   when the question and the context are in the same language, and 2) Generalized Cross-lingual Transfer  when the question is in one language  and the context is in another language .  %In both cases, MLQA is zero-shot because it does not provide training data in any language. \avi{Maybe remove this previous sentence?} % TyDiQA consists of QA examples in English and 8 other languages.  TyDiQA is designed for XLT only. Both datasets are challenging for multilingual QA due to the large number of languages and the variety of linguistic phenomena they encompass .  Ideally, we want to build QA systems for all existing languages but it is impractical to collect manually labeled training data for all of them.  In the absence of labeled data,  suggested several research directions for pushing the boundaries in multilingual QA, including zero-shot QA, exploring data augmentation with machine translation, as well as effective transfer learning. These are avenues we explore in our work in addition to asking the following research questions:\\             \\      Prior work proposes zero-shot transfer learning from English SQuAD data  to other languages using only a pre-trained LM and competitive results are achieved  on MLQA  and TyDiQA . We venture beyond zero-shot training by first exploring data augmentation  on top of their underlying model. We achieve this by using translation methodologies  to augment the English training data.       We use machine translation to obtain additional silver labeled data allowing us to improve cross-lingual transfer at a low cost.  Our approach introduces several multilingual extensions to the SQuAD training data: translating just the questions but keeping the context in English, translating just the context but keeping the question in English, and translating the question and the context to other languages. This enables us to augment the original English human-labeled training examples with 14 times more multilingual silver-labeled QA pairs.\\          \\         %To do better MLQA, we believe it is important that the model      Our hypothesis is that we can make the cross-lingual QA transfer more effective if we can bring the embeddings in a multilingual pre-trained LM closer to each other in the same semantic space. To answer a question in French it should suffice to train the system on Hindi and not be necessary to train a system on the target language:  hence, French and Hindi should look as if they are the same language.     We propose two approaches to explore cross-lingual transfer:              In our first approach, we propose a novel strategy based on adversarial training  . We investigate how the addition of a language-adversarial task during QA finetuning for a pretrained LM can significantly improve the cross-lingual transfer performance while causing the embeddings in the LM to become less language-dependent.           In our second approach, we develop a novel Language Arbitration Framework  to consolidate the embedding representation across languages using properties of the translation.         We train additional auxiliary tasks  making sure an English question and its translation in Arabic produces the same answer when they see the same input context in Spanish. The intuition behind language arbitration is that while we are training the model on English and translated examples, the proposed multi-lingual objectives bring the language-specific embeddings closer to the English embeddings.\\               Overall, our main contributions in this paper are as follows:         \setlength          more multi-lingual          % ``pseudo'' human-labeled QA pairs than SQuAD.         silver-labeled QA pairs than SQuAD.               
"," Prior work on multilingual question answering has mostly focused on using large multilingual pre-trained language models  to perform zero-shot language-wise learning: train a QA model on English and test on other languages. In this work, we explore strategies that improve cross-lingual transfer by bringing the multilingual embeddings closer in the semantic space.  Our first strategy augments the original English training data with machine translation-generated data. This results in a corpus of multilingual silver-labeled QA pairs that is 14 times larger than the original training set. In addition, we propose two novel strategies, language adversarial training and language arbitration framework, which significantly improve the  cross-lingual transfer performance and result in LM embeddings that are less language-variant. Empirically, we show that the proposed models outperform the previous zero-shot baseline on the recently introduced multilingual MLQA and TyDiQA  datasets.",351
"   %   Researchers' ability to automate natural language processing has grown exponentially over the past few years, particularly with the advent of the Transformer architecture . Despite the fact that recent machine learning methods achieve impressive and almost human-level performance on tasks such as dialogue modeling  and natural language generation , many intelligent voice assistants still rely on rule-based architectures and cached responses in open domain dialogue . This is primarily due to the lack of controls in deep learning architectures for producing specific phrases, tones, or topics, which makes these models inherently unpredictable and therefore too risky for most entities - corporate or otherwise - who wish to deploy public-facing intelligent agents. For example, it is often desirable for a conversational agent to maintain a specific identity  throughout an exchange of dialogue and it is currently impossible to condition deep learning algorithms to maintain a coherent identity across dialogue without training them on highly specialized  data sets. Fine-tuning on these specialized data sets comes with an additional, significant cost: it can lead to catastrophic forgetting of the language model . Despite this aspect of fine-tuning, current state-of-the-art methods  require fine-tuning  of the entire network when their original data set proves unsuitable for a given task , even if the language being modeled is the same across tasks. Furthermore, models produced by current methods are almost entirely uninterpretable and therefore generally difficult to test for egregious failure cases.  %   In this paper, we address both the issue of content control as well as that of catastrophic forgetting induced by fine-tuning. We define `content control' as being able to command a network to either incorporate or eschew an exact word, phrase, topic, style, or sentiment in its output, and therefore attempt a more granular level of control than the purely topic/style-level control that has been published in recent literature . We also introduce an alternative to fine-tuning neural language models and demonstrate through experimentation that the high-cost of overwriting model weights through fine-tuning  often fails to induce the desired behavior in generalized settings.  %is inspired by the ``No Free Lunch"" theorems introduced by Wolpert \& Macready  in that we seek to avoid training a neural network to simultaneously model language and act on explicit commands.  Instead,  we recast the problem of control in natural language generation as one of combining separate models - one of the natural language itself and one of high-level command responses - to produce desired linguistic output. In doing so, we develop a framework for interpreting and subsequently controlling the hidden activations of a pretrained neural network without any adjustments being made to the pretrained model. This framework is biologically consistent with the findings of Knutson et al., who discovered that neural pathways in humans are inhibited by other neuron clusters , and has applications to other neural network architectures and questions outside the domain of controllable text generation.  
"," %   Current solutions to the problem of controlling generative neural language models are usually formulated under a training paradigm in which the language model is trained to simultaneously model natural language and respond to high-level commands. We recast the problem of control in natural language generation as that of learning to interface with a pretrained language model to generate desired output, just as Application Programming Interfaces  control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network  learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model in real time to produce desired outputs, such that no permanent changes are made to the weights of the original language model.     It is notoriously difficult to control the behavior of artificial neural networks such as generative neural language models. We recast the problem of controlling natural language generation as that of learning to interface with a pretrained language model, just as Application Programming Interfaces  control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network  learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model to produce desired outputs. Importantly, no permanent changes are made to the weights of the original model, allowing us to re-purpose pretrained models for new tasks without overwriting any aspect of the language model. We also contribute a new data set construction algorithm and GAN-inspired loss function that allows us to train NPI models to control outputs of autoregressive transformers. In experiments against other state-of-the-art approaches, we demonstrate the efficacy of our methods using OpenAI闁炽儲鐛 GPT-2 model, successfully controlling noun selection, topic aversion, offensive speech filtering, and other aspects of language while largely maintaining the controlled model's fluency under deterministic settings. %Finally, we describe the ethical implications of this work. %   Applications for this approach include re-purposing a pretrained model  for a new task without a specialized data set in the problem domain. We present experimental results from training several NPI models to control the outputs of OpenAI's GPT-2 language model , as well as a novel data curation approach in which hidden activations of an uninterpretable pretrained model are associated with specific outputs. Finally, we describe potential methods whereby NPIs might be leveraged to interpret the inner workings of pretrained networks, as well as the related ethical implications of this work.",352
"  Emotion analysis of user-generated content  available on the web provides insights toward making meaningful decisions. Micro-blog platforms such as Twitter has gained profuse popularity for textual content holding people's opinions. The past decade has seen the active growth in emotion analysis models in many domains. Recently there has been an increasing interest in analysis of emotions of informal short texts such as tweets. In this paper, we introduce and analyze a system to accurately identify the emotions of the individual tweets with the associated intensities~\footnote{Intensity refers to the degree or amount of an emotion}.  % explain why it is important to analyze emotions  Analyzing emotions in social media such as twitter benefits society in a number of ways. Policymakers can use emotional information in social media to accurately identify concerns of people when making decisions. Monitoring social media for health issues benefits not only public health but also government decision makers. Furthermore, organizations can monitor opinion of the public on their products and services to provide better service to the society. Once emotions are recognized, emotion intensity can be used to prioritize the major concerns.  Studies in emotion analysis have often focused on emotion classification. However, emotions may exhibit varying levels of intensities. Here, emotion intensity can be defined as the degree or the intensity of particular emotion felt by the speaker. Additionally, we may observe multiple emotions simultaneously in the same tweet with varying intensities.   One purpose of this study is to develop a model to accurately identify the emotions and associated emotion intensities for a given tweet. In this paper, we propose a transfer learning approach backed by a neural network classifier and a regressor. Although the proposed neural network alone is inadequate to beat the benchmark, we show that features learned when training the above neural networks can be used to improve the overall performance when combined with other features.  Another purpose of this study is to explain how the input word level features affect the features extracted by the neural network.  % [complete the actual findings here] The findings should make an important contribution in understanding how features are used in a neural network and to effectively select features to improve the effectiveness of extracted features.   Our main contributions of this study:      \pagebreak  Major challenge in using deep learning to train emotion intensity prediction models is the lack of large labeled datasets. More recently, emoji and hashtags were used in studies to create large naturally labeled datasets. However, it is not possible to use a similar technique to obtain the intensity of emotions. Furthermore, creating a large dataset manually is time consuming and expensive.  are some existing datasets for emotional intensity prediction. Due to the limited amount of task-specific training data the previous researches have opted for transfer learning approaches~ and traditional machine learning. However, in this paper we argue that even with reasonable size dataset we can train a neural network to obtain good performance provided that there is proper regularization. Additionally, we show that features learned when training the neural network can be combined with other features to improve the overall performance of emotion intensity prediction.   % [explain methodology in brief]  []   \resizebox{0.4\textwidth}{!}{% {lllll}  & Train & Dev & Test & Total \\ anger & 1,701 & 388 & 1,002 & 3,091 \\ fear & 2,252 & 389 & 986 & 3,627 \\ joy & 1,616 & 290 & 1,105 & 3,011 \\ sadness & 1,533 & 397 & 975 & 2,905 \\ \hline % }   In \S, we outline related works on sentiment and emotion mining. Next, in \S we will discuss the datasets used in this study. After, we introduce the background and our methodology in \S and \S accordingly. Then, in \S we will discuss the evaluation results. Finally, we will conclude this paper in \S.  
"," In this paper, we present an experiment on using deep learning and transfer learning techniques for emotion analysis in tweets and suggest a method to interpret our deep learning models. The proposed approach for emotion analysis combines a Long Short Term Memory  network with a Convolutional Neural Network . Then we extend this approach for emotion intensity prediction using transfer learning technique. Furthermore, we propose a technique to visualize the importance of each word in a tweet to get a better understanding of the model. Experimentally, we show in our analysis that the proposed models outperform the state-of-the-art in emotion classification while maintaining competitive results in predicting emotion intensity.",353
"  Online reviewing for businesses becomes more and more important nowadays, where customers can publish their reviews for businesses, and other potential customers or shop owners can view them. Positive feedback from customers may prosper the store businesses, while negative one could have opposite consequences. Yelp, one of the largest company founded in 2004 for publishing crowd-sourced reviews about businesses, provides one open dataset, Yelp Open Dataset , which has tremendously many data about businesses, reviews, and users. Such dataset has been proven to be a good material for personal, educational, and academic purposes.  Among multiple tasks on the Yelp Open Dataset, predicting ratings for restaurants based their reviews is one of fundamental and important tasks. This task can help Yelp classify reviews into proper groups for its recommendation system, detect anomaly reviews to protect businesses from malicious competitions, and assign rating to texts automatically.  Yelp review rating prediction can be done in multiple ways, such as sentiment analysis and 5-star rating classification. In this paper, we will focus on rating prediction for restaurants based only on their review texts. This task can be viewed as a multiclass classification problem, where the input is the textual data , and output is the predicted class . We will apply both machine learning and deep learning models. After analyzing data distribution, splitting datasets, and extracting features, we will use four machine learning methods, including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine  . Then we will focus on four transformer-based models, including BERT , DistilBERT , RoBERTa , and XLNet , where several different architectures will be tried with hyperparameter tuning. This project is done on {Google Colab}, where multi-processors and GPUs are available. The code is publicly available at GitHub .   
","    We predict restaurant ratings from Yelp reviews based on Yelp Open Dataset. Data distribution is presented, and one balanced training dataset is built. Two vectorizers are experimented for feature engineering. Four machine learning models including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine are implemented. Four transformer-based models containing BERT, DistilBERT, RoBERTa, and XLNet are also applied. Accuracy, weighted $ F_1 $ score, and confusion matrix are used for model evaluation. XLNet achieves 70\% accuracy for 5-star classification compared with Logistic Regression with 64\% accuracy.",354
"  Language processing requires tracking information over multiple timescales. To be able to predict the final word ``timescales"" in the previous sentence, one must consider both the short-range context  and the long-range context . How do humans and neural language models encode such multi-scale context information? Neuroscientists have developed methods to study how the human brain encodes information over multiple timescales during sequence processing. By parametrically varying the timescale of intact context, and measuring the resultant changes in the neural response, a series of studies  showed that higher-order regions are more sensitive to long-range context change than lower-order sensory regions. These studies indicate the existence of a ``hierarchy of processing timescales"" in the human brain. More recently,  used a time-resolved method to investigate how the brain builds a shared representation, when two groups of people processed the same narrative segment preceded by different contexts. By directly mapping the time required for individual brain regions to converge on a shared representation in response to shared input, we confirmed that higher-order regions take longer to build a shared representation. Altogether, these and other lines of investigation suggest that sequence processing in the brain is supported by a distributed and hierarchical structure: sensory regions have short processing timescales and are primarily influenced by the current input and its short-range context, while higher-order cortical regions have longer timescales and track longer-range dependencies .  How are processing timescales organized within recurrent neural networks  trained to perform natural language processing? Long short-term memory networks   have been widely investigated in terms of their ability to successfully solve sequential prediction tasks. However, long-range dependencies have usually been studied with respect to a particular linguistic function , and there has been less attention on the broader question of how sensitivity to prior context -- broadly construed --  is functionally organized within these RNNs. Therefore, drawing on prior work in the neuroscience literature, here we demonstrate a model-free approach to mapping processing timescale in RNNs. We focused on existing language models that were trained to predict upcoming tokens at the word level  and at the character level . The timescale organization of these two models both revealed that the higher layers of LSTM language models contained a small subset of units which exhibit long-range sequence dependencies; this subset includes previously reported units  as well as previously unreported units.  After mapping the timescales of individual units, we asked: does the processing timescales of each unit in the network relate to its functional role, as measured by its connectivity? The question is motivated by neuroscience studies which have shown that in the human brain, higher-degree nodes tend to exhibit slower dynamics and longer context dependence than lower-degree nodes . More generally, the primate brain exhibits a core periphery structure in which a relatively small number of ``higher order閳 and high-degree regions  maintain a large number of connections with one another, and exert a powerful influence over large-scale cortical dynamics . Inspired by the relationships between timescales and network structure in the brain, we set out to test corresponding hypotheses in RNNs:  Do units with longer-timescales tend to have higher degree in neural language models? and  Do neural language models also exhibit a ``core network"" composed of functionally influential high-degree units? Using an exploratory network-theoretic approach, we found that units with longer timescales tend to have more projections to other units. Furthermore, we identified a set of medium-to-long timescale ``controller"" units which exhibit distinct and strong projections to control the state of other units, and a set of long-timescale ``integrator units"" which showed influence on predicting words where the long context is relevant. In summary, these findings advance our understanding of the timescale distribution and functional organization of LSTM language models, and provide a method for identifying important units representing long-range contextual information in RNNs.  
"," In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the ``processing timescales闁 of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network  with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: ``controller闁 units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while ``integrator闁 units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models.\footnote{The code and dataset to reproduce the experiment can be found at \url{https://github.com/sherrychien/LSTM_timescales}}",355
"    We summarize our contribution as follows:     
"," Keyphrase Generation  is the task of generating central topics from a given document or literary work, which captures the crucial information necessary to understand the content. Documents such as scientific literature contain rich meta-sentence information, which represents the logical-semantic structure of the documents.  However, previous approaches ignore the constraints of document logical structure, and hence they mistakenly generate keyphrases from unimportant sentences. To address this problem, we propose a new method called Sentence Selective Network  to incorporate the meta-sentence inductive bias into KG. In SenSeNet, we use a straight-through estimator for end-to-end training and incorporate weak supervision in the training of the sentence selection module. Experimental results show that SenSeNet can consistently improve the performance of major KG models based on seq2seq framework, which demonstrate the effectiveness of capturing structural information and distinguishing the significance of sentences in KG task.",356
"   % With the recent development of end-to-end text-to-speech  system, the synthesised speech has achieved high intelligibility and quality in various languages . Recently neural network based text-to-speech  systems have achieved certain success in prosody and naturalness of synthesized speech over conventional methods .  % Because Chinese is non-alphabet and its character set is very large, grapheme-to-phoneme  is essential when hiring end-to-end model in Chinese . By applying encoder-decoder framework with attention , these systems can directly predict speech parameters from graphemes or phonemes by learning acoustic and prosodic patterns via a flexible mapping from linguistic to acoustic space.  % But they still can only model part of prosody structural information from raw text  because of their limited model capacity, resulting poor expressiveness even prosody errors. % V_1021But they still can only model part of prosody structural information from raw text  resulting in poor expressiveness even prosody errors. However, the learnt prosodic patterns only contain part of prosodic structural information , resulting in poor prosody and naturalness performance even improper prosody.  % So additional prosody structure information is important to improve the naturalness of synthesized speech for text-to-speech system. % V5: So adding prosody information, such as prosody structure annotations, in encoder-decoder based models is important to improve the expressiveness of synthesized speech in TTS systems. % The G2P module converts the text input into a sequence of phonemes with tones, after which the intelligibility and naturally of synthesised Chinese speech can perform better than the conventional TTS . % However, the limited coverage of phoneme permutation in training data causes the decline of ability to predict prosody, resulting in unnatural prosody and unexpected pause.   %   % V4: There are many attempts to improve the prosody prediction ability of TTS system by introducing prosody structure information explicitly. % V5: Prosody structure annotations have been successfully applied in TTS systems to improve expressiveness. % V1020: To improve expressiveness of synthesized speech, directly adding prosodic structure annotations, such as Tones and break indices  labels  and The MATE meta-scheme % v_1021:To improve expressiveness of synthesized speech, adding prosodic structure annotations such as tones and break indices  labels  or other prosodic structure annotation  to input sequence of encoder-decoder based models has been proposed. To further improve prosody and naturalness of synthesized speech, adding prosodic structure annotations such as tones and break indices  labels  or other prosodic structure labels  to the input sequence of neural network based TTS models has been proposed. Prosodic structure annotations need to be subjectively labeled from speech, which is time-consuming.  Although these annotations can be automatically annotated by training another prosodic structure prediction model , the accuracy of predicted prosodic structure labels is still limited by using subjectively labeled annotations as the ground-truths. The high correlation between syntactic structure and prosodic information has been proved by successful syntactic-to-prosodic mapping . % V1020: The syntactic parsing models trained with a large text database with rich grammatical structure  provide text in TTS dataset with usefully syntactic structure information. A set of rule-based syntactic features such as part-of-speech  and positions of the current word in parent phrases are proposed and used in hidden Markov model  based acoustic model . % So subjective labeled prosodic structure annotations can be replaced with syntactic structure information, which obtained from text without referring to speech. % In hidden markov model  based acoustic model, a set of rules to create syntactic features including part of speech  and positions of the current word in parent phrases are hired as syntactic structure information to improve prosody and naturalness exceeds prosodic structure annotations in comprehensiveness and granularity . % This provides us with another method to implicitly improve prosody using syntactic structure information, which exceeds using prosody structure information explicitly in comprehensiveness and granularity.  % Early in the hidden markov based TTS model, rich syntactic context instead of prosody structure information is used to improved prosody of synthesized . % The word relation based features  proposed by  are prior features, which require expert knowledge to be designed. % to explore syntactic information from parse tree, to improve the generalization of synthesised speech. To utilize more syntactic structure information, phrase structure based feature  and word relation based feature  are proposed in neural network based TTS . PSF and WRF expand the set of syntactic features used in HMM model. More features such as highest-level phrase beginning with current word  and lowest common ancestor  are further introduced to model syntactic structure .  However, the expanded features are still manually designed features rather than automatically learned high-level representations. PSF only contains features from limited layers of the whole syntactic tree structure. WRF only exposes the information of partial nodes and edges from the whole syntactic parse tree.  % PSF and WRF can only model the syntactic relation among limited subtrees rather than the whole syntactic parse tree structure. % contain feature from syntactic tree structure by design. % needs  and expert knowledge to select  % V1020: which makes it harder to extract useful information and leads to instability. % and the way to select the specific layers from parse tree, which makes it harder to extract useful information and leads to instability. % V1020: And WRF focuses on the relation between two adjacent words in parsing tree structure, which can only model limited information from the whole syntactic parse tree. For example, one of WRF features is highest-level phrase beginning with current word . % WRF only models . %  which expand partial higher structure . % limited by manual selection strategy, WRF only considers the influence of former word on next word and specific layer of parent nodes, so cannot model the whole structure parse tree.  % This makes the prosody performance largely determined by the selected strategy, and at the same time very unstable. %In Fig., we show a example of how synthesised speech from phoneme sequence input  is different from reference speech  because of failing to respect syntax structure.  % Without parsing tree's limit, the third word ""cu4 jin4"" is pronounced separately .  % Besides, without parsing tree information, synthesised speech does not pause between the fifth word ""ti2 xiao4"" and the sixth word ""shi4"", which have a obvious gap in parsing tree reflected in reference speech . % simply plugging these parsing tree information during TTS does not perform well. Limited by the manual design rule, these features have some disadvantages to model syntax tree structure information. Firstly, using phrase structure feature needs to fix the number of tree layers and the way to select specific layer, while using word relation feature has to make the model select only part of parse tree structure, which cannot be proved to be the most useful part for prosody modeling. This makes the prosody performance largely determined by the selected strategy, and at the same time very unstable. Secondly, word relation feature only consider the former words' influence on next word and ignore the impact of the backward structure importance. Last but not least, manual design features require very high accuracy of syntax tree annotation, which can not be easily achieved. Otherwise, Otherwise under the influence of manual selection strategy, the destructive influence of mislabeling on prosody prediction will be magnified.  % A syntactic parse tree traversal based method is proposed to learn syntactic representation and employed in neural machine translation . To maker better use of the syntactic information, motivated by the syntactic parse tree traversal approach in neural machine translation , we propose a syntactic representation learning method to further improve the prosody and naturalness of synthesized speech in neural network based TTS.  % To make a better use of the syntactic information, in this paper, we propose a syntactic representation learning method to further improve the prosody in neural network based TTS. % which also known as phrase structure parsing, for TTS system to control prosody more effective.  Syntactic parse tree is linearized into two constituent label sequences through left-first and right-first traversal. % Word level bidirectional  Then syntactic representations are extracted from the constituent label sequences using different uni-directional GRU network for each sequence. After which, the syntactic representations are up-sampled from word level to phoneme level and concatenated with phoneme embeddings.  Tacotron 2 is employed to generate spectrogram from the concatenated syntactic representations and phoneme embeddings, with Griffin-Lim  to reconstruct the waveform. % directly Nuclear-norm maximization loss  is introduced to the constituent label embedding layer to enhance discriminability and diversity.  Compared to only hiring left-first traversal , right-first traversal is proposed to alleviate the ambiguity.  Experimental results show that our proposed model outperforms the baseline in terms of prosody and naturalness. Mean opinion score  increases from  to  compared with the baseline approach . % compared to baseline approach, with  is  from a one-way ANOVA test. ABX preference rate exceeds the baseline approach by . % One-way ANOVA test reveals a significant improvement . % We go further to explore how the enhanced controllability of prosody can benefit eliminate ambiguity. For sentences with multiple different syntactic parse trees, prosodic differences can be clearly perceived from corresponding synthesized speeches.  %We linearize a phrase parse tree into a structural label sequence and propose a rnn-based model to learn useful syntactic information by itself, and experimental shows significantly better than the method of manually extracting features. %To our best known, we first exploite syntactic information to chinese TTS system and first to apply syntactic information to lower input level than word. %We have also introduce rank loss of syntactic label embedding to enhance the ability of the syntax structure to control prosody, which expanded the specific application of parsing tree information, including different sentences in the same parsing tree structure to bring the same prosodic structure, and different trees in the same sentence to produce different prosodic readings. The latter brings solutions to the ambiguity caused by grammatical structure  
"," Syntactic structure of a sentence text is correlated with the prosodic structure of the speech that is crucial for improving the prosody and naturalness of a text-to-speech  system.  Nowadays TTS systems usually try to incorporate syntactic structure information with manually designed features based on expert knowledge.  In this paper, we propose a syntactic representation learning method based on syntactic parse tree traversal to automatically utilize the syntactic structure information.  Two constituent label sequences are linearized through left-first and right-first traversals from constituent parse tree. Syntactic representations are then extracted at word level from each constituent label sequence by a corresponding uni-directional gated recurrent unit  network.  Meanwhile, nuclear-norm maximization loss is introduced to enhance the discriminability and diversity of the embeddings of constituent labels.  Upsampled syntactic representations and phoneme embeddings are concatenated to serve as the encoder input of Tacotron2.  Experimental results demonstrate the effectiveness of our proposed approach, with mean opinion score  increasing from $3.70$ to $3.82$ and ABX preference exceeding by $17\%$ compared with the baseline. In addition, for sentences with multiple syntactic parse trees, prosodic differences can be clearly perceived from the synthesized speeches.",357
" Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Many semantic parsing methods are based on the principle of semantic compositionality ~, of which the main idea is to put together the meanings of utterances by combining the meanings of the parts~. However, these methods suffer from heavy dependence on handcrafted grammars, lexicons, and features.  To overcome this problem, many neural semantic parsers have been proposed and achieved promising results~. %\textcolor{red}{However, compared to compositional semantic parsers, neural semantic parsers are not aware of the compositional structure of utterances, which often limits their generalization between various compound-complex utterances: However, due to the lack of capturing compositional structures in utterances, neural semantic parsers usually have poor generalization ability to handle unseen compositions of semantics~. For example, a parser trained on ``'' and ``'' may not perform well on ``''.    & \textcolor{red}{How many} \textcolor{green}{rivers run through} \textcolor{blue}{the states bordering colorado}\\ M & \\ {Q1}} & \textcolor{blue}{the states bordering colorado}\\ M1 & \\ {Q2}} & \textcolor{green}{rivers run through} \textcolor{blue}{state\}\\ M3 & \\  \\ {*}{Geo} & \\ &  \\ {*}{\tabincell{c}{Complex\\WebQuestions}} & \\ & \\ & \quad\quad\\ & \quad\quad\\ & \quad\quad}\\ {*}{\tabincell{c}{Formulas}} & \\ & \\ { % then, for each train sample , we use this preliminary base parser to check whether spans in  can be parsed to \textcolor{red}{be} a part of  \textcolor{red}{or not}. If true, we leverage these spans as pseudo supervision signals for training the utterance segmentation model, and thereby do not require any handcraft templates or additional labeled data.} %The key to implement this framework is to address the challenge of lacking labeled data for utterance segmentation. %We achieve this through cooperative training of the segmentation model and the base parser: %leverage pre-trained base parser to derive synthetic supervision signals for training the segmentation model, then leverage the segmentation model to derive synthetic supervision signals for updating the base parser.  % \textcolor{green}{Moreover, considering that there are usually no labeled data for utterance segmentation, we propose to search for reasonable segmentation points from utterances via the base parser, and use them as a distant supervision. This improves the domain adaptability of our framework.}  % While lacking the direct supervision for segmentation model, we seek to address this challenge in a distantly supervised way. % shaped like  %\textcolor{red}{ %Firstly, we train the base parser, and use it to search for and evaluate all viable ways to segment training utterances. %Then, these segmentations are leveraged as distant supervision for training the utterance segmentation model and fine-tuning the base neural semantic parser.}  In summary, our proposed framework has four advantages:  the base parser learns to parse simpler spans instead of whole complex utterances, thus alleviating the training difficulties and improving the compositional generalization ability;  our framework is flexible to incorporate various popular encoder-decoder models as the base parser;  our framework does not require any handcraft templates or additional labeled data for utterance segmentation; % our framework addresses the challenge of lacking labeled data for utterance segmentation through cooperative training.  our framework improves the interpretability of neural semantic parsing by providing explicit alignment between spans and partial meaning representations.  We conduct experiments on three datasets: Geo~, ComplexWebQuestions~, and Formulas . They use different forms of meaning representations: FunQL, SPARQL, and Spreadsheet Formula. Experimental results show that our framework consistently improves the performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gain: Geo , Formulas , ComplexWebQuestions .  [htbp]      from it and parses the span to {a partial meaning representation}}. We obtain span meanings via neural semantic parsing, then piece them to assemble whole-utterance meaning.}   
"," Neural semantic parsers usually fail to parse long and complex utterances into correct meaning representations, due to the lack of exploiting the principle of compositionality. To address this issue, we present a novel framework for boosting neural semantic parsers via iterative utterance segmentation. Given an input utterance, our framework iterates between two neural modules: a segmenter for segmenting a span from the utterance, and a parser for mapping the span into a partial meaning representation. Then, these intermediate parsing results are composed into the final meaning representation. One key advantage is that this framework does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the parser provides pseudo supervision for the segmenter. Experiments on Geo, ComplexWebQuestions and Formulas show that our framework can consistently improve performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gains: Geo $63.1\to 81.2$, Formulas $59.7\to 72.7$, ComplexWebQuestions $27.1\to 56.3$.",358
"     Word alignment is a task of finding the corresponding words in a sentence pair  and used to be a key component of statistical machine translation . Although word alignment is no longer explicitly modeled in neural machine translation , it is often leveraged to interpret and analyze NMT models . Word alignment is also used in many other scenarios, such as imposing lexical constraints on the decoding process , improving automatic post-editing  and providing guidance for translators in computer-aided translation .  Recently, unsupervised neural alignment methods have been studied and outperformed GIZA++  on many alignment datasets . However, these methods are trained with a translation objective, which computes the probability of each target token conditioned on source tokens and previous target tokens. This will bring noisy alignments when the prediction is ambiguous . To alleviate this problem, previous studies modify Transformer  by adding alignment modules to re-predict the target token , or computing an additional alignment loss on the full target sequence . Moreover,  propose an extraction method that induces alignment when the to-be-aligned target token is the decoder input.   Although these methods have demonstrated their effectiveness, they have two drawbacks. First, they retain the translation objective which is not tailored for word alignment. Consider the example in Figure . When predicting target token ``Tokyo'', the translation model may wrongly generate ``1968'' as it only considers the previous context, which will result in an incorrect alignment link . A better modeling is needed for obtaining more accurate alignments. Second, they need an additional guided alignment loss  to outperform GIZA++, which requires inducing alignments for entire training corpus.  In this paper, we propose a self-supervised model specifically designed for the word alignment task, namely Mask-Align. Our model masks each target token and recovers it with the source and the rest of the target tokens. For example, as shown in Figure , the target token ``Tokyo'' is masked and re-predicted. During this process, our model can identify that only the source token ``Tokio'' has not been translated yet, so the to-be-predicted target token ``Tokyo'' is aligned to ``Tokio''. Comparing with the translation model, this masked modeling method is highly related to word alignment, and based on that our model generates more accurate predictions and alignments.  % We model the target token conditioned on all other tokens in both source and target, which will disambiguate the prediction and thus lead to an accurate alignment ). As the vanilla transformer architecture requires sequential time to model this probability, we modify the attention in the decoder by separating the queries from keys and values and  % updating only the former in each layer. This allows our model to predict all target tokens in a single forward pass without information leakage. Besides, we also propose a variant of attention called leaky attention that allieviates the unexpected high attention weights on some specific tokens such as periods, which is helpful for the alignment extraction from attention matrix. Finally, we leverage the attention weights from the models in two directions by incorporating an agreement loss in the training process.  % Experiments on four public datasets show that our model significantly outperforms all existing statistical and neural methods without using guided alignment loss.  To summarize, the main contributions of our work are listed as follows:         
"," Neural word alignment methods have received increasing attention recently. These methods usually extract word alignment from a machine translation model. However, there is a gap between translation and alignment tasks, since the target future context is available in the latter. In this paper, we propose Mask-Align, a self-supervised model specifically designed for the word alignment task. Our model parallelly masks and predicts each target token, and extracts high quality alignments without any supervised loss. In addition, we introduce leaky attention to alleviate the problem of unexpected high attention weights on special tokens. Experiments on four language pairs show that our model significantly outperforms all existing unsupervised neural baselines and obtains new state-of-the-art results.  % However, the original translation objective ignores the future context in the target, which is available in the alignment task.",359
" The sequence-to-sequence  models~, which learn to map an arbitrary-length input sequence to another arbitrary-length output sequence, have successfully tackled a wide range of language generation tasks. % including machine translation, text summarization, question generation, to name a few.  Early seq2seq models have used recurrent neural networks to encode and decode sequences, leveraging attention mechanism  that allows the decoder to attend to a specific token in the input sequence to capture long-term dependencies between the source and target sequences. Recently, the Transformer~, which is an all-attention model that effectively captures long-term relationships between tokens in the input sequence as well as across input and output sequences, has become the de facto standard for most of the text generation tasks due to its impressive performance. Moreover, Transformer-based language models trained on large text corpora  have shown to significantly improve the model performance on text generation tasks. %Seq2seq tasks are becoming increasingly more important, as  show that most of text-based language problems can be cast into sequence-to-sequence problems.  However, a crucial limitation of seq2seq models is that they are mostly trained only with , where ground truth is provided at each time step and thus  never exposed to incorrectly generated tokens during training ), which hurts its generalization. This problem is known as the ``exposure bias"" problem  and often results in the generation of low-quality texts on unseen inputs. Several prior works tackle the problem, such as using reinforcement learning  to maximize non-differentiable reward . %  --- BLEU or Rouge.   Another approach is to use RL or gumbel softmax  to match the distribution of generated sentences to that of the ground truth, in which case the reward is the discriminator output from a Generative Adversarial Network  . Although the aforementioned approaches improve the performance of the seq2seq models on text generation tasks, they either require a vast amount of effort in tuning hyperparameters or stabilize training. %Moreover,  show that RL methods for machine translation often do not optimize the expected reward and the performance gain is attributed to the side effects, such as increasing the peakiness of the output distribution.      In this work, we propose to mitigate the exposure bias problem with a simple yet effective approach, in which we  a positive pair of input and output sequence to negative pairs, to expose the model to various valid or incorrect sentences. Na鑼倂ely, we can construct negative pairs by simply using random non-target sequences from the batch~. However, such a na鑼倂e construction yields meaningless negative examples that are already well-discriminated in the embedding space ), which we highlight as the reason why existing methods~ require large batch size. This is clearly shown in Fig., where a large portion of positive-negative pairs can be easily discriminated without any training, which gets worse as the batch size decreases as it will reduce the chance to have meaningfully difficult examples in the batch. Moreover, discriminating positive and na鑼倂e negative pairs becomes even more easier for models pretrained on large text corpora.   To resolve this issue, we propose principled approaches to automatically generate negative and positive pairs for constrastive learning, which we refer to as  . Specifically, we generate a negative example by adding a small perturbation to the hidden representation of the target sequence, such that its conditional likelihood is minimized ). Conversely, we construct an additional positive example ) by adding a large amount of perturbation to the hidden representation of target sequence such that the perturbed sample is far away from the source sequence in the embedding space, while enforcing it to have high conditional likelihood by minimizing Kullback-Leibler  divergence between the original conditional distribution and perturbed conditional distribution. This will yield a negative example that is very close to the original representation of target sequence in the embedding space but is largely dissimilar in the semantics, while the generated positive example is far away from the original input sequence but has the same semantic as the target sequence. This will generate difficult examples that the model fails to correctly discriminate , Fig.2), helping it learn with more meaningful pairs.  To verify the efficacy of our method, we empirically show that it significantly improves the performance of seq2seq model on three conditional text generation tasks, namely machine translation, text summarization and question generation. Our contribution in this work is threefold:   [itemsep=1mm, parsep=0pt, leftmargin=*]       
"," Recently, sequence-to-sequence  models with the Transformer architecture have achieved remarkable performance on various conditional text generation tasks, such as machine translation. However, most of them are trained with  with the ground truth label given at each time step, without being exposed to incorrectly generated tokens during training, which hurts its generalization to unseen inputs, that is known as the ``exposure bias"" problem. In this work, we propose to mitigate the conditional text generation problem by contrasting positive pairs with negative pairs, such that the model is exposed to various valid or incorrect perturbations of the inputs, for improved generalization. However, training the model with na閼煎俥 contrastive learning framework using random non-target sequences as negative examples is suboptimal, since they are easily distinguishable from the correct output, especially so with models pretrained with large text corpora. Also, generating positive examples requires domain-specific augmentation heuristics which may not generalize over diverse domains. To tackle this problem, we propose a principled method to generate positive and negative samples for contrastive learning of seq2seq models. Specifically, we generate negative examples by adding small perturbations to the input sequence to minimize its conditional likelihood, and positive examples by adding  large perturbations while enforcing it to have a high conditional likelihood. Such ``hard'' positive and negative pairs generated using our method guides the model to better distinguish correct outputs from incorrect ones. We empirically show that our proposed method significantly improves the generalization of the seq2seq on three text generation tasks --- machine translation, text summarization, and question generation.",360
"   %缁楊兛绔村▓纰夌窗headline瀵板牓鍣哥憰 With the rapid growth of information spreading throughout the Internet, readers get drown in the sea of documents, and will only pay attention to those articles with attractive headlines that can catch their eyes at first sight. On one hand, generating headlines that can trigger high click-rate is especially important for different avenues and forms of media to compete for user's limited attention. On the other hand, only with the help of a good headline, can the outstanding article be discovered by readers.                     %缁楊兛绗佸▓纰夌窗閹存垳婊戦惃鍕侀崹瀣簼绠為幀搴濈疄閸 To generate better headlines, we first analyze what makes the headlines attractive. By surveying hundreds of headlines of popular websites, we found that one important feature that influences the attractiveness of a headline is its content. For example, when reporting the same event, the headline ``Happy but not knowing danger: Children in India play on the poisonous foam beach'' wins over 1000 page views, while the headline ``Chennai beach was covered with white foam for four days in India'' only has 387 readers. The popular headline highlights the fact that ``the beach is poisonous and affects children'',  which will concern more people than ``white foam''. On the other hand, the style of the headline also has a huge impact on attractiveness. For example, the headline ``Only two people scored thousand in the history of NBA Finals'' attracts fewer people than the headline ``How hard is it to get 1000 points in the NBA finals? Only two people in history!'', due to its conversational style that makes readers feel the need to see the answer to this question.      %缁楊兛绨╁▓纰夌窗challenge:婵″倷缍嶉惌銉╀壕attractive Most of the recent researches regard the headline generation task merely as a typical summarization task . This is not sufficient because a good headline should not only capture the most relevant content of an article but also be attractive to the reader. However, attractive headline generation tasks were paid less attention by researchers.  tackle this task by adversarial training, using an attractiveness score module to guide the summarization process.  introduce a parameter sharing scheme to disentangle the attractive style from the attractive text. However, previous works neglect the fact that attractiveness is not just about style, but also about content. %     the negative samples generated by the pre-trained model can be non-fluent, inconsistent, and incoherent, which makes it difficult for the scorer to learn the attractiveness standard given such huge noise.           Based on the above analysis, we propose a model named  , which learns to write attractive headlines from both style and content perspectives.  These two attractiveness attributes are learned from an attractive prototype headline, \ie the headline of the document in the training dataset that is most similar to the input document. First, DAHG separates the attractive style and content of the prototype headline into latent spaces, with two auxiliary constraints to ensure the two spaces are indeed disentangled. Second, the learned attractive content space is utilized to iteratively polish the input document, emphasizing the parts in the document that are attractive. Finally, the decoder generates an attractive headline from the polished input document representation under the guidance of the separated attractive style space. Extensive experiments on the public Kuaibao dataset show that DAHG outperforms the summarization and headline generation baselines in terms of ROUGE metrics, BLEU metrics, and human evaluations by a large margin. Specifically, DAHG triggers 22\% more clicks than the strongest baseline.           %缁楊剙娲撳▓纰夌窗閹崵绮╟ontribution The major contributions of this paper are as follows:   We devise a disentanglement mechanism to divide the attractive content and style space from the attractive prototype headline.  We propose to generate an attractive headline with the help of disentangled content space under the style guidance.  Experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations.      
"," Eye-catching headlines function as the first device to trigger more clicks, bringing reciprocal effect between producers and viewers. Producers can obtain more traffic and profits, and readers can have access to outstanding articles. When generating attractive headlines, it is important to not only capture the attractive content but also follow an eye-catching written style.  In this paper, we propose a Disentanglement-based Attractive Headline Generator  that generates headline which captures the attractive content following the attractive style. Concretely, we first devise a disentanglement module to divide the style and content of an attractive prototype headline into latent spaces, with two auxiliary constraints to ensure the two spaces are indeed disentangled. The latent content information is then used to further polish the document representation and help capture the salient part. %The latent attractive content space further helps to distill salient and attractive knowledge from the input document. Finally, the generator takes the polished document as input to generate headline under the guidance of the attractive style.  Extensive experiments on the public Kuaibao dataset show that DAHG achieves state-of-the-art performance.  Human evaluation also demonstrates that DAHG triggers 22\% more clicks than existing models.",361
"  Task-specific finetuning of pretrained deep networks has become the dominant paradigm in contemporary NLP, achieving state-of-the-art results across a suite of natural language understanding tasks . While straightforward and empirically effective, this approach is difficult to scale to multi-task, memory-constrained settings , as it requires shipping and storing a full set of model parameters for each task. Inasmuch as these models are learning generalizable, task-agnostic language representations through self-supervised pretraining, finetuning the entire model for each task seems especially profligate.   A popular approach to parameter-efficiency with pretrained models is to learn sparse models for each task where a subset of the final model parameters  are exactly zero~. Such approaches often face a steep sparsity/performance tradeoff, and a substantial portion of nonzero parameters  are still typically required to match the performance of the dense counterparts. An alternative is to use multi-task learning or feature-based transfer for more parameter-efficient transfer learning with pretrained models~. These methods learn only a small number of additional parameters  on top of a shared model. However, multi-task learning generally requires access to all tasks during training to prevent catastrophic forgetting~, while feature-based transfer learning  is typically outperformed by full finetuning~.    Adapters~ have recently emerged as a promising approach to parameter-efficient transfer learning within the pretrain-finetune paradigm~.  Adapter layers are smaller, task-specific modules that are inserted between layers of a pretrained model, which remains fixed and is shared across tasks.  These approaches do not require access to all tasks during training, making them attractive in settings where one hopes to obtain and share performant models as new tasks arrive in stream.   find that adapter layers trained on BERT can match the performance of fully finetuned BERT on the GLUE benchmark  while only requiring 3.6\% additional parameters  per task.   In this work, we consider a similar setting as adapters but propose a new  approach with the goal of even more parameter-efficient transfer learning.  Diff pruning views finetuning as learning a task-specific \underline{diff}erence  vector%\footnote{Similar to the  command in Unix operating systems.}    \ that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks.   In order to learn this vector, we reparameterize the task-specific model parameters as , where the pretrained parameter vector  is finetuned. The diff vector is regularized with a differentiable approximation to the -norm penalty~ to encourage sparsity. This approach can become  parameter-efficient as the number of tasks increases as it only requires storing the nonzero positions and weights of the diff vector for each task. The cost of storing the shared pretrained model remains constant and is amortized across multiple tasks.  On the GLUE benchmark~, diff pruning can match the performance of the fully finetuned BERT baselines  while finetuning only  of the pretrained parameters per task, making it a potential alternative to adapters for parameter-efficient transfer learning.   
"," While task-specific finetuning of pretrained networks has led to significant empirical advances in NLP, the large size of networks makes finetuning difficult to deploy in multi-task, memory-constrained settings. We propose  as a simple approach to enable parameter-efficient transfer learning within the pretrain-finetune framework. This approach views finetuning as learning a task-specific ``diff"" vector that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks. The diff vector is adaptively pruned during training with a differentiable approximation to the $L_0$-norm penalty to encourage sparsity. Diff pruning becomes parameter-efficient as the number of tasks increases, as it requires storing only the nonzero positions and weights of the diff vector for each task, while the cost of storing the shared pretrained model remains constant. It further does not require access to all tasks during training, which makes it attractive in settings where tasks arrive in stream or the set of tasks is unknown. We find that models finetuned with diff pruning can match the performance of fully finetuned baselines on the GLUE benchmark while only modifying 0.5$\%$ of the pretrained model's parameters per task. Our code is available at \url{https://github.com/dguo98/DiffPruning}}",362
"   Goal-oriented dialogue systems is a hot topic in machine learning research. The systems have widespread applications in the industry and are the foundation of many successful products, including Alexa, Siri, Google Assistant, and Cortana. One core component of a dialog system is spoken language understanding , which consists of two main problems, intent classification  and slot labeling  . In IC, we attempt to classify the goal of a user query, usually input in text or transcribed by automatic speech recognition  system from audio. SL, similar to the named-entity recognition  problem, aims to label each token in a query an entity type. The only difference is that entity types in SL are domain-specific and based upon dialog ontology. Recent advances in neural models have enabled greatly improved SLU .  However, two significant challenges hinder the broad application and expansion of the SLU models in industrial settings. First of all, neural methods require a large amount of labeled data for training . SLU is often coupled with the ontology of the underlying dialog system and thus domain-dependent. Collecting a large number of in-domain labeled data for neural models is prohibitively expensive and time-consuming. Secondly, the performance of SLU models in practice often suffers from fluctuations due to various types of noises. One common noise is adaptation data perturbation. In many industrial applications such as cloud services\footnote{Alexa ASK: https://developer.amazon.com/en-US/alexa/alexa-skills-kit; Google DialogFlow: https://dialogflow.com/}, the SLU model is built by fine-tuning  a pre-trained, shared network to the target domain with data provided by developers. The developers often have a limited background in SLU and machine learning. Thus the data provided varies in quality and is subject to different types of perturbations, such as missing or replaced data samples  and typos. Another common noise comes from the mismatch of input modalities between adaptation and inference stages. For instance, the model is adapted with human transcription yet deployed to understand ASR decoded text, or the input at adaptation and inference stages relies on the recognition of different versions of ASR models. Given that most neural methods comprise a large number of parameters and are heavily optimized for the training  data provided, the resulting model is usually sensitive to these noises. The requirement of noise-free adaptation and inference conditions also prohibits the use of neural SLU techniques because it is often infeasible to achieve such conditions.  Transfer learning and meta-learning are two conventional techniques that have been applied to address the challenge of data scarcity. Transfer learning usually refers to pre-training initial models using mismatched domains with rich human annotations and then adapting the models with limited labels in targeted domains. Previous works  have shown promising results in applying transfer learning to SLU. Note that pre-training discussed here covers methods including using a pre-trained language model like BERT  directly and further training downstream tasks on data in mismatched domains with the pre-trained model. In the following, we focus on the latter due to utilizing data from other domains better and yielding higher accuracy. In recent years, meta-learning has gained growing interest among the machine learning fields for tackling few-shot learning  scenarios. Model-Agnostic Meta-Learning   focuses on learning parameter initialization from multiple subtasks, such that the initialization can be fine-tuned with few labels and yield good performance in targeted tasks. Metric-based meta-learning, including prototypical networks   and matching networks , aim to learn embedding or metric space which can be generalized to domains unseen in the training set after adaptation with a small number of examples from the unseen domains. Recent work unveils excellent potential in applying meta-learning techniques to SLU in the few-shot learning context .  As compared to data scarcity, another challenge for SLU, the robustness against noises, is also gaining attention. Simulated ASR errors are used to augment training data for SLU models . Researchers also leverage information from confusion networks or lattices , and adversarial training techniques  for models to learn query embeddings that are robust against ASR errors. For text input, methods have also been explored on model robustness against noises from misspelling and acronym . In contrast to these noise types that have gained attention, to our best knowledge, there is no prior work investigating the impact of missing or replaced examples in adaptation data. Moreover, the intersection of data scarcity and noise robustness is unexplored. Since the scarcity of labeled data and data noisiness usually co-occur in SLU applications , the lack of studies in the intersectional areas hinders the use of neural SLU models and its expansion to broader use cases.  Given the deficiency, we establish a novel few-shot noisy SLU task by introducing two common types of natural noise, adaptation example missing/replacing and modality mismatch, to the previously defined few-shot IC/SL splits . The task is built upon three public datasets, ATIS , SNIPS , and TOP . We further propose a noise-robust few-shot SLU model based on ProtoNets for the established task. In summary, our primary contributions are 3-fold: 1) formulating the first few-shot noisy SLU task and evaluation framework, 2) proposing the first working solution for the few-shot noisy SLU with the existing ProtoNet algorithm, and 3) in the context of noisy and scarce learning examples, comparing the performance of the proposed method with conventional techniques, including MAML and fine-tuning based adaptation.   
","    Recently deep learning has dominated many machine learning areas, including spoken language understanding . However, deep learning models are notorious for being data-hungry, and the heavily optimized models are usually sensitive to the quality of the training examples provided and the consistency between training and inference conditions. To improve the performance of SLU models on tasks with noisy and low training resources, we propose a new SLU benchmarking task: few-shot robust SLU, where SLU comprises two core problems, intent classification  and slot labeling . We establish the task by defining few-shot splits on three public IC/SL datasets, ATIS, SNIPS, and TOP, and adding two types of natural noises  to the splits. We further propose a novel noise-robust few-shot SLU model based on prototypical networks. We show the model consistently outperforms the conventional fine-tuning baseline and another popular meta-learning method, Model-Agnostic Meta-Learning , in terms of achieving better IC accuracy and SL F1, and yielding smaller performance variation when noises are present.",363
"  In the modern world, social media is playing its part in several ways, for instance in news dissemination and information sharing, social media outlets, such as Twitter, Facebook, and Instagram, have been proved very effective . However, it also comes with several challenges, such as collecting information from several sources, detecting and filtering misinformation . Similar to other events and pandemics, being one of the deadly pandemics in the history, COVID-19 has been the subject of discussion over social media since its emergence. Without any surprise, a lot of misinformation about the pandemic are circulated over social networks. In order to identify misinformation spreaders and filter fake news about COVID-19 and 5G conspiracy, a task namely ""FakeNews: Corona Virus and 5G Conspiracy Multimedia Twitter-Data-Based Analysis"" has been proposed in the benchmark MediaEval 2020 competition .   This paper provides a detailed description of the methods proposed by team DCSE\_UETP for the fake news detection task. The task consists of two parts, namely  text-based misinformation detection , and  structure-based misinformation detection . The first task  is based on textual analysis of COVID-19 related information shared on Twitter during January 2020 and 15th of July 2020, and aims to detect different types of conspiracy theories about COVID-19 and its vaccines, such as that ""the 5G weakens the immune system and thus caused the current corona-virus pandemic etc., . In the SMD task, the participants are provided with a set of graphs, each representing a sub-graph of Twitter, and corresponds to a single tweet where the vertices of the graphs represent accounts. Similar to TMD, in this task, the participants need to detect and differentiate between 5G and other COVID-19 conspiracy theories.   
"," The paper presents our solutions for the MediaEval 2020 task namely FakeNews: Corona Virus and 5G Conspiracy Multimedia Twitter-Data-Based Analysis. The task aims to analyze tweets related to COVID-19 and 5G conspiracy theories to detect misinformation spreaders. The task is composed of two sub-tasks namely  text-based, and  structure-based fake news detection. For the first task, we propose six different solutions relying on Bag of Words  and BERT embedding. Three of the methods aim at binary classification task by differentiating in 5G conspiracy and the rest of the COVID-19 related tweets while the rest of them treat the task as ternary classification problem. In the ternary classification task, our BoW and BERT based methods obtained an F1-score of .606\% and .566\% on the development set, respectively. On the binary classification, the BoW and BERT based solutions obtained an average F1-score of .666\% and .693\%, respectively. On the other hand, for structure-based fake news detection, we rely on Graph Neural Networks  achieving an average ROC of .95\% on the development set.",364
"   Recurrent neural networks are the basis of the state-of-the-art models in natural language processing, including language modeling , machine translation  and named entity recognition . It is needless to say that complex learning tasks require relatively large networks with millions of parameters to be accomplished. However, large neural networks need more data and/or strong regularization techniques to be trained successfully and avoid overfitting. Without the means to collect more data, which is the case in the majority of real-world problems, data augmentation and regularization methods are standard alternative practices to overcome this barrier.  Data augmentation in natural language processing is limited, and often task-specific . On the other hand, adopting the same regularization methods that are originally proposed for feed-forward  networks needs to be done with extra care to avoid hurting the network's information flow between consecutive time-steps. To overcome such limitations, we present Sequence Mixup: a set of training methods, regularization techniques, and data augmentation procedures for RNNs. Sequence Mixup can be considered as the RNN-generalization of input mixup  and manifold mixup , which are already introduced for feed-forward neural networks. Generally speaking, the core idea behind mixup strategies is to mix training samples in the network's input or hidden layers, where by mix, we simply mean to consider random convex combinations of pairs of samples as alternatives for the actual training data points. Mixup in non-recurrent networks has led to smoother decision boundaries, more robustness to adversarial examples, and better generalization compared to many rival regularization methods . Here, we extend input mixup to RNNs and also propose two variants of manifold mixup, namely Pre-Output Mixup  and Through-Time Mixup , where mixing occurs in the hidden space of the RNN. POM and TTM differ from each other in the way information flow is passed from one time-step to the next.  In order to elucidate the effect of sequence mixup during the learning stage, consider the classification of half-moons data plotted in figure  with a simple two-timestep RNN. We have also added some levels of noise to the original data points to make the classification task more challenging. Figures  and  show the learned decision boundaries from noisy data via regular training and Pre-Output Mixup, respectively. As can be seen, mixup expands the margin between the classes and increases the decision boundary levels, which in turn renders a smoother decision boundary with less certainty about nearby cross-class samples. Intuitively speaking, this type of training creates artificial samples whose labels and hidden states are obtained from intermixing those of the original samples, in a respective manner. Based on our experiments, applying sequence mixup has improved both the test F-1 score and loss of BiLSTM-CRF model  on CoNLL-03 data  .   [t] 	 	 	 	 Original half-moons data and its learned decision boundary. Noise-corrupted half-moons and decision boundaries learned with  regular training and  Pre-Output Mixup .} 	   We have also provided a theoretical analysis on the impact of our regularization techniques in the asymptotic regime where network widths become increasingly large, and learning rates become infinitesimally small. In a nutshell, our analysis reveals that as long as the number of hidden state neurons, which we denote by  in this work, is less than the number of distinct classes in a classification problem, both POM and TTM cannot achieve a zero training error regardless of how large the training dataset is or how deep the neural networks become. Moreover, we show that as long as  is less than twice the number of classes, the hidden-state generating section of the RNN acts as a memoryless unit and produces hidden states that are almost independent of previous time-steps. On the other hand, given that  is chosen sufficiently large, both POM and TTM are able to divide the hidden representation space of the RNN into a set of orthogonal affine subspaces, where each subspace is an indicator of a unique class. We refer to this property as spectral compression of sequence mixup, which is a similar behaviour to that of manifold mixup for feed-forward networks.  The rest of the paper is organized as follows: Section  reviews a number of related works to this problem. In Section , we propose Sequence Mixup, describe its challenges and specifications in detail, and also present our theoretical analysis.  Section  is devoted to our experiments on real-world data. Finally, Section  concludes the paper. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
"," In this paper, we extend a class of celebrated regularization techniques originally proposed for feed-forward neural networks, namely Input Mixup  and Manifold Mixup , to the realm of Recurrent Neural Networks . Our proposed methods are easy to implement and have a low computational complexity, while leverage the performance of simple neural architectures in a variety of tasks. We have validated our claims through several experiments on real-world datasets, and also provide an asymptotic theoretical analysis to further investigate the properties and potential impacts of our proposed techniques. Applying sequence mixup to BiLSTM-CRF model  to Named Entity Recognition task on CoNLL-2003 data  has improved the F-1 score on the test stage and reduced the loss, considerably. @ce.sharif.edu,~motahari@sharif.edu} {https://github.com/ArminKaramzade/SequenceMixup.}}",365
" Sentiment classification is the task of analyzing a piece of text to predict the orientation of the attitude towards an event or opinion. The sentiment of a text can be either positive or negative. Sometimes, a neutral perspective is also considered for classification. SA has many different applications, such as reducing the early age suicide rate by identifying cyberbullying , discouraging unwarranted activities towards a particular community through hate-speech detection , and monitoring public response towards a proposed government bill  among many others.    The task of SA has achieved superior improvement in other languages, i.e. English - about 97.1\% accuracy for 2-class  and 91.4\% accuracy for 3-class SA . But only a few research works have been published for the SA in Bengali. This is because we lack quality datasets in Bengali for training a computation model for the sentiment classification. However, in the last few years, we have seen the rise of Internet users in the Bengali domain mostly due to the development of wireless network infrastructure throughout South East Asia. This resulted in a massive increase in the total number of online social network users as well as newspaper readers. So it became comparatively easier to collect the public comments posted online on the Bengali news websites.         BERT_{BSA}, that performs better compared to other existing models that are trained with word2vec or fastText embedding. We discuss the model and in Section .     $ and compare it to other models trained with Word2Vec and fastText embeddigns using the 2-class and 3-class Bengali SA datasets. We discuss the results in the Section .     %   % \makeatletter % \patchcmd{\@makecaption} %   { %   {} %   {} % \makeatletter % \patchcmd{\@makecaption} %   {\\} %   {.\ } %   {} %   {} % \makeatother % \def\tablename{Table}   
"," Sentiment analysis  in Bengali is challenging due to this Indo-Aryan language's highly inflected properties with more than 160 different inflected forms for verbs and 36 different forms for noun and 24 different forms for pronouns. The lack of standard labeled datasets in the Bengali domain makes the task of SA even harder. In this paper, we present manually tagged 2-class and 3-class SA datasets in Bengali. We also demonstrate that the multi-lingual BERT model with relevant extensions can be trained via the approach of transfer learning over those novel datasets to improve the state-of-the-art performance in sentiment classification tasks. This deep learning model achieves an accuracy of 71\% for 2-class sentiment classification compared to the current state-of-the-art accuracy of 68\%. We also present the very first Bengali SA classifier for the 3-class manually tagged dataset, and our proposed model achieves an accuracy of 60\%. We further use this model to analyze the sentiment of public comments in the online daily newspaper. Our analysis shows that people post negative comments for political or sports news more often, while the religious article comments represent positive sentiment. The dataset and code is publicly available \footnote{ https://github.com/KhondokerIslam/Bengali\_Sentiment}.",366
"  Methods for automatically learning phone- or word-like units from unlabelled speech audio could enable speech technology in severely low-resourced settings and could lead to new cognitive models of human language acquisition. The goal in unsupervised representation learning of phone units is to learn features which capture phonetic contrasts while being invariant to properties like the speaker or channel. Early approaches focussed on learning continuous features. In an attempt to better match the categorical nature of true phonetic units, more recent work has considered discrete representations. One approach is to use a self-supervised neural network with an intermediate layer that quantizes features using a learned codebook. While the discrete codes from such vector quantized  networks have given improvements in intrinsic phone discrimination tasks, they still encode speech at a much higher bitrate than true phone sequences.  As an example, the top of Figure shows the code indices from a vector-quantized variational autoencoder  overlaid on the input spectrogram. While there is some correspondence between the code assignments and the true phones , and although there is some repetition of codes in adjacent frames , the input speech are often assigned to codes that are distinct from those of surrounding frames. This is not surprising since the VQ model is not explicitly encouraged to do so. The result is an encoding at a much higher bitrate  than that of true phone sequences .    In this paper we consider ways to constrain VQ models so that contiguous feature vectors are assigned to the same code, resulting in a low-bitrate segmentation of the speech into discrete units. We specifically compare two VQ segmentation methods. Both of these are based on a recent method for segmenting written character sequences. The first method is a greedy approach, where the closest adjacent codes are merged until a set number of segments are reached. The second method allows for an arbitrary number of segments. A squared error between blocks of feature vectors and VQ codes are used together with a penalty term encouraging longer-duration segments. The optimal segmentation is found using dynamic programming.  We apply these two segmentation approaches using the encoders and codebooks of the two VQ models from . The first is a type of VQ-VAE. The second is a vector-quantized contrastive predictive coding  model. The combination of these two models with the two segmentation approaches gives a total of four VQ segmentation models to consider. %Applying both these models with both segmentation approaches gives a total of four model combinations. We evaluate these on four different tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The last-mentioned is particularly important since the segmentation and clustering of % word-like units %from unlabelled speech remains a major but important challenge.  On most metrics in the four tasks the combination of the VQ-VAE with the penalized dynamic programming approach is the best VQ segmentation method. Example output is shown in the middle of Figure. Compared to other existing methods, it does not achieve state-of-the-art performance in all four evaluation tasks. However, it achieves reasonable performance at a much lower bitrate than most existing methods. This is noteworthy since, while most of the other methods have been tailored to the respective tasks, a single VQ segmentation approach can be used without any alteration directly to a range of problems. 
"," We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized~ neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized method generally performs best. While results are only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate.",367
"   Natural language has provided a key cohesive ingredient for pushing the boundaries of technological advances beyond individuals to the 4th industrial revolution. In textual form, it provides a long term, stable, knowledge base, which can be used to preserve knowledge across generations. Digital evolution in the last century has greatly accelerated this preservation process and provided a means to extract hidden meaning and information from texts, largely considered illegible and indecipherable for human beings. Natural Language Processing  is a divergent field, with state-of-the-art research initiative looking towards resolving the various challenges of automatic information extraction. Foremost, amongst these challenges is the ability to identify the various concepts and their relationship, which form the epitome of the target corpus . For humans and machines, cause-effect represents an essential relation, which provides ample support for the reasoning and decision making process . Automatic causality detection has benefited greatly from numerous dedicated research efforts . However, challenges such as the dynamicity of syntax and semantics, and in particular the evolution of vocabulary have hindered the development and usage of any generic and cross-domain solution . On the other hand, applications such as information retrieval , question answering , and event reasoning and predictions  have gained valuable improvements through the identification of cause-effect relationships. \\ The commonly used approaches for causality detection, fall into two categories: pattern-based traditional rule bases, and machine learning based automatic classification and entity extraction . Pattern based approaches are based on partial or complete expert intervention for crafting and verifying the conditions based on the syntactic and semantic analysis of the corpus. This approach, requires intensive human effort and lacks cross-domain generalization. Even after utilizing a substantial amount of human time, the extracted rules cannot cover all possible linguistic patterns and are usually not usable beyond the original domain/corpus. Such an approach also suffers from the diversity in linguistic typology, leading to rules formed for a language based on the Subject-Verb-Object sentence structure  not being compatible with those based on other structures such as Subject-Object-Verb and others .\\ Automatic machine learning based approaches utilize labeled datasets for extracting causality relationships from unseen data and thereby requires less expert intervention, relatively. With this approach, most human time is spent on labeling the data and verifying the results, while providing a reusable model for cross-domain applications. However, any evolution of labels and change in text can render the model unusable. Additionally, machine learning models, are typically independent of the linguistic topology features and can be customized to work on any sentence structure albeit with some effort towards creating and optimizing language vectors, and incorporating natural heuristics derived from syntactically labelled  or a well distributed large corpus  .\\ A solution to managing change in the machine learning models and reducing the expert intervention is available as Transfer Learning, where the machine can learn a new tasks by reusing a foundational model, originally employed for a different but related task in another domain . Such a cross-domain application may not replicate the original performance benchmarks, and thereby requires some model tuning and tweaking before becoming useful. Model tuning is achieved with the help of a human expert who provides feedback to the machine learning model for improving its learning tasks, a technique more commonly known as active learning . To gain benefits of these two approaches active transfer learning is applied to various tasks in diverse domains , transferring similar models and improving its performance in a single workflow. This performance is mainly improved by enhancing the pre-trained model with few annotated dataset and expert involvement from the new domain.\\ Causality mining as an application of causality detection is typically based on two tasks, which includes identification of causal triggers, and causal pairs participating in each relationship . Also known as causal connectives; causal triggers are transitive verbs which form a bridge between causality concepts and identify the cause and its effect. Leveraging the sentence structuring in English language, typical causality relation identification methodologies, found in research literature, follow the Noun Phrase  - Verb  - NP pattern which corresponds to either Cause - Trigger - Effect or Effect - Trigger - Cause forms   . Based on this heuristic, Kaplan and Berry-Bogge  provided an early model for creating and using handcrafted linguistic template for causality detection. Kalpana Raja et al. , built upon the same idea in addition to identifying and organizing a dictionary based on causal trigger keywords, which was then used to define patterns for causality detection. R. Girju et al.  refined the process of identifying the causal verbs by utilizing the WordNet dictionary. Cole et al.  utilized a syntactic parser to convert the SVO structures into SVO triples, which were then passed through various rule based filters for causality detection. S. Zhao et al. , pointed towards the existence of diversity in the manner each causal trigger expresses causality. However, the syntactic structure of causal sentences and the way the trigger invokes the causality, can provide satisfactory categorization of the causal triggers, enabling smart application of the causality identification filters. Son Doan et al.  presented an application of causal mining by marking several verbs and nouns as causal triggers for extracting causal relations from twitter messages. Girju and Moldovan  proposed a semi-supervised approach towards causality relation identification by using the underlying linguistic patterns of the corpus.\\ Many other automatic causal pattern identification methodologies have relied on the evolution of machine learning models. In particular,  has presented a causal relation extraction model using unsupervised learning to detect the noun phrases corresponding to the subject and object of the sentence. By analysing an unannotated raw corpus and using Expected Maximization along with a Naive Bayes classifier, the authors were able to precisely identify 81.29\% of causal relations. \\ On the other hand, E. Blanco et al.  utilized a supervised learning approach by first annotating ternary instances as being a causal relation or not, and then applied Bagging with C4.5 decision trees to achieve a precision of over 95\% in causal relations ad above 86\% in non causal ones. These and many other machine learning approaches have been comprehensively classified by , which indicates a general trend towards the utilizing of the same, as the models become more mature and stable. Of particular interest are the word embedding methods, which due to their requirement of unsupervised data, scalability, and accuracy have piqued the interest of the NLP research community. \\ Several initiatives have already led to the state-of-the-art results in completing NLP tasks such as sentiment analysis, text classification, topic modeling, and relation extraction . Zeng et al.  classified relations in the SemEval Task 8 dataset using deep convolution neural networks . Nguyen et al.  introduced positional embedding to the input sentence vector in CNNs for improved relation extraction. Silva et al.  proposed a deep learning  based causality extraction methodology that can detect causality along with its direction. The author addressed the causality detection problem as a three class classification problem, where class 1 indicates the annotated pairs has causal relation with direction entity1  entity2, class 2 implies the causal relation has the direction entity2  entity1, and class 3 entities are non-causal.\\ Ning An et al.  has utilized a word embedding with cosine similarity based approach, which uses an initial causal seed list to identify the causal relationships as a multi-class  classification problem. With one-hot encoding the authors, convert the causal verbs in the seed list and the verbs identified in Noun Phrase-Verb Phrase-Noun Phrase ternary into encoding vectors. These vectors are then converted into Embedding vectors using Continuous Skip-Gram based on a Wikipedia dataset of 3.7 million articles. Finally the encoded vectors are then compared using cosine similarity and the pair with maximum similarity above a pre-defined threshold value of 0.5 are used to classify the causal relationship and evolve the seed list. This method achieved an average F-score of 78.67\%. While this methodology presents a significant improvement on previous research initiatives towards causal relationship identification, it suffers from low accuracy, due to its focus on causal verb identification based on a small initial seed list and its limited extension, and classification based, solely on these verbs meanwhile losing context of the causal phrase. \\ In this paper we present a novel causal relationship identification framework, which outperform, in the domain of causality mining in clinical text. This framework uses a multi-dimensional approach, which resolves syntactic and semantic matching problems in clinical textual data, providing causal knowledge which is useful to summarize clinical text for quick review, create patient personas for reapplication of medical procedures and predictive analysis, discovering medical knowledge from volumnous data sources, and provide evidence supporting clinical decision making.\\ This novel framework identifies causal phrases using automatic seed list generation from training data set, seed expansion using transfer learning, causal phrase generation, and BERT based phrase embedding and semantic matching. It then applies semantic enrichment on the causal phrases using Unified Medical Language System , to extend  healthcare terms with their semantic and uniquely identifiable corresponding codes. Finally, the trained model is evolved based on expert feedback, by employing active learning.\\  In the presented approach, we extracted the initial causal seed list from SemEval Task 8 dataset and expanded it by utilizing synonyms from WordNet dictionary, pre-trained Google News model , ConceptNet Numberbatch Model , and Facebook Fasttext Model . We then generated causal quads using dependency based linguistic patterns for identifying the subject, object, causal verb, and a confidence measure. Causal triples, under a threshold, were then filtered from the quads and converted into embedding vectors using BERT to create an initial model. This trained model was used to identify candidate causal triples in unseen textual data, which were then semantically enriched from UMLS and converted into a causal quad by augmenting a confidence score. The semantically enriched causal quads were then verified by an expert by increasing or decreasing the confidence value and used to evolve the trained model, iteratively.\\ This detailed methodology is presented in section , with details workflows in section  and its results following in section . Finally, section  will conclude the paper.        
"," Objective: Causality mining is an active research area, which requires the application of state-of-the-art natural language processing techniques. In the healthcare domain, medical experts create clinical text to overcome the limitation of well-defined and schema driven information systems. The objective of this research work is to create a framework, which can convert clinical text into causal knowledge. \\ Methods: A practical approach based on term expansion, phrase generation, BERT based phrase embedding and semantic matching, semantic enrichment, expert verification, and model evolution has been used to construct a comprehensive causality mining framework. This active transfer learning based framework along with its supplementary services, is able to extract and enrich, causal relationships and their corresponding entities from  clinical text.\\ Results: The multi-model transfer learning technique when applied over multiple iterations, gains performance improvements in terms of its accuracy and recall while keeping the precision constant. We also present a comparative analysis of the presented techniques with their common alternatives, which demonstrate the correctness of our approach and its ability to capture most causal relationships.\\ Conclusion: The presented framework has provided cutting-edge results in the healthcare domain. However, the framework can be tweaked to provide causality detection in other domains, as well. \\ Significance: The presented framework is generic enough to be utilized in any domain, healthcare services can gain massive benefits due to the voluminous and various nature of its data. This causal knowledge extraction framework can be used to summarize clinical text, create personas, discover medical knowledge, and provide evidence to clinical decision making.",368
" Content based websites such as Quora, Reddit, StackOverflow are primarily used for seeking genuine answers to questions. People from different domains put up their questions and educators or people knowledgeable in a certain field answer them. One major impediment to a plain sailing execution of information exchange is the proliferation of toxic comments. The key challenge is to weed out such toxic comments termed as Insincere Questions. An Insincere Question is designated as a comment intended to make a statement than to look for genuine answers.  An Insincere Question is characterised by:        This major class of problem pertains to Text classification which has been a benchmark problem of evaluating various research advancements in natural language processing. While traditional machine learning algorithms such as naive bayes, logistic regression and decision trees can be rightfully applied to this problem, they suffer with major impediments in their constructs. Vanilla RNNs, Gated Recurrent Unit and Long Short Term Memory Networks replaced their usage as the new state of the art. Even though LSTMs and GRUs performed well, they failed to capture the dependencies in long range sentences. Now with the advent of Transfer Learning, Language model pre-training has proven to be useful in learning universal language representations. Researchers in the field are developing new and better language models at an unprecedented speed. Applying these new state of the art models could improve current methods and replace manual labeling tasks for text classification, but also find widespread application in similar other fields, such as machine translation and question answering. In this paper, we test this by applying new transformer models from the BERT-family to improve the current method of binary text classification in the context of Insincere Questions Classification. We make use of the Quora Insincere Questions Classification dataset  for this purpose We find that all of our models achieve remarkable results in classifying the given  data , with BERT achieving the best results compared to RoBERTa, DistilBERT, and ALBERT. This indicates that the models are well equipped to take over tasks that researchers have previously solved in less optimal ways.    
","  The internet today has become an unrivalled source of information where people converse on content based websites such as Quora, Reddit, StackOverflow and Twitter asking doubts and sharing knowledge with the world. A major arising problem with such websites is the proliferation of toxic comments or instances of insincerity wherein the users instead of maintaining a sincere motive indulge in spreading toxic and divisive content. The straightforward course of action in confronting this situation is detecting such content beforehand and preventing it from subsisting online. In recent times Transfer Learning in Natural Language Processing has seen an unprecedented growth. Today with the existence of transformers and various state of the art innovations, a tremendous growth has been made in various NLP domains. The introduction of BERT has caused quite a stir in the NLP community. As mentioned, when published, BERT dominated performance benchmarks and thereby inspired many other authors to experiment with it and publish similar models. This led to the development of a whole BERT-family, each member being specialized on a different task. In this paper we solve the Insincere Questions Classification problem by fine tuning four cutting age models viz BERT, RoBERTa, DistilBERT and ALBERT",369
"     The term  measures how much energy the reader will have to expend in order to understand a writing at optimal speed and find interesting. Readability measuring formulas, such as Automated Readability Index  , Flesch Reading Ease , and Dale閳ユ弲hall formula  calculate a score that estimates the grade level or years of education of a reader based on the U.S. education system, which is illustrated in Figure . These formulas are still used in many widely known commercial readability measuring tools such as  and . This measurement plays a significant role in many places, such as education, health care, and government . Government organizations use it to ensure that the official texts meet a minimum readability requirement. For instance, the Department of Insurance at Texas has a requirement that all insurance policy documents have a Flesch Reading Ease  score of 40 or higher, which translates to the reading level of a first-year undergraduate student based on the U.S. education system. A legal document which is hard to read can lead someone to sign a contract without understanding what they are agreeing to. Another common usage area is the healthcare sector to ensure the proper readability of the care and treatment documents . Better readability will attract visitors or readers of different websites or blogs, whereas poor readability may decrease the number of readers . Readability measures are also often used to assess the financial documents such as annual reports of a company閳ユ獨 economic performance so that the information is more transparent to the reader .  is a disorder that causes difficulties with skills associated with learning, namely reading and writing, which affects up to 20\% of the general population. Readability formulas have been applied to measure the difficulty of reading texts for people with dyslexia .   The scores from readability formulas have been generally found to correlate highly with the actual readability of a text written in the English language. The adaptation of readability formulas to no-English texts is not straightforward. Measuring readability is also essential for every non-English language, but not all of the readability formulas mentioned above are language-independent. These formulas require some resources like a 3000-word list, which is easily understandable by fourth-grade American students, syllable counting dictionary, stemmer, lemmatizer etc. Resource availability for Natural Language Processing  research is an obstacle for some low-resource-languages . In this paper, we aim to develop a readability analysis tool for the Bengali Language. Bengali is the native language of Bangladesh, also used in India  and has approximately 230 million native speakers. Despite being the  most spoken language in the world, Bengali suffers from a lack of fundamental resources for NLP. For a low resource language like Bengali, the research in this area so far can be considered to be narrow and sometimes incorrect.  tried to adapt the formula-based approaches used for the English language. Unfortunately, it isn't straightforward as these formulas are developed for U.S. based education system and which predicts U.S. grade level of the reader. Since the Bangladeshi education system and grade levels are different from U.S., therefore, the mapping is faulty and led to incorrect results. There is a strong relationship between reading skills and human cognition, which varies depending on different age groups . Therefore, to eliminate this incompatibility, in this paper, we map grade level to different age groups to present age-to-age comparison. Moreover,  used traditional machine learning models to address this task on a very small scale dataset, which isn't publicly available. There are readability analysis tools available for  ,  ,  , and   language. Unfortunately, no such tool is available for Bengali language that can validate the readability of a text. On the other hand, there is no large-scale human annotated readability analysis dataset available to train supervised neural models for this extremely low-resource language. Our main contributions are summarized as follows:       
","  Determining the readability of a text is the first step to its simplification. In this paper, we present a readability analysis tool capable of analyzing text written in the Bengali language to provide in-depth information on its readability and complexity. Despite being the $7^{th}$ most spoken language in the world with  million native speakers, Bengali suffers from a lack of fundamental resources for natural language processing. Readability related research of the Bengali language so far can be considered to be narrow and sometimes faulty due to the lack of resources.  Therefore, we correctly adopt document-level readability formulas traditionally used for U.S. based education system to the Bengali language with a proper age-to-age comparison. Due to the unavailability of large-scale human-annotated corpora, we further divide the document-level task into sentence-level and experiment with neural architectures, which will serve as a baseline for the future works of Bengali readability prediction. During the process, we present several human-annotated corpora and dictionaries such as a document-level dataset comprising  documents with 12 different grade levels,  a large-scale sentence-level dataset comprising more than  sentences with simple and complex labels, a consonant conjunct count algorithm and a corpus of  words to validate the effectiveness of the algorithm, a list of  easy words, and an updated pronunciation dictionary with more than  words. These resources can be useful for several other tasks of this low-resource language. \footnote{We make our Code \& Dataset publicly available at \url{https://github.com/tafseer-nayeem/BengaliReadability} for reproduciblity.}",370
" A typical text retrieval system uses a multi-stage retrieval pipeline, where documents flow through a series of ``funnels``  that discard unpromising candidates using increasingly more complex and accurate ranking components. These systems have been traditionally relying on simple term-matching techniques to generate an initial list of candidates . In that, retrieval performance is adversely affected by a mismatch between query and document terms, which is known as a vocabulary gap problem.  The vocabulary gap can be mitigated by learning  dense or sparse representations for effective first-stage retrieval. Despite recent success in achieving this objective , existing studies have have at least one of the following flaws:        brute-force ).       This motivated us to develop a carefully-tuned traditional, i.e., non-neural, system, which we evaluated in {the MS MARCO document ranking task} .  Our objectives are:        Our submission  achieved MRR=0.298 on the hidden validation set  and outperformed all other traditional systems. It was the first system  that outstripped several neural baselines.  According to our own evaluation  on TREC NIST data , our system achieves NDCG@10 equal to 0.584 and 0.558 on  2019 and 2020 queries, respectively. It, thus, outperforms a tuned BM25 system by 6-7\%: NDCG@10 is equal to 0.544 and 0.524 on 2019 and 2020 queries, respectively. {We posted two notebooks to reproduce results}:      {The first notebook} reproduces all steps necessary to download the data, preprocess it, and train all the models.     {The second notebook} operates on preprocessed data in \flexneuart\   JSONL format. It does not require running MGIZA to generate IBM Model 1 .   
"," This short document describes a traditional IR  system that achieved MRR@100 equal to 0.298 on the MS MARCO Document Ranking leaderboard . Although inferior to most BERT-based models,  it outperformed several neural runs ,  including two submissions that used a large pretrained Transformer model for re-ranking. We provide software and data to reproduce our results.",371
" Figurative language, or a figure of speech , is phrasing that goes beyond the literal meaning of words to get a message or point across. Writers and poets use figurative language to build imagery and elicit aesthetic experiences. %A handful of figurative types help make foreign concepts familiar and graspable, including but not limited to simile , metaphor , irony, etc. %.  In computational linguistics, figurative language processing  has long been an interesting research topic, including both detection  and generation tasks . [htbp] {!}{ {|c|c|l|}   \multicolumn{3}{|c|}{Writing Polishment with Simile} \\ {|c|}{Before} & \multicolumn{2}{l|}{[c]{@{}l@{}}Looking at his bloodthirsty eyes, everyone felt\\ horrible and couldn't help but step back.} \\ {|c|}{After} & \multicolumn{2}{l|}{[c]{@{}l@{}}Looking at his bloodthirsty eyes, everyone felt\\ horrible \underline{as if they were being stared at by a }\\ \underline{serpent}, and couldn't help but step back.} \\   \multicolumn{3}{|c|}{Other Figurative Language Generation} \\   Task & Status & \multicolumn{1}{c|}{Text} \\ {*}{Metaphor} & After & She devoured his novels. \\ l@{}}Non-\\ ironic & [c]{@{}l@{}}Tried to leave town and my phone died, \\ what a failure. \\   \multirow{-2}{*}{[c]{@{}c@{}}Irony} & Ironic & [c]{@{}l@{}}\underline{Nice} to leave town and my phone died, \\ \underline{definition of success.} \\ , none of existing work has ever investigated simile generation given a plain text, which is indispensable for amplifying writing with similes. % very few works have explored simile generation in the field of FLP polishing text with simile interpolation. % interpolation for text polishment.  Although sequence-to-sequence models work well for story generation , irony generation , or metaphor and personification generation , it is non-trivial for these models to generate proper and creative simile for a given text. In particular, writing polishment with similes is a unique task because it requires to together address the challenges listed below:%that together make writing polishment with similes a unique task.    %Apparently, one of the biggest challenge for most text polishing studies is data insufficiency. Either it's the lack of labelled data for continuous figurative language generation, such as metaphor  and personification , or the lack of parallel data for style transfer on text attributes such as sentiment, formality , offensivity , political slant  and irony  etc. Apart from expensive human annotation, previous works either adopted semi-supervised methods to construct new datasets, or applied complex unsupervised approaches to deal with this issue. In contrast, obtaining simile data is relatively cheap, since it can be identified with clear patterns such as the occurrence of connecting words as ``like''. Even better, there are a rich dozen of simile patterns in Chinese , which further facilitates the automatic construction of simile data.   %In the field of figurative language processing however, while the detection tasks have been thoroughly explored , very few studies actually focused on the generation task , almost all existing works are limited by the lack of annotated or parallel data to a great deal.    %Despite its simple form, simile plays a vital role for written narratives to be attractive. A creative and coherent simile that occurs at proper position of a narration will greatly improve the reading experience . However, existing work on metaphor generation are mostly non-contextual and only focus on continuous generation.  developed a web-driven approach for simile generation within a single sentence.  only focused on generating unconditional verb-oriented metaphors, which requires a pair of fit word  and target word as input. Although  studied the contextual metaphorical generation of poetry, the generation is still in continuous manner, which always generates next lines given previous lines. Hence, none of these works shed lights on polishing plain narrations with both the simile generation and positioning.     %Beyond all that, current researches on text editing or style transfer mostly focused on single sentence rephrasing towards various text attributes such as sentiment, formality , offensivity , political slant  and irony  etc. . Most existing approaches could not be directly applied in the case of narration simile polishment, since our objective is not to rephrase given sentences on the whole. Rather, the proposed task is to generate similes only at proper locations without changing anything from the original writing.   %Meanwhile, there has been great progress in neural generation approaches in recent years due to rapid growth of model architectures as well as available corpus , resulting in various creative applications, from chatbots , to livebot commenting , to streamlined video captioning , etc. Unfortunately, in the field of figurative language processing, despite well-studied detection tasks , the lack of labelled or parallel data limits the research on generation tasks to a great deal .  To this end, we propose a new task of Writing Polishment with Simile 閳ユ敄o firstly decide where to put a simile within plain input text, then figure out what content to generate as a coherent simile. To facilitate our research, we propose a new Chinese Simile  dataset, which contains roughly 5.5 million similes in fictional contexts. %from Chinese online fictions with 92\% simile extraction precision. %For model design, We also set up a benchmark model Locate\&Gen to validate the feasibility and potentials of WPS task. Locate\&Gen model is a two-stage biased generation model upon the framework of transformer encoder-decoder . At the first step, it locates a pointer position for simile interpolation, and then generates a location-specific simile using a novel insertion bias. The two-stage design allows both automatic and semi-automatic inference modes to assist writing polishment flexibly. To summarize, our contributions are three-folded:  } a large-scale Chinese Simile  dataset for public research, which contains millions of similes with contexts extracted from Chinese online fictions.   %Hence, in contrast with the romance of open machine story/metaphor generation discussed above, SP aims at practically improving narrative writings in the hands of human writers. It also poses several new challenges for AI as follows: 1) There are no existing large-scale figurative language corpus, and its annotation is fairly difficult since annotators need to be familiar with writing techniques. 2) Besides generating coherent simile that must be faithful to the original context, the model should also learn to put the generated ingredients at proper position. In this work, we address the SP problem and propose two-staged Locate\&Gen model. In order to obtain large-scale simile dataset, we adopt Chinese simile patterns\footnote[1]{Different from English, there are dozens of patterns for simile expressions in Chinese like ""婵傝棄鍎"", ""閹鎶"", ""娴犲じ缍"", ""鐎规稑顩"", ""娣囥劎鍔"", ""婵″倽瀚"", ""閻樼懓顩"", etc., all standing for the meaning of ""as if"". Also note that similes and metaphors can be exchanged easily by exchanging simile patterns with verbs such as ""Be"" or ""Become"".} to automatically extract sentences containing similes. 
"," A simile is a figure of speech that directly makes a comparison, showing similarities between two different things, e.g. ``Reading papers can be dull sometimes, like watching grass grow"". Human writers often interpolate appropriate similes into proper locations of the plain text to vivify their writings. However, none of existing work has explored neural simile interpolation, including both locating and generation. In this paper, we propose a new task of Writing Polishment with Simile  to investigate whether machines are able to polish texts with similes as we human do. Accordingly, we design a two-staged Locate\&Gen model based on transformer architecture. Our model firstly locates where the simile interpolation should happen, and then generates a location-specific simile. We also release a large-scale Chinese Simile  dataset containing 5 million similes with context. The experimental results demonstrate the feasibility of WPS task and shed light on the future research directions towards better automatic text polishment.%with model achieving 76.9\% simile positioning accuracy and decent performance on generation metrics as well as human evaluations.  %Current studies on figurative language generation are either non-contextual or focus only on continuous left-to-right generation manner, which is impractical for polishing written narratives with simile embellishments which may take place at any position of the original content. In this paper, we propose Simile Positioning \& Generation  task闁炽儲鏁刼 first decide a proper insertion position of simile then generate coherent simile content in plain narrations, to investigate whether computational methods are able to refine written narratives with similes as human novelists do. We introduce a large-scale Chinese Simile  dataset, which contains millions of similes with contexts extracted automatically from Chinese online fictions of various types. We establish baseline Insert\&Gen model performances based on SOTA transformer architecture. The experimental results demonstrate the feasibility of SPG task with model achieving 76.9\% accuracy on simile positioning and decent performance on generation metrics and human evaluations.  %Human author is capable of applying figurative language to bring their stories to life, so that readers ""devour"" his vivid narratives. Current researches mainly focused on the continuous story generation  as well as global text editing or style transfer, topics around machines learning to apply figurative techniques for writing is seldom discussed. In this paper, we propose a new task of Simile Positioning\&Generation , which aims to decorate plain narrative sentences with similes at appropriate positions, while being faithful to the original writings. We introduce a large-scale Chinese Simile  dataset, containing millions of contextual similes extracted automatically from Chinese online fictions of various types. We establish baseline Locate\&Gen model performances based on SOTA transformer architecture. The experimental results demonstrate the feasibility of SPG task with model achieving around 80\% accuracy on simile positioning and decent performance on generation metrics and human evaluations.",372
"  A contract is a legally binding agreement that recognizes and governs the rights and duties of the parties to the agreement. Correctly composing contracts is crucial to ensure its legal validity. In many real-world scenarios, a standard contract is prepared by {, which may severely impair the legal validity of the contract.  Contract review is widely used by companies to check contract inconsistencies. However, contract review is labor-intensive and costly. Big companies have to hire tens of thousands of lawyers to conduct contract review, and it is estimated that Fortune Global  and Fortune  companies spend about 35281299,62194.05\%90.90\%$.  Our contributions are summarized as follows:   We formulate the {  framework to address the CIC problem. In PBR, we propose a  that extends the Transformer encoder architecture to efficiently model meaningless blanks.   We collected and labeled  a large-scale Chinese contract corpus for CIC. The experimental results show the promising performance of our PBR method.     
"," Contract consistency is important in ensuring the legal validity of the contract. In many scenarios, a contract is written by filling the blanks in a precompiled form. Due to carelessness, two blanks that should be filled with the same  content may be incorrectly filled with different  content. This will result in the issue of {  problem, and design an end-to-end framework, called { to address the challenge of modeling meaningless blanks. BlankCoder adopts a two-stage attention mechanism that adequately associates a meaningless blank with its relevant descriptions while avoiding the incorporation of irrelevant context words. Experiments conducted on real-world datasets show the promising performance of our method with a balanced accuracy of $94.05\%$ and an F1 score of $90.90\%$ in the CIC problem.",373
"  Building a human-like open-domain conversational agent  has been one of the milestones in artificial intelligence . Early conversational agents are primarily based on rules , e.g., Eliza , the first CA developed in 60's, simulates a Rogerian psychotherapist based on hand-crafted pattern matching rules. In recent years, with the advancement of data-driven neural networks, neural open-domain conversational models are becoming dominant .  Recent efforts in open-domain neural conversational models are primarily aiming to improve the response diversity  and endowing responses with knowledge , personality , emotion  and empathy .  All the efforts mentioned above are focusing on models that passively respond to user messages. However, in many real-world scenarios, e.g., conversational recommendation, psychotherapy and education, conversational agents are required to actively lead the conversation by smoothly changing the conversation topic to a designated one. For example, during a casual conversation, the agent may actively lead the user to a specific product or service that the agent wants to introduce and recommend.  In this paper, we follow the line of research in  and study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. As illustrated in Figure , given a target keyword ``juice"" and a random starting keyword ``comics"", the agent is required to converse with the user in multiple exchanges and lead the conversation to ``juice"". The challenge of this problem lies in how to balance the tradeoff between maximizing keyword transition smoothness and minimizing the number of turns taken to reach the target. On the one hand, passively responding to the user solely based on the conversation context would achieve high smoothness but may take many turns to reach the target, but on the other hand, directly jumping to the target word by ignoring the conversation context would minimize the number of turns but produce non-smooth keyword transitions.   proposed to break down the problem into two sub-problems: next-turn keyword selection and keyword-augmented response retrieval.  proposed a next-turn keyword predictor and a rule-based keyword selection strategy to solve the first sub-problem, allowing the agent to know what is the next keyword to talk about given the conversation history and the target keyword. In addition,  proposed a keyword-augmented response retrieval model to solve the second sub-problem, allowing the agent to produce a response that is relevant to the selected keyword.    However, there are two major limitations in existing studies . First, the training and evaluation datasets for next-turn keyword prediction are directly extracted from conversations without human annotations, thus, the majority of the ground-truth keyword transitions are noisy and have low correlations with human judgements. As illustrated in Figure , only a few keyword transitions in a conversation are considered relevant. In fact, in our human annotation studies of over 600 keyword transitions, we found that around 70\% of keyword transitions in the next-turn keyword prediction datasets are rated as not relevant, which renders the trained next-turn keyword predictor in existing studies less reliable.  Second, the rule-based keyword selection strategy primarily leverages the cosine similarity between word embeddings to select keywords that are closer to the target keyword. Word embeddings are trained based on the distributional hypothesis that words that have similar contexts have similar meanings, which may not reflect how humans relate words in conversational turn-taking.  In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs  for both next-turn keyword selection and keyword-augmented response retrieval. Humans rely on commonsense to reason, and commonsense reasoning plays an important role in the cognitive process of conversational turn-taking . Relying on a CKG for keyword transition would allow the agent to select a more target-related keyword for the next-turn.  Moreover, we leverage commonsense triplets from the CKG using Graph Neural Networks  for both next-turn keyword prediction and keyword-augmented response retrieval to achieve more accurate predictions.   In summary, our contributions are as follows:       
"," We study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. Solving this problem enables the application of conversational agents in many real-world scenarios, e.g., recommendation and psychotherapy. The dominant paradigm for tackling this problem is to 1) train a next-turn keyword classifier, and 2) train a keyword-augmented response retrieval model. However, existing approaches in this paradigm have two limitations: 1) the training and evaluation datasets for next-turn keyword classification are directly extracted from conversations without human annotations, thus, they are noisy and have low correlation with human judgements, and 2) during keyword transition, the agents solely rely on the similarities between word embeddings to move closer to the target keyword, which may not reflect how humans converse. In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs  for both keyword transition and response retrieval. Automatic evaluations suggest that commonsense improves the performance of both next-turn keyword prediction and keyword-augmented response retrieval. In addition, both self-play and human evaluations show that our model produces responses with smoother keyword transition and reaches the target keyword faster than competitive baselines.",374
"   Despite of remarkable progress made in NMT recently , most NMT systems are still prone to translation errors caused by noisy input sequences. One common type of input noise is homophone noise, where words or characters are mis-recognized as others with same or similar pronunciation in ASR or input systems for non-phonetic languages , as illustrated by the example in Table.   Previous works suggest that incorporating phonetic embeddings into NMT and augmenting training data with adversarial examples with injected homophone noise would alleviate this issue. Intuitively, humans usually have no trouble in disambiguating sentences corrupted with moderate homophone noise via context and syllable information. We propose a human-inspired robust NMT framework tailored to homophone noise for Chinese-English translation, which is composed of a homophone noise detector  and a syllable-aware NMT  model.  []  \toprule[1pt] Clean Input~&~{UTF8}{gbsn}瀵よ桨绔撮幍鐏忓繐顒    \\ Output of NMT~&~build a primary school \\ {3pt}{3pt} Noisy Input~&~{UTF8}{gbsn}瀵ょuline{鐠佺晽閹电亸蹇擃劅  \\  Output of NMT~&~suggest a primary school \\  {3pt}{3pt} Mixed Transcript~&~{UTF8}{gbsn}瀵ょuline{yi}閹电亸蹇擃劅  \\  Output of Ours~&~build a primary school\\  {gbsn}鐠佺敍end{CJK}''  in the noisy input is a homophone corresponding to the original character ``{UTF8}{gbsn}娑''  in the clean input. The erroneous character ``{UTF8}{gbsn}鐠佺敍end{CJK}'' is replaced with its Chinese Pinyin ``yi'' in the mixed transcript which enables NMT to translate correctly.}  %   Due to the lack of data annotated with homophone noise, we propose to train our detector on monolingual data in a self-supervised manner, where Chinese characters sequences as input and their corresponding syllables sequence as label to predict the possibility that a character is homophone noise. The identified homophone errors from a source sentence are then converted into corresponding syllables to produce a new source sequence mixed with characters and syllables. Augmenting bilingual training data with instances where original source sentences are substituted with their corresponding character-syllable-mixed sequences, we train the SANMT model to translate such unconventional inputs. To examine the effectiveness of our proposed model, we conduct extensive experiments on both artificial noisy test sets and a real-world noise test set with homophone noise in speech translation  scenario. The test set will be released soon. Our experimental results on ChineseEnglish translation clearly show that the proposed method is not only significantly superior to previous approaches in alleviating the impact of homophone noise on NMT, but also achieves a substantial improvement on the clean text. %Due to the lack of data annotated with homophone noise, we propose to train our detector on monolingual data in a self-supervised manner, where Chinese characters are automatically transformed into syllables to predict homophone noise. The identified homophone errors from a source sentence are then converted into corresponding syllables to produce a new source sequence mixed with characters and syllables. Augmenting training data with instances where original source sentences are substituted with their corresponding character-syllable-mixed sequences, we train the SANMT model to translate such unconventional inputs. To examine the effectiveness of our proposed model, we conduct extensive experiments on both artificial noisy test sets and a real-world noise test set with homophone noise in speech translation  scenario. The test set will be released soon. Our experimental results on ChineseEnglish translation clearly show that the proposed method is not only significantly superior to previous approaches in alleviating the impact of homophone noise on NMT, but also achieves a substantial improvement on the clean text.    
"," In this paper, we propose a robust neural machine translation  framework. The framework consists of a homophone noise detector and a syllable-aware NMT model to homophone errors. The detector identifies potential homophone errors in a textual sentence and converts them into syllables to form a mixed sequence that is then fed into the syllable-aware NMT. Extensive experiments on Chinese$\rightarrow$English translation demonstrate that our proposed method not only significantly outperforms baselines on noisy test sets with homophone noise, but also achieves a substantial improvement on clean text.",375
" In recent years, there has been a dramatic surge in the adoption of voice assistants such as Amazon Alexa, Apple Siri, and Google Assistant. Customers use them for a variety of tasks such as playing music and online shopping.  These voice assistants are built on complex Spoken Language Understanding  systems that are typically too large to store on an edge device such as a mobile phone or a smart speaker. Hence, user traffic is routed through a cloud server to process requests. This has led to privacy concerns and fueled the push for tiny AI and edge processing, where the user requests are processed on the device itself.   Traditional SLU systems consist of a two-stage pipeline, an Automatic Speech Recognition  component that processes customer speech and generates a text transcription , followed by a Natural Language Understanding  component that maps the transcription to an actionable hypothesis consisting of intents and slots . An end-to-end  system that goes directly from speech to the hypothesis would help make the SLU system smaller and faster, allowing it to be stored on an edge device. It could potentially also be better optimized than a pipeline since it eliminates cascading errors.  However, E2E systems are not used in practice because they have some key issues. These systems are hard to build since they consist of large neural components such as transformers and require massive amounts of E2E training data. They also don't make use of the vastly available training data for the ASR and NLU components that could be used to enhance their performance, because the examples in these datasets may not be aligned to create an E2E training sample. Another issue is feature expansion, a scenario where a new domain, with new intents and slots, is added to the voice assistant's capabilities. Here, developers typically only have access to some synthetically generated text-hypothesis examples. Speech data isn't readily available and it is very expensive to collect. E2E models thus fail as they require lots of new audio and hypothesis data to learn this new domain.  In this work, we build an E2E model that mitigates these issues using transfer learning. We call it the Audio-Text All-Task  Model. AT-AT is an E2E transformer-based model that is jointly trained on multiple audio-to-text and text-to-text tasks. Examples of these tasks include speech recognition , hypothesis prediction from speech , masked LM prediction , and hypothesis prediction from text . Our model achieves this by converting data from all these tasks into a single audio-to-text or text-to-text format. Figure shows this joint training phase in detail. Our findings indicate that there is significant knowledge transfer taking place from multiple tasks, which in turn helps in downstream model performance. We see that the AT-AT pretrained model shows improved performance on SLU hypothesis prediction on internal data collected from Alexa traffic. We also report state-of-the-art results on two public datasets: FluentSpeech , and SNIPS Audio .   Furthermore, since our model contains a text encoder, it can consume both audio and text inputs to generate a target sequence. By jointly training on both audio-to-text and text-to-text tasks, we hypothesize that this model learns a shared representation for audio and text inputs. This allows us to simply train on new text-to-text data and get audio-to-text performance for free, giving us a way to do E2E hypothesis prediction in a zero-shot fashion during feature expansion. We test this approach on an internal dataset from Alexa traffic, and an external dataset, Facebook TOP . Since TOP consists of only text data, we collected speech data for the test split using an internal tool at Amazon. We will soon release this dataset.  In summary, our contributions are as follows.        
"," Voice Assistants such as Alexa, Siri, and Google Assistant typically use a two-stage Spoken Language Understanding pipeline; first, an Automatic Speech Recognition  component to process customer speech and generate text transcriptions, followed by a Natural Language Understanding  component to map transcriptions to an actionable hypothesis. An end-to-end  system that goes directly from speech to a hypothesis is a more attractive option. These systems were shown to be smaller, faster, and better optimized. However, they require massive amounts of end-to-end training data and in addition, don't take advantage of the already available ASR and NLU training data.  In this work, we propose an E2E system that is designed to jointly train on multiple speech-to-text tasks, such as ASR  and SLU , and text-to-text tasks, such as NLU . We call this the Audio-Text All-Task  Model and we show that it beats the performance of E2E models trained on individual tasks, especially ones trained on limited data. We show this result on an internal music dataset and two public datasets, FluentSpeech and SNIPS Audio, where we achieve state-of-the-art results. Since our model can process both speech and text input sequences and learn to predict a target sequence, it also allows us to do zero-shot E2E SLU by training on only text-hypothesis data  from a new domain. We evaluate this ability of our model on the Facebook TOP dataset and set a new benchmark for zeroshot E2E performance. We will soon release the audio data collected for the TOP dataset for future research.",376
"  Neural Machine Translation   has achieved state of the art in various MT systems, including rich and low resource language pairs . However, the quality of low-resource MT is quite unpretentious due to the lack of parallel data while it has achieved better results on systems of the available resource. Therefore, low-resource MT is one of the essential tasks investigated by many previous works .    Recently, some works present MT systems that have achieved remarkable results for low-resource language . Inspired by these works, we collect data from the TED Talks domain, then attempt to build multilingual MT systems from French, English-Vietnamese. Experiments demonstrate that both language pairs: French-Vietnamese and English-Vietnamese have achieved significant performance when joining the training. %  Although multilingual MT can reduce the sparse data in the shared space by using word segmentation, however, rare words still exist, evenly they are increased more if languages have a significant disparity in term vocabulary. Previous works suggested some strategies to reduce rare words such as using translation units at sub-word and character levels or generating a universal representation at the word and sentence levels . These help to downgrade the dissimilarity of tokens shared from various languages. However, these works require learning additional parameters in training, thus increasing the size of models.   Our paper presents two methods to augment the translation of rare words in the source space without modifying the architecture and model size of MT systems:  exploiting word similarity. This technique has been mentioned by previous works . They employ monolingual data or require supervised resources like a bilingual dictionary or WordNet, while we leverage relation from the multilingual space of MT systems.  Adding a scalar value to the rare word embedding in order to facilitate its translation in the training process.  %  Due to the fact that NMT tends to have bias in translating frequent words, so rare words  often have less opportunity to be considered. Our ideal is inspired by the works of .  and  proposed various solutions to urge for translation of rare words, including modification embedding in training. They only experimented with recurrent neural networks  while our work uses the state-of-the-art transformer architecture.  transforms the word embedding of a token into the universal space, and they learn plus parameters while our method does not.  We apply our strategies in our fine-tuning processes, and we show substantial improvements of the systems after some epochs only.    Monolingual data are widely used in NMT to augment data for low-resource NMT systems . Back-translation  is known as the most popular technique in exploiting target-side monolingual data to enhance the translation systems while the self-learning method  focuses on utilizing source-side monolingual data. Otherwise, the dual-learning strategy  also suggests using both source- and target-side monolingual data to tackle this problem. Our work investigates the self-learning method  on the low-resource multilingual NMT systems specifically related to Vietnamese. Besides, monolingual data are also leveraged in unsupervised or zero-shot translation.  % learn the lexical relative between one token on a source language and the other once from another source language without modifying the system architecture as well as the model size. We also do not use any additional resources in our systems.   The main contributions of our work are:   \setlength{     In section 2, we review the transformer architecture used for our experiments. The brief of multilingual translation is shown in section 3. Section 4 presents our methods to deal with rare words in multilingual translation scenarios. The exploitation of monolingual data for low-resource multilingual MT is discussed in section 5. Our results are described in section 6, and related work is shown in section 7. Finally, the paper ends with conclusions and future work. % 
"," % Prior works have demonstrated that a low-resource language pair can be benefited from a multilingual machine translation  system which relies on the jointly training many language pairs. In this paper, we propose two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese,  English-Vietnamese. The first strategy learns  dynamically word similarity of tokens in the shared space among source languages whilst the other one augments the translation ability of rare words through updating their embeddings during the training. In addition, we attempt to leverage monolingual data which is generated from multilingual MT to reinforce synthetic parallel in the data sparsity situation. We show that significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and release datasets for the research community.  Prior works have demonstrated that a low-resource language pair can benefit from multilingual machine translation  systems, which rely on many language pairs' joint training. This paper proposes two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese and English-Vietnamese. The first strategy is about dynamical learning word similarity of tokens in the shared space among source languages while another one attempts to augment the translation ability of rare words through updating their embeddings during the training. Besides, we leverage monolingual data for multilingual MT systems to increase the amount of synthetic parallel corpora while dealing with the data sparsity problem. We have shown significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and released our datasets for the research community.",377
" % Fabian: Describing what it is Entity linking  is the task of mapping entity mentions in text documents to standard entities in a given knowledge base. For example, the word ``Paris'' is : It can refer either to the capital of France or to a hero of Greek mythology. Now given the text ``Paris is the son of King Priam'', the goal is to determine that, in this sentence, the word refers to the Greek hero, and to link the word to the corresponding entity in a knowledge base such as YAGO  or DBpedia .  %Intriguingly, the Greek hero also goes by the name of ``Alexander''. Thus, the words ``Paris'' and ``Alexander'' are , and if they both refer to the Greek hero in some input text, they both have to be linked to the same entity in the knowledge base.  % Fabian: Describing why it's important In the biomedical domain, entity linking maps mentions of diseases, drugs, and measures to normalized entities in standard vocabularies. It is an important ingredient for automation in medical practice, research, and public health. Different names of the same entities in Hospital Information Systems seriously hinder the integration and use of medical data. If a medication appears with different names, researchers cannot study its impact, and patients may erroneously be prescribed the same medication twice.   % Fabian: Describing why it's difficult The particular challenge of biomedical entity linking is not the ambiguity: a word usually refers to only a single entity. Rather, the challenge is that the surface forms vary markedly, due to abbreviations, morphological variations, synonymous words, and different word orderings.  For example, ``Diabetes Mellitus, Type 2'' is also written as ``DM2'' and ``lung cancer'' is also known as ``lung neoplasm malignant''. In fact, the surface forms vary so much that all the possible expressions of an entity cannot be known upfront. This means that standard disambiguation systems cannot be applied in our scenario, because they assume that all forms of an entity are known. %, and thus they cannot be applied in our scenario.  One may think that variation in surface forms is not such a big problem, as long as all variations  of an entity are sufficiently close to its canonical form. Yet, this is not the case. For example, the phrase ""decreases in hemoglobin"" could refer to at least 4 different entities in MedDRA, which all look alike:  ""changes in hemoglobin"", ""increase in hematocrit"", ""haemoglobin decreased"", and ""decreases in platelets"". In addition, biomedical entity linking cannot rely on external resources such as  alias tables, entity descriptions, or entity co-occurrence, which are often used in classical entity linking settings.   % Fabian: what has been done For this reason, entity linking approaches have been developed particularly for biomedical entity linking. Many methods use deep learning: the work of  casts biomedical entity linking as a ranking problem,  leveraging convolutional neural networks .  More recently, the introduction of BERT has advanced the performance of many NLP tasks, including in the biomedical domain .  BERT creates rich pre-trained representations on unlabeled data and achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. However, considering the number of parameters of pre-trained BERT models,  the improvements brought by fine-tuning them come with a heavy computational cost and memory footprint.  This is a problem for energy efficiency, for smaller organizations, or in poorer countries.  In this paper, we introduce a very lightweight model that achieves a performance statistically indistinguishable from the state-of-the-art BERT-based models. The central idea is to use an alignment layer with an attention mechanism,  which can capture the similarity and difference of corresponding parts between candidate and mention names. Our model is 23x smaller and 6.4x faster than BERT-based models on average; and more than twice smaller and faster than the lightweight BERT models. Yet, as we show, our model achieves comparable performance on all standard benchmarks. Further, we can show that adding more complexity to our model is not necessary: the entity-mention priors, the context around the mention, or the coherence of extracted entities  do not improve the results any further. \footnote{All data and code are available at  \url{https://github.com/tigerchen52/Biomedical-Entity-Linking}.}   
"," Biomedical entity linking aims to map biomedical mentions, such as diseases and drugs, to standard entities in a given knowledge base.  The specific challenge in this context is that the same biomedical entity can have a wide range of names,  including synonyms, morphological variations, and names with different word orderings.  Recently, BERT-based methods have advanced the state-of-the-art by allowing for rich representations of word sequences. However, they often have hundreds of millions of parameters and require heavy computing resources, which limits their applications in resource-limited scenarios. Here, we propose a lightweight neural method for biomedical entity linking, which needs just a fraction of the parameters of a BERT model and much less computing resources.  Our method uses a simple alignment layer with attention mechanisms to capture the variations between mention and entity names. Yet, we show that our model is competitive with previous work on standard evaluation benchmarks.",378
" Although deep neural networks have recently been contributing to state-of-the-art advances in various areas , %in NLP problems ,  such black-box models may not be deemed reliable in situations where safety needs to be guaranteed, such as legal judgment prediction and medical diagnosis. Interpretable deep neural networks are a promising way to increase the reliability of neural models. To this end, extractive rationales, i.e., subsets of features of instances on which models rely for their predictions on the instances, can be used as evidence for humans to decide whether or not to trust a predicted result and, more generally, to trust a~model.   Previous works mainly use selector-predictor types of neural models to provide extractive rationales, i.e., models composed of two modules:  a selector that selects a subset of important features, and  a  predictor that makes a prediction based solely on the selected features. For example,  and  use a selector network to calculate a selection probability for each token in a sequence, then sample a set of tokens that is exclusively passed to the predictor. %The supervision is solely on the answer given by the prediction. %One then calculates a loss between the result given by the predictor and the ground-truth answer.  An additional typical desideratum in natural language pro\-cessing  tasks is that the selected tokens form a semantically fluent rationale. To achieve this,  added a non-differential regularizer that encourages any two adjacent tokens to be simultaneously selected or unselected. %The selector and predictor are jointly trained in a REINFORCE-style manner [cite Williams 92] because the sampling process and the regularizer are not differentiable.  further improved the quality of the rationales by using a Hard Kuma regularizer that also encourages any two adjacent tokens to be selected or unselected together. %, which is differentiable.    One drawback of previous works is that the learning signal for both the selector and the predictor comes mainly from comparing the prediction of the selector-predictor model with the ground-truth answer. %, while the predictor tells the selector to what extent the selected features contribute to the prediction, it does not directly tell the selector what kind of features are still missing or over-selected for a correct prediction.  Therefore, the exploration space to get to the correct rationale is large, decreasing the chances of converging to the optimal rationales and predictions.  Moreover, in NLP applications, the regularizers commonly used for achieving fluency of rationales treat all adjacent token pairs in the same way. This often leads to the selection of unnecessary tokens due to their adjacency to informative~ones. %Intuitively, if two tokens frequently occur adjacently, then they are more likely to be simultaneously selected or unselected. These important adjacent token pairs should receive more priority by the regularizer.   In this work, we first propose an alternative method to rationalize the predictions of a neural model. Our method aims to squeeze more information from the predictor in order to guide the selector in selecting the rationales. Our method trains two models: a ``guider"" model that solves the task at hand in an accurate but black-box manner, and a selector-predictor model that solves the task while also providing rationales. We use an adversarial-based method to encourage the final information vectors generated by the two models to encode the same information. We use an information bottleneck technique in two places: ~to encourage the features selected by the selector to be the least-but-enough features, and ~to encourage the final information vector of the guider model to also contain the least-but-enough information for the prediction.    Secondly, we propose using language models as regularizers for rationales in natural language understanding tasks. A language model  regularizer encourages rationales to be fluent subphrases, which means that the rationales are formed by consecutive tokens while avoiding unnecessary tokens to be selected simply due to their adjacency to informative tokens. %novel regularizer for the consecutiveness and semantic fluency of the rationale in NLP applications. This regularizer is based on a language model , which gives priority to important adjacent tokens so that they are simultaneously being selected.  The effectiveness of our LM-based regularizer is proved by both mathematical derivation and experiments. All the further details are given in the Appendix of the extended  paper.  Our contributions are briefly summarized as follows:         
","  Explaining the predictions of AI models is paramount in safety-critical applications, such as in legal or medical domains. One form of explanation for a prediction is an extractive rationale, i.e., a subset of features of an instance that lead the model to give its prediction on the instance.  Previous works on generating extractive rationales usually employ a two-phase model: a selector that selects the most important features  followed by a predictor that makes the prediction based exclusively on the selected features. One disadvantage of these works is that the main signal for learning to select features comes from the comparison of the answers given by the predictor and the ground-truth answers. In this work, we propose to squeeze more information from the predictor via an information calibration method. More precisely, we train two models jointly: one is a typical neural model that solves the task at hand in an accurate but black-box manner, and the other is a selector-predictor model that additionally produces a rationale for its prediction. The first model is used as a guide to the second model. We use an adversarial-based technique to calibrate the information extracted by the two models such that the difference between them is an indicator of the missed or over-selected features. In addition, for natural language tasks, we propose to use a language-model-based regularizer to encourage the extraction of fluent rationales. Experimental results on a sentiment analysis task as well as on three tasks from the legal domain show the effectiveness of our approach to rationale extraction.",379
" % background Sentence semantic matching is a fundamental Natural Language Processing~ task that tries to infer the most suitable label for a given sentence pair. For example, Natural Language Inference~ targets at classifying the input sentence pair into one of the three relations~. Paraphrase Identification~ aims at identifying whether the input sentence pair expresses the same meaning. Figure gives some examples with different semantic relations from different datasets.    % Current state As a fundamental technology, sentence semantic matching has been applied successfully into many NLP fields, e.g., information retrieval, question answering, and dialog system.  Currently, most work leverages the advancement of representation learning techniques to tackle this task.  They focus on input sentences and design different architectures to explore sentence semantics comprehensively and precisely.  Among all these methods, BERT plays an important role.  It adopts multi-layer transformers to make full use of large corpus~ for the powerful pre-trained model.  Meanwhile, two self-supervised learning tasks~ are designed to better analyze sentence semantics and capture as much information as possible.  % more citation Based on BERT, plenty of work has made a big step in sentence semantic modeling.    In fact, since relations are the predicting targets of sentence semantic matching task, most methods do not pay enough attention to the relation learning.  They just leverage annotated labels to represent relations, which are formulated as one-hot vectors.  However, these independent and meaningless one-hot vectors cannot reveal the rich semantic information and guidance of relations, which will cause an information loss.  ~ has observed that different relations among sentence pairs imply specific semantic expressions.  Taking Figure as an example, most sentence pairs with ``contradiction'' relation contain negation words~.  ``entailment'' relation often leads to exact numbers being replaced with approximates~.  ``Neutral'' relation will import some correct but irrelevant information~.  Moreover, the expressions between sentence pairs with different relations are very different.  Therefore, the comparison and contrastive learning among different relations~ can help models to learn more about the semantic information implied in the relations, which in turn helps to strengthen the sentence analysis ability of models. They should be treated as more than just meaningless one-hot vectors.   One of the solutions for better relation utilization is the embedding method inspired by Word2Vec.  Some researchers try to jointly encode the input sentences and labels in the same embedding space for better relation utilization during sentence semantic modeling.  Despite the progress they have achieved, label embedding method requires more data and parameters to achieve better utilization of relation information.  It still cannot fully explore the potential of relations due to the small number of relation categories or the lack of explicit label embedding initialization.   To this end, in this paper, we propose a novel \fullname~approach to make full use of relation information in a simple but effective way.  In concrete details, we first utilize pre-trained BERT to model semantic meanings of the input words and sentences from a global perspective.  Then, we develop a CNN-based encoder to obtain partial information~ of sentences from a local perspective.  Next, inspired by self-supervised learning methods in BERT training processing, we propose a Relation of Relation~ classification task to enhance the learning ability of \shortname~for the implicit common features corresponding to different relations.    Moreover, a triplet loss is used to constrain the model, so that the intra-class and inter-class relations are analyzed better.   Along this line, input sentence pairs with the same relations will be represented much closer and vice versa further apart.  Relation information is properly integrated into sentence pair modeling processing, which is in favor of tackling the above challenges and improving the model performance.  Extensive evaluations of two sentence semantic matching tasks  demonstrate the effectiveness of our proposed \shortname~and its advantages over state-of-the-art sentence semantic matching baselines.    
"," 	% background 	Sentence semantic matching is one of the fundamental tasks in natural language processing, which requires an agent to determine the semantic relation among input sentences.  	% current state 	Recently, deep neural networks have achieved impressive performance in this area, especially BERT.  	% problem 	Despite their effectiveness, most of these models treat output labels as meaningless one-hot vectors, underestimating the semantic information and guidance of relations that these labels reveal, especially for tasks with a small number of labels.  	% solution 	To address this problem, we propose a \fullname~for sentence semantic matching. 	 	Specifically, we first employ BERT to encode the input sentences from a global perspective. 	Then a CNN-based encoder is designed to capture keywords and phrase information from a local perspective.  	To fully leverage labels for better relation information extraction, we introduce a self-supervised relation of relation classification task for guiding \shortname~to consider more about relations.  	Meanwhile, a triplet loss is employed to distinguish the intra-class and inter-class relations in a finer granularity. 	% result 	Empirical experiments on two sentence semantic matching tasks demonstrate the superiority of our proposed model.  	As a byproduct, we have released the codes to facilitate other researches.",380
" 	Discovering novel user intents is important to improve the service quality in dialogue systems. By analyzing the discovered new intents, we may find underlying user interests, which could provide business opportunities and guide the improvement direction.  	 	 	Intent discovery has attracted much attention in recent years. Many researchers regard it as an unsupervised clustering problem, and they manage to incorporate some weak supervised signals to guide the clustering process. For example,~ propose a hierarchical semantic clustering model and collect web page clicked information as implicit supervision for intent discovery.~ utilize a semantic parsing graph as extra knowledge to mine novel intents during clustering.~ benefit from the consensus predictions of multiple clustering techniques to discover similar semantic intent-wise clusters.~ cluster questions into user intent categories under the supervision of structured outputs.~ extract intent features with an autoencoder and automatically label the intents with a hierarchical clustering method. 	  	However, all of the above methods fail to leverage the prior knowledge of known intents. These methods assume that the unlabeled samples are only composed of undiscovered new intents. A more common case is that some labeled data of known intents are accessible and the unlabeled data are mixed with both known and new intents. As illustrated in Figure, we may have a few labeled samples  of known intents in advance. The remaining known and new intent samples are all unlabeled. Our goal is to find known intents and discover new intents with the prior knowledge of limited labeled data. Our previous work CDAC+ directly tackles this problem. Nevertheless, it uses pairwise similarities as weak supervised signals, which are ambiguous to distinguish a mixture of unlabeled known and new intents. Thus, the performance drops with more new intents. 	 	To summarize, there are two main difficulties in our task. On the one hand, it is challenging to effectively transfer the prior knowledge from known intents to new intents with limited labeled data. On the other hand, it is hard to construct high-quality supervised signals to learn friendly representations for clustering both unlabeled known and new intents. 	 	To solve these problems, we propose an effective method to leverage the limited prior knowledge of known intents and provide high-quality supervised signals for feature learning.  As illustrated in Figure, we firstly use the pre-trained BERT model to extract deep intent features. Then, we pre-train the model with the limited labeled data under the supervision of the softmax loss. We retain the pre-trained parameters and use the learning information to obtain well-initialized intent representations. Next, we perform clustering on the extracted intent features and estimate the cluster number   by eliminating the low-confidence clusters. 	 	As most of the training samples are unlabeled, we propose an original alignment strategy to construct high-quality pseudo-labels as supervised signals for learning discriminative intent features. For each training epoch, we firstly perform k-means on the extracted intent features, and then use the produced cluster assignments as pseudo-labels for training the neural network. However, the inconsistent assigned labels cannot be directly used as supervised signals, so we use the cluster centroids as the targets to obtain the alignment mapping between pseudo-labels in consequent epochs. Finally, we perform k-means again for inference. Benefit from the relatively consistent aligned targets, our method can inherit the history learning information and boost the clustering performance. 	 	We summarize our contributions as follows. Firstly, we propose a simple and effective method that successfully generalizes to mass of new intents and estimate the number of novel classes with limited prior knowledge of known intents. Secondly, we propose an effective alignment strategy to obtain high-quality self-supervised signals by learning discriminative features to distinguish both known and new intents. Finally, extensive experiments on two benchmark datasets show our approach yields better and more robust results than the state-of-the-art methods.  	 	
"," 		Discovering new intents is a crucial task in dialogue systems. Most existing methods are limited in transferring the prior knowledge from known intents to new intents. They also have difficulties in providing high-quality supervised signals to learn clustering-friendly features for grouping unlabeled intents. In this work, we propose an effective method, Deep Aligned Clustering, to discover new intents with the aid of the limited known intent data. Firstly, we leverage a few labeled known intent samples as prior knowledge to pre-train the model. Then, we perform k-means to produce cluster assignments as pseudo-labels. Moreover, we propose an alignment strategy to tackle the label inconsistency problem during clustering assignments. Finally, we learn the intent representations under the supervision of the aligned pseudo-labels. With an unknown number of new intents, we predict the number of intent categories by eliminating low-confidence intent-wise clusters. Extensive experiments on two benchmark datasets show that our method is more robust and achieves substantial improvements over the state-of-the-art methods. The codes are released at \url{https://github.com/thuiar/DeepAligned-Clustering}.",381
" The U.S.~NIH's precision medicine  initiative calls for designing treatment and preventative interventions considering genetic, clinical, social, behavioral, and environmental exposure variability among patients. The initiative rests on the widely understood finding that considering individual variability is critical in tailoring healthcare interventions to achieve substantial progress in reducing disease burden worldwide. Cancer was chosen as its near term focus with the eventual aim of expanding to other conditions. As the biomedical research enterprise strives to fulfill the initiative's goals, computing needs are also on the rise in drug discovery, predictive modeling for disease onset and progression, and in building NLP tools to curate information from the evidence base being generated.   [hbt]   ll@{}}     \toprule     Facet & Input\\      \midrule     Disease & Melanoma \\     Genetic variation & BRAF  \\     Demographics & 64-year-old female \\     \midrule     Disease & Gastric cancer \\     Genetic variation & ERBB2 amplification\\     Demographics & 64-year-old male\\            In a dovetailing move, the U.S.~NIST's  TREC  has been running a PM track since 2017 with a focus on cancer. The goal of the TREC-PM task is to identify the most relevant biomedical articles and clinical trials for an input patient case. Each case is composed of    a disease name,   a gene name and genetic variation type, and  demographic information . Table shows two example cases from the 2019 track. So the search is ad hoc in the sense that we have a free text input in each facet but  the    facets themselves highlight the PM related attributes that ought to characterize the retrieved documents. We believe this style of faceted retrieval is going to be more common across medical IR tasks for many conditions as the PM initiative continues its mission.     The vocabulary mismatch problem is a prominent issue in medical IR given the large variation in the expression of medical concepts and events. For example, in the query ``What is a potential side effect for Tymlos?'' the drug is referred by its brand name. Relevant scientific literature may contain the generic name Abaloparatide more frequently. Traditional document search engines have clear limitations on resolving   mismatch issues. The IR community has extensively explored methods to address the vocabulary mismatch problem, including query expansion based on relevance feedback, query term re-weighting, or query reconstruction by optimizing the query syntax.  Several recent studies highlight exploiting neural network models for query refinement in document retrieval  settings.   address  this issue by generating a transformed query from the initial query using a neural model.  They use reinforcement  learning  to train it where an agent  learns to reformulate the initial query to maximize the expected return  through actions . In a different approach,   use RL for sentence ranking for extractive summarization.    In this paper, building on the BERT architecture, we focus on a different hybrid document scoring and reranking setup involving three components: .~a document relevance classification model, which predicts  whether a document is relevant to the given query ; .~a keyword extraction model which spots tokens in a document that are likely to be seen in PM related queries; and .~an abstractive document summarization model that generates a pseudo-query given the document context and a facet type  via the BERT encoder-decoder setup. The keywords ) and the pseudo-query ) are together compared with the original query to generate a score. The scores from all the components are combined to rerank top   documents returned with a basic Okapi BM25 retriever from a Solr index of the corpora. %This is critical because neural document-query matching and summarization are expensive operations that cannot practically scale to the full corpus.  Our main innovation is in pivoting from the focus on queries by previous methods to emphasis on transforming candidate documents into pseudo-queries via summarization. Additionally, while generating the pseudo-query, we also let the   decoder output concept codes from biomedical terminologies that capture disease and gene names. We do this by embedding both words and concepts in a common semantic space before letting the decoder generate summaries that include concepts. Our overall architecture was evaluated using the TREC-PM datasets  with the 2019 dataset used as the test set. The results show an absolute  improvement in P@10 compared to prior best approaches while obtaining a small  gain in R-Prec. Qualitative analyses also highlight how the summarization is able to focus on document segments that are highly relevant to patient cases.  
"," Information retrieval  for precision medicine  often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patient. Other factors such as demographic attributes, comorbidities, and social determinants may also be pertinent. As such, the retrieval problem is often formulated as ad hoc search but with multiple facets  that may need to be incorporated. In this paper, we present a document reranking approach that combines neural query-document matching and text summarization toward such retrieval scenarios. Our architecture builds on the basic BERT model with three specific components for reranking: . document-query matching . keyword extraction and . facet-conditioned abstractive summarization. The outcomes of  and  are used to essentially transform a candidate document into a concise summary that can be compared with the query at hand to compute a relevance score. Component  directly generates a matching score of a candidate document for a query. The full architecture benefits from the complementary potential of document-query matching and the novel document transformation approach based on summarization along PM facets. Evaluations using NIST's TREC-PM track datasets  show that our model achieves state-of-the-art performance. To foster reproducibility, our code is made available here: \url{https://github.com/bionlproc/text-summ-for-doc-retrieval}.",382
"  . }   In real-world dialogue systems, a substantial portion of all user queries are ambiguous ones for which the system is unable to precisely identify the underlying intent.  %For example, nearly 30\% of user queries in a real-world QA system are ambiguous questions. % Can't give statistics in an academic paper without mentioning details We observed that many such queries in our question answering  system exhibited one of the following two characteristics. [partopsep=0pt,topsep=0pt]      %The ambiguous questions in our QA system can be summarized into 2 types:\\ %  \\ %        % We will illustrate the method of finding such intents set in detail in the methodology section. % %  introduced methods to ask clarification questions for information that is missing from a given linguistic context.  use generative model to generate clarification questions for solving entity ambiguities. But it has some obstacles to use these methods in real application. One reason is users in real world sometimes doesn't respond as clarification question expected like just reply ``I'm not sure"". Compared withing ask a clarification question, we directly list potential ambiguities as options.  proposed a query refinement method based on reinforcement learning, which helps to improve search results in search engine. Limited to the form of a dialogue system, it's not practical to show long list of potential results in dialogue. We aimed to interact with user by concise phrases to clarify user's question. % % A similar idea from  also suggests that, a conversational interface may be easier for users to clarify their needs given precise choices rather than expecting them to come up with particular terms.  % The complete question clarification process in our work is illustrated in Figure . Through real-world application experiments, our method has a lower rate on transferring to human agents and significant higher CTR  than other baselines. Our method also performs better than other baselines on the recall of potential FAQs on our annotated corpora.  %This paper focuses on closed-domain question clarification in dialogue, solving all kinds of ambiguous questions in one method.   {0pt} {0pt}     %    %% This part is comparison between related works.  % We investigated related works to clarify ambiguous questions in QA. The classic solution is to rank the most semantic similar questions [ranker ref] to the ambiguous questions. However, considering the limitation to display information in a dialogue based QA system, generally only the three results can be displayed, resulting in that this method cannot cover enough potential clear questions. In our experiments, we use the relevance ranker as the baseline for comparison. The results show that the human transferring rate of our method is much lower than the ranking method. The second method is to ask clarification questions . . However, the method of generative clarification question has some limitations in the real-word QA system. The biggest obstacle is that the user's answer space maybe to too open to answer, which complicates the dialogue. In addition, there is a lot of works to disambiguate questions through question refinement, but most refinement methods usually supplements information by a single key point, which not able to achieve all the key point recall we mentioned earlier.  % Question clarification is essential for a question answering system. In a real-world QA system, nearly 30\% of the user queries are ambiguous questions. Without clarification, dialogue participants risk missing information and ambiguous failing to achieve mutual understanding. The ability to ask clarification questions is one of the key desired components of conversational systems .  introduced methods to ask clarification questions for information that is missing from a given linguistic context.  use generative model to generate clarification questions for solving entity ambiguities.  % However, it is difficult to achieve a high success rate. For example, ``how to apply?"" is ambiguous, because there are too many products related to the ``apply"". By asking only one option question, such as ``Do you want to apply for a credit card?"" or two options question, such as ``Do you want to apply for a credit card or a loan ?"", which are both less efficient. Phenomena mentioned above exist in our real world customer service robot  system. CSRobot based on FAQ question answering is widely used in the real world, especially in the financial industry. When user enter a question in CSRobot system , information is retrieved by computing semantic similarity between user question and pre-manually prepared FAQ. Due to factors such as user's age, gender, geography, familiarity with our system, and urgency of user's problem, user may enter many ambiguous questions. In our CSRobot environment, the ratio is nearly 30\%. The ambiguous questions in our system can be summarized into 5 types:  Missing subject or object, e.g. ``how to apply"", ``how to change it back"",  Missing predicate, e.g. ``credit card"", ``my QR code"",  Missing of all subject predicates and objects,  e.g. ``How benefit"", ``its not right"",  Entity ambiguous,  e.g. ``My health insurance"", because health insurance contains many sub-categories,  Misspelling ambiguous. ``how to exist"" , ``exist"" may be misspelling of ``exit"". In this work, we focus on asking clarification questions using intents recommendation in FAQ-based question answering system. Previous methods either solve missing information questions or solve entity ambiguity questions, while our proposed method can handle both missing information and entity ambiguous mentioned above.   % The complete question clarification process in our work can be seen in Figure . The user enters an incomplete or ambiguous question, and agent recommends a list of candidate intents, each of which clarifies the user's question and can be clicked. Then user clicks on an intent associated with himself, and the agent finds a list of related FAQ in the FAQ knowledge base with the clarified question. Our work focuses on recommend a list of candidate intents for question clarification. A similar idea from  also suggests that, a conversational interface may be easier for users to clarify their needs given precise choices rather than expecting them to come up with particular terms.   % introduce question clarification as collection partition thought in detail   %   % One of the challenges in designing this method is how to design a cold start scenario. We use the end-to-end sequential intents recommendation method based on reinforcement learning for user question clarification. We did not use supervised method mainly because it is difficult for human annotators directly labeling intents related to user's ambiguous question . The reward is designed to recommend the closest clear question list and maximize the information gain after clicking one intent for better question clarification. We conducted offline and online experiments in a real-world CSRobot environment and collected the data of more than 100 million online real-users' interactions with our system in one month. To the best of our knowledge, we are the first to use intents recommendation for question clarification on real-world CSRobot environment, and interactions with more than 100 million of real users. The experiments proved the effectiveness and scalability of our proposed method. Contributions are summarized as follows:  %  %       %% FORMATTING  {NTCIR-13} [1]{{\mbox{#1}}} [1]{{}} {{\metricfont{Y!S1}}} {{\metricfont{GOV2}}} {\metric{RBP}} {\metric{P}} %{\metric{AP}} {\metric{MAP}} {\metric{NDCG}} {\metric{ERR}} {\metric{BPref}} {\metric{Qmeasure}}  [1]{\mbox{\Pat@}} [1]{\mbox{\RBP@}} [2]{\mbox{\RBP@}} [1]{\mbox{\NDCG@}} [1]{\mbox{\ERR@}} [1]{\mbox{\AP}} [1]{\mbox{\AP}} [1]{\mbox{\NDCG}} [1]{\mbox{\ERR}}  } {} {}  {\method{RRF}\xspace} %-- Baselines {\method{GBRT}} {\method{LSTM}} {\method{DQN}} {\method{DoDQN}} {\method{DoDDQN}} {\method{DDQN}} {\method{PER-DoDQN}} {\method{PER-DoDDQN}} {\method{PER}} {\method{MLP}}   %{\method{GBDT-BL}} %{\method{GBRT-BL}} %{\method{LambdaMART-BL}} %{\method{GBDT-Budget-BL}} %{\method{QL-BL}} % % %{\method{AdaRank-BL}} %{\method{WLM-BL}} % %%-- Experimental methods %{\method{LM-C3-Cost}} %{\method{LM-C3-CE}} %{\method{LM-C3-Rnd}} %{\method{GBDT-C3-Cost}} %{\method{GBDT-C3-CE}} %{\method{GBDT-C3-Rnd}} %{\method{GBRT-C3-Cost}} %{\method{GBRT-C3-CE}} %{\method{GBRT-C3-Rnd}} %{\method{LambdaMART-C3-Cost}} %{\method{LambdaMART-C3-CE}} %{\method{LambdaMART-C3-Rnd}} % %{\method{LM-C3-C}} %{\method{LM-C3-E}} %{\method{LM-C3-F}} %{\method{GBDT-C3-C}} %{\method{GBDT-C3-E}} %{\method{GBDT-C3-F}} %{\method{GBRT-C3-C}} %{\method{GBRT-C3-E}} %{\method{GBRT-C3-F}} %{\method{LambdaMART-C3-C}} %{\method{LambdaMART-C3-E}} %{\method{LambdaMART-C3-F}}  %-- Tools {} {}  %-- misc formatting \def\D{} \def\C{} %-- Misc control commands } }}} [1]{\makebox[15mm][l]{ \raggedright{#1.}\\[0.5ex]} [1]{\makebox[15mm][l]{ \raggedright{#1.}\\} %--- Ranking Stuff   {\var{Ans}} {\var{docweight}_{d}} } {\var{pivot}} }} {t_{\mbox{}}} {_{d,t})}}  {\rangle d,f_{d,t} \langle} {\mbox{}}\xspace} {\mbox{tfidf}\xspace} {}} {}} }\xspace} } } } }\xspace} {_d}} %--- Ops %% [1]{\mbox{	extsc{#1}}} [1]{\mbox{{#1}}}  {\opstyle{WAND}\xspace}  {\opstyle{MAXSCORE}\xspace}  {\opstyle{PST}\xspace} {\opstyle{Greedy-TAAT}\xspace} {\opstyle{TAAT}\xspace} {\opstyle{DAAT}\xspace} %--- Misc   {{}}} } {\mbox{\mbox{ {{{{{{{{} {} {} {} {\mbox{ {\opstyle{LMDS}} {\mbox{{_{\mbox{}}\xspace}} {\mbox{{\mbox{\method{NeWT}\xspace}} {\mbox{\method{NewSys}\xspace}}  {\method{Lynx\xspace}}  {\method{Terrier\xspace}} {{{{ {{  {R-Data}      {\mbox{}} {T_{\mbox{}}} {T_{\mbox{}}}   {\operatornamewithlimits{argmin}} {\operatornamewithlimits{argmax}} {\operatornamewithlimits{max}} {\operatornamewithlimits{lim}}   %-- Sizes    %-- maths }} {n_{\mbox{\tiny max}}} {{*$} {\makebox[\onedigit]{~}}    {1} [1]{{*** 	[\thetodocount]  ***\addtocounter{todocount}{1}}} % % File acl2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith   \documentclass[11pt]{article} \usepackage{coling2020} \usepackage{times} \usepackage{url} \usepackage{latexsym}  \renewcommand{\UrlFont}{\ttfamily % For formal tables \usepackage[normalem]{ulem} \usepackage{xcolor} %%xl: I need xcolour.... \usepackage{algorithm} \usepackage{algpseudocode} \usepackage{amsmath} \usepackage{mathrsfs}  \usepackage{amssymb} \usepackage{subfigure} \usepackage{makecell} \usepackage{mathtools} \usepackage[font=rm]{caption} % \usepackage{subcaption} \DeclareCaptionType{copyrightbox} \usepackage{shortvrb} \usepackage{tabularx} \usepackage{verbatim} \usepackage{xspace} \usepackage{listings} \lstset{basicstyle= \usepackage[multiple]{footmisc} \usepackage[all]{nowidow} \usepackage{balance} % This is not strictly necessary, and may be commented out, % but it will improve the layout of the manuscript, % and will typically save some space. \usepackage{microtype} \usepackage{wrapfig} % %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \usepackage[medium,compact]{titlesec} \usepackage{enumitem}     \author{       Xiang Hu\footnotemark[2]    \\ %       Ant Financial Services Group\footnotemark[2]\\    Hasso Plattner Institute, University of Potsdam\footnotemark[3]\\     \\\And    Zujie Wen\footnotemark[2] \\ %   Rutgers University\\    %       \\\And    Yafang Wang \footnotemark[2] \thanks{\ \  corresponding author, email: yafang.wyf@antfin.com}   \\ %   Ant Financial Services Group\\  %    %   \thanks{Corresponding author, Email: yafang.wyf@antfin.com}   \\\And    Xiaolong Li\footnotemark[2] \\ %   Rutgers University\\    %       \\\And   Gerard de Melo\footnotemark[3]  \\ %   Ant Financial Services Group\\  %    \\    tu   }  \date{}     %闁俺绻冮崣宥夋６閻ㄥ嫭鏌熷蹇旂窞濞撳懎褰查懗钘夌敨閺夈儲妫ゅ▔鏇㈩暕閺堢喓娈戦悽銊﹀煕閸欏秹顩敍灞芥躬閻喎鐤勭化鑽ょ埠娑擃厼绱╅崗銉ょ瑝绾喖鐣鹃幀褋淇uestion reformulation閺傝纭跺瀵伴弮鐘崇《娴兼媽顓搁幍閺堝缍旈崷銊ュ讲閼宠姤褋鍌氭礈濮濄倖鍨滄禒顑垮▏閻€劋绔寸粔宥勬唉娴滄帒绱￠惃鍕６妫版ɑ绶炲〒鍛煙濞夋洏 % \todo{explain defect of previous works} Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of human interaction, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a reinforcement model to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The model is trained using reinforcement learning with a deep policy network.  We evaluate our model based on real-world user clicks and demonstrate significant improvements across several different experiments. % The ability to ask clarification questions to solve ambiguity and missing information phenomena is essential for question answering systems. The current research mainly uses questions generation or questions ranking to ask a clarification question, which lead to low success rate and redundant information. Insufficient use of the graphic user interface  results in more interactions with users. There is usually no guarantee for replying the user after the clarification. To solve these problems, we propose a question clarification method based on intents recommendation. intents are extracted from the historical Frequently Asked Questions of our system. The recommended intents can provide more concise candidates for user to click. Once an intent is clicked, the system guaranteed to provide a clear question list relative to the real question. We use the reinforcement learning method to recommend intents, and the most challenging problem is cold start. The reward is designed to recommend the most relevant clear question list and maximize the information gain after clicking one intent for better question clarification. The method we proposed for question clarification can solve both ambiguity and missing information phenomena. Experiments on interactions with more than 100 million real-world online users shows the effectiveness of this method.         
"," %闂侇偅淇虹换鍐矗瀹ュ锛栭柣銊ュ閺岀喎顕ｈ箛鏃傜獮婵炴挸鎳庤ぐ鏌ユ嚄閽樺鏁ㄩ柡澶堝劜濡倕鈻旈弴銏╂殨闁哄牏鍠撳▓鎴︽偨閵婏箑鐓曢柛娆忕Ч椤╊參鏁嶇仦鑺ヨ含闁活亞鍠庨悿鍕寲閼姐倗鍩犲☉鎿冨幖缁扁晠宕楅妷銈囩憹缁绢収鍠栭悾楣冨箑瑜嬫穱uestion reformulation闁哄倽顫夌涵璺侯嚗鐎典即寮悩宕囥婂ù鍏煎椤撴悂骞嶉柡鍫濐槹缂嶆棃宕烽妸銉ヨ闁煎疇濮よ閸屾碍绀堟慨婵勫栭崹婊勭椤戝灝鈻忛柣鈧妺缁斿绮斿鍕攭濞存粍甯掔槐锟犳儍閸曨垱锛栧Λ鐗埳戠欢鐐层掗崨顔界厵婵炲娲 % \todo{explain defect of previous works} Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of human interaction, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a reinforcement model to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The model is trained using reinforcement learning with a deep policy network.  We evaluate our model based on real-world user clicks and demonstrate significant improvements across several different experiments. % The ability to ask clarification questions to solve ambiguity and missing information phenomena is essential for question answering systems. The current research mainly uses questions generation or questions ranking to ask a clarification question, which lead to low success rate and redundant information. Insufficient use of the graphic user interface  results in more interactions with users. There is usually no guarantee for replying the user after the clarification. To solve these problems, we propose a question clarification method based on intents recommendation. intents are extracted from the historical Frequently Asked Questions of our system. The recommended intents can provide more concise candidates for user to click. Once an intent is clicked, the system guaranteed to provide a clear question list relative to the real question. We use the reinforcement learning method to recommend intents, and the most challenging problem is cold start. The reward is designed to recommend the most relevant clear question list and maximize the information gain after clicking one intent for better question clarification. The method we proposed for question clarification can solve both ambiguity and missing information phenomena. Experiments on interactions with more than 100 million real-world online users shows the effectiveness of this method.",383
" %Discourse Parsing is a key NLP %an important task, aiming to establish a better understanding of multi-sentential natural language. %, which is inherently ambiguous and intent-driven.  %Most research in the area thereby focuses on one of the two main discourse theories RST  or PDTB , both proposed over a decade ago. Discourse Parsing is a key Natural Language Processing  task for processing multi-sentential text. Most research in the area focuses on one of the two main discourse theories -- RST  or PDTB . The latter thereby postulates shallow discourse structures, combining adjacent sentences and mainly focuses on explicit and implicit discourse connectives. The RST discourse theory, on the other hand, proposes discourse trees over complete documents in a constituency-style manner, with tree leaves as so called Elementary Discourse Units , representing span-like sentence fragments. Internal tree-nodes encode discourse relations between sub-trees as a tuple of \{Nuclearity, Relation\}, where the nuclearity defines the sub-tree salience in the local context, and the relation further specifies the type of relationship between the binary child nodes   with automatically inferred discourse structures and nuclearity attributes from large-scale sentiment datasets already reached state-of-the-art  performance on the inter-domain discourse parsing task. Similarly,  infer latent discourse trees from the text classification task, and  employ the downstream task of summarization using a transformer model to generate discourse trees. Outside the area of discourse parsing, syntactic trees have previously been inferred according to several strategies, e.g. . %including: Discrete decisions frameworks using a Gumbel-softmax component , applying a reinforcement approach to syntactic parsing , using the reconstruction error of adjacent spans as an indicator for syntactic coherence within a sentence  or by employing a CKY approach to select syntactic trees from a soft model .  In general, the approaches mentioned above  %to automatically annotate text with discourse structures or syntactic trees  have shown to capture valuable structural information. Some models outperform baselines trained on human-annotated datasets , others have proven to enhance diverse downstream tasks . However, despite these initial successes, one critical limitation that all aforementioned models share is the task-specificity, possibly only capturing downstream-task related information. %of discourse,  This potentially compromises the generality of the resulting trees, as for instance shown for the model using text classification data  in .  %For instance, the approach by  uses document-level sentiment information to inform the discourse tree generation, with others %have been  %using summarization data  or sentence-level sentiment cues  to achieve the results.  In order to alleviate this limitation of task-specificity, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending the latent tree induction framework proposed by  with an auto-encoding objective. %.  Our system thereby extracts important knowledge from natural text by optimizing both the underlying tree structures and the distributed representations. We believe that the resulting discourse structures effectively aggregate related and commonly appearing patterns in the data by merging coherent text spans into intermediate sub-tree encodings, similar to the intuition presented in . However, in contrast to the approach by , our model makes discrete structural decisions, rather than joining possible subtrees using a soft attention mechanism. We believe that our discrete tree structures allow the model to more efficiently achieve the autoencoder objective in reconstructing the inputs, directly learning how written language can be aggregated in the wild . In general, the proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and further problems outside of NLP, like tree-planning  and decision-tree generation . Yet, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to %complement task-specific models in  generate much larger and more diverse discourse treebanks.    
"," Discourse information, as postulated by popular discourse theories, such as RST and PDTB, has been shown to improve an increasing number of downstream NLP tasks, showing positive effects and synergies of discourse with important real-world applications. While methods for incorporating discourse become more and more sophisticated, the growing need for robust and general discourse structures has not been sufficiently met by current discourse parsers, usually trained on small scale datasets in a strictly limited number of domains. This makes the prediction for arbitrary tasks noisy and unreliable. The overall resulting lack of high-quality, high-quantity discourse trees poses a severe limitation to further progress.  In order the alleviate this shortcoming, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to generate larger and more diverse discourse treebanks. In this paper we are inferring general tree structures of natural text in multiple domains, showing promising results on a diverse set of tasks.  %With this paper, we intend to initiate a new line of research on inferring discourse structures in an unbiased manner. %With a growing need for robust and general discourse structures in many downstream tasks and real-world applications, the current lack of high-quality, high-quantity discourse trees poses a severe shortcoming. %In order the alleviate this limitation, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop such method to complement task-specific models in generating much larger and more diverse discourse treebanks.",384
"  Retrieval technique or response selection is a very popular and elegant approach  to framing a chatbot i.e. open-domain dialog system. Given the conversation context, a retrieval-based chatbot aims to select the most appropriate utterance as a response from a pre-constructed database. %that saves a large number of human written utterances. In order to balance the effectiveness and efficiency, mosts of the retrieval-based chatbots  employ coarse-grained selection module to recall a set of candidate  that are semantic coherent with the conversation context to speed up processing.  % 鐠囧瓨妲戦敍姘妧娑斿孩鏅ラ悳鍥ф嫲閺佸牊鐏夐獮鏈电瑝閼宠棄鍙忛柈銊ュ絿瀵 % 閸滃elated work闁插矂娼伴柌宥咁槻娴滃棴绱濋惄瀛樺复閸掔姴骞撻敍 To the best of our knowledge, there are two kinds of approaches  to build a coarse-grained selection module in retrieval-based chatbots:  sparse representation:  TF-IDF or BM25  is a widely used method. It matches keywords with an inverted index and can be seen as representing utterances in highdimensional sparse vectors ; %This method runs very quickly, but lacks rich semantic information.  dense representation:  Large scale pre-trained langauge models , e.g. BERT  are commonly used to obtain the semantic representation of utterances, which could be used to recall semantic coherent candidates by using cosine similarity . %Due to the high computational burden of similarity calculating, %this method runs slowly, but could consider rich semantic information %.  % 鐠囧瓨妲戦惄顔煎楠炶埖妫ょ化鑽ょ埠娑擃亜顕В鏃撶礉鐠囧瓨妲戠紒鍡氬Ν閸欘垯浜掗崷銊ョ杽妤犲奔鑵戦幍鎯у煂閿涘矂妲撻弰搴＄杽妤犲瞼绮ㄩ弸婊冩嫲dense vectors閻ㄥ墜eakness閿涘瞼鍔ч崥搴＄穿閸戠儤鍨滄禒顒傛畱閸欙缚绔存稉鐚祌oposed method % Luan2020SparseDA鏉╂瑤閲滅拋鐑樻瀮娑旂喕顕╅弰搴濈啊BM25閺堝妞傞崐娆愭櫏閺嬫粍娲挎總 % Dense 閺鐟版倳 BERT So far, there is no systematic comparison between these two kinds of approaches in retrieval-based chatbots, and which kind of method is most appropriate in real scenarios is  still an open question that confuses researchers in dialog system community. Thus, in this paper, we first conduct extensive experiment to compare these two approaches from four important aspects:   effectiveness;  search time cost;  index storage occupation;  human evaluation. Extensive experiment results on four popular response selection datasets  demonstrate that the dense representation  significantly outperforms the sparse representation at the expense of  the lower speed and bigger storage than sparse representation, which is unsufferable in real scenarios. Then, in order to overcome the fatal weaknesses of dense representation methods, we propose an ultra-fast, low-storage and highly effective  Deep Semantic Hashing Coarse-grained selection module  %based on a given dense representation method, which effectively balances the effectiveness and efficiency. Specifically,  we first stack a novel hashing optimizing module that consists of two autoencoders on a given  dense representation method. Then, three well designed loss functions are used to optimize  these two autoencoders in hashing optimizing module:  preserved loss;  hash loss;  quantization loss. After training, the autoencoders could effectively preserve rich semantic and similarity information of the dense vectors into the hash codes, which are very computational and storage efficient .      The rest of this paper is organized as follows: we introduce the important concepts and background covered in our paper in Section 2. The experiment settings is presented in Section 3. In Section 4, we systematically compare the current two kinds of methods in coarse-grained selection module:   sparse representation;  dense representation. In Section 5, we introduce our proposed DSHC model, and detailed experiment results are elaborated. In Section 6, we conduct the case study. Finally, we conclude our work in Section 7. Due to the page limitation, more details and extra analysis can be found in Appendix.  
","   We study the coarse-grained selection module in retrieval-based chatbot.   Coarse-grained selection is a basic module in a retrieval-based chatbot,   which constructs a rough candidate set from the whole database to speed up the interaction with customers.   So far, there are two kinds of approaches for coarse-grained selection module:     sparse representation;  dense representation.   To the best of our knowledge, there is no systematic comparison between these two approaches in retrieval-based chatbots,   and which kind of method is better in real scenarios is still an open question.   In this paper, we first systematically compare these two methods from four aspects:     effectiveness;  index stoarge;  search time cost;  human evaluation.   Extensive experiment results demonstrate that dense representation method    significantly outperforms the sparse representation,    but costs more time and storage occupation.   In order to overcome these fatal weaknesses of dense representation method,    we propose an ultra-fast, low-storage, and highly effective    Deep Semantic Hashing Coarse-grained selection method, called DSHC model.   Specifically, in our proposed DSHC model,   a hashing optimizing module that consists of two autoencoder models is    stacked on a trained dense representation model,   and three loss functions are designed to optimize it.   The hash codes provided by hashing optimizing module effectively    preserve the rich semantic and similarity information in dense vectors.   Extensive experiment results prove that,   our proposed DSHC model can achieve much faster speed and lower storage than sparse representation,   with limited performance loss compared with dense representation.   Besides, our source codes have been publicly released for future research\footnote{\url{https://github.com/gmftbyGMFTBY/HashRetrieval}}.",385
"  With huge quantities of natural language documents, search engines have been essential for the time saved on information retrieval tasks. Usually, deployed search engines achieve the task of ranking documents by relevance according to a query. \\ Recently, research has focused on the task of extracting the span of text that exactly matches the user's query through Machine  Reading Comprehension and Question Answering. \\ Question Answering deals with the extraction of the span of text in a short paragraph that exactly answers a natural language question. Recent deep learning models based on heavy pretrained language models like BERT achieved better than human performances on this tasks .  \\ One could try to apply QA models for the Open-Domain Question Answering paradigm which aims to answer questions taking a big amount of documents as knowledge source. Two main issues emerge from this : first, applying 100M parameters language models to potentially millions of documents requires unreasonable GPU-resources. Then, QA models allow to compare spans of text coming exclusively from a single paragraph while in the open-domain QA paradigm, one needs to compare spans of text coming from a wide range of documents. \\ Our system, as done in previous work, deals with the resources issue thanks to a Retriever module, based on the BM25 algorithm, that allows to reduce the search space from millions of articles to a hundred of paragraphs. The second issue is tackled by adding a deep learning based Scorer module that re-ranks with more precision the paragraphs returned by the Retriever. Eventually, the Extractor module uses a QA deep learning model to extract the best span of text in the first paragraph returned by the Scorer. To avoid a heavy and hardly scalable pipeline consisting of two huge deep learning models, we parallelize the re-ranking and span extraction tasks thanks to multitask learning : while maintaining high performances, it allows to significantly reduce both memory requirements and inference time. Our system achieve state-of-the-art results on the open-squad benchmark.  
","   In this paper, we introduce MIX : a   multi-task deep learning approach to solve Open-Domain Question  Answering. First, we design our system as a multi-stage pipeline made of 3 building blocks : a BM25-based Retriever, to reduce the search space; RoBERTa based Scorer and Extractor, to rank retrieved paragraphs and extract relevant spans of text respectively. Eventually, we further improve computational efficiency of our system to deal with the scalability challenge : thanks to multi-task learning,   we parallelize the close tasks solved by the Scorer and the Extractor. Our system is on par with state-of-the-art performances on the squad-open benchmark while being simpler conceptually.",386
"  Named Entity Recognition  is the task of identifying the span and the class of a Named Entity  in unstructured text. NEs typically include but are not limited to persons, companies, dates, and geographical locations .   Legal NER is a central task in language processing of legal documents, especially for extracting key information such as the name of the parties in a case, the court name or the case number, or references to laws or judgements, to name a few. The extracted NEs could be integrated in legal research workflows for functionalities such as search, document anonymization or case summarization  thereby enabling and expediting insights for legal professionals .  NER is commonly formalized as a sequence labeling task: each token of the document is assigned a single label that indicates whether the token belongs to an entity from a predefined set of categories . To create a training dataset in such a format the annotator is required to manually label each token in a sentence with the respective category. In this format, both the NE and the location of the NE in the source text are known. This format of training data is what we refer to hereafter as 閳ユ笀old standard閳 data. Obtaining the required voluminous gold standard data to train such models is, therefore, a laborious and costly task.    In this paper, we perform NER in filed lawsuits in US courts. Specifically, we aim to identify the party names in each case, i.e. the names of the plaintiffs and the defendants, in a large collection of publicly available cases from more than 200 courts in different US jurisdictions. The party names have been identified by legal annotators but their exact location in the text is unknown. In this respect, we do not have access to 閳ユ笀old standard閳 training data even though the target NEs are available. This feature of our dataset introduces a key difference of our task to most NER tasks.  One solution to this problem is to generate the 閳ユ笀old standard閳 training data by searching for the locations of the known NEs in the source text . By performing this additional transformation to our data, we would be able to train sequence labeling NER models. For the following reasons, this solution is nontrivial. First, as our source text is also extracted from scanned PDF files , it contains Optical Character Recognition  mistakes and/or typos which may not be present in the target NEs. Second, besides the potential OCR errors at the character level, the closely spaced, two-column page layouts that can be often found as headers in the filed cases, represent an additional challenge for the OCR, which tends to concatenate the text across columns . In such cases, the tokens that make up the NEs in the source text may be intertwined with other words and/or sentences. Third, variations of the names may be also present in the source text and in our human-generated labels, such as presence of first and/or middle names whole or as initials and, to a lesser extent, typos.     [h]     To address some of the challenges imposed by the format of our training data and inspired by the work in the field of abstractive summarization, we propose to reformulate the NER task, not as a sequence labeling problem, but as a text-to-text sequence generation problem with the use of a pointer generator network . With this reformulation, in contrast to sequence labeling, we do not require knowledge of the NE閳ユ獨 locations in the text as training labels. A recent study by  proposed a different formulation of the NER task as a question answering task and achieved state-of-the-art performance in a number of published NER datasets . In this study, we adopt a hybrid extractive-abstractive architecture, based on recurrent neural networks coupled with global  attention and copying  attention  mechanisms . The proposed architecture can be successfully used for abstractive summarization since it can copy words from the source text via pointing and can deal effectively with out-of-vocabulary  words 閳 words that have not been seen during training. Our approach is conceptually simple but empirically powerful and we show that the pointer generator outperforms the typical NER architectures in the case of noisy and lengthy inputs where the NE's location in the text is not known.   In addition, we examine how our approach can be used for the related NER task of case number extraction. The case number is a unique combination of letters, numbers and special characters as a single token and are, therefore, particularly challenging for NER models as they are often dealt with as OOV words by the model. As in the party names task discussed above, in the case number task we do not have 閳ユ笀old standard閳 labels of the case number閳ユ獨 location in the text. We show that a character level sequence generation network can dramatically increase our ability to extract case numbers from the source text, compared to a word level sequence generation network.  The rest of the paper is organized as follows. In Section 2, we discuss related work in the field of NER in the legal domain. In Section 3, we describe our proposal of NER as a text-to-text sequence generation task in the absence of gold standard data and formulate the task in two ways:  as a combination of automatically labeling the NE's location and then using the conventional sequence labeling method for NER, and  as a text-to-text sequence generation task where the NEs are directly generated as text. Section 4 presents our experimental design, results and analysis. Section 5 presents the case number case study. Finally, we conclude and discuss directions for future work.  
"," Named Entity Recognition  is the task of identifying and classifying named entities in unstructured text. In the legal domain, named entities of interest may include the case parties, judges, names of courts, case numbers, references to laws etc. We study the problem of legal NER with noisy text extracted from PDF files of filed court cases from US courts. The 闁炽儲绗old standard闁 training data for NER systems provide annotation for each token of the text with the corresponding entity or non-entity label. We work with only partially complete training data, which differ from the gold standard NER data in that the exact location of the entities in the text is unknown and the entities may contain typos and/or OCR mistakes. To overcome the challenges of our noisy training data, e.g. text extraction errors and/or typos and unknown label indices, we formulate the NER task as a text-to-text sequence generation task and train a pointer generator network to generate the entities in the document rather than label them. We show that the pointer generator can be effective for NER in the absence of gold standard data and outperforms the common NER neural network architectures in long legal documents.",387
" Query reformulation and paraphrase generation techniques are employed for a variety of purposes in natural language processing , such as dialogue generation , machine translation , and especially in question answering  systems . Generating coherent and clean texts can reduce potential errors in downstream systems. In the cases when users are at the receiving end of NLP pipelines, it is essential to show them fluent and human-like languages before they lose faith and recede into requiring human agents for the sake of better understanding and communication. In search or question answering systems, query reformulation aims to paraphrase or restructure original question sequences, transforming them into ones that are more interpretable with natural well-formedness in both grammar and semantics. Typically, users may not have the patience to input an entirely grammatical or coherent question, which can cause issues for the downstream components to understand and give accurate predictions or answers. When human representatives are present, an originally noisy query or question can be reiterated and rephrased to double-check with users what they are asking for. This is a costly operation if every convoluted question needs to be restated. By having an NLP model to reformulate input queries, reformulations are fed back to users to confirm their original intentions in an automated way. As a result, unnecessary errors are eliminated and noises are prevented from propagating in an NLP pipeline, which can contain a series of models such as intent classification, information retrieval and question answering.  Traditionally, rule-based and statistical methods have been studied for paraphrase and reformulation generation . The advent of sequence-to-sequence learning   made it feasible to train deep neural networks as a new paradigm. We investigate how to paraphrase and denoise queries and generate well-formed reformulations using Seq2Seq learning models such as LSTMs  and transformers . Following the framework from AQA , a Seq2Seq model is pre-trained on supervised tasks and further tuned using reinforcement learning  on a machine comprehension QA dataset SearchQA , learning from a pre-trained BiDAF  QA system that generates rewards. SearchQA is a suitable and challenging dataset as queries contain noisy phrases and the associated contexts are concatenated web text snippets from Google's search engine. Our goal is to obtain a model that can generate better-formed reformulations based on the original query sequences and achieve good QA performance with these reformulations. We use transfer learning  from pre-trained transformers with text-to-text task formulations . In our approach, pre-trained T5 models are first fine-tuned on paraphrase generation  and denoising  datasets to gain general paraphrasing capabilities. Then, reinforcement learning of downstream QA rewards is performed to further encouraged the model to produce task-specific reformulations. To our knowledge, this is a first attempt to fine-tune text-to-text transformers with RL, nudging the model to generate reward-acquiring query trajectories to get better answers. We show that fine-tuned text-to-text transformers are better starting points for RL as they are more sample efficient in achieving the same level of QA performance, acquiring rewards faster than the previous AQA approach that uses translation-based LSTMs. T5 models also generate reformulations with better readability and can generalize to out-of-sample data. We provide a new way to evaluate fluency on a sequence level using an trained metric on the well-formedness   dataset, which is based on real evaluations from humans, a more reliable source than widely-used algorithmic metrics based on overlapping n-grams.   
"," Query reformulation aims to alter potentially noisy or ambiguous text sequences into coherent ones closer to natural language questions. In this process, it is also crucial to maintain and even enhance performance in a downstream environments like question answering when rephrased queries are given as input. We explore methods to generate these query reformulations by training reformulators using text-to-text transformers and apply policy-based reinforcement learning algorithms to further encourage reward learning. Query fluency is numerically evaluated by the same class of model fine-tuned on a human-evaluated well-formedness dataset. The reformulator leverages linguistic knowledge obtained from transfer learning and generates more well-formed reformulations than a translation-based model in qualitative and quantitative analysis. During reinforcement learning, it better retains fluency while optimizing the RL objective to acquire question answering rewards and can generalize to out-of-sample textual data in qualitative evaluations. Our RL framework is demonstrated to be flexible, allowing reward signals to be sourced from different downstream environments such as intent classification.",388
" % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  %.     %      % % final paper: en-us version      % %       % space normally used by the marker %     This work is licensed under a Creative Commons  %     Attribution 4.0 International License. %     License details: %     \url{http://creativecommons.org/licenses/by/4.0/}. %}  Relation extraction aims to extract relations between entities in text, where distant supervision proposed by automatically establishes training datasets by assigning relation labels to instances that mention entities within knowledge bases. However, the wrong labeling problem can occur and various multi-instance learning methods have been proposed to address it. Despite the wrong labeling problem, each instance in distant supervision is crawled from web pages, which is informal with many noisy words and can express multiple similar relations. This problem is not well-handled by previous approaches and severely hampers the performance of conventional neural relation extractors. To handle this problem, we have to address two challenges:  Identifying and gathering spotted relation information from low-quality instances;  Distinguishing multiple overlapped relation features from each instance.    First, a few significant relation words are distributed dispersedly in the sentence, as shown in Figure, where words marked in red brackets represent entities, and italic words are key to expressing the relations. For instance, the clause ``evan\_bayh son of birch\_bayh"" in S1 is sufficient to express the relation /people/person/children of evan\_bayh and birch\_bayh. Salient relation words are few in number and dispersedly in S1, while others excluded from the clause can be regarded as noise. Traditional neural models have difficulty gathering spotted relation features at different positions along the sequence because they use Convolutional Neural Network  or Recurrent Neural Network  as basic relation encoders, which model each sequence word by word and lose rich non-local information for modeling the dependencies of semantic salience. Thus, a well-behaved relation extractor is needed to extract scattered relation features from informal instances.  Second, each instance can express multiple similar relations of two entities. As shown in Figure, Changsha and Hunan possess the relations /location/location/contains and /location/province/capital in S2, which have similar semantics, introducing great challenges for neural extractors in discriminating them clearly. Conventional neural methods are not effective at extracting overlapped relation features, because they mix different relation semantics into a single vector by max-pooling or self-attention. Although  first propose an attentive capsule network for multi-labeled relation extraction, it treats the CNN/RNN as low-level capsules without the diversity encouragement, which poses the difficulty of distinguishing different and overlapped relation features from a single type of semantic capsule. Therefore, a well-behaved relation extractor is needed to discriminate diverse overlapped relation features from different semantic spaces.  To address the above problem, we propose a novel Regularized Attentive Capsule Network  to identify highly overlapped relations in the low-quality distant supervision corpus. First, we propose to embed multi-head attention into the capsule network, where attention vectors from each head are encapsulated as a low-level capsule, discovering relation features in an unique semantic space. Then, to improve multi-head attention in extracting spotted relation features, we devise relation query multi-head attention, which selects salient relation words regardless of their positions. This mechanism assigns proper attention scores to salient relation words by calculating the logit similarity of each relation representation and word representation. Furthermore, we apply disagreement regularization to multi-head attention and low-level capsules, which encourages each head or capsule to discriminate different relation features from different semantic spaces. Finally, the dynamic routing algorithm and sliding-margin loss are employed to gather diverse relation features and predict multiple specific relations. We evaluate RA-CapNet using two benchmarks. The experimental results show that our model achieves satisfactory performance over the baselines. Our contributions are summarized as follows:       {0pt}     {0pt}       
","   Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network  to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our model achieves significant improvements in relation extraction.",389
" 	Identifying the user's open intent plays a significant role in dialogue systems. As shown in Figure, we have two known intents for specific purposes, such as book flight and restaurant reservation. However, there are also utterances with irrelevant or unsupported intents that our system cannot handle. It is necessary to distinguish these utterances from the known intents as much as possible. On the one hand, effectively identifying the open intent can improve customer satisfaction by reducing false-positive error. On the other hand, we can use the open intent to discover potential user needs. 	 	We regard open intent classification as an -class classification task as suggested in, and group open classes into the  class . Our goal is to classify the n-class known intents into their corresponding classes correctly while identifying the  class open intent. To solve this problem,~ propose the concept of open space risk as the measure of open classification.~ reduce the open space risk by learning the closed boundary of each positive class in the similarity space. However, they fail to capture high-level semantic concepts with SVM.  	~ manage to reduce the open space risk through deep neural networks , but need to sample open classes for selecting the core hyperparameters.~ use the softmax probability as the confidence score, but also need to select the confidence threshold with negative samples.~ replace softmax with the sigmoid activation function, and calculate the confidence thresholds of each class based on statistics. However, the statistics-based thresholds can not learn the essential differences between known classes and the open class.~ propose to learn the deep intent features with the margin loss and detect unknown intents with local outlier factor. However, it has no specific decision boundaries for distinguishing the open intent, and needs model architecture modification.  	 	Most of the existing methods need to design specific classifiers for identifying the open class and perform poorly with the common classifier. Moreover, the performance of open classification largely depends on the  decision conditions. Most of these methods need negative samples for determining the suitable decision conditions. It is also a complicated and time-consuming process to manually select the optimal decision condition, which is not applicable in real scenarios.  	 	To solve these problems, we use known intents as prior knowledge, and propose a novel post-processing method to learn the adaptive decision boundary  for open intent classification. As illustrated in Figure, we first extract intent representations from the BERT model. Then, we pre-train the model under the supervision of the softmax loss. We define centroids for each known class and suppose known intent features are constrained in the closed ball areas. Next, we aim to learn the radius of each ball area to obtain the decision boundaries. Specifically, we initialize the boundary parameters with standard normal distribution and use a learnable activation function as a projection to get the radius of each decision boundary.  	 	The suitable decision boundaries should satisfy two conditions. On the one hand, they should be broad enough to surround in-domain samples as much as possible. On the other hand, they need to be tight enough to prevent out-of-domain samples from being identified as in-domain samples. To address these issues, we propose a new loss function, which optimizes the boundary parameters by balancing both the open space risk and the empirical risk. The decision boundaries can automatically learn to adapt to the intent feature space until balance with the boundary loss. We find that our post-processing method can still learn discriminative decision boundaries to detect the open intent even without modifying the original model architecture. 	 	We summarize our contribution as follows. Firstly, we propose a novel post-processing method for open classification, with no need for prior knowledge of the open class. Secondly,  we propose a new loss function to automatically learn tight decision boundaries adaptive to the feature space. To the best of our knowledge, this is the first attempt to adopt deep neural networks to learn the adaptive decision boundary for open classification. Thirdly, extensive experiments conducted on three challenging datasets show that our approach obtains consistently better and more robust results compared with the state-of-the-art methods.  	[t!]} 			\toprule 			Dataset & Classes & \#Training & \#Validation & \#Test & Vocabulary Size & Length  \\ 			\midrule 			BANKING & 77 & 9,003 & 1,000 & 3,080 & 5,028 & 79 / 11.91\\ 			OOS & 150 & 15,000 & 3,000 & 5,700 & 8,376 & 28 / 8.31 \\ 			StackOverflow & 20 & 12,000 & 2,000 & 6,000 & 17,182 & 41 / 9.18 \\ 			 		 	 	
"," 		Open intent classification is a challenging task in dialogue systems. On the one hand, we should ensure the classification quality of known intents. On the other hand, we need to identify the open  intent during testing. Current models are limited in finding the appropriate decision boundary to balance the performances of both known and open intents. In this paper, we propose a post-processing method to learn the adaptive decision boundary  for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we use the well-trained features to automatically learn the adaptive spherical decision boundaries for each known intent. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open samples and is free from modifying the model architecture. We find our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods.\footnote{Code: https://github.com/thuiar/Adaptive-Decision-Boundary}",390
"    Neural Machine Translation   yields  state-of-the-art translation performance when a large number of parallel sentences are available. However, only a few parallel corpora are available for the majority of language pairs and domains. It has been known that NMT does not perform well in the specific domains where the domain-specific corpora are limited, such as medical domain. As such, high-quality domain-specific machine translation  systems are in high demand whereas general purpose MT has limited applications.  There are many studies of domain adaptation for NMT, which can be mainly divided into two categories: data-centric and model fine-tuning.  Data-centric methods focus on  selecting or generating target domain data from general domain corpora, which is effective and well explored.  In this paper, we focus on the second approach. Fine-tuning is  very common in domain adaptation, which first trains a base model on the general domain data and then fine-tunes it on each target domain . However, unconstrained or full fine-tuning  requires very careful hyper-parameter tuning,  and is prone to over-fitting on the target domain as well as forgetting on the general domain. To tackle these problems, researchers have proposed several constructive approaches, with the view to limiting the size or plasticity of parameters in the fine-tuning stage, which can be roughly divided into two categories: regularization and partial-tuning strategy. Regularization methods often integrate extra training objectives to prevent parameters from large deviations, such as model output regularization , elastic weight consolidation  . Regularization methods, which impose arbitrary global constraints on parameter updates, may further restrict the adaptive process of the network, especially when domain-specific corpora are scarce. Partial-tuning methods either freeze several sub-layers of the network and fine-tune the others, or integrate domain-specific adapters into the network. By only fine-tuning the domain-specific part of the model, they can alleviate the over-fitting and forgetting problem in fine-tuning. However, the structure designed to adapting is usually hand-crafted, which relies on experienced experts and the adapter brings additional parameters. Therefore,  a more adaptive, scalable, and parameter-efficient approach for domain adaptation is very valuable and worth well studying.  [!ht]                          In this paper, we propose \method, a novel domain adaptation method via adaptive structure pruning. Our motivation is inspired from Continual Learning  and  that a randomly-initialized, dense neural network contains a sub-network which  can match the test accuracy of the original network after training for at most the same number of iterations.   We therefore suppose that multiple  machine translation models for different domains can share different sparse subnetworks within a single neural network.   Specifically, we first apply a standard pruning technique to automatically uncover the subnetwork from a well-trained NMT model in the general domain.  The  subnetwork is capable of  reducing the parameter without compromising accuracy. Therefore, it has the potential to keep as much general information as possible.   Then we freeze this informative sparse network and leave the unnecessary  parameters unfixed for the target  domain, which enables our approach to be parameter efficient, and eases the scalability of the approach to more domains.  The capacity of these non-fixed parameters can be tuned to match the requirements of the target domain, while keeping the parameters of the general domain. Our method successfully circumvents catastrophic forgetting problem and retains the quality on the general domain.  As the benefits of the flexible design, \method can be easily extended to other transfer learning problems, such as multilingual machine translation.      We summarize our main contribution as follows:       problems in domain adaptation.     , \method outperforms several strong competitors including Fine-tuning, EWC, Model Distillation, Layer Freeze and Adapter in target domain test set without the loss of general domain performance.      and ZhEn, which shows the possibilities of training a single model to serve different domains without performance degradation.              % --------------------Background-------------------- 
"," Fine-tuning is a major approach for domain adaptation in Neural Machine Translation .  However, unconstrained fine-tuning requires very careful hyper-parameter tuning otherwise it is easy to fall into over-fitting on the target domain and degradation on the general domain.  To mitigate it, we propose \method, a novel domain adaptation method via gradual pruning.  It learns tiny domain-specific subnetworks for tuning. During adaptation to a new domain, we only tune its corresponding subnetwork.  \method alleviates the over-fitting and the degradation problem without model modification. Additionally, with no overlapping between domain-specific subnetworks, \method is also capable of sequential multi-domain learning.    Empirical experiment results show that \method outperforms several strong competitors in the target domain test set without the quality degradation of the general domain in both single and multiple domain settings. \footnote{The source code and data are available at \url{https://github.com/ohlionel/Prune-Tune}}",391
" As an important task in a dialogue system, response selection aims to find the best matched response from a set of candidates given the context of a conversation. The retrieved responses usually have natural, fluent and diverse expressions with rich information owing to the abundant resources. Therefore, response selection has been widely used in industry and has attracted great attention in academia.  Most existing studies on this task pay more attention to the matching problem between utterances and responses, but with insufficient concern for the reasoning issue in multi-turn response selection. Just recently, MuTual, the first human-labeled reasoning-based dataset for multi-turn dialogue, has been released to promote this line of research. Reasoning is quite different from matching in the conversations. Specifically, matching focuses on capturing the relevance features between utterances and responses, while reasoning not only needs to identify key features , but also needs to conduct inference based on these clue words. The challenges of this new task include:  how to identify the clue words in utterances, which is fundamental for inference;  how to conduct inference according to the clue words in utterances. Figure illustrates a motivating example. To infer the current time, we must first identify the clue words `10:45' in  and `15 minutes' in . Then we must conduct a logical inference based on these clue words in  and .    To tackle these challenges, first, we need better contextual representation for identifying the clue words in conversations. This is because clue word identification inevitably relies on the context of a conversation. Although previous literature publications have achieved promising results in context modeling, there are still several limitations of these approaches. More concretely, the existing studies either concatenate the utterances to form context or process each utterance independently, leading to the loss of dependency relationships among utterances or important contextual information. It has been validated that the chronological dependency between utterances, as well as the semantical dependency between utterances, are crucial for multi-turn response selection. Thus, how to model the dependencies in utterances remains a challenging problem for context representation.  Second, we need to devise a new strategy to collect the clue words scattered in multiple utterances and need to reason according to these clue words. In recent years, we have witnessed great success in KBQA  and MRC  tasks. However, new obstacles emerge for transferring current reasoning approaches in KBQA and MRC to conversational reasoning.  A clear reasoning path based on entities in a well-structured knowledge base exists in KBQA, but there is no similar reasoning path in utterances.  Current approaches on MRC conduct inference based on graph while taking shared entities as nodes, while it is difficult to construct such graphs based on entities in short utterances, which usually suffer from greater coreference resolution, poor content and serious semantic omission problems in comparison with document text.  In this paper, we propose a new model named GRN  which can tackle both challenges in an end-to-end way. We first introduce two pre-training tasks called NUP  and UOP  which are specially designed for response selection. NUP endows GRN with context-aware ability for semantical dependency, and UOP facilitates GRN with the ability to capture the chronological dependency. These customized pre-training methods are beneficial for modeling dependencies contained in utterances to achieve better context representation. We perform task-adaptive pre-training with the combined NUP and UOP tasks based on the ALBERT model. To conduct reasoning based on clue words, we devise a graph neural network called UDG , which not only models the dependencies between utterances with each utterance as a node but also collects the clue words from different utterances. Reasoning is achieved by propagating the messages of clue words between nodes along various utterance paths on UDG, and this graph reasoning structure realizes the inference based on an utterance-level context vector with local perspective. On the other hand, we also implement a reasoning network by the output of the trained model and self-attention mechanism. This sequence reasoning structure realizes the inference based on the highly summarized context vector with global perspective. To summarize, we make the following contributions:       ^{plus}$ datasets.   
"," We investigate response selection for multi-turn conversation in retrieval-based chatbots. Existing studies pay more attention to the matching between utterances and responses by calculating the matching score based on learned features, leading to insufficient model reasoning ability. In this paper, we propose a graph reasoning network  to address the problem. GRN first conducts pre-training based on ALBERT using next utterance prediction and utterance order prediction tasks specifically devised for response selection. These two customized pre-training tasks can endow our model with the ability of capturing semantical and chronological dependency between utterances. We then fine-tune the model on an integrated network with sequence reasoning and graph reasoning structures. The sequence reasoning module conducts inference based on the highly summarized context vector of utterance-response pairs from the global perspective. The graph reasoning module conducts the reasoning on the utterance-level graph neural network from the local perspective. Experiments on two conversational reasoning datasets show that our model can dramatically outperform the strong baseline methods and can achieve performance which is close to human-level.",392
" As a fundamental task in natural language processing , coherence analysis can benefit various downstream tasks, such as sentiment analysis  and document summarization . Rhetorical Structure Theory   is one of the most influential theories of text coherence, under which a document is represented by a hierarchical discourse tree, which consists of a set of semantic units organized in the form of a dependency structure, labeled with their rhetorical relations.$Equal contribution.} As shown in Figure , the leaf nodes of an RST discourse tree are basic text spans called Elementary Discourse Units , and the EDUs are iteratively connected by rhetorical relations  to form larger text spans until the entire document is included.  The rhetorical relations are further categorized to Nucleus and Satellite based on their relative importance, in which Nucleus corresponds to the core part while Satellite corresponds to the subordinate part. While manual coherence analysis under the RST theory is labor-intensive and requires specialized linguistic knowledge, a discourse parser serves to automatically transform a document into a discourse tree. Document-level discourse parsing consists of three sub-tasks: hierarchical span splitting, rhetorical nuclearity determination, and rhetorical relation classification.    Models for RST-style discourse parsing have made much progress in the past decade. While statistical methods utilize hand-crafted lexical and syntactic features , data-driven neural approaches reduce feature-engineering labor by effective representation learning, and are capable of characterizing implicit semantic information. Neural networks are first used as feature extractors along with traditional shift-reduce approaches  or dynamic programming approaches . Then,  bridges the gap between neural and traditional methods by an end-to-end transition-based neural parser via an encoder-decoder architecture. Recently, pointer networks are introduced to achieve linear-time complexity, and models with top-down parsing procedures achieve favorable results on sentence-level discourse analysis tasks .  However, there is still much space for improvement in document-level discourse parsing. First, compared to sentence-level parsing, document-level parsing is more challenging due to the deeper tree structures and longer dependencies among EDUs: in the benchmark dataset RST Discourse Tree Bank  , the average EDU number at the document level is 56, which is 20 times larger than that of sentence-level parsing. Thus modeling context information across a long span is essential, especially if considering a top-down parsing procedure where poor accuracy at the top of the tree will propagate toward the leaf nodes. Second, the three sub-tasks of discourse parsing strongly rely on nuanced semantic judgments, which require comprehensive contextual representation with various types of linguistic information. Take discourse relation classification for example, explicit relations are overtly signaled by a connective word such as ``although'' and ``because'', which can be determined by lexical and syntactic features. However, this approach can not be readily adapted to implicit discourse relations determination, as it requires high-order features with semantic information. Moreover, to compensate for the lack of large-scale corpora, prior work in neural modeling has leveraged inductive biases through syntactic features such as part-of-speech tagging to improve performance. However, such models still suffer from insufficient linguistics information from the lack of data, thus they are incapable of acquiring deeper and richer contextual representations useful for discourse processing.  In this paper, to tackle the aforementioned challenges, we propose a document-level neural discourse parser with robust representation modeling at both the EDU and document level, based on a top-down parsing procedure. To take advantage of widely-adopted vector representations that encode rich semantic information, we first exploit a large-scale pre-trained language model as a contextual representation backbone.  Then we incorporate boundary information with implicit semantic and syntactic features to the EDU representations, and introduce a hierarchical encoding architecture to more comprehensively characterize global information for long dependency modeling. To improve inference accuracy and alleviate the aforesaid error propagation problem, we present breadth-first span splitting to propose a layer-wise beam search algorithm.  We train and evaluate our proposed model on the benchmark corpus RST-DT\footnote{https://catalog.ldc.upenn.edu/LDC2002T07} , and achieve the state-of-the-art performance on all fronts, significantly surpassing previous models while approaching the upper bound of human performance. We also conduct extensive experiments to analyze the effectiveness of our proposed method.  
"," Document-level discourse parsing, in accordance with the Rhetorical Structure Theory , remains notoriously challenging. Challenges include the deep structure of document-level discourse trees, the requirement of subtle semantic judgments, and the lack of large-scale training corpora. To address such challenges, we propose to exploit robust representations derived from multiple levels of granularity across syntax and semantics, and in turn incorporate such representations in an end-to-end encoder-decoder neural architecture for more resourceful discourse processing. In particular, we first use a pre-trained contextual language model that embodies high-order and long-range dependency to enable finer-grain semantic, syntactic, and organizational representations. We further encode such representations with boundary and hierarchical information to obtain more refined modeling for document-level discourse processing. Experimental results show that our parser achieves the state-of-the-art performance, approaching human-level performance on the benchmarked RST dataset.",393
" Due to the substantial growth and effortless access to the Internet in recent years, an enormous amount of unstructured textual contents have generated. It is a crucial task to organize or structure such a voluminous unstructured text in manually. Thus, automatic classification can be useful to manipulate a huge amount of texts, and extract meaningful insights which save a lot of time and money. Text categorization is a classical NLP problem which aims to categorize texts into organized groups. It has a wide range of applications like machine translation, question answering, summarization, and sentiment analysis. There are several approaches available to classify texts according to their labels. However, deep learning method outperforms the rule-based and machine learning-based models because of their ability to capture sequential and semantic information from texts . We propose a classifier using CNN , and BiLSTM  to classify technical texts in the computer science domain. Furthermore, by sequentially adding these networks, remarkable accuracy in several shared classification tasks can be obtained. The rest of the paper is organized as follows: related work given in section 2. Section 3 describes the dataset. The framework described in section 4. The findings presented in section 5.   %%%%%%%%%%%% Related Work %%%%%%%%% 
"," This paper illustrates the details description of technical text classification system and its results that developed as a part of participation in the shared task TechDofication 2020. The shared task consists of two sub-tasks:  first task identify the coarse-grained technical domain of given text in a specified language and  the second task classify a text of computer science domain into fine-grained sub-domains. A classification system  is developed to perform the classification task using three techniques: convolution neural network , bidirectional long short term memory  network, and combined CNN with BiLSTM. Results show that CNN with BiLSTM model outperforms the other techniques concerning task-1 of sub-tasks  and task-2a. This combined model obtained $f_1$ scores of 82.63 , 81.95 , 82.39 , 84.37 , and 67.44  on the development dataset. Moreover, in the case of test set, the combined CNN with BiLSTM approach achieved that higher accuracy for the subtasks 1a , 1b , 1c , 1g  and 2a .",394
" The traditional task-oriented dialogue systems, which focuses on providing information and performing actions by the given databases or APIs, often meet the limitation that the DB/API can not cover enough necessary cases. A good enhance can be achieved with lots of relevant domain knowledge in the form of descriptions, FAQs and customer reviews, which we call unstructured knowledge. Track 1 of the 9th Dialogue System Technology Challenges , Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access, aims at generating a response based on dialogue history and unstructured knowledge access. The whole task can be divided into three subtasks, knowledge-seeking turn detection, knowledge selection and knowledge-grounded response. Test set of this track includes seen and unseen parts. The unseen test set are collected on different domains, entities, and locales, aiming to evaluate models' generalization ability.   Knowledge-seeking turn detection, as the first subtask, needs to determine whether the related knowledge is contained in the unstructured knowledge base. In other words, this subtask can be modeled as a binary classification problem. If the model predicts that there exists related knowledge, then subtask 2  will search for the most relevant knowledge snippets and then pass them to the generation process . If the model predicts that there is no related knowledge for the specific question, the remaining two subtasks will not be performed. In this paper, we first conduct an entity matching for each question and then add the domain label from matching results to the end of dialogue history as model input.  Knowledge selection is to retrieve the most relevant knowledge snippets from the database according to the dialogue history and provide information for the subsequent response generation. The dialogue history is a conversation between the human speaker and the machine. Close to the end of the conversation, the human speaker brings up a question about a certain place  or service . The given knowledge database consists of question-answer pairs involving diverse facts and is organized by different domains and entities. % Note that the knowledge-seeking turn detection model determines whether our dialog system needs to access the knowledge database before generating the response.  % We perform knowledge selection for the samples  that requires relevant knowledge in the database. The retrieved knowledge snippets provide information for the subsequent response generation.  % Information retrieval  techniques are widely applied to search for related candidates in retrieval-based knowledge-grounded system. Some researchers  compute the traditional tf-idf score to search the most relevant document to the user's query, while others leverage the power of neural networks to learn the ranking score directly through an end-to-end learning process. Recently, due to the significant improvements on numerous natural language processing tasks, large scale pre-trained language models have also been applied to better model the semantic relevance in knowledge selection.  In this paper, we first apply retrieval techniques to narrow down the searching space and then use a neural network initialized by a pre-trained model to formulate the ranking function. % We propose two base models for the knowledge selection, and the final ensemble model combines the predictions of different base models to improve the selection performance.  % The Retrieve \& Rank model first gathers the knowledge snippets of potentially relevant entities from the knowledge base, then a ranking model is trained to select the most plausible knowledge snippets from the retrieved candidates. % Different from the Retrieve \& Rank model, Three-step model divides the ranking model into three cascade parts to rank domain, entity and documents respectively in order to force the model to take the knowledge hierarchy into account. % We also ensemble these two models together and experiments show the ensemble model has a better performance than two base model separately.   % briefly introduce the three-step pipeline model.   Knowledge-grounded response generation requests to give a response automatically from the model using dialogue history and unstructured knowledge as input. There are two different types of dialogue systems, retrieval-based system, and generation-based system. Retrieval-based dialogue system, giving responses from a list of candidate sentences, only has fixed answer forms in candidate sets. To deal with our problem, which needs more flexible and natural responses, the generation-based model is a better choice. Dialogue generation requires an encoder to represent the input and a decoder to generate the response. The network often needs to minimize the cross-entropy loss between the output and the ground truth. In this paper, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism.  % Pre-trained language models make a great progress on dialogue generation. Note that bi-directional model is not designed for dialogue generation task, and thus PLATO and  PLATO-2 use uni- and bi-directional processing for pre-training. Moreover, large-scale Reddit and Twitter conversations are utilized to further pre-train the generation model to reduce data distribution gaps. Furthermore, a latent variable  is used to capture one-to-many relations of post-response pairs.  As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics. In the following sections, we will explain the details of our proposed model. Experiment results will be shown next with some analysis and conclusions.  
"," Task-oriented conversational modeling with unstructured knowledge access, as track 1 of the 9th Dialogue System Technology Challenges , requests to build a system to generate response given dialogue history and knowledge access. This challenge can be separated into three subtasks,  knowledge-seeking turn detection,  knowledge selection, and  knowledge-grounded response generation. We use pre-trained language models, ELECTRA and RoBERTa, as our base encoder for different subtasks. For subtask 1 and 2, the coarse-grained information like domain and entity are used to enhance knowledge usage. For subtask 3, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism. Meanwhile, some useful post-processing strategies are performed on the model's final output to make further knowledge usage in the generation task.  As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics.",395
"  %    Recent years have witnessed the rapid advancement of online recruitment platforms. With the increasing amount of online recruitment data, more and more interview related studies have emerged such as person-job  fit and automatic analysis of asynchronous video interviews , which aim to enable automated job recommendation and candidate assessment. Among these studies, person-job fit is to casting the task as a supervised text match problem. Given a set of labeled data , it aims to predict the matching label between the candidate resumes and job description. More recently, deep learning has enhanced person-job fit methods by training more effective text match or text representations models. AVI is to determine whether the candidate is hirable by evaluating the answers of interview questions. In AVIs, an interview is usually considered as a sequence of questions and answers containing salient socials signals. To evaluate the candidates more comprehensively, AVI models will extract the features of video , text, and voice in the process of answering questions. In this work, we focus on the scoring of multiple QA pairs,  we only extract the features of text modality and define this task as the scoring competency of candidates rather than the score of whether or not to be employed. Based on the anatomy of the human interviewers' evaluation process, the solutions consist of two stages:  analyzing and evaluating individual QA pair one by one, then acquiring the evaluation status, and  grading the competency of the candidate based on the evaluation status of multiple QA pairs.        For the first stage, existing methods tend to employ text matching or attentional text matching algorithms to evaluate QA pairs, which feeds the concatenated representation of the question and the answer to the subsequent classifier. As we all know, questions in an asynchronous video interview are not limited to specific domains. That is to say, candidates can answer questions according to their work or study experience. In this way, the candidates' answers will be varied and it is difficult to evaluate the answer accurately only by text matching. Intuitively, it is more reasonable to evaluate QA pairs through the semantic interaction between questions and answers. A critical challenge along this line is how to reveal the latent relationships between each question and answer.  %Intuitively, experienced interviewers could discover the semantic-level correlation between interview questions and candidates' answers, then obtain a preliminary judgement on the answer to the current question, and finally give an assessment based on the judgements of several problems. Therefore,  %In this work, we propose a sentence-level reasoning GNN to assess the single QA pair at the semantic interaction level. Graph neural networks  can learn effective representation of nodes by encoding local graph structures and node attributes. Due to the compactness of model and the capability of inductive learning, GNNs are widely used in modeling relational data and logical reasoning. Recently, ~ proposed a GNN variant, Named ExpressGNN, to strike a nice balance between the representation power and the simplicity of the model in probabilistic logic reasoning.~ constructed the DialogeGCN to address context propagation issues present in the RNN-based methods. Specifically, they leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Inspired by, we present a sentence-level relational GCN to represent the internal temporal and QA interaction dependency in the process of answering questions. %Recently, graph neural network or graph emebedding has attracted wide attention. Graph neural networks have been effective at tasks thought to have rich relational structure and can preserve global structure information of a graph in graph emebedding.   %In this work, we aim to address the task of automatically scoring the textual answer of candidates at the semantic interaction level.  %The automatic short answer scoring  is a task of estimating a score of a short text answer written as response to a given prompt on the basis of whether the answer satisfies the rubrics prepared by a human in advance. ASAS systems have mainly been constructed to markedly reduce the scoring cost of human rater.   %鐟欏嫬鍨鍫ユ閸掕泛鐣鹃敍灞芥礈濮濄倕顕梻顕顣介崪灞芥礀缁涙棃妫跨拠顓濈疅娴溿倓绨伴惃鍕閹烘ɑ妯夊妤佹纯閸旂娀鍣哥憰 %閸ョ偓膩閸ㄥ婀梻顕顣介幒銊ф倞娴犺濮熸稉濂僥ep learning has proven to be effective in long text NLP tasks. Due to the lack of information in the short sentence of the ASAS corpus, it seems not good enough in the ASAS task.  For the second stage of grading the candidate, based on the representation of QA pairs, exists methods prefer to encoder question-answer pairs as a sequence directly. However, this kind of approaches lead to insufficient interaction between the semantic information of question and answer pairs. Therefore, it is difficult to ensure the rationality and explainability of the evaluation. To mitigate this issue, in the first stage, we present a semantic-level graph attention network  to model the interaction states of each QA session.    %Automatic scoring of answer transcriptions in job interview aims to evaluate multiple question-answer pairs.  %To alleviate this limitation of previous approaches, To this end, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic scoring of answer transcriptions  in job interviews. Specifically, the proposed sentence-level relational graph convolutional neural network  is used to capture the contextual dependency, and the semantic-level Reasoning graph attention network  is applied to acquire the latent interaction states. And the contribution of our work can be summarized as follows:    
"," %Automatic scoring of answer transcripts in job interview aims to evaluate multiple question-answer pairs. The key challenge is how to conduct deep interaction on the semantic level for each question-answer pair, and give the evaluation results combined with multiple interaction states. Recent studies either use text matching approaches to evaluate each question-answer pair roughly, or employ the sequential model to deal with disordered question-answer pairs which fail to take advantages of the semantic association between questions and answers, and the logical connection between question-answer pairs. In this work, we propose a hierarchical reasoning Graph Neural Network  for the automatic assessment of multi-question answering. Specifically, we construct a sentence-level reasoning GNN to assess the single question-answer pair. Based on these graphs, we propose a document-level reasoning GNN to model the interaction states of question-answer pairs. The first module utilizes each sentence in the question and answer to establish the connection between them. The second module adopts a graph convolutional network to encoder interaction states of each pair and aggregates evidence with graph attention mechanism for predicting the final score. Empirical results on Chinese and English interview datasets show that our proposed model outperforms both sequence-based and pre-training based  benchmark models.  %We address the task of automatically scoring the answer competency of candidates based on textual features from the automatic speech recognition transcriptions. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and give the evaluation results combined with multiple interaction states. Recent studies either use text matching approaches to evaluate each QA pair roughly, or employ the sequential model to deal with disordered QA pairs which fail to take advantages of the semantic association between questions and answers, and the logical connection between QA pairs. In this work, we propose a hierarchical reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level reasoning GNN to assess the single QA pair. Based on these graphs, we propose a document-level reasoning GNN to model the interaction states of QA pairs. The first module utilizes each sentence in the question and answer to establish the connection between them. The second module adopts a graph convolutional network to encoder interaction states of each pair and aggregates evidence with graph attention mechanism for predicting the final score. Empirical results conducted on CHNAT and ENGIAT  clearly validate that our proposed model outperforms both text matching based benchmark models.  %We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the video job interview. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and then give the evaluation results combined with multiple interaction states. Recent studies tend to use text matching approaches to evaluate each QA pair roughly, which fails to take advantage of the semantic association between questions and answers. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the latent semantic interaction of sentences in the question or the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit with a global fusion mechanism to aggregates evidence of temporal QA pairs for the final score. Empirical results conducted on CHNAT  clearly validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.    We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the asynchronous video job interview . The key challenge is how to construct the dependency relation between questions and answers, and conduct the semantic level interaction for each question-answer  pair. However, most of the recent studies in AVI focus on how to represent questions and answers better, but ignore the dependency information and interaction between them, which is critical for QA evaluation. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the dependency information of sentences in or between the question and the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit encoder to represent the temporal question-answer pairs for the final prediction. Empirical results conducted on CHNAT  validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.",396
"  Social media is a unique source of information. On the one hand, their low cost, easy access and distribution speed make it possible to quickly share the news. On the other hand, the quality and reliability of social media news is difficult to verify . This is the source of a lot of false information that has a negative impact on society.   Over the past year, the world has been watching the situation developing around the novel coronavirus pandemic. The COVID-19 pandemic has become a significant newsworthy event of 2020. Therefore, news related to COVID-19 are actively discussed on social media and this topic generates a lot of misinformation. Fake news related to the pandemic have large-scale negative social consequences, they provoke huge public rumor spreading and misunderstanding about the COVID-19 and aggravate effects of the pandemic. Moreover, recent studies  show an increase in symptoms such as anxiety and depression in connection with the pandemic. This is closely related to the spread of misinformation, because fake news can be more successful when the population is experiencing a stressful psychological situation . The popularity of fake news on social media can rapidly increase, because the rebuttal is always published too late. In this regard, there is evidence that the development of tools for automatic COVID-19 fake news detection plays a crucial role in the regulation of information flows.  In this paper, we present our approach for the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in English  that attracted 433 participants on CodaLab. This approach achieved the weighted F1-score of 98.69  on the test set among 166 submitted teams in total.  The rest of the paper is organized as follows. A brief review of related work is given in Section 2. The definition of the task has been summarized in Section 3, followed by a brief description of the data used in Section 4. The proposed methods and experimental settings have been elaborated in Section 5. Section 6 contains the results and error analysis respectively. Section 7 is a conclusion.  
"," The COVID-19 pandemic has had a huge impact on various areas of human life. Hence, the coronavirus pandemic and its consequences are being actively discussed on social media. However, not all social media posts are truthful. Many of them spread fake news that cause panic among readers, misinform people and thus exacerbate the effect of the pandemic. In this paper, we present our results at the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in English. In particular, we propose our approach using the transformer-based ensemble of COVID-Twitter-BERT  models. We describe the models used, the ways of text preprocessing and adding extra data. As a result, our best model achieved the weighted F1-score of 98.69 on the test set  of this shared task that attracted 166 submitted teams in total.",397
" Identifying emotion in dialogues is one of the most challenging tasks in the area of natural language processing, which is very important for building the dialogue systems . Though sentiment analysis is being studied in natural language community for a long time , understanding multiple emotions expressed in chats and conversations comes up with relatively harder challenges  for many reasons.   Various individuals may respond differently towards the same comment. Absence of voice modulations and facial expressions in informal chatting makes it difficult to capture emotion in the conversation. This type of problem is earlier addressed in EmoContext  and EmotionX-2018 . People during chatting often take help of emojis, figures and message contractions not only to reduce effort and shorten comments but also to express their emotions better. In this case, GIFs play an essential role . Often social media users reply with GIFs only, without any text response. Therefore, to understand the different emotions associated with a GIF in the reply, it is necessary to consider the comment and its relation with its associated reply. This type of problem is earlier addressed in EmotionX-2019 challenge  where the task was to predict emotions in spoken dialogues and chats.   The enhanced and better expressiveness of GIFs in comparison to other popular graphics-based media, such as emojis and emoticons have made their utilization amazingly mainstream on social media and a significant expansion to online human communication which has motivated the introduction of EmotionGIF 2020  shared task.     Given an unlabelled tweet and its reply , the challenge is to recommend the possible categories its GIF response may belong to. All tweets in the training set have their GIF responses with some tweets having their text responses as well. The task requires to return a non-empty subset of 1-6 categories from among 43 possible GIF categories for a given unlabelled tweet.      & text & reply & categories & mp4 \\ \hline 47 & what is sleep & - & [""shrug"", ""idk""] & 4f69c84cb....mp4 \\ 42 & Are your ready for America & - & [""slow\_clap"", ""applause"", & \\ & to Open Up Again? & &  ""yes""] &  9acf47aab3....mp4 \\ 95 & we as a collective have been & & & \\ & traumatized by men with J & & & \\ & names. & Woahhhhh lol & [""yes""] & 12d85340f....mp4 \\ \hline     In this paper, we develop a deep learning framework for predicting the categories of a GIF response for an unlabelled tweet. We build multiple deep neural-based systems, such as CNN , Bidirectional Gated Recurrent Unit  , and Bidirectional Long Short Term Memory Networks  . We support our models with an attention mechanism  that emphasizes on the important parts of a given input tweet. We combine multiple basic models to result in a couple of stacked architectures , and finally, we report the final predictions from a majority voting-based ensemble  method that combines all the developed models. Our proposed frameworks are less complex than the standard transformer models, but can provide reasonably good results and can be trained using local GPU support conveniently.  The rest of the paper is organized as follows: Section 2 gives a brief description of the dataset and the various preprocessing measures applied to them. The details of the proposed methodologies are discussed in section 3. In Section 4, we discuss the various experimental details and their results. Finally, we conclude the paper in section 5.  
"," In this paper, we describe the systems submitted by our IITP-AINLPML team in the shared task of SocialNLP 2020, EmotionGIF 2020, on predicting the category of a GIF response for a given unlabelled tweet.  For the round 1 phase of the task, we propose an attention-based Bi-directional GRU network trained on both the tweet  and their replies  and the given category for its GIF response. In the round 2 phase, we build several deep neural-based classifiers for the task and report the final predictions through a majority voting based ensemble technique. Our proposed models attains the best Mean Recall  scores of 52.92\% and 53.80\% in round 1 and round 2, respectively.",398
"   Machine translation has been shown to exhibit gender bias , and several solutions have already been proposed to mitigate it . The general gender bias in Natural Language Processing  has been mainly attributed to data . Several studies show the pervasiveness of stereotypes in book collections , or Bollywood films , among many others. As a consequence, our systems trained on this data exhibit biases. Among other strategies, several studies have proposed to work in data augmentation to balance data  or forcing gender-balanced datasets . In parallel, other initiatives focus on documenting our datasets  to prioritize transparency.  However, data is not the only reason for biases, and recent studies show that %algorithms and training strategies matter.  our models can be trained in a robust way to reduce the effects of data correlations . In , the authors explored available mitigations and by increasing dropout, which resulted in improving how the models reasoned about different stereotypes in WinoGender examples .   The purpose of the current paper is to explore if the Multilingual Neural Machine Translation  architecture can impact the amount of gender bias. To answer this question, we compare MNMT architectures trained with the same data and quantify their amount of gender bias with the standard WinoMT evaluation benchmark . Results show that the Language-Specific encoders-decoders  exhibit less bias than the Shared encoder-decoder . Then, we analyze and visualize why the MNMT architecture impacts mitigating or amplifying this bias by studying its internal workings. We study the amount of gender information that the source embeddings encode, and we see that Language-Specific surpasses Shared in these terms, allowing for a better prediction of gender.  Additionally, and taking advantage that both Shared and Language-Specific are based on the Transformer , we study the coefficient of variation in the attention , which shows that the attention span is narrower for the Shared system than for the Language-Specific one. Therefore, the context taken into account is smaller for the Shared system, which causes a higher gender bias.   %We observe that this is caused by using a Shared encoder-decoder with several languages since pairwise Bilingual systems have a wider attention span. Given the similarities of not sharing modules and parameters across languages in both Bilingual and Language-Specific, this characteristic  of the Bilingual systems prevails in the language-specific architecture.   Finally, we also do a manual analysis to investigate which biases have a linguistic explanation. %implications in gender bias has the target language from the linguistic and social point of view.    
"," Multilingual Neural Machine Translation architectures mainly differ in the amount of sharing modules and parameters among languages. In this paper, and from an algorithmic perspective, we explore if the chosen architecture, when trained with the same data, influences the gender bias accuracy. Experiments in four language pairs show that Language-Specific encoders-decoders exhibit less bias than the Shared encoder-decoder architecture. Further interpretability analysis of source embeddings and the attention shows that, in the Language-Specific case, the embeddings encode more gender information, and its attention is more diverted. Both behaviors help in mitigating gender bias.",399
" Commonsense question answering  is recently an attractive field in that it requires systems to understand the common sense information beyond words, which are normal to human beings but nontrivial for machines. There are plenty of datasets that are proposed for this purpose, for instance, CommonsenseQA , CosmosQA , WIQA . Different from traditional machine reading comprehension  tasks such as SQuAD  or NewsQA  that the key information for answering the questions is directly given by the context paragraph, solving commonsense questions requires a more comprehensive understanding of both the context and the relevant common knowledge, and further reasoning out the hidden logic between them. There are varieties of knowledge bases that meet the need, including text corpora like Wikipedia, and large-scale knowledge graphs .  Recent popular solution resorts to external supporting facts from such knowledge bases as evidence, to enhance the question with commonsense knowledge or the logic of reasoning . However, the quality of the supporting facts is not guaranteed, as some of them are weak in interpretability so that do not help the question answering. Specifically, current methods are mainly two-fold. The first group of methods  pre-train language models on those external supporting facts  so that the models could remember some of the common knowledge, which is empirically proven by Tandon et al.  and Trinh and Le . The second group of methods  incorporates the question with knowledge subgraphs or paths that carry information such as relation among concepts or show multi-hop reasoning process. The structured information is typically encoded via graph models such as GCN , and after which merged with the question features. Generally, current methods all handle evidence by brute force, without further selection or refinement according to the interpretability of the supporting facts. But as the example shown in Figure, some of the supporting facts do not interpret the question, regardless that they are semantically related. Thus, there is need for models that will further our processing of the evidence.  In this paper, we introduce a new recursive erasure memory network  that further refines the candidate supporting fact set. The REM-Net consists of three main components: a query encoder, an evidence generator, and a novel recursive erasure memory  module. Specifically, the query encoder is a pre-trained encoder that encodes the question. The evidence generator is a pre-trained generative model that produces candidate supporting facts based on the question. Compared with those retrieved supporting facts, the generated facts provides new question-specific information beyond the existing knowledge bases. The REM module refines the candidate supporting fact set by recursively matching the supporting facts and the question in feature space to estimate each fact's quality. This estimation helps both updating the question feature and the supporting fact set. The question feature is updated by a residual term, whereas the supporting fact set is updated by removing the low-quality facts. Compared with the standard attention mechanisms  that allocate weights to the supporting facts once, the multi-hop operation in REM module widens the gap of how much each supporting fact contributes to the question answering by the number of recursive steps their features are incorporated for the feature update. Therefore this procedure leads to a refined use of given supporting facts.  We conduct experiments on two commonsense QA benchmarks, WIQA  and CosmosQA . The experimental results demonstrate that REM-Net outperforms current methods, and the refined supporting facts are more qualified for the questions. Our contributions are mainly three-fold:      
"," When answering a question, people often draw upon their rich world knowledge in addition to the particular context. While recent works retrieve supporting facts/evidence from commonsense knowledge bases to supply additional information to each question, there is still ample opportunity to advance it on the quality of the evidence. It is crucial since the quality of the evidence is the key to answering commonsense questions, and even determines the upper bound on the QA systems' performance. In this paper, we propose a recursive erasure memory network  to cope with the quality improvement of evidence. To address this, REM-Net is equipped with a module to refine the evidence by recursively erasing the low-quality evidence that does not explain the question answering. Besides, instead of retrieving evidence from existing knowledge bases, REM-Net leverages a pre-trained generative model to generate candidate evidence customized for the question. We conduct experiments on two commonsense question answering datasets, WIQA and CosmosQA. The results demonstrate the performance of REM-Net and show that the refined evidence is explainable.",400
"  We typically train neural machine translation  systems on human-translated parallel texts, then ask them to decode previously-unseen source sentences.  Trained parameter values induce a distribution  over all pairs of source/target strings .  Given a new source string , the NMT decoder searches for the best target string :        = \argmax_y P   This optimization is unsolvable for general recurrent neural networks , while  present an exact optimization search algorithm for consistent NMT models.       {|c}{Decoder beam = 512} \\ pair & Length ratio & Empty ratio \\   Our training data does not contain any source strings translated to empty strings, so why does NMT learn to assign high probability to empty translations? Our findings are:      
","  We investigate why neural machine translation  systems assign high probability to empty translations. We find two explanations. First, label smoothing makes correct-length translations less confident, making it easier for the empty translation to outscore them. Second, NMT systems use the same, high-frequency EoS word type to end all target sentences, regardless of length. This creates an implicit smoothing that increases the relative probability zero-length translations.  Using different EoS types in target sentences of different lengths exposes this implicit smoothing.",401
"   Understanding emotion in human social conversations or chitchat has gained popularity in the natural language processing community due to its usefulness in developing human-like conversational agents. Emotions revealed in social chitchat are rather complex. It has many categories of emotions to distinguish due to subtle variations present in human emotion. For example, Sadness and Disappointment are pursued and dealt differently in human conversations. Also, the listeners' reaction to emotions is not always a straightforward mirroring effect of the speakers' emotions. Rather it can be more neutral and convey a specific intent, as is evident from the dialogue example in Table .   [ht!] {|r X|}  \\ Listener:&oh no! That閳ユ獨 scary! What do you think it is? \\ Speaker:&I don閳ユ獩 know, that閳ユ獨 what閳ユ獨 making me anxious. \\ Listener:&I閳ユ獡 sorry to hear that. \\ ; Agreeing; Acknowledging; Sympathizing; Encouraging; Consoling: Suggesting; Wishing; and Neutral . They have automatically annotated the EmpatheticDialogues dataset  with 32 fine-grained emotions and the 9 empathetic response intents and discovered frequent emotion-intent exchange patterns in human social conversations. They observe that this type of dataset tagged with fine-grained emotions and response intents could train neural chatbots to generate empathetically appropriate responses conditioned on a selected emotion or intent. However, for this purpose, a large-scale emotion and intent labeled dataset is even more desirable. Curating such a dataset is technically challenging because 1) annotating such a large-scale dataset require human labor that is costly, and 2) given the fine-granularity of the emotion and intent labels, the human labeling task is more difficult compared to more generic Angry-Happy-Sad. As a result, existing manually labeled emotional dialogue datasets such as IEMOCAP , MELD , and DailyDialogue  are smaller in scale and contain only a limited set of emotions , with simpler dialogue responding strategies, or both. Also, existing datasets often contain a label Neutral or Other for responses that do not convey emotion, which introduces vagueness and limits the ability of automatic agents that use such datasets in learning useful response strategies.   % , EmotionLines , and EmoContext   To fill the above gap, we curate a novel large-scale dialogue dataset, OSED , containing 1M emotional dialogues from movie subtitles, in which each dialogue turn is automatically annotated with 32 fine-grained emotions and 9 empathetic response intents. Movie subtitles well approximate human social conversations and how emotion is handled in them. It is one of the major sources to learn emotional variations and corresponding response strategies. To reduce the cost of human labeling and the complexity of labeling dialogues with fine-grained emotions and intents, we devise a semi-automated human computation task to collect fine-grained emotion and intent labels for a small set of movie dialogues . We then follow a semi-supervised approach to expand the labeled data and train a dialogue emotion classifier to automatically annotate 1M emotional dialogues.   The process of curating the dataset consists of several stages. First, we apply automatic turn and dialogue segmentation methods on movie subtitles in the OpenSubtitles  corpus  and obtain close to 9M dialogues. After data cleaning and removing duplicates, we reduce its size to 4M. Then, we apply a weak labeler, EmoBERT  trained on the EmpatheticDialogues dataset , to label utterances in OS dialogues and filter 1M emotional dialogues . Thirdly, with semi-supervised learning methods, we refine EmoBert and obtain EmoBert+, a more advanced dialogue emotion classifier trained on OS dialogues. To evaluate EmoBert+, we compare it with FastText. The former is more accurate than FastText. Finally, we use EmoBert+ to label dialogues in OSED initial to obtain the final 1M OSED dataset. We evaluate the quality of the resultant dataset by visually inspecting the emotion-intent flow patterns that occur in the dataset and checking if they conform to the patterns of human social conversations discovered in existing work . Figure  summarizes the process of creating OSED. The data curation pipeline we follow substantially reduces the cost of human labor, while ensuring quality annotations.   Our contributions in this paper are three-fold. 1) We curate a dialogue dataset, OSED, containing 1M emotional dialogues labeled with 32 fine-grained emotions and 9 empathetic response intents. Compared to existing dialogue datasets tagged with emotions, OSED is more general-purpose, significantly larger, and contains more fine-grained emotions and empathetic response strategies. 2) We outline the complex pipeline used to derive this dataset and evaluate the annotation quality using visualization methods. 3) We release our fine-grained emotion classifier used to annotate the OSED dataset, which can be used as a general-purpose classifier capable of recognizing fine-grained emotions and empathetic response intents in social chitchat.     
"," We propose a novel large-scale emotional dialogue dataset, consisting of 1M dialogues retrieved from the OpenSubtitles corpus and annotated with 32 emotions and 9 empathetic response intents using a BERT-based fine-grained dialogue emotion classifier. This work explains the complex pipeline used to preprocess movie subtitles and select good movie dialogues to annotate. We also describe the semi-supervised learning process followed to train a fine-grained emotion classifier to annotate these dialogues. Despite the large set of labels, our dialogue emotion classifier achieved an accuracy of $65\%$ and was used to annotate 1M emotional movie dialogues from OpenSubtitles. This scale of emotional dialogue classification has never been attempted before, both in terms of dataset size and fine-grained emotion and intent categories. Visualization techniques used to analyze the quality of the resultant dataset suggest that it conforms to the patterns of human social interaction.",402
"  Neural machine translation  has advanced significantly in recent years . In particular, the Transformer model has become popular for its well-designed architecture and the ability to capture the dependency among positions over the entire sequence . Early systems of this kind stack 4-8 layers on both the encoder and decoder sides , and the improvement often comes from the use of wider networks . More recently, researchers try to explore deeper models for Transformer. Encouraging results appeared in architecture improvements by creating direct pass from the low-level encoder layers to the decoder , and proper initialization strategies .  Despite promising improvements, problems still remain in deep NMT. Deep Transformer stacked by dozens of encoder layers always have a large number of parameters, which are computationally expensive and memory intensive. For example, a 48-layer Transformer is  larger than a 6-layer system and  slower for inference. It is difficult to deploy such models on resource-restricted devices, such as mobile phones. Therefore, it is crucial to compress such heavy systems into light-weight ones while keeping their performance.  Knowledge distillation is a promising method to address the issue. Although several studies  have attempted to compress the 12-layer BERT model through knowledge distillation, effectively compressing extremely deep Transformer NMT systems is still an open question in the MT community. In addition, these methods leverage sophisticated layer-wise distillation loss functions to minimize the distance between the teacher and the student models, which requires huge memory consumption and enormous training cost.  In this paper, we investigate simple and efficient compression strategies for deep Transformer. We propose a novel Transformer compression approach ) to transfer the knowledge from an extremely deep teacher model into a shallower student model. We disturb the computation order among each layer group during the teacher training phase, which is easy to implement and memory friendly. Moreover, to further enhance the performance of the teacher network, we introduce a vertical ``dropout''  into training by randomly omitting sub-layers to prevent co-adaptations of the over-parameterized teacher network. Although similar technique has been discussed in 's work, we believe that the finding here is complementary to theirs. Both Gpkd and regularization training methods can be well incorporated into the teacher training process, which is essential for obtaining a strong but light-weight student model.  \pgfdeclarepatternformonly{soft horizontal lines}{\pgfpointorigin}{\pgfqpoint{100pt}{1pt}}{\pgfqpoint{100pt}{3pt}}% {   \pgfsetstrokeopacity{0.3}   \pgfsetlinewidth{0.1pt}   \pgfpathmoveto{\pgfqpoint{0pt}{0.5pt}}   \pgfpathlineto{\pgfqpoint{100pt}{0.5pt}}   \pgfusepath{stroke} }  \pgfdeclarepatternformonly{soft crosshatch}{\pgfqpoint{-1pt}{-1pt}}{\pgfqpoint{6pt}{6pt}}{\pgfqpoint{5pt}{5pt}}% {   \pgfsetstrokeopacity{0.3}   \pgfsetlinewidth{0.4pt}   \pgfpathmoveto{\pgfqpoint{4.5pt}{0pt}}   \pgfpathlineto{\pgfqpoint{0pt}{4.5pt}}   \pgfpathmoveto{\pgfqpoint{0pt}{0pt}}   \pgfpathlineto{\pgfqpoint{4.5pt}{4.5pt}}   \pgfusepath{stroke} }  \definecolor{ugreen}{rgb}{0,0.5,0}      = [line width=0.5pt,minimum height=2.4em,minimum width = 1em,inner sep=3pt,rounded corners=1pt,font=\tiny]   \tikzstyle{legend_node} = [minimum height=1em,minimum width = 1em,draw,thick,inner sep=3pt,rounded corners=1pt,font=;   	_{1}};   	_{1}};   	_{1}};   	_{n}\mathrm{Group}_{1}\mathrm{Group}_{2}L_{1}L_{2}L_{3}2.4630.630.503.2$ times with almost no loss in BLEU.    
","     Recently, deep models have shown tremendous improvements in neural machine translation . However, systems of this kind are computationally expensive and memory intensive. In this paper, we take a natural step towards learning strong but light-weight NMT systems. We proposed a novel group-permutation based knowledge distillation approach to compressing the deep Transformer model into a shallow model. The experimental results on several benchmarks validate the effectiveness of our method. Our compressed model is $8\times$ shallower than the deep model, with almost no loss in BLEU. To further enhance the teacher model, we present a Skipping Sub-Layer method to randomly omit sub-layers to introduce perturbation into training, which achieves a BLEU score of 30.63 on English-German newstest2014. The code is publicly available at https://github.com/libeineu/GPKD.",403
"  {S}{emantic} role labeling , also known as shallow semantic parsing, conveys the meaning of a sentence by forming a predicate-argument structure for each predicate in a sentence, which is generally described as the answer to the question ""Who did what to whom, where and when?"". The relation between a specific predicate and its argument provides an extra layer of abstraction beyond syntactic dependencies  , such that the labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Given a sentence in Figure , SRL pipeline framework consists of 4 subtasks, including predicate identification , predicate disambiguation , arguments identification  and arguments classification . SRL is a core task of natural language processing  having wide range of applications such as neural machine translation , information extraction , question answering , emotion recognition from text , document summarization  etc.   Semantic role labeling can be categorized into two categories, span and dependency. Both types of SRL are useful for formal semantic representations but dependency based SRL is better for the convenience and effectiveness of semantic machine learning. Johansson and Nugues  concluded that the best dependency based SRL system outperforms the best span based SRL system through gold syntactic structure transformation. The same conclusion was also verified by Li et al.  through a solid empirical verification. Furthermore, since 2008, dependency based SRL has been more studied as compared to span based SRL. With this motivation, we focus on dependency based SRL, which is mainly popularized by CoNLL-2008 and CoNLL-2009 shared tasks .   The traditional approaches to SRL focus on feature engineering which struggles in apprehending discriminative information  while neural networks are proficient enough to extract features automatically . Specifically, since large scale empirical verification of Punyakanok et al. , syntactic information has been proven to be extremely beneficial for SRL task. Later works  achieve satisfactory performance for SRL with syntax-agnostic models which creates conflict with the long-held belief that syntax is essential for high-performance SRL .  The study of Li et al.  shows that the empirical results from neural models on the less importance of syntax indicate a potential challenge and despite the satisfactory performance of syntax-agnostic SRL systems, the reasons behind the absence of syntax in these models are three-fold. First, the effective incorporation of syntax in neural SRL models is quite challenging as compared to traditional approaches. Second, neural SRL models may cover partial syntactic clues more or less. Third, syntax has always been a complicated formalism in linguistics and its not easy to encode syntax for later usage. %Despite the satisfactory performance of syntax-agnostic SRL models, the reasons behind the absence of syntax in these models are two-fold. First, the effective incorporation of syntax information in neural SRL models is quite challenging. Second, the unreliability of syntactic parsers on account of the risk of erroneous syntactic input may lead to error proliferation. This has been proven by Li et al.  through a strong empirical verification. They show that the effective method of syntax incorporation and the high quality of syntax can promote SRL performance.%   
"," Semantic role labeling  aims at elaborating the meaning of a sentence by forming a predicate-argument structure. Recent researches depicted that the effective use of syntax can improve SRL performance. However, syntax is a complicated linguistic clue and is hard to be effectively applied in a downstream task like SRL. This work effectively encodes syntax using adaptive convolution which endows strong flexibility to existing convolutional networks. The existing CNNs may help in encoding a complicated structure like syntax for SRL, but it still has shortcomings. Contrary to traditional convolutional networks that use same filters for different inputs, adaptive convolution uses adaptively generated filters conditioned on syntactically-informed inputs. We achieve this with the integration of a filter generation network which generates the input specific filters. This helps the model to focus on important syntactic features present inside the input, thus enlarging the gap between syntax-aware and syntax-agnostic SRL systems. We further study a hashing technique to compress the size of the filter generation network for SRL in terms of trainable parameters. Experiments on CoNLL-2009 dataset confirm that the proposed model substantially outperforms most previous SRL systems for both English and Chinese languages.",404
"     {}             The randomly sampling-based user simulator neglects the fact that human learning supervision is often accompanied by a curriculum . For instance, when a human-teacher teaches students, the order of presented examples is not random but meaningful, from which students can benefit . Therefore, this randomly sampling-based user simulators bring two issues:   issue:  since the ability of the dialogue agent does not match the difficulty of the sampled user goal, it takes a long time for the dialogue agent to learn the optimal strategy . For example, in the early learning phase, it is possible that the random sampling method arranges the dialogue agent to learn more complex user goals first, and then learn simpler user goals.   issue:  using random user goals to collect experience online is not stable enough, making the learned dialogue policy unstable and difficult to reproduce. Since RL is highly sensitive to the dynamics of the training process, dialogue agents trained with stable experience can guide themselves more effectively and stably than dialogue agents trained with instability.     Most previous studies of dialogue policy have focused on the efficiency issue, such as reward shaping , companion learning , incorporate planning , etc. However, stability is a pre-requisite for the method to work well in real-world scenarios. It is because, no matter how effective an algorithm is, an unstable online leaned policy may be ineffective when applied in the real dialogue environment. This can lead to bad user experience and thus fail to attract sufficient real users to continuously improve the policy. As far as we know, little work has been reported about the stability of dialogue policy. Therefore, it is essential to address the stability issue.    In this paper,  we propose a novel policy learning framework that combines curriculum learning and deep reinforcement learning,  namely Automatic Curriculum Learning-based Deep Q-Network . As shown in Figure, this framework replaces the traditional random sampling method in the user simulator with a teacher policy model that arranges a meaningful ordered curriculum and dynamically adjusts it to help dialogue agent  for automatic curriculum learning. As a scheduling controller for student agents, the teacher policy model arranges students to learn different user goals in different learning stages without any requirement of prior knowledge. Sampling the user goals that match the ability of student agents regarding different difficulty of each user goal, can not only increases the feedback of the environment to the student agent but also makes the learning of the student agent more stable.  There are two criteria for evaluating the sampling order of each user goal: the learning progress of the student agent and the over-repetition penalty. The learning progress of the student agent emphasizes the efficiency of each user goal, encouraging the teacher policy model to choose the user goals that match the ability of the student agent to maximize the learning efficiency of the student agent. The over-repetition penalty emphasizes the sampled diversity, preventing the teacher policy model from cheating\footnote[1]{The teacher policy model repeatedly selects user goals that the student agent has mastered to obtain positive rewards.}. The incorporation of the learning progress of the student agent and the over-repetition penalty reflects both sampled efficiency and sampled diversity to improve efficiency as well as stability of ACL-DQN.   Additionally, the proposed ACL-DQN framework can equip with different curriculum schedules. Hence, in order to verify the generalization of the proposed framework, we propose three curriculum schedule standards for the framework for experimentation: i) Curriculum schedule A: there is no standard, only a single teacher model; ii) Curriculum schedule B: user goals are sampled from easiness to hardness in proportion; iii) Curriculum schedule C: ensure that the student agents have mastered simpler goals before learning more complex goals.   Experiments have demonstrated that the ACL-DQN significantly improves the dialogue policy through automatic curriculum learning and achieves better and more stable performance than DQN. Moreover, the ACL-DQN equipped with the curriculum schedules can be further improved. Among the three curriculum schedules we provided, the ACL-DQN under curriculum schedule C with the strength of supervision and controllability, can better follow up on the learning progress of students and performs best. In summary, our contributions are as follows:       
"," Dialogue policy learning based on reinforcement learning is difficult to be applied to real users to train dialogue agents from scratch because of the high cost. User simulators, which choose random user goals for the dialogue agent to train on, have been considered as an affordable substitute for real users. However, this random sampling method ignores the law of human learning, making the learned dialogue policy inefficient and unstable. We propose a novel framework, Automatic Curriculum Learning-based Deep Q-Network , which replaces the traditional random sampling method with a teacher policy model to realize the dialogue policy for automatic curriculum learning. The teacher model arranges a meaningful ordered curriculum and automatically adjusts it by monitoring the learning progress of the dialogue agent and the over-repetition penalty without any requirement of prior knowledge. The learning progress of the dialogue agent reflects the relationship between the dialogue agent's ability and the sampled goals' difficulty for sample efficiency. The over-repetition penalty guarantees the sampled diversity. Experiments show that the ACL-DQN significantly improves the effectiveness and stability of dialogue tasks with a statistically significant margin. Furthermore, the framework can be further improved by equipping with different curriculum schedules, which demonstrates that the framework has strong generalizability.",405
" Exponential growths of micro-blogging sites and social media not only provide platforms for empowering freedom of expressions and individual voices, but also enables people to express anti-social behavior such as online harassment, cyberbullying, rumors, and spreading hatred statements. %In recent years, micro-blogging sites and social media sites have grown exponentially, enabling the users to express anti-social behavior, false political or religious rumor, and spreading hatred activities. Besides, abusive or threatening speech that expresses prejudice against a certain group, which religious, political, geopolitical, personal, and gender abuse are very common and on the basis of race, religion, and sexual orientation are getting pervasive. United Nations Strategy and Plan of Action on Hate Speech defines hate speech as ``any kind of communication in speech, writing or behaviour, that attacks or uses pejorative or discriminatory language with reference to a person or a group on the basis of who they are, in other words, based on their religion, ethnicity, nationality, race, colour, descent, gender or other identity factor''.  Bengali is spoken by 230 million people in Bangladesh and India, making it one of the major languages in the world. Although, a rich language with a lot of diversity, Bengali is severely low-resourced for natural language processing~, which is due to the scarcity of computational resources such as language models, labeled datasets, and efficient machine learning~ methods required for different NLP tasks. Similar to other major languages like English, the use of hate speech in Bengali is also getting rampant. This is mainly due to unrestricted access and use of social media and digitalization. Some examples of Bengali hate speech and their respective English translations are shown in  that are either directed towards a specific person or entity or generalized towards a group. These examples signify how severe Bengali hateful statements could be. Nevertheless, there is a potential chance that these could lead to serious consequences such as hate crimes, regardless of languages, geographic locations, or ethnicity.                   Automatic identification of hate speech and creating awareness among people is very challenging. However, manual reviewing and verification from a vast amount of online content is not only labor-intensive but also time-consuming. Nevertheless, accurate identification requires automated, robust, and efficient machine learning~ methods. Compared to traditional ML and neural network~-based approaches, state-of-the-art~ language models are becoming increasingly effective. On a serious drawback: a prediction made by many models can neither be traced back to the input, nor it is clear why the output is transformed in a certain way. This makes even the most efficient DNN models `black-box' methods. On the other hand, the General Data Protection Regulation~ by the European Parliament enforces the `right to explanation', which prohibits the use of ML for automated decisions unless a clear explanation of the logic used to make each decision is well explained. Therefore, how a prediction is made by an algorithm should be as transparent as possible in order to gain human trust.    %Recent research efforts from both the NLP and ML communities have proven to be very useful for well-resourced languages like English. %Nevertheless, accurate identification requires automated, robust, and efficient machine learning~ methods. As state-of-the-art language models becoming increasingly effective, their decisions should be made as transparent as possible in order to improve human trust. %Some of these techniques are based on the model閳ユ獨 local gradient information while other methods seek to redistribute the function閳ユ獨 value on the input variables, typically by reverse propagation in the neural network graph. Bach et al. proposed specific propagation rules for neural networks . These rules were shown to produce better explanations than e.g. gradient-based techniques not only for computer vision but also text data. To overcome the shortcomings of `black-box'-based methods and inspired by the outstanding success of transformer language models~, we propose an explainable approach for hate speech detection from under-resourced Bengali language. Our approach is based on the ensemble of several BERT variants, including monolingual Bangla BERT-base, m-BERT~, and XLM-RoBERTa. Further, we not only provide both global and local explanations of the predictions, in a post-hoc fashion but also provide the measure of explanations in terms of faithfulness.  The rest of the paper is structured as follows: \Cref{rw} reviews related work on hate speech and Bengali word embedding. \Cref{section:3} describes the data collection and annotation process. \Cref{nettwork} describes the process of Bengali neural embedding, network construction, and training. \Cref{er} illustrates experiment results, including a comparative analysis with baseline models on all datasets. \Cref{con} summarizes this research with potential limitations and points some possible outlook before concluding the paper.  
","   The exponential growths of social media and micro-blogging sites not only provide platforms for empowering freedom of expressions and individual voices, but also enables people to express anti-social behavior like online harassment, cyberbullying, and hate speech. Numerous works have been proposed to utilize the textual data for social and anti-social behavior analysis, by predicting the contexts mostly for highly-resourced languages like English. However, some languages are under-resourced, e.g., South Asian languages like Bengali, that lack  computational resources for accurate natural language processing~. In this paper, we propose an explainable approach for hate speech detection from the under-resourced Bengali language, which we called DeepHateExplainer. In our approach, Bengali texts are first comprehensively preprocessed, before classifying them into political, personal, geopolitical, and religious hates, by employing the neural ensemble method of different transformer-based neural architectures~. Subsequently, important~ terms are identified with sensitivity analysis and layer-wise relevance propagation~, before providing human-interpretable explanations. Finally, to measure the quality of the explanation~, we compute the comprehensiveness and sufficiency. Evaluations against machine learning~ and deep neural networks~ baselines yield F1 scores of 84\%, 90\%, 88\%, and 88\%, for political, personal, geopolitical, and religious hates, respectively, outperforming both ML and DNN baselines.%, during 3-fold cross-validation tests.",406
"  Sentence embeddings map sentences into a vector space. The vectors capture rich semantic information that can be used to measure semantic textual similarity~ between sentences or train classifiers for a broad range of downstream tasks~. State-of-the-art models are usually trained on supervised tasks such as natural language inference~, or with semi-structured data like question-answer pairs~ and translation pairs~. However, labeled and semi-structured data are difficult and expensive to obtain, making it hard to cover many domains and languages. Conversely, recent efforts to improve language models include the development of masked language model  pre-training from large scale unlabeled corpora .  While internal MLM model representations are helpful when fine-tuning on downstream tasks, they do not directly produce good sentence representations, without further supervised  or semi-structured  fine-tuning.  In this paper, we explore an unsupervised approach, called Conditional Masked Language Modeling , to effectively learn sentence representations from large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on sentence level representations produced by adjacent sentences. The model therefore needs to learn effective sentence representations in order to perform good MLM. Since CMLM is fully unsupervised, it can be easily extended to new languages. We explore CMLM for both English and multilingual sentence embeddings for 100+ languages.  Our English CMLM model achieves state-of-the-art performance on SentEval~, even outperforming models learned using supervised signals. Moreover, models training on the English Amazon review data using our multilingual vectors exhibit strong multilingual transfer performance on translations of the Amazon review evaluation data to French, German and Japanese, outperforming existing multilingual sentence embedding models by  for non-English languages and by  on the original English data.   We further extend the multilingual CMLM to co-train with parallel text  retrieval task, and finetune with cross-lingual natural language inference  data, inspired by the success of prior work on multitask sentence representation learning~ and NLI learning~. We achieve performance  better than the previous state-of-the-art multilingual sentence representation model . Language agnostic representations require semantically similar cross-lingual pairs to be closer in representation space than unrelated same-language pairs~. While we find our original sentence embeddings do have a bias for same language sentences, we discover that removing the first few principal components of the embeddings eliminates the self language bias.  The rest of the paper is organized as follows. \Cref{sec:cmlm} describes the architecture for CMLM unsupervised learning. In \Cref{sec:en_cmlm} we present CMLM trained on English data and evaluation results on SentEval. In \Cref{sec:en_cmlm} we apply CMLM to learn sentence multilingual sentence representations. Multitask training strategies on how to effectively combining CMLM, bitext retrieval and cross lingual NLI finetuning are explored. In \Cref{sec:analysis}, we investigate self language bias in multilingual representations and how to eliminate it.  The contributions of this paper can be summarized as follows:  A novel pre-training technique CMLM for unsupervised sentence representation learning on unlabeled corpora .  An effective multitask training framework, which combines unsupervised learning task CMLM with supervised learning Bitext Retrieval and cross-lingual NLI finetuning.  An evaluation benchmark for multilingual sentence representations.  A simple and effective algebraic method to remove same language bias in multilingual representations. The pre-trained models are released at \url{https://tfhub.dev/s?q=universal-sentence-encoder-cmlm}.   
"," This paper presents a novel training method, Conditional Masked Language Modeling , to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval~, even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval~ and natural language inference~ tasks outperforms the previous state-of-the-art multilingual models by a large margin. We explore the same language bias of the learned representations, and propose a principle component based approach to remove the language identifying information from the representation while still retaining sentence semantics.",407
" Many seemingly convincing rumors such as ``Most humans only use 10 percent of their brain'' are widely spread, but ordinary people are not able to rigorously verify them by searching for scientific literature. In fact, it is not a trivial task to verify a scientific claim by providing supporting or refuting evidence rationales, even for domain experts.  %Such The situation worsens as misinformation is proliferated  %by the  on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes more and more crucial for combating  %against  the spread of misinformation.  %There are many existing datasets and %the corresponding  %systems for fact-verification tasks %, emphasizing on  %in various domains, such as Wikipedia , social media , and politics . These tasks  %are  The existing fact-verification tasks usually consist of three sub-tasks: document retrieval, rationale sentence extraction, and fact-verification. However, due to the nature of scientific literature that requires domain knowledge, it is challenging to collect a large scale scientific fact-verification dataset, and further, to perform fact-verification under a low-resource setting with limited training data.  collected a scientific claim-verification dataset, SciFact, and proposed a scientific claim-verification task: given a scientific claim, find evidence sentences that support or refute  %such the claim  %from  in a corpus of scientific paper abstracts.  also proposed a simple, pipeline-based, sentence-level model, VeriSci, as a baseline solution based on .  %Despite the simplicity of VeriSci ,  VeriSci is a pipeline model that runs modules for abstract retrieval, rationale sentence selection, and stance prediction sequentially, and thus the error generated from  %the an upstream module may propagate to the downstream modules. To overcome this drawback, we hypothesize that a module jointly optimized on multiple sub-tasks may mitigate the error-propagation problem to improve the overall performance.  %On the other hand,  In addition, we observe that a complete set of rationale sentences usually contains multiple inter-related sentences from the same paragraph. Therefore, we propose a novel, paragraph-level, multi-task learning model for the SciFact task.  In this work, we employ compact paragraph encoding, a novel strategy of computing sentence representations using BERT-family models. We directly feed an entire paragraph as a single sequence to BERT, so that the encoded sentence representations are already contextualized on the neighbor sentences by taking advantage of the attention mechanisms in BERT. In addition, we jointly train the modules for rationale selection and stance prediction as multi-task learning  by leveraging the confidence score of rationale selection as the attention weight of the stance prediction module. Furthermore, we compare two methods of transfer learning that mitigate the low-resource issue: pre-training and domain adaptation . Our experiments show that: % the compact paragraph encoding method is beneficial over separately computing sentence embeddings, and  with negative sampling, the joint training of rationale selection and stance prediction is beneficial over the pipeline solution. %\todo{you may want to create a list of contribution. -Violet}  [leftmargin=*]      method is beneficial over separately computing sentence embeddings.       
"," Even for domain experts, it is a non-trivial task to verify a scientific claim by providing supporting or refuting evidence rationales. The situation worsens as misinformation is proliferated on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes crucial for combating the spread of misinformation. % collected a scientific claim-verification dataset, SciFact, to facilitate research on scientific claim-verification.  In this work, we propose a novel, paragraph-level, multi-task learning model for the SciFact task by directly computing a sequence of contextualized sentence embeddings from a BERT model and jointly training the model on rationale selection and stance prediction.",408
" Self attention networks  have been widely studied on many natural language processing  tasks, such as machine translation , language modeling  and natural language inference . It is well accepted that SANs can leverage both the local and long-term dependencies through the attention mechanism, and are highly parallelizable thanks to their position-independent modeling method.  However, such position-independent models are incapable of explicitly capturing the boundaries between sequences of words, thus overlook the structure information that has been proven to be robust inductive biases for modeling texts . Unlike RNNs that model sequential structure information of words by using memory cells, or CNNs that focus on learning local structure dependency of words via convolution kernels, SANs learn flexible structural information in an indirect way almost from scratch. One way to integrate structural information into SAN models is via pre-training, such as BERT , which learns to represent sentences by using unsupervised learning tasks on the large-scale corpus. Recent studies  have shown the ability of pre-training models on capturing structure information of sentences.  Another method to deal with structural information is introducing structure priors into SANs by mask strategies.   proposed the directional self-attention mechanism, which employs two SANs with the forward and backward masks respectively to encode temporal order information.   introduced the Gaussian prior to the transformers for capturing local compositionality of words. Admittedly, structure priors can strengthen the model's capability of modeling sentences and meanwhile assist in capturing proper dependencies. With the help of these learned structure priors, SANs can model sentences accurately even in resource-constrained conditions. [!t] 	 	    Though these models get success on many NLP tasks, these studies commonly focus on integrating one single type of structure priors into SANs, thus fail at making full use of multi-head attentions. One straightforward advantage of using the multi-head attentions lies in the fact that different heads convey different views of texts . In other words, multi-head attentions enable the model to capture the information of texts at multiple aspects, which in return brings thorough views when modeling the texts.  Besides, it is well accepted that one type of structural prior can only reveal part of the structural information from one single perspective. A variety of types of structural priors are needed in order to gain complete structural information of texts. This can be achieved by introducing different structural priors into different parts of attention heads, where different structural priors can complement each other, guiding the SAN models to learn proper dependencies between words. Therefore, to gain a better representation of the texts, a desirable solution should make full use of the multi-head attention mechanism and utilize multiple types of structural priors.  To better alleviate the aforementioned problems, in this paper, we propose a lightweight self attention network, i.e., the Multiple Structural Priors Guided Self Attention Network . The novel idea behind our model lies in the usage of the multi-mask based multi-head attention , which helps our model to better capture different types of dependencies between texts. Thanks to the MM-MH Attention mechanism, our model can capture multiple structural priors, which in return brings benefits in modeling sentences.  Especially, the structural priors we employed come from two categories: the sequential order and the relative position of words. Since the standard SANs are incapable of distinguishing the order between words, we apply the direction mask  directly to each attention head. Motivated by the Bidirectional RNNs , we split the attention heads into two parts. For a given word, we apply the forward mask to the first half of attention heads, which allows it to attend on only the previous words when modeling the reference word. Accordingly, the backward mask is applied to the rest of the attention heads.  Since the direction masks take no consideration of the difference between long-distance words and nearby words, we employ the second category of structural prior as a complement, which could be measured by the distance between pair of words. We integrate two types of distance masks into different attention heads. The first one we utilized is the word distance mask, which describes the physical distance between each pair of words. Besides, for the purpose of capturing the latent hierarchical structure of sentences, we integrate another kind of distance information, i.e., dependency distance that is defined as the distance between each pair of words on a dependency syntax tree. The word distance mask helps our model to focus on the local words and the dependency distance mask enables our model to capture the hierarchical relationships between words. Consequently, they provide our model the ability of capturing the local and non-local dependency of words properly.  To illustrate the effectiveness of our model, we conduct experiments on two NLP tasks: natural language inference and sentiment classification. Experimental results show that MS-SAN outperforms other baselines and achieves a competitive performance comparing with the state-of-the-art models.   Our contributions are listed as follows:    
"," Self attention networks  have been widely utilized in recent NLP studies. Unlike CNNs or RNNs, standard SANs are usually position-independent, and thus are incapable of capturing the structural priors between sequences of words. Existing studies commonly apply one single mask strategy on SANs for incorporating structural priors while failing at modeling more abundant structural information of texts. In this paper, we aim at introducing multiple types of structural priors into SAN models, proposing the Multiple Structural Priors Guided Self Attention Network  that transforms different structural priors into different attention heads by using a novel multi-mask based multi-head attention mechanism. In particular, we integrate two categories of structural priors, including the sequential order and the relative position of words. For the purpose of capturing the latent hierarchical structure of the texts, we extract these information not only from the word contexts but also from the dependency syntax trees. Experimental results on two tasks show that MS-SAN achieves significant improvements against other strong baselines.",409
"  Building intelligent conversation systems is a long-standing goal of artificial intelligence and has attracted much attention in recent years . A central challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a pool of candidate responses .  [tb]  \resizebox{0.95{ {|l|}  Dialogue Context Between Two Speakers A and B \\  A: Would you please recommend me a good TV series \\ to watch during my spare time?  \\ B: Absolutely! Which kind of TV series are you most\\ interested in?  \\ A: My favorite type is fantasy drama. \\ B: I think both Game of Thrones and The Vampire \\ Diaries are good choices.  \\ }  \\   \\ P1: Awesome, I believe both of them are great TV \\ series! I will first watch Game of Thrones. \; \\  %Difficult  \\ %P2: Agree! I am a huge fan of Jon Snow. \\ P2: Cool! I think I find the perfect things to kill my\\ \; weekends. \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \;\\ }  \\  \\ N1: This restaurant is very expensive. \; \; \; \; \; \; \\  %Difficult  \\ %R2: Thanks very much for your advice! & \\ % R2: Modern Family is an American TV sitcom. & \\ N2: Iain Glen played Ser Jorah Mormont in the HBO \\ \; fantasy series Game of Thrones. \; \; \; \; \;\\   }        To tackle the response selection problem, different matching models are developed to measure the matching degree between a conversation context and a response candidate . Despite their differences, most prior works train the matching models with training data constructed by a simple heuristic. For each dialogue context, the human-written response is considered as positive  and the responses from other dialogue contexts are considered as negative . In practice, the negative responses are often randomly sampled and the training objective is to ensure that the positive responses score higher than the negative ones.  Recently, some researchers  has raised the concern that randomly sampled negative responses are often too trivial . Models trained with such negative data lacks the ability to handle strong distractors during testing. In general, the problem stems from the ignorance of the diversity in context-response matching; all random responses are treated as equally negative regardless of their distracting strength. For example, in Table , two negative responses  are presented. For N1, one can easily dispel its legality %as there is no lexical overlap and off-topic semantic meaning.  as it does not follow the topic discussed in the dialogue context. %as its semantic meaning  is obviously off the topic ,  On the other hand, judging a strong distractor like N2 can be difficult as its content overlaps significantly with the context . Only with close observation, %we find that N2 does not properly reply the context . %the semantic incoherence between the context and N2 can be spotted.  we find that N2 does not strongly maintain the coherence of the discussion, i.e., it starts a parallel discussion about an actor in Game of Thrones rather than elaborating on the enjoyable properties of the TV series. %Similarly, we can observe the same phenomena on the positive side.  Similarly, the positive side has the same phenomena. For the positive response P1, one can easily confirm its legality as it naturally replies the context. As for P2, while it expatiates on the enjoyable properties of the TV series, it doesn't exhibit any obvious matching clues, such as lexical overlap with the context. %share any lexical overlap with the context.  Thus, to correctly identify P2, the relationship between P2 and the context has to be carefully reasoned by the model.  %Game of Thrones, the character name Jon Snow  has not appeared in the context. Thus, to correctly identify it, the relationship between Jon Snow and Game of Thrones has to be carefully reasoned by the model.  To conclude, the above observations suggest that, to accurately recognize different positive and negative responses, the model is required to possess different levels of discriminative capability.  %require different levels of model capability to accurately recognize. %\textcolor{red}{brandenwang: since the difficulty of random sampled negative responses is diverse. Besides, we observe that such diversity also applies to the positive responses: in some cases, the relationship between context and response is explicit and easy to identify, while in others, it is difficult to find the implicit relationship between the context and response. These two kinds of diversity may result in an unstable training process and poor accuracy in real-world applications.}  %Motivated by the intuition that one should first learn to deal with easy cases before handling harder ones, we propose to employ the idea of curriculum learning   to tackle the task of response selection.   Inspired by the aforementioned observations, we propose to employ the idea of curriculum learning   for a better learning of response selection models.  %to better learn matching models for response selection.  CL is reminiscent of the cognitive process of human being, the core idea is first learning easier concepts and then gradually transitioning to learning more complex concepts based on some pre-defined learning schemes. %.  In various NLP tasks ), CL has demonstrated its benefit in improving the model performance as well as the learning convergence.%, such as , leading to improved model performance as well as faster learning convergence.%better generalization.  %which has been successfully applied to many machine learning tasks . The core idea of CL is first learning easier concepts and then gradually transitioning to learning more complex concepts.  %.  The key to applying CL is to specify an appropriate learning scheme under which all training examples are gradually learned %. . In this work, we tailor-design a hierarchical curriculum learning  framework according to the characteristics of the concerned response selection task. Our HCL framework consists of two complementary curriculum strategies, namely corpus-level curriculum  and instance-level curriculum , covering the two distinct aspects of response selection. Specifically, in CC, the model gradually increases its ability in finding matching clues between the context and the positive response. As for IC, it progressively strengthens the model's ability in identifying the mismatch information between the context and negative responses. To order all positive and negative examples, we need to assess millions of possible context-response combinations in the training data. To overcome this computational challenge, we propose to use a fast neural ranking model to assign learning priorities to all training examples based on their pairwise context-response similarity score.   Notably, our proposed learning framework is independent to the choice of matching models. % and it can be conveniently implemented without any additional modelling effort . Therefore, for a comprehensive evaluation, we test our approach with three representative matching models, including the latest advance brought by pre-trained language models. Results on two benchmark datasets demonstrate that the proposed learning framework leads to remarkable performance improvement across all evaluation metrics.   In summary, our contributions are:  We propose a new hierarchical curriculum learning framework to tackle the task of response selection; % We design a decomposable neural model which works coherently with the proposed learning framework; and  and  Experimental results on two benchmark datasets demonstrate that our approach can significantly improve the performance of strong matching models, including the state-of-the-art one.  [t]  	{3pt}  	 denotes the positive response). For each training instance, we show three associated negative responses  whose difficulty level increase from the bottom to the top. In the negative examples, the words that also appear in the dialogue context are marked as italic.} %	         
"," We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that random negatives are often too trivial to train a reliable model, we propose a hierarchical curriculum learning  framework that consists of two complementary curricula: % Motivated by the idea of curriculum learning, we propose a new hierarchical curriculum learning framework which consists of two curriculum strategies:  corpus-level curriculum ; and  instance-level curriculum . In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response. On the other hand, IC progressively strengthens the model's ability in identifying the mismatched information between the dialogue context and a response. Empirical studies on two benchmark datasets with three state-of-the-art matching models demonstrate that the proposed HCL significantly improves the model performance across various evaluation metrics\footnote{All data, code and models are made publicly available at https://github.com/yxuansu/HCL/.}.",410
" Sequence-to-Sequence  learning~ has advanced the state of the art in various natural language processing  tasks, such as machine translation~, text summarization~, and grammatical error correction~. Seq2Seq models are generally implemented with an encoder-decoder framework, in which a multi-layer encoder summarizes a source sequence into a sequence of representation and another multi-layer decoder produces the target sequence conditioned on the encoded representation.   Recent studies reveal that fusing the intermediate encoder layers  is beneficial for Seq2Seq models, such as layer attention~, layer aggregation~, and layer-wise coordination~. Despite its effectiveness, not much is known about how fusing encoder layer representations work. The intuitive explanation is that fusing encoder layers exploits surface and syntactic information embedded in the lower encoder layers~.  However, other studies show that attending to lower encoder layers  does not improve model performance~, which is conflicted with existing conclusions. It is still unclear why and when fusing encoder layers should work in Seq2Seq models.  This paper tries to shed light upon behavior of Seq2Seq models augmented with EncoderFusion method. To this end, we propose a novel {.  Based on this observation, we simplify the EncoderFusion approaches by only connecting the encoder embedding layer to softmax layer . The SurfaceFusion approach shortens the path distance between source and target embeddings, which can help to learn better bilingual embeddings with direct interactions. Experimental results on several Seq2Seq NLP tasks show that our method consistently outperforms both the vanilla Seq2Seq model and the layer attention model.  Extensive analyses reveal that our approach produces more aligned bilingual word embeddings by shortening the path distance between them, which confirm our claim.  Our main contributions are as follows:  %    
"," Encoder layer fusion  is a technique to fuse all the encoder layers  for sequence-to-sequence  models, which has proven effective on various NLP tasks. However, it is still not entirely clear why and when EncoderFusion should work. In this paper, our main contribution is to take a step further in understanding EncoderFusion. Many of previous studies believe that the success of EncoderFusion comes from exploiting surface and syntactic information embedded in lower encoder layers. Unlike them, we find that the encoder embedding layer is more important than other intermediate encoder layers.  In addition, the uppermost decoder layer consistently pays more attention to the encoder embedding layer across NLP tasks. Based on this observation, we propose a simple fusion method, SurfaceFusion, by fusing only the encoder embedding layer for the softmax layer. Experimental results show that SurfaceFusion outperforms EncoderFusion on several NLP benchmarks, including machine translation, text summarization, and grammatical error correction.   It obtains the state-of-the-art performance on WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive analyses reveal that SurfaceFusion learns more expressive bilingual word embeddings by building a closer relationship between relevant source and target embeddings. Source code is freely available at \url{https://github.com/SunbowLiu/SurfaceFusion}.  . Recent studies have found that widening the bottleneck by fusing the surface features from lower level representations can boost the performance of Seq2Seq, but none of them explain the intrinsic mechanism of this benefit. In this paper, we take the first step to probe into the essence of the bottleneck on three typical Seq2Seq tasks, i.e.~machine translation, text summarization, and grammatical error correction. We observe that the representation learning of higher decoder layer suffers from the bottleneck, and thus propose a simple yet effective surface fusion method to mitigate the issue. The results over a variety of benchmarks confirm the effectiveness of the proposed method. Source code will be released. \fi",411
"  Word segmentation is a fundamental and challenging task in text classification and other NLP applications. Word segmenter determines the boundaries of words in the shape of beginning and ending. It has been largely investigated in many space-delimited languages including English, Arabic, Urdu and non-space delimited languages including Chinese, Japanese, and Burmese . However, the word segmentation in low-resource Sindhi language has not been studied well, mainly due to the lack of language resources.   Sindhi word segmentation exhibits the space omission and space insertion  problems. Although, the white spaces between words are a good sign for predicting word boundaries,  the space omission and space insertion between words bring ambiguity in the segmentation process. Therefore, the SWS task is a challenging problem because of resource scarcity, lack of standard segmentation benchmark corpus, and rich morphological features in Sindhi language. Previously, little work has been proposed to address the SWS problem by employing dictionary-based and rule-based approaches. Thus, the existing approaches lack the applicability towards open-source implementation due to following reasons,  inability to deal with out-of-vocabulary words,  less robust on the large datasets, and  lower segmentation accuracy. Our proposed novel deep SGNWS model has the capability of dealing with such issues for SWS with the Subword Representation Learning  approach.   Recently, deep neural architectures have largely gained popularity in NLP community by greatly simplifying the learning and decoding in a number of NLP applications including word segmentation with neural word embedding and powerful recurrent neural architectures. More recently, self-attention has also become a  popular approach to boost the performance of neural models. Therefore, we tackle the SWS problem by taking advantage of BiLSTM, self-attention, SRL, and CRF without relying on external feature engineering.  In this paper, we propose a language-independent neural word segmentation model for Sindhi. The proposed model efficiently captures the character-level information with subword representation learning. We convert segmentation into a sequence tagging problem using B, I, E, S, X tagging scheme. Where B denotes [Beginning], I [Inside], E [Ending] of a word in the given corpus, S [Single] is used for the tagging of a single or special character in the unlabeled text, and X tag is used for [hard-space] between words. We train task-oriented Sindhi word representations with character-level subword approach. To the best of our knowledge, this is the first attempt to tackle SWS as a sequence labeling task. We provide the open-source implementation for further investigation\footnote{https://github.com/AliWazir/Neural-Sindhi-word-segmenter}. Our novel contributions are listed as follows:        % The remaining parts of the paper are organized in the following sequence;  Section  presents the related work on SWS and its morphology, the evolution and usage of Recurrent Neural Networks  and its variants of LSTM, BiLSTM and GRU in the text segmentation in various languages. Section  presents an overview of Sindhi writing system and segmentation challenges, followed by the proposed methodology in Section  where RNN variants are employed for our task. Section  presents the experiments and results analysis. Lastly, Section  concludes this paper. 
"," Deep neural networks employ multiple processing layers for learning text representations to alleviate the burden of manual feature engineering in Natural Language Processing . Such text representations are widely used to extract features from unlabeled data. The word segmentation is a fundamental and inevitable prerequisite for many languages. Sindhi is an under-resourced language, whose segmentation is challenging as it exhibits space omission, space insertion issues, and lacks the labeled corpus for segmentation.  In this paper, we investigate supervised Sindhi Word Segmentation  using unlabeled data with a Subword Guided Neural Word Segmenter  for Sindhi. In order to learn text representations, we incorporate subword representations to recurrent neural architecture to capture word information at morphemic-level, which takes advantage of Bidirectional Long-Short Term Memory , self-attention mechanism, and Conditional Random Field .  Our proposed SGNWS model achieves an F1 value of  98.51\% without relying on feature engineering. The empirical results demonstrate the benefits of the proposed model over the existing Sindhi word segmenters.   % , such as dictionaries, morphological analyzers, or rules, for the Sindhi word segmentation. The conducted extensive empirical study demonstrates the benefits of the proposed model over the existing Sindhi word segmenters and state-of-the-art deep learning approaches.",412
"   Indonesian colloquialism is everyday and everywhere, e.g. in social media posts and conversational transcripts. Yet, existing research on Indonesian NLP models including NMTs often disregards qualitative analysis when the models are given strictly colloquial inputs. This is mainly due to the fact that the data readily available for training and testing the models are in formal Indonesian. %This follow naturally due to the fact that the models are style-agnostic, that is,   Colloquial Indonesian has several different word choices from formal language due to the diversity of regional languages and dialects. We define the spoken colloquial as a clean colloquial. In addition, in written media,  colloquial Indonesian is often abbreviated, disemvoweled, or written with voice alteration, which we define as the noisy colloquial .    [ht!]    }           & Example \\          {Ayo} \textcolor{red}{bertemu} \textcolor{orange}{dengan} \textcolor{purple}{pak} Ridho \\          ~Google Translate & Let's meet Pak Ridho \\           ~Gold-standard & Let's meet Mr. Ridho \\ \rule{0pt}{3.5ex}          Clean colloquial & \textcolor{blue}{kuy} \textcolor{red}{ketemu} \textcolor{orange}{sama} \textcolor{purple}{pak} Ridho \\          ~Google Translate & kuy met Pak Ridho \\ \rule{0pt}{3.5ex}          Noisy colloquial & \textcolor{blue}{kuy} \textcolor{red}{ktemu} \textcolor{orange}{sm} \textcolor{purple}{pk} ridho \\          ~Google Translate & I met you at happy time \\           from Malang, \textcolor{red}{bertemu-ketemu}, \textcolor{orange}{dengan-sama} from Betawi). Social-media text introduces additional typographical noise, such as diemvowelling .}         To better evaluate English-Indonesian MT systems against colloquial text, we first create 2 new test-sets of Indonesian-English colloquial pairs. The first test is a clean colloquial taken from a YouTube transcript. The second test-set is a noisy colloquial from Twitter annotated by our team of annotators. We found that NMT systems trained on formal dataset did not perform very well on these test-sets.  Next, we develop synthetic colloquial text data by performing word-level translation of several words in the formal text into a colloquial form based on a word-to-word dictionary. By combining the formal dataset and the synthesized colloquial dataset, we increase the NMT performance on the colloquial test-set by 2.5 BLEU points.   
","  Neural machine translation  is typically domain-dependent and style-dependent, and it requires lots of training data. State-of-the-art NMT models often fall short in handling colloquial variations of its source language and the lack of parallel data in this regard is a challenging hurdle in systematically improving the existing models. In this work, we develop a novel colloquial Indonesian-English test-set collected from YouTube transcript and Twitter. We perform synthetic style augmentation to the source formal Indonesian language and show that it improves the baseline Id-En models  over the new test data. %Our experimental data and code are available on github.com.",413
"  Large-scale language models have greatly advanced NLP research in various sub-areas, such as question answering, text summarization, story generation and so on . However, these generation models still suffer from at least three major problems when applied to the dialogue system building, 1) generic and repeated responses ,   2) inconsistent statements with the dialogue context , and 3) uncontrollable task-oblivious replies  .  Many previous studies have attempted to address these problems . For instance,  penalized repetitive and inconsistent behaviors with unlikelihood loss in open-domain chats.  detected and rewrote the contradicting responses to achieve a more consistent personality.  However, these methods optimize the language model by minimizing the loss in supervised learning, which may lead to exposure bias and uninterpretable behaviors, and consequently,  makes it harder for humans to regulate the model.   To alleviate these problems, previous work has explored RL-based methods in dialogue system building . %For instance,  integrated the goal of coherent into the reward design  and made the first step towards .designed for better generation.   However, such methods not only rely on hand-crafted user simulators that are inherently hard to build , but also require meaningful rewards that are difficult to design. To address these issues, we propose to teach the model to extract a policy directly from the data and learn from its own mistakes without the use of simulators. Leveraging decoding methods such as Nucleus Sampling , the language model finetuned on a persuasion task is able to generate lexically diverse response candidates given the same context. %One example is shown in Figure.  Some candidates are appropriate, while others are repetitive or inconsistent with the context. These good and bad examples are used as positive and negative feedback to the model through meaningful rewards in RL, and help refine the language model. During testing, to fully utilize the refined language model, we use it to generate multiple candidates again,  and filter out the repetition and inconsistency afterwards. Beyond being nonrepetitive and consistent, a good response also needs to accomplish the dialogue task, in our case, to persuade people. Therefore, we ask humans to demonstrate the persuasion process, and build a response imitator to imitate these human demonstrations and select the most persuasive response.  The above issues in language models are especially salient in complex strategic dialogue tasks such as persuasion and negotiation. These dialogues involve both a specific task goal and social contents to build rapport for better task completion, and therefore, have richer and more complicated language structures . Furthermore, due to their inherent similarity to task-oriented and open-domain dialogues, improvements made on these systems would also help in both dialogue settings. Therefore, we choose a strategic donation persuasion task  to perform our study, and conduct both automatic and human evaluations to evaluate our models.     This work  makes multiple contributions. First, we propose DialGAIL, an RL-based generative algorithm to refine MLE-based language models for dialogue  generation without the use of user simulators.  Second, we design an effective and practicable framework for strategic dialogue systems that achieves state-of-the-art performance on a complex persuasion task, with only small amount of human demonstration efforts.  %Such system achieves more diverse, consistent and fluent conversations with better persuasion outcomes on a complex persuasion task compared to the MLE-based baselines.   %a framework to automatically detect repetitive and inconsistent responses, and imitate human demonstration to select persuasive responses.  %Furthermore, experiments show that our model produces more diverse, consistent and fluent conversations with better persuasion outcomes on a complex persuasion task compared to the MLE-based baselines.  Previous dialogue research has mostly focused on pure task-oriented dialogues and pure social conversations; but looking forward, it becomes more and more important to pay attention to strategic dialogues that involves both task and social components. We sincerely hope this work could inspire more research and discussions on strategic dialogues in the community.   % how to refine the dialogue generation with limited amount of data? MLE fine-tuning woldn't work with the limited data % social content + a specific end-goal --> persuasionforgood. advance research in this area % how to easily get a usable lm without computational resources? % explore the possibility to apply GAIL in dialogue generation in a simple way  % the first to explore GAIL % raise more attention in persuasion in the community % with small amount of human demo % task-independent in repetition detection strengthen    
"," Despite the recent success of large-scale language models on various downstream NLP tasks, the repetition and inconsistency problems still persist in dialogue response generation. Previous approaches have attempted to avoid repetition by penalizing the language model's undesirable behaviors in the loss function. However, these methods focus on token-level information and can lead to incoherent responses and uninterpretable behaviors. To alleviate these issues, we propose to apply reinforcement learning to refine an MLE-based language model without user simulators, and distill sentence-level information about repetition, inconsistency and task relevance through rewards. In addition, to better accomplish the dialogue task, the model learns from human demonstration to imitate intellectual activities such as persuasion, and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback.% We will release the code and data upon acceptance.",414
"   Large-scale pre-training has draw much attention in both the community of Compute Vision  and Natural Language Processing  due to its strong capability of generalization and efficient usage of large-scale data. Firstly in CV, a series of models were designed and pre-trained on the large-scale dataset ImageNet, such as AlexNet , VGG  and ResNet , which effectively improved the capability of image recognition for numerous tasks. Recent years have witnessed the burst of pre-training in NLP, such as BERT , RoBERTa , XLNet  and BART , which greatly improve the capability of language understanding and generation. However, the above researches towards the single-modal learning and can only be used in single-modal  scenarios. %which greatly restricts their ability to process multi-modal  information. In order to adapt to multi-modal scenarios, a series of multi-modal pre-training methods were proposed and pre-trained on the corpus of image-text pairs, such as ViLBERT , VisualBERT  and UNITER , which greatly improve the ability to process multi-modal information. However, these models can only utilize the limited corpus of image-text pairs and cannot be effectively adapted to single-modal scenarios . %Moreover, the size of the corpus of image-text pairs is very limited, and large scale of single-modal data can't be effectively utilized.     A smarter AI system should be able to process different modalities of information effectively. There are large scale of data in different modalities on the Web, mainly textual and visual information. The textual knowledge and the visual knowledge usually can enhance and complement with each other. As the example shown in Figure , it's difficult to answer the question correctly only with the visual information in the image.  However, if we connect the visual information to the textual information which describes the background of a baseball game, it's very easy to determine the correct answer. Also, the visual information can make it easier to understand the scene described by the text. The research in neuroscience by  reveals that the parts of the human brain responsible for vision can learn to process other kinds of information, including touch and sound. Inspired by the research, we propose to design a unified-modal architecture UNIMO which can process multi-scene and multi-modal data input, including textual, visual and vision-and-language data, as shown in Figure .  The greatest challenge to unify different modalities is to align and unify them into the same semantic space which are generalizable to different modalities of data. Existed cross-modal pre-training methods try to learn cross-modal representations based on only limited image-text pairs by simple image-text matching and masked language modeling . They can only learn specific representations for image-text pairs, which are not generalizable for single-modal scenarios. So their performance will drop dramatically when applied to language tasks . In this work, UNIMO learns visual representations and textual representations in similar ways, and unify them into the same semantic space via cross-modal contrastive learning  based on a large-scale corpus of image collections, text corpus and image-text pairs.  %Our unified-modal architecture can utilize large scale of image collections and text corpus, and align the visual and textual information into the same semantic space via cross-modal contrastive learning on image-text pairs. %Effectively utilizing large-scale of images and text corpus can improve the capability of vision and textual understanding respectively. UNIMO effectively utilizes the large-scale of text corpus and image collections to learn general textual and visual representations.  The CMCL aligns the visual representation and textual representation, and unifies them into the same semantic space based on image-text pairs. To facilitate different levels of semantic alignment between vision and language, we propose to utilize a series of text rewriting techniques to improve the diversity of cross-modal information. As shown in Figure , we utilize back-translation to generate several positive examples for an image-text pair. Also, to enhance the detail semantic alignment between text and image, we further parse the caption to scene graph  and randomly replace either the objects, attributes or relations in the caption to generate various negative samples. Sentence-level retrieval and replacement is also utilized to enhance the sentence-level alignment. In this way, our model can effectively unify different levels of visual and textual representations into the same semantic space.  The unified-modal architecture mainly has the following advantages compared with previous methods:      
","  Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data  or limited multi-modal data . In this work, we propose a unified-modal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections can be utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning  is leveraged to align the textual and visual information into a unified semantic space over a corpus of image-text pairs. As the non-paired single-modal data is very rich, our model can utilize much larger scale of data to learn more generalizable representations. Moreover, the textual knowledge and visual knowledge can enhance each other in the unified semantic space. The experimental results show that UNIMO significantly improves the performance of several single-modal and multi-modal downstream tasks.",415
" %What is ToD Task-oriented dialogue systems  are the core technology of the current state-of-the-art smart assistant . These systems are either modularized, Natural Language Understanding , Dialogue State Tracking , Dialogue Policy  and Natural Language Generation , or end-to-end, where a single model implicitly learn how to issue APIs  and system responses .   % what is the current problem we are trying to solve These systems are often updated with new features based on the user needs, e.g., adding new slots and intents, or even completely new domains. However, existing dialogue models are trained with the assumption of having a fixed dataset at the beginning of the training, and they are not designed to add new domains and functionalities through time, without incurring the high cost of a whole system retraining. Therefore, the ability to acquire new knowledge continuously, a.k.a. Continual Learning , is crucial in the design of a dialogue system. Figure shows an high-level intuition of CL in ToDs.     In this setting the main challenge is catastrophic forgetting. This phenomena happens since there is a distributional shift between the tasks in the curriculum which leads to catastrophic forgetting the previously acquired knowledge. To overcome this challenge three kind of methods are usually deployed such as loss regularization, for avoiding to interfere with the previously learned task, rehearsal, which uses an episodic memory to recall previously learned tasks, and architectural, which adds task-specific parameters for each learned task. However, architectural methods are usually not considered as a baseline, especially in sequence-to-sequence generation tasks, because they usually require a further step during testing for selecting which parameter to use for the given task.    To the best our knowledge, Continual Learning  in task-oriented dialogue systems is mostly unexplored or it has been studied in specific settings  using only few tasks learned continuously. Given the importance of the task in the dialogue setting, we believe that a more comprehensive investigation is required, especially by comparing multiple settings and baselines. Therefore in this paper: [noitemsep]         In Section we introduce the basic concepts and notation used throughout the paper, for both task-oriented dialogue modelling and continual learning, in Section we introduce the proposed architectural CL method, in Section we describe datasets, baselines, evaluation metrics and experimental settings, and Section we describe the main findings of the paper.    % Based on our experimental results, we discovered that two technique are particularly effective, but they both have a linear cost with respect to the number of learned tasks. To elaborate, in rehearsal-based methods the number of samples stored in the episodic-memory grows linearly with the learned task, while instead in architectural methods the number of parameters grows linearly. Hence concluding that there is not an absolute best  when comparing different methods based on the resources needed, both in term of additional parameters or sample stored in memory.    
"," Continual learning in task-oriented dialogue systems can allow us to add new domains and functionalities through time without incurring the high cost of a whole system retraining. In this paper, we propose a continual learning benchmark for task-oriented dialogue systems with 37 domains to be learned continuously in four settings, such as intent recognition, state tracking, natural language generation, and end-to-end. Moreover, we implement and compare multiple existing continual learning baselines, and we propose a simple yet effective architectural method based on residual adapters. Our experiments demonstrate that the proposed architectural method and a simple replay-based strategy perform comparably well but they both achieve inferior performance to the multi-task learning baseline, in where all the data are shown at once, showing that continual learning in task-oriented dialogue systems is a challenging task. Furthermore, we reveal several trade-off between different continual learning methods in term of parameter usage and memory size, which are important in the design of a task-oriented dialogue system. The proposed benchmark is released together with several baselines to promote more research in this direction.",416
" % Background: % What is MT, history of MT, and current state of MT % What is NMT, current state of NMT % Reason: % Sufficient and necessity condition for writing this article % Organization of this article   [!t]           Machine Translation  is an important task that aims to translate natural language sentences using computers. The early approach to machine translation relies heavily on hand-crafted translation rules and linguistic knowledge. As natural languages are inherently complex, it is difficult to cover all language irregularities with manual translation rules. With the availability of large-scale parallel corpora, data-driven approaches that learn linguistic information from data have gained increasing attention. Unlike rule-based machine translation, Statistical Machine Translation  learns latent structures such as word alignments or phrases directly from parallel corpora. Incapable of modeling long-distance dependencies between words, the translation quality of SMT is far from satisfactory. With the breakthrough of deep learning, Neural Machine Translation  has emerged as a new paradigm and quickly replaced SMT as the mainstream approach to MT.  Neural machine translation is a radical departure from previous machine translation approaches. On the one hand, NMT employs continuous representations instead of discrete symbolic representations in SMT. On the other hand, NMT uses a single large neural network to model the entire translation process, freeing the need for excessive feature engineering.  The training of NMT is end-to-end as opposed to separately tuned components in SMT. Besides its simplicity, NMT has achieved state-of-the-art performance on various language pairs. In practice, NMT also becomes the key technology behind many commercial MT systems.  As neural machine translation attracts much research interest and grows into an area with many research directions, we believe it is necessary to conduct a comprehensive review of NMT. In this work, we will give an overview of the key ideas and innovations behind NMT. We also summarize the resources and tools that are useful and easily accessible. We hope that by tracing the origins and evolution of NMT, we can stand on the shoulder of past studies, and gain insights into the future of NMT.  The remainder of this article is organized as follows: Section will review the methods of NMT. We first introduce the basics of NMT, and then we selectively describe the recent progress of NMT. We focus on methods related to architectures, decoding, and data augmentation. Section will summarize the resources such as parallel or monolingual corpora that are publicly available to researchers. Section will describe tools that are useful for training and evaluating NMT models. Finally, we conclude and discuss future directions in Section.  
"," Machine translation  is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation  has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions. %Machine translation  is an important sub-field of natural language processing which aims to translate natural language sentences between different languages using computers. Recent years has witnessed the great success of end-to-end neural machine translation  models, which has dominated the mainstream approach in commercial machine translation systems. In this work, we first provide a broad review of the methods and challenges in NMT. We introduce three basic components in NMT methods, namely modeling, inference, and learning. The modeling part starts with the encoder-decoder framework and the celebrated attention mechanism, which is followed by Recurrent Neural Networks , Convolutional Neural Networks , and Self-Attention Networks  as potential instances in an NMT architecture. The inference part focuses on the generation of translation sentences from NMT models, which consists of autoregressive,  non-autoregressive, and bidirectional decoding methods. The learning part concentrates on the methods that enhances the expressive capacity of NMT models to learn from data. We highlight the design of training objectives and the use of monolingual data in this part. In addition to the three basic parts, we highlight some of the most significant challenges in NMT, including open vocabulary, prior knowledge integration, as well as the interpretability and robustness issues. Then we summarize useful resources and tools for MT research and maintainance. Finally, we conclude with a discussion of promising future research directions.",417
" NMT is the task of transforming a source sequence into a new form in a particular target language using deep neural networks. Such networks commonly have an encoder-decoder architecture , in which an encoder maps a given input sequence to an intermediate representation and a decoder then uses the same representation to generate candidate translations. Both encoder and decoder are neural networks that are trained jointly. Due to the sequential nature of the NMT task, early models usually relied on recurrent architectures , or benefited from the sliding feature of convolutional kernels to encode/decode variable-length sequences .   Recently, Transformers  have shown promising results for NMT and become the new standard in the field. They follow the same concept of encoding and decoding but in a relatively different fashion. A Transformer is fundamentally a feed-forward model with its unique neural components  that alter the traditional translation pipeline accordingly. Therefore, it is expected if such a model behaves differently than its recurrent or convolutional counterparts. Our goal in this research is to study this aspect in the presence of noise.     NMT engines trained on clean samples provide high-quality results when tested on similarly clean texts, but they break easily if noise appears in the input . They are not designed to handle noise by default and Transformers are no exception. Many previous works have focused on this issue and studied different architectures . In this work, we particularly focus on Transformers\footnote{We assume that the reader is already familiar with the Transformer architecture.} as they are relatively new and to some extent understudied.   A common approach to make NMT models immune to noise is fine-tuning , where a noisy version of input tokens is intentionally introduced during training and the decoder is forced to generate correct translations despite deformed inputs. FT is quite useful for almost all situations but it needs to be run with an optimal setting to be effective. In our experiments, we propose a slightly different learning-rate scheduler to improve FT. We also define a new extension that not only modifies input words but also adds complementary tokens to the target side. We refer to this extension as Target Augmented Fine-Tuning , which is the first contribution of this paper.   In our study, we realized that data augmentation techniques  might not be sufficient enough for some cases and we need a compatible training process and neural architecture to deal with noise. Therefore, we propose Controlled Denoising  whereby noise is added to source sequences during training and the encoder is supposed to fix noisy words before feeding the decoder. This approach is implemented via an auxiliary loss function and is similar to adversarial training. CD is our second contribution.   CD only takes care of noise on the encoder side, so we propose a Dual-Channel Decoding  strategy to study what happens if the decoder is also informed about the input noise. DCD supports multi-tasking through a -channel decoder that samples target tokens and corrects noisy input words simultaneously. This form of fusing translation knowledge with noise-related information has led to interesting results in our experiments. DCD is the third and last contribution of this work.   The remainder of the paper is organised as follows: First, we review previously reported solutions for the problem of noise in NMT in Section , then we present details of our methods and the intuition behind them in Section . To validate our methods, we report experimental results in Section . Finally, we conclude the paper and discuss possible future directions in Section .  
"," Transformers  have brought a remarkable improvement in the performance of neural machine translation  systems, but they could be surprisingly vulnerable to noise. Accordingly, we tried to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behaviour of conventional models for the problem of noise but it seems Transformers are understudied in this context.  Therefore, we introduce a novel data-driven technique to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two new extensions to the original Transformer, that modify the neural architecture as well as the training process to handle noise. We evaluated our techniques to translate the English--German pair in both directions. Experimental results show that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to $10$\% of entire test words are infected by noise.",418
"   Cross-lingual word embeddings  represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora  or bilingual dictionaries . However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning   or adversarial training .         Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods . In later work,  showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it.     In this paper, we propose an alternative approach that does not have this limitation, but can still work without any parallel resources. The core idea of our method is to fix the target language embeddings, and learn aligned embeddings for the source language from scratch. This prevents structural mismatches that result from independently training embeddings in different languages, as the learning of the source embeddings is tailored to each particular set of target embeddings. For that purpose, we use an extension of skip-gram that leverages translated context words as anchor points. So as to translate the context words, we start with a weak initial dictionary, which is iteratively improved through self-learning, and we further incorporate a restarting procedure to make our method more robust. Thanks to this, our approach can effectively work without any human-crafted bilingual resources, relying on simple heuristics  or an existing unsupervised mapping method to build the initial dictionary. Our experiments confirm the effectiveness of our approach, outperforming previous mapping methods on bilingual dictionary induction and obtaining competitive results on zero-shot cross-lingual transfer learning on XNLI.     
","  Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary  as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task.",419
" Robust and accurate detection of hate speech is important for minimizing the risk of harm to online users. However, this task has proven remarkably difficult and concerns have been raised about the performance, generalizability and fairness of existing systems. A key challenge in the research community is the lack of high quality datasets that can be freely shared among researchers, have finegrained annotations, contain challenging edge-case content and are not unduly biased by the over-representation of certain demographic groups.  We address these problems in online hate classification by utilizing a system for dynamic data collection, model training and evaluation. Specifically, we use a human-and-model-in-the-loop approach, whereby an initial model is trained and annotators are then tasked with entering content that would fool it into making an incorrect classification. Models are then retrained on all collected data and the process is repeated. New rounds of data are collected, with annotators still trying to trick the improved models by entering the most difficult and unusual forms of content. In this way models `learn from the worst' as the more challenging content they are shown, the faster they will hopefully improve.  The dataset formation was organized into four $} Third, as part of the dataset we present over 14,000 challenging contrastive examples, which were created during the dynamic data generation process.  Dynamic dataset generation through a human-and-model-in-the-loop approach offers several advantages over static datasets. First, problems can be addressed as work is conducted -- rather than creating the dataset and then discovering any inadvertent design flaws, as would be the case with static benchmarks. Second, the model-in-the-loop means that annotators' work is guided by the model; they receive real-time feedback from the model about how effectively different strategies are beating it; this lets them target their efforts to exploit key weaknesses, creating a dataset with many hard-to-classify entries. Third, the dataset can be constructed to better meet the requirements of machine learning; our dataset is balanced, comprising 54\% hate. It includes hate targeted against a large number of targets, providing variety for the model to learn from, and many entries were directed to counter established problems in hate detection model training, such as overfitting on keywords.    
"," We present a first-of-its-kind large synthetic training dataset for online hate classification, created from scratch with trained annotators over multiple rounds of dynamic data collection. We provide a 40,623 example dataset with annotations for fine-grained labels, including a large number of challenging contrastive perturbation examples. Unusually for an abusive content dataset, it comprises 54\% hateful and 46\% not hateful entries. We show that model performance and robustness can be greatly improved using the dynamic data collection paradigm. The model error rate decreased across rounds, from 72.1\% in the first round to 35.8\% in the last round, showing that models became increasingly harder to trick -- even though content become progressively more adversarial as annotators became more experienced. Hate speech detection is an important and subtle problem that is still very challenging for existing AI methods. We hope that the models, dataset and dynamic system that we present here will help improve current approaches, having a positive social impact.",420
"   Speech separation, also known as cocktail party problem, aims to separate target speech from interference background . It is often used as the front end of speech recognition for improving the accuracy of human-machine interaction. Conventional speech separation technologies include computational auditory scene analysis , non-negative matrix factorization , HMM-GMM , and minimum mean square error . Recently, deep learning based speech separation becomes a new trend , which is the focus of this paper. According to whether speakers閳 information is known as a prior, deep-learning-based speech separation techniques can be divided into three categories, which are speaker-dependent , target-dependent, and speaker-independent speech separation.    Speaker-dependent speech separation needs to known the prior information of all speakers, which limits its practical applications. Nowadays, the research on speech separation is mostly speaker-independent and target-dependent.      Speaker-independent speech separation based on deep learning faces the speaker permutation ambiguity problem. In order to solve this problem, two techniques have been proposed. The first one is deep clustering %     .     It projects each time-frequency unit to a higher-dimensional embedding vector by a deep network, and conducts clustering on the embedding vectors for speech separation.     The second technique is permutation invariant training %     . For each training mixture, it picks the permutation of the speakers that has the minimum training error among all possible permutations to train the network.    % Besides, there are some other effective algorithm based on deep learning, such as deep ensemble learning and deep attractor network.  Target-dependent speech separation based on deep learning aims to extract target speech from a mixture given some prior knowledge on the target speaker. The earliest speech separation method takes the target speaker as the training target . It has to train a model for each target speaker, which limits its practical use. To prevent training a model for each target speaker, speaker extraction further takes speaker codes extracted from a speaker recognition system as part of the network input . Some representative speaker extraction methods are as follows.  applies a context adaptive deep neural network to extract the target speaker through a speaker adaptation layer. It takes the estimated mask and ideal binary mask as the training objective.  proposed a temporal spectrum approximation loss to estimate a phase sensitive mask for the target speaker.  generalized the end-to-end speaker-independent speech separation  to the end-to-end speaker extraction. [t]         % It is more practical when only registered speakers need to be responded, such as speaker diarization and speech recognition .  The aforementioned methods are all single-channel methods. Although they work well in clean scenarios, their performance degrades significantly in reverberant scenarios. To improve the performance of speech separation in reverberant scenarios, many multichannel methods were proposed, which has the following two major forms. The first form combines spatial features that are extracted from microphone arrays, such as interaural time difference and interaural level difference, with spectral features as the input of single-channel speech separation networks . The second form uses a deep network to predict a mask for each speaker at each channel, and then conducts beamforming for each speaker . For brevity, we call this method deep beamforming. Some methods combined the above two forms for boosting their advantages together in reverberant scenarios, e.g. .  The aforementioned multichannel methods are only studied with traditional fixed arrays, such as linear arrays or spherical arrays. However, for  far-field speech separation problems with high reverberation, they suffer significant performance degradation. How to maintain the estimated speech at the same high quality throughout an interested physical space is of broad interests.  Ad-hoc microphone array, which is a group of randomly distributed microphones collaborating with each other, is a solution to the problem. Figure  gives a comparison example where a target speaker extraction problem with a fixed array is on the left and that with an ad-hoc microphone array on the right. From the figure, we see that, compared with the fixed array that is far from the target speaker, the ad-hoc microphone array has several apparent advantages. First, an ad-hoc microphone array may put a number of microphones around the target speaker, which significantly reduced the probability of far-field speech processing. By channel selection, it might be able to form a local microphone array around the target speaker. At last, it may be able to incorporate application devices of various physical sizes.   [t]                In literature, ad-hoc microphone arrays have consistently been an important research topic . However, they face many practical problems due to the lack of important priors. Recently,  addresses the difficulties of ad-hoc microphone arrays, such as lack of priors and insufficient estimation of variables, by deep learning for the first time. The proposed method, named deep ad-hoc beamforming , was originally designed for speech enhancement only, which predicts segment-level signal-to-noise-ratio  by deep neural networks for supervised channel selection. Later on, some speech separation methods based on ad-hoc microphone arrays were proposed.  proposed a transform-average-concatenate strategy for a filter-and-sum network  to realize the channel reweighting/selection ability for ad-hoc microphone arrays. Because ad-hoc microphone arrays lack the prior of the number and spatial distribution of microphones,  proposed a network architecture by interleaving inter-channel processing layers and temporal processing layers to leverage information across time and space alternately. %{ ASR in ad-hoc microphone array...}  % The filter-and-sum network   first conducts pre-separation on a selected reference microphone by estimating its beamforming filters, and then estimates the beamforming filters of all remaining microphones based on pair-wise cross-channel features. They further improved the channel reweighting/selection ability of FaSNet by a transform-average-concatenate strategy  for ad-hoc microphone arrays.   However, existing deep learning based speech separation with ad-hoc microphone arrays are all speaker-independent. To our knowledge, target-dependent speech separation with ad-hoc microphone arrays are far from explored yet. In many applications, extracting and tracking target speech is of more interests than separating a mixture into its components. This is particularly the case for ad-hoc microphone arrays, where several speakers may locate far apart and talk independently.   In this paper, we propose a target-dependent speech separation algorithm with ad-hoc microphone arrays, named DAB based on speaker extraction . Our algorithm consists of three components: first, we propose a supervised channel selection based on speaker extraction, which applies bi-directional long short-term memory  networks to estimate the utterance-level SNR of the target speaker. Then, we employ the heuristic channel selection algorithms in  to pick the channels with high SNRs. We further apply a single-channel speaker extraction algorithm to the selected channels for the mask estimation problem of the target speech. At last, we use the estimated masks to derive a beamformer for the target speaker, such as minimum variance distortionless response  . Experimental results on a WSJ0-adhoc corpus show that the proposed DABse performs well in reverberant environments.   The rest of the paper is organized as follows. We introduce the signal model of the speaker extraction problem with ad-hoc microphone arrays in Section . Then, we present the deep ad-hoc beamforming system based on speaker extraction in Section . In Section , we present the experimental results. Finally, we conclude this study in Section .   
"," % abstract %\parttitle{First part title} %if any %Text for this section. %\parttitle{Second part title} %if any %Text for this section. Recently, the research on ad-hoc microphone arrays with deep learning has drawn much attention, especially in speech enhancement and separation. Because an ad-hoc microphone array may cover such a large area that multiple speakers may locate far apart and talk independently, target-dependent speech separation, which aims to extract a target speaker from a mixed speech, is important for extracting and tracing a specific speaker in the ad-hoc array. However, this technique has not been explored yet. In this paper, we propose deep ad-hoc beamforming based on speaker extraction, which is to our knowledge the first work for target-dependent speech separation based on ad-hoc microphone arrays and deep learning. The algorithm contains three components. First, we propose a supervised channel selection framework based on speaker extraction, where the estimated utterance-level SNRs of the target speech are used as the basis for the channel selection. Second, we apply the selected channels to a deep learning based MVDR algorithm, where a single-channel speaker extraction algorithm is applied to each selected channel for estimating the mask of the target speech. We conducted an extensive experiment on a WSJ0-adhoc corpus. Experimental results demonstrate the effectiveness of the proposed method.",421
" In this paper, we tackle the problem of screening a finite pool of documents, where the aim is to retrieve relevant documents satisfying a given set of predicates that can be verified by human or machines . In this context, if a document does not satisfy at least one predicate, it is treated to be irrelevant. A predicate represents a property, a unit of meaning, given in natural language . By this means a predicate might be interpreted in a variety of ways in text, so making keywords-based search hard to reach high recall while keeping a decent level of precision . We interpret the screening problem as high recall problem, i.e., the aim is to retrieve all relevant documents maximizing precision. %we assume predicates and candidate documents are given  % Since predicates can be interpreted in a variety of ways, it makes the problem of document screening very challenging especially when there is a little training data.  The screening finds application in many domains, such as i) in systematic literature reviews ; % -SLRs-  AND papers studying older adults }; ii) database querying - where items filtered  in/out based on predicates ; iii) hotel search - where the hotels retrieve  based upon filters of interest . Consequently,  the document screening is an instance of finite pool binary classification problems  , where we need to classify a finite set of objects minimizing cost. % As an instance of the problem, we choose the screening phase of SLRs what makes the problem rather challenging since each review is different and has a unique set of predicates . Typically, authors of an SLR retrieve a candidate pool of documents executing a keywords-based query on a database such as Scopus. To avoid missing papers, the query tends to be inclusive, which means that it returns hundreds or thousands of results  that are later manually screened by researchers based on predefined predicates. For example, researchers might look for papers that describe all of the following predicates at the same time: 1) ""include papers that study older adults 85+ years"", 2) ""include papers conducted randomized controlled trial"", 3) "" include papers about behavioral intervention"". Therefore, here we have the conjunctive query of three inclusive predicates. A bottleneck of the screening process is the predicate evaluation, i.e.,  identifying which of the given predicates are satisfied in a current document. For example, in literature reviews, authors validate predicates, however, this is time-consuming, exhaustive, and very expensive .   An effective technique to solve screening problems is crowdsourcing where the crowd can solve even complex screening tasks with high accuracy and lower cost compared to expert screening . However, achieving a good performance in crowd-based screening requires a deep understanding of how to design tasks and model their complexity , how to test and filter workers , how to aggregate results into a classification decision, or how to improve worker engagement .   Machine learning  algorithms have also made a very impressive progress in solving complex screening tasks. However, obtaining a sufficiently large set of training data is still a key bottleneck for accurate ML classifiers. Active learning   accelerates this process by minimizing the size of training data that is required to train better classifiers via selecting the most informative instances for annotation. The effectiveness of AL have been proven in many domains , but most of the work considers single-label cases while multi-label AL problems have been far less investigated. The challenge in applying AL to a multi-label classification problem is that the algorithm should measure the unified informativeness of each unlabeled item across all labels. The state of the art multi-label AL strategies follow an object-wise  labeling, where the AL algorithm first finds the relevance scores  of  pairs, and then aggregates these scores to find the informativeness of items . However, it may ignore the interaction between labels .     We investigate how to efficiently combine crowdsourcing and ML for item screening. It is a challenging task since the budget is limited and there are countless number of ways to spend it on the problem. We propose a multi-label AL screening specific sampling technique for querying unlabelled items for annotating. Our algorithm takes a decision how to choose unlabeled data to annotate by crowd workers in order to maximize the performance of a screening task. Unlike existing multi-label AL approaches that rely on global labeling, we choose the local labeling method, where for each label  we determine the relevancy to each item.  
"," In this paper, we explore how to efficiently combine crowdsourcing and machine intelligence for the problem of document screening, where we need to screen documents with a set of machine-learning filters. Specifically, we focus on building a set of machine learning classifiers that evaluate documents, and then screen them efficiently. It is a challenging task since the budget is limited and there are countless number of ways to spend the given budget on the problem. We propose a multi-label active learning screening specific sampling technique -objective-aware sampling- for querying unlabelled documents for annotating. Our algorithm takes a decision on which machine filter need more training data and how to choose unlabeled items to annotate in order to minimize the risk of overall classification errors rather than minimizing a single filter error.  We demonstrate that objective-aware sampling significantly outperforms the state of the art active learning sampling strategies. % on multi-filter classification problems.",422
"   One of the hallmarks of human intelligence is the ability to generalize seamlessly across heterogeneous sensory inputs and different cognitive tasks. We see objects, hear sounds, feel textures, smell odors, and taste flavors to learn underlying concepts present in our world. Much of AI's existing progress in multimodal learning, however, focuses primarily on a fixed set of predefined modalities and tasks that are consistent between training and testing. As a result, it is unclear how to transfer knowledge from models trained for one modality  to another  at test time. This scenario is particularly important for low-resource target modalities where unlabeled data is scarce and labeled data is even harder to obtain . In the unimodal case, this is regarded as meta-learning or few-shot learning. In contrast, we formally define the cross-modal generalization setting as a learning paradigm to train a model that can  quickly perform new tasks in a target modality  and  doing so while being trained on a different source modality. In this paper, we study the data and algorithmic challenges for cross-modal generalization to succeed. %Such a learning paradigm is particularly useful in leveraging high-resource source modalities to help low-resource target modalities, where unlabeled data is scarce and labeled data is even harder to obtain, such as audio from low-resource languages, real-world environments, and medical images.      As a motivating example, Figure illustrates a scenario where large-scale image classification benchmarks can help audio classification, which is a less studied problem with fewer large-scale benchmarks. In this ambitious problem statement, a key research question becomes: how can we obtain generalization across modalities despite using separate encoders for different source  and target  modalities? The technical challenge involves aligning shared knowledge learned from source image tasks with target audio tasks. Our problem statement differs from conventional meta-learning and domain adaptation where one can take advantage of the same source and target modality with shared encoders which helps generalization by having the same representation space. In our case, the discrepancies in modalities requires one to learn new output concepts expressed in new input modalities. As a result, cross-modal generalization requires new ideas to synchronize  multimodal sources and targets. What is the minimal extra supervision required to perform this alignment?  In this paper, we formalize the conditions required for successful generalization and show that another level of supervision is necessary under partial observability across modalities and tasks. Supervision comes in the form of cross-modal meta-alignment  to capture a space where representations of similar concepts in different modalities are close together while ensuring quick generalization to new tasks . We introduce a novel algorithm called  expressed in new input modalities. %We show that existing domain adaptation, meta-learning, and transfer learning approaches are unable to bridge the gap between such heterogeneous paradigms where both input modalities and output tasks are different.  % emphasize cant share encoders, need explicit alignment % emphasize different label space, generalize meta-learning % formulate crossmodal ml and therefore we propose meta alignment % first para ok. like to learn but different modalities. % second para. compared to ml and da, 1 critical issue when trying to do crossmodal - have hetero data between source and target. cant use shortcut such as same encoder for images of different domains. need different encoders 1 for each. how do we solve this? need another level of supervision to help - where meta alignment comes in. what we propose - a technique to address the core technical challenge of crossmodal ml which is how to learn different encoders. meta alignment is a way to do that, a contrastive learning approach.  %To account for this technical challenge, we formalize the conditions required for successful generalization and show that another level of supervision is necessary under partial observability across modalities and tasks. This form of supervision comes in the form of cross-modal alignment to capture a space where representations of similar concepts in different modalities are close together while ensuring quick generalization to new low-resource tasks . Our analysis leads to a novel algorithm based on contrastive learning called  expressed in new input modalities. We show that existing domain adaptation, meta-learning, and transfer learning approaches are unable to bridge the gap between such heterogeneous paradigms where both input modalities and output tasks are different.  % how do we handle limited resource modalities and task, we explore cross-modal approach % note: define modality, concept, task % note: a better way of saying cross-modal cross-task  %, which allows us to learn a classifier for transfer from source to target tasks. %This makes it particularly suitable for generalization across modalities and tasks due to the presence of unseen concepts and annotations in the target modality. %We show that this space:  groups similar concepts expressed across different modalities,  is well-clustered across concepts, and  generalizes well to new concepts, making it particularly suitable for generalization across modalities and tasks. %While our first attempt at meta-alignment uses strong pairings across source and target modalities , we further provide an extension to use only weak pairs between modalities. Weak pairs represent coarse groupings of semantic correspondence which better capture the many-to-many relations between real-world multimodal data  and allow us to use large banks of weakly paired multimodal data available on the internet and prepared for machine learning studies such as video data  and image captioning data . %Finally, we quantify the trade-offs between labeling more data in the target modality versus obtaining better source-target alignment.  %provide theoretical justification to quantify the benefits of our approach: { ZIYIN TODO} \zing[ziyin: should mention and focus on the difficulty of definition and formalization] %instead of a classical generalization error in the target modality that scales wrt the sample complexity of the target modality, our approach is bounded by the sample complexity in the source modality. As a result, the error is therefore reduced with ample samples in the source modality and a well-aligned space.  We present experiments on three cross-modal tasks: generalizing from  text to image,  image to audio, and  text to speech. In all cases, the goal is to classify data from a new target modality given only a few  labeled samples. %We find that \names\ accurately performs few-shot alignment of concepts from different modalities, thereby allowing generalization from concepts in the source modality to new concepts in the target modality. We perform extensive experiments to compare with related approaches including target modality meta-learning that would be expected to perform well since they have seen thousands of labeled examples from the target modality during meta-training. Surprisingly, \names\ is competitive with these baselines and significantly outperforms other cross-modal approaches. In addition, we study settings where the target modality suffers from noisy or limited data, a scenario particularly prevalent in low-resource modalities. %While this setting makes it difficult to directly train in the target modality, our approach efficiently leverages cross-modal information to perform well.   
"," The natural world is abundant with concepts expressed via visual, acoustic, tactile, and linguistic modalities. Much of the existing progress in multimodal learning, however, focuses primarily on problems where the same set of modalities are present at train and test time, which makes learning in low-resource modalities particularly difficult. In this work, we propose algorithms for cross-modal generalization: a learning paradigm to train a model that can  quickly perform new tasks in a target modality  and  doing so while being trained on a different source modality. We study a key research question: how can we ensure generalization across modalities despite using separate encoders for different source and target modalities? Our solution is based on meta-alignment, a novel method to align representation spaces using strongly and weakly paired cross-modal data while ensuring quick generalization to new tasks across different modalities. We study this problem on 3 classification tasks: text to image, image to audio, and text to speech. Our results demonstrate strong performance even when the new target modality has only a few  labeled samples and in the presence of noisy labels, a scenario particularly prevalent in low-resource modalities. %Despite vast differences in these raw modalities, humans seamlessly perceive multimodal data, learn new concepts, and show extraordinary capabilities in generalizing across input modalities. %In addition, our method works particularly well when the target modality suffers from noisy or limited labels, a scenario particularly prevalent in low-resource modalities. %, sometimes outperforming within modality few-shot baselines that have seen thousands of labeled examples from that target modality during meta-training. %\zing[Ziyin: heterogeneous -> multimodal? since we are assuming there is an underlying shared space, so maybe not heterogeneous] %\zing[Ziyin: since this is the first sentence in the intro, maybe remove this?] %Similarly, truly general artificial intelligence  systems must learn to generalize across multiple input modalities and output tasks. %In this work, we define and propose algorithms for a new notion of generalization:  %, languages, and concepts. %We believe that our proposed methods could open new doors towards better generalization in multimodal AI systems.",423
"  Program source code contains rich structure information, like the syntax structure and control or data flow. Learning from these structures has been a hot topic in the area of deep learning on source code. In recent years, instead of applying basic sequential neural models, researchers have used more complex neural networks to capture the explicit structure of source code. Most researches use abstract syntax trees  as they are easy-to-acquire for most programming languages and semantically equivalent to source code.  A problem of ASTs is that they do not explicitly reflect structural information beyond syntax dependencies, like control and data flow. A viable solution is adding different types of control and data flow edges on ASTs to generate program graphs, and apply graph neural networks  on programs to learn their representations . However, these approaches do not consider that apart from control or data flow edges, the nodes and edges of the original ASTs are also differently typed. For example, in ASTs, some nodes refer to identifiers, and some nodes define upper-level structures as control flows. For parent-child links, the relation between a function definition node to its function body or one of its arguments is apparently different. We believe if we explicitly add node and edge types to programs graphs, it will help neural models to understand programs better.  Our idea of adding types to nodes and edges in AST coincides with the concept of heterogeneous graphs. Heterogeneous graphs, or heterogeneous information networks , refer to a group of graphs with multiple types of nodes and edges. A typical example of heterogeneous graphs is knowledge graphs, in which the nodes are different types of entities, and the edges represent different relations. In this paper, we propose an approach for building heterogeneous program graphs from ASTs. To obtain the type of AST nodes and edges, we use the abstract syntax description language   grammar.  After we acquire heterogeneous graphs for code snippets, we need to find a GNN model to effectively represent these graphs. Although some existing GNN-for-code works  have pointed out that there exist different types for AST nodes, they only consider node type in the initial node embedding and neglect their differences in the message passing  step. So we turn our sight to the field of heterogeneous graph embeddings. Recently, heterogeneous graph neural networks have become widely used in heterogeneous graph embedding. Unlike traditional graph neural networks, heterogeneous graph neural networks are capable of integrating node and edge type information in the message passing stage and map different types of nodes to different feature space. We use heterogeneous graph transformer   on our heterogeneous program graphs to calculate the representation of programs.  We evaluate our approach on two tasks: comment generation and method naming, with two Python datasets from different domains. These two tasks can be seen as two different forms of code summarization, so both of them require understanding the semantics of the input code snippets. The results show that our approach outperforms existing GNN models and other state-of-the-art approaches, indicating the extra benefit of bringing heterogeneous graph information to source code.  To summarize, our contributions are:  To our knowledge, we are the first to put forward the idea of representing programs as heterogeneous graphs and apply heterogeneous GNN on source code snippets.  We propose an approach of using ASDL grammars to build heterogeneous program graphs from program ASTs.  We evaluate our approach on two different tasks involving graph-level prediction on source code snippets. Our approach outperforms other GNN models on both comment generation and method naming tasks.  
"," Program source code contains complex structure information, which can be represented in structured data forms like trees or graphs. To acquire the structural information in source code, most existing researches use abstract syntax trees . A group of works add additional edges to ASTs to convert source code into graphs and use graph neural networks to learn representations for program graphs. Although these works provide additional control or data flow information to ASTs for downstream tasks, they neglect an important aspect of structure information in AST itself: the different types of nodes and edges. In ASTs, different nodes contain different kinds of information like variables or control flow, and the relation between a node and all its children can also be different.  To address the information of node and edge types, we bring the idea of heterogeneous graphs to learning on source code and present a new formula of building heterogeneous program graphs from ASTs with additional type information for nodes and edges. We use the ASDL grammar of programming language to define the node and edge types of program graphs. Then we use heterogeneous graph neural networks to learn on these graphs. We evaluate our approach on two tasks: code comment generation and method naming. Both tasks require reasoning on the semantics of complete code snippets. Experiment results show that our approach outperforms baseline models, including homogeneous graph-based models, showing that leveraging the type information of nodes and edges in program graphs can help in learning program semantics.",424
" [htb] % trim is left bottom right top , clip, width=\textwidth]{topic_discovery_diagram}      % Every day pharmaceutical companies receive numerous medical inquiries related to their products from patients, healthcare professionals, research institutes, or public authorities from a variety of sources .  % These medical inquiries may relate to drug-drug-interactions, availability of products, side effects of pharmaceuticals, clinical trial information, product quality issues, comparison with competitor products, storage conditions, dosing regimen, and the like.  % On the one hand, a single medical inquiry is simply a question of a given person searching for a specific information related to a medicinal product. On the other hand, a plurality of medical inquiries from different persons may provide useful insight into matters related to medicinal products and associated medical treatments. % Examples of these insights could be early detection of product quality or supply chain issues, anticipation of treatment trends and market events, improvement of educational material and standard answers/frequently asked question coverage, potential changes in treatment pattern, or even suggestions on new possible indications to investigate. % From a strategic perspective, this information could enable organizations to make better decisions, drive organization results, and more broadly create benefits for the healthcare community.   % transition paragraph - machine learning can help However, obtaining high-level general insights is a complicated task since pharmaceutical companies receive  copius amounts of medical inquiries every year. Machine learning and natural language processing represent a promising route to automatically extract insights from these large amounts of unstructured  medical text. % % % text mining in general and in the biomedical domain Natural language processing and text mining techniques have been widely used in the medical domain, with particular emphasis on electronic health records.  In particular, deep learning has been successfully applied to medical text, with the overwhelming majority of works in supervised learning, or representation learning  to learn specialized word vector representations . % %There is little work however on unsupervised learning from unstructured medical text.  Conversely, the literature on unsupervised learning for medical text is scarce despite the bulk of real-world medical text being unstructured, without any labels or annotations. % Unsupervised learning from unstructured medical text is mainly limited to the development of topic models based on latent Dirichlet allocation . Examples of applications in the medical domain are clinical event identification in brain cancer patients from clinical reports, modeling diseases and predicting clinical order patterns from electronic health records, or detecting cases of noncompliance to drug treatment from patient forums. % Only recently, word embeddings and unsupervised learning techniques have been combined to analyze unstructured medical text to study the concept of diseases, medical product reviews, or to extract informative sentences for text summarization.  % real-world corpus of medical inquiries and its challenges In this work, we combine biomedical word embeddings and unsupervised learning to discover topics from real-world medical inquiries received by Bayer\texttrademark. % A real-world corpus of medical inquiries presents numerous challenges. From an inquirer  perspective, often the goal is to convey the information requested in as few words as possible to save time. This leads to an extensive use of acronyms, sentences with atypical syntactic structure, occasionally missing verb or subject, or inquiries comprising exclusively a single noun phrase. % Moreover, since medical inquiries come from different sources, it is common to find additional  information related to the text source; examples are references to internal computer systems, form frames  alongside with the actual form content, lot numbers, email headers and signatures, city names.  % % mixture of layman and medical language The corpus contains a mixture of layman and medical language depending  on the inquirer being either a patient or a healthcare professional. Style and content of medical inquiries vary quite substantially according to which therapeutic areas  a given medicinal product belongs to.  % add sentence to refer to the text representation %as one can see from Fig., As already mentioned, medical inquiries are short. More specifically, they comprise less than fifteen words in the vast majority of cases.  % Standard techniques for topic modelling based on LDA do not apply, since the main assumption - each document/text is a distribution over topics - clearly does not hold given that the text is short.  % Approaches based on pseudo-documents or using auxiliary information are also not suitable since no meaningful pseudo-document nor auxiliary information are available for medical inquiries. % Moreoever, these models aim to learn semantics  directly from the corpus of interest. However, the recent success of pretrained embeddings shows that it is beneficial to include semantics learned on a general  corpus, thus providing semantic information difficult to obtain from smaller corpora. This is particularly important for limited data and short text settings. To this end, there has been recently some work aimed at incorporating word embeddings into probabilistic models similar to LDA  and that - contrary to LDA - satisfies the single topic assumption .  Even though these models include  semantic information in the topic model, it is not evident how to choose the required hyper-parameters, for example determining an appropriate threshold when filtering semantically related word pairs. Concurrently to our work, document-level embeddings and hierarchical clustering have been combined to obtain topic vectors from news articles and a question-answer corpus.  % summary Here, we propose an approach based on specialized biomedical word embeddings and unsupervised learning to discover topics from short, unstructured, real-world medical inquiries. This approach - schematically depicted in Fig. - is then used to discovery topics in medical inquiries received by Bayer\texttrademark\ Medical Information regarding the oncology medicinal product Stivarga\texttrademark.    
"," %141 words % the motivation Millions of unsolicited medical inquiries are received by pharmaceutical companies every year.  It has been hypothesized that these inquiries represent a treasure trove of information, potentially giving insight into matters regarding medicinal products and the associated medical treatments.  % the challenge However, due to the large volume and specialized nature of the inquiries, it is difficult to perform timely, recurrent, and comprehensive analyses. % the solution Here, we propose a machine learning approach based on natural language processing and unsupervised learning to automatically discover key topics in real-world medical inquiries from customers. This approach does not require ontologies nor annotations.  % the results The discovered topics are meaningful and medically relevant, as judged by medical information specialists, thus demonstrating that unsolicited medical inquiries are a source of valuable customer insights. % the implications and outlook Our work paves the way for the machine-learning-driven analysis of medical inquiries in the pharmaceutical industry, which ultimately aims at improving patient care.",425
" Dynamic models of text aim at characterizing temporal changes in patterns of document generation. Most successful dynamic language models are Bayesian in nature, and lag behind state-of-the-art deep language models in terms of expressibility. A natural space to study some of the temporal aspects of language is that of the large review datasets found in e-commerce sites.  The availability of millions of reviewed items, such as business or services, books or movies, whose reviews have been recorded in time scales of years, opens up the possibility to develop deep scalable models that can predict the change in taste and preference of users as time evolves. Originally, the interaction of users in these e-commerce sites were studied in the context of collaborative filtering, where the goal was to predict user ratings, based on user interaction metrics. Here we aim to look directly at the content of reviews as time evolves.  %More KDD probably, to much focus on the ratings and recommendations  %-------- %The shear size of e-commerce and review web sites naturally lend itself to the development of data mining tools which are able to provide users with a way to sort out relevant information. This is the task assigned to recommender systems.  Originally kick started by the Netflix competition, matrix factorization  methods through collaborative filtering, aim at predicting user ratings based on user interaction metrics. This rating based methods are lacking as they are unable to clarify the nature of the user preferences, in particular how those preferences change on time. In order to address this issue, methodologies that exploit costumers reviews are gaining attention.  %--------- Costumer reviews provide a rich and natural source of unstructured data which can be leverage to improve recommender system performance . Indeed, reviews are effectively a form of recommendation. % Recently, a variety of deep learning solutions for recommendation have profit from their ability to extract latent representations from review data, encoding rich information related to both users and items. % %Review  content naturally encodes  % This type of data  % Review content is of contextual nature, as the text arises from the interaction of user preferences and items at hand.  % Time represents yet another dimension of context, as user preference and item availability change with time % -- and indeed, % causal and temporal relations have been known to improve the performance of recommender systems  .  % Despite this fact, % recent natural language processing  methodologies for rating and reviews  lag behind at incorporating temporal structure in their language representations. In the present work we exploit recurrent neural network  models for point processes, and feed them neural representations of text, to characterize costumer reviews. Our goal is to capture the changes in user taste and item importance during time, and to exploit those changes to better predict when are new reviews arriving, and what do they actually say. We summarize our contributions as follows:      {}  % [ht!] %       %      %      %     We present the related work in Section  and introduce our model in Section . The baseline models used for comparison in this paper are presented in Section . The experimental setup and results are presented in Section . Finally, in Section  we conclude and discuss future work.  
"," Deep neural network models represent the state-of-the-art methodologies for natural language processing.  % Here we build on top of these methodologies to incorporate temporal information and model how review data changes with time. % Specifically, we use the dynamic representations of recurrent point process models, % % which encode the nonlinear relations between content and timing of the reviews received by e.g. businesses or services,  % which encode the history of how business or service reviews are received in time,  % to generate instantaneous language models with improved prediction capabilities.  % Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations.  % % as that encoded in recurrent point process models, and improve the predictive power of these model by incorporating the text representations.  % % Our methodologies resemble that of a hierarchical model, whereupon the temporal information is used as a  representation for the language model.  % We provide recurrent network and temporal convolution solutions for modeling the review content. % We deploy our methodologies in the context of recommender systems,  % as to enhance the expressibility of current models, % effectively characterizing the change in preference and taste of users as time evolves. Source code is available at .",426
"  Most authentication methods commonly used today rely on users setting custom passwords to access their accounts and devices. Password-based authentications are popular due to their ease of use, ease of implementation and the established familiarity of users and developers with the method.   However studies show that users tend to set their individual passwords predictably, favoring short strings, names, birth dates and reusing passwords across sites.  Since chosen passwords exhibit certain patterns and structure, it begs the question whether it is possible to simulate these patterns and generate passwords that a human user realistically might have chosen.  Password guessing is an active field of study, until recently dominated by statistical analysis of password leaks and construction of corresponding generation algorithms . These methods rely on expert knowledge and analysis of various password leaks from multiple sources to generate rules and algorithms for efficient exploitation of learned patterns.  On the other hand, in recent years major advances in machine-driven text generation have been made, notably by novel deep-learning based architectures and efficient training strategies for large amounts of training text data. These methods are purely data driven, meaning they learn only from the structure of the input training text, without any external knowledge on the domain or structure of the data. % Deep learning models have recently shown remarkable performance concerning text classification and text generation.  Major advancements in the field have been fueled by the development in several central directions such as:   	 mechanisms}. Considering a token  within a textual environment, the idea is to develop a flexible notion of context that connects the given token with other pieces of the textual environment. Intuitively, this allows the learning model to better grasp the textual structure , thus leading to eventual improvements in terms of text classification/generation/interpretability. Among others, well-known attention-based examples are given by BERT, ELMO, GPT and various further types of Transformers. 	 	. Remarkable  \\progress has been made in designing more flexible deep learning structures . The success of these deep neural networks can to a large extent be attributed to their representation capability, i.e. they create an appropriate transformation of the data  that renders the data easier to handle and solve a given problem. In this regard a central class of deep learning models is given by the so-called 	extbf{autoencoders} whose goal is not only to create a meaningful and useful data representation/transformation  but also to be able to go back and reconstruct the initial data from the representation . An upshot is that one could generate new data by sampling points in the representation space and then decoding back. 	 	. The above tools would not be as efficient, had not it been for the corresponding methods to select  the parameters and weights of the neural networks. Among others these include appropriate momentum and annealing-driven stochastic gradient descents, Wasserstein regularization and variational approaches.   In this paper we will continue the exploration of data driven deep-learning text generation methods for the task of password-guessing. While some applications to password guessing already show promising results, most frameworks still can not reach or surpass state-of-the-art password generation algorithms. % On the other hand, considering password guessing problems, some popular frameworks  as well as a large body of state-of-art research suggest that advanced deep learning methodologies are still to be further explored.  Ideally, one would attempt to design more efficient password-guessing models aided by neural networks and cutting-edge practices.  Our findings and contributions can be summarized as follows:                   
","     Password guessing approaches via deep learning have recently been investigated with significant breakthroughs in      their ability to generate novel, realistic password candidates.     In the present work we study a broad collection of deep learning and probabilistic based models in the light of password guessing:      attention-based deep neural networks, autoencoding mechanisms and generative adversarial networks.      We provide novel generative deep-learning models in terms of variational autoencoders exhibiting state-of-art sampling performance,     yielding additional latent-space features such as interpolations and targeted sampling.     Lastly, we perform a thorough empirical analysis in a unified controlled framework over well-known datasets .      Our results not only identify the most promising schemes driven by deep neural networks, but also illustrate the strengths of each approach in terms of generation variability and sample uniqueness.",427
" % 1 page  % Definition and importance of the causality knowledge.  % causality knowledge, as an important knowledge for artificial intelligence  systems, has been proven helpful in many downstream tasks, especially in the NLP domain. % % In this work, we follow ConceptNet and COPA to focus on the causal relations between daily events. % However, due to the lack of a high-quality and large-scale causality knowledge resource, the application of causality knowledge in downstream tasks is still limited.  Humans possess a basic knowledge about facts and understandings for commonsense of causality in our everyday life.  For example, if we leave five minutes late, we will be late for the bus; if the sun is out, it's not likely to rain; and if we are hungry, we need to eat. %Causality is an important commonsense reasoning that humans use all the time,  Such causality knowledge has been shown to be helpful for many NLP tasks. Thus, it is valuable to teach machines to understand causality.   Causal relations in the commonsense domain are typically contributory and contextual.  %   By contributory\footnote{The other two levels are absolute causality  and conditional causality , which commonly appear in the scientific domain rather than our daily life.}, we mean that the cause is neither necessary nor sufficient for the effect, but it strongly contributes to the effect.  By contextual, we mean that some causal relations only make sense in a certain context. The contextual property of causal relations is important for both the acquisition and application of causal knowledge. For example, if some people tell the AI assistant  ``they are hungry'' in a meeting, a basic assistant may suggest them to order food because it has the knowledge that `being hungry' causes `eat food'. A better assistant may suggest ordering food after the meeting because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the meeting context.  % \ye{I made small adaptation to this paragraph } % For example, if a person is in the middle of a meeting, he/she may tell the AI assistant  that he/she is hungry, a good AI assistant may suggest him/her to eat some food because it has the knowledge that `being hungry' cause `eat food', but an extraordinary AI assistant may suggest that ``I can help order some food for you to eat after the meeting'' because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the context of a meeting. Without understanding the contextual property of causal knowledge, achieving such a level of intelligence would be challenging.  To help machines better understand the causality commonsense, many efforts have been devoted into developing the causality knowledge bases.  For example, ConceptNet and ATOMIC leverage  human-annotation to acquire small-scale but high-quality causality knowledge. After that, people try to leverage linguistic patterns  to acquire causality knowledge from textual corpus. However, causality knowledge, especially those trivial knowledge for humans, are rarely formally expressed in documents, a pure text-based approach might struggle at covering all causality knowledge. Besides that, none of them take the aforementioned contextual property of causal knowledge into consideration, which may restrict their usage in downstream tasks.     % Causal relations in the commonsense domain are typically contributory and contextual.  % By contributory\footnote{The other two levels of causality are absolute causality  and conditional causality , which commonly appear in the scientific domain rather than our daily life.}, we mean that the cause is neither necessary nor sufficient for the effect, but it strongly contributes to the effect.  % By contextual, we mean that some causal relations only make sense in a certain context. % The contextual property of causal relations is important for both the acquisition and application of causality knowledge. % For example, if some people tell the AI assistant  ``they are hungry'' in a meeting, a basic assistant may suggest them to order food because it has the knowledge that `being hungry' causes `eat food'. A better assistant may suggest ordering food after the meeting because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the meeting context.  % Without understanding the contextual property of causality knowledge, achieving such a level of intelligence would be challenging.  % [t] %      % % limitation of existing acquisition methods % Conventional approaches  \ye{i think this should be more elaborated. maybe give an example?} However, two drawbacks of these approaches significantly limit their usage in downstream tasks: % [leftmargin=*] %     %  %      %     In this paper, we propose to ground causality knowledge into the real world and explore the possibility of acquiring causality knowledge from visual signals .  By doing so, we have three major advantages:  Videos can be easily acquired and can cover rich commonsense knowledge that may not be mentioned in the textual corpus;  Events contained in videos are naturally ordered by time. As discussed by, there exists a strong correlation between temporal and causal relations, and thus such time-consecutive images can become a dense causality knowledge resource;  Objects from the visual signals can act as the context for detected causality knowledge, which can remedy the aforementioned lack of contextual property issue of existing approaches.   To be more specific, we first define the task of mining causality knowledge from time-consecutive images and propose a high-quality dataset .  To study the contextual property of causal relations, for each pair of events, we provide two kinds of causality annotations: one is the causality given certain context and the other one is the causality without context.  Distribution analysis and case studies are conducted to analyze the contextual property of causality. An example from Vis-Causal is shown in Figure, where the causal relation between ``dog is running'' and ``blowing leaves'' only makes sense when the context is provided because the dog is running on the leaves, so its high speed and quickly-moved pow cause the leaves blow around. Without the context  ``leaves on the ground'', this causal relation is implausible. After that, we propose a Vision-Contextual Causal  model, which can effectively leverage both the pre-trained textual representation and visual context to acquire causality knowledge and can be used as a baseline method for future works. Experimental results demonstrate that even though the task is still challenging, by jointly leveraging the visual and contextual representation, the proposed model can better identify meaningful causal relations from time-consecutive images. To summarize, the contributions of this paper are three-fold:  We formally define the task of mining contextual causality from the visual signal;  We present a high-quality dataset Vis-Causal;  We propose a Vision-Contextual Causal  model to demonstrate the possibility of mining contextual causality from the vision signal. % Experimental results prove that considering context is crucial for understanding causality and representing the visual context with textual representation is helpful. % Further analysis shows that the proposed task is still challenging for current models, and we may need to consider injecting external knowledge to better understand the videos and acquire causality knowledge. % \ye{there's no real reference to the text part in the into, NLP people might think it's not suitable for ACL? maybe add that the models use some description and objects which are represented in a textual form}   %   % 
","  Causality knowledge is crucial for many artificial intelligence systems. Conventional textual-based causality knowledge acquisition methods typically require laborious and expensive human annotations. As a result, their scale is often limited. Moreover, as no context is provided during the annotation, the resulting causality knowledge records  typically do not take the context into consideration. To explore a more scalable way of acquiring causality knowledge, in this paper, we jump out of the textual domain and investigate the possibility of learning contextual causality from the visual signal. Compared with pure text-based approaches, learning causality from the visual signal has the following advantages:  Causality knowledge belongs to the commonsense knowledge, which is rarely expressed in the text but rich in videos;  Most events in the video are naturally time-ordered, which provides a rich resource for us to mine causality knowledge from;  All the objects in the video can be used as context to study the contextual property of causal relations. In detail, we first propose a high-quality dataset Vis-Causal and then conduct experiments to demonstrate that with good language and visual representation models as well as enough training signals, it is possible to automatically discover meaningful causal knowledge from the videos. Further analysis also shows that the contextual property of causal relations indeed exists, taking which into consideration might be crucial if we want to use the causality knowledge in real applications, and the visual signal could serve as a good resource for learning such contextual causality. Vis-Causal and all used codes are available at: \url{https://github.com/HKUST-KnowComp/Vis_Causal}. % In detail, we first identify events from the videos, which are represented with natural sentences, and then leverage the visual signal to predict the contextual causal relations among these events.     % In this work, we mimic how human beings learn causality and explore the possibility of acquiring causality knowledge with visual signal. % To do so, we first define the task of mining contextual causality knowledge from visual signals, which aims at evaluating models' abilities to identify causal relation given certain visual context, and then employ the crowd-sourcing to annotate a high-quality dataset Vis-Causal. % On top of that, we propose a Vision-Contextual Causal  model that can utilize the images as context to better acquire causality knowledge. % Different from existing \revisehm{causality knowledge acquisition works}, \revisehm{to the best of our knowledge, }the proposed solution \revisehm{is the first one that }has the potential to preserve contextual property  of causal relations.",428
"  % The advent of deep learning techniques has dramatically improved accuracy of speech recognition models . Deep learning techniques first saw success by replacing the Gaussian Mixture Model  of the Acoustic Model  part of the conventional speech recognition systems  with the Feed-Forward Deep Neural Networks  , further with Recurrent Neural Network  such as the  Long Short-Term Memory  networks  or Convonlutional Neural Networks . In addition to this, there have been improvements in noise robustness by using models motivated by auditory processing , data augmentation techniques , and beam-forming . Thanks to these advances, voice assistant devices such as Google Home  and Amazon Alexa have been widely used at home environments.  Nevertheless, it was not easy to run such high-performance speech recognition systems  on devices largely because of the size of the Weighted Finite State Transducer   handling the lexicon and the language model.  Fortunately, all-neural end-to-end  speech recognition systems were introduced which do not need a large WFST or an n-gram Language Model   . These complete end-to-end systems have started surpassing the performance of the conventional WFST-based decoders with a very large training  dataset  and a better choice of target unit  such as Byte Pair Encoded  subword units.  In this paper, we provide a comprehensive review of the various components and algorithms of an end-to-end speech recognition system. In Sec., we give a brief overview of the various neural building blocks of an E2E Automatic Speech Recognition  model. The most popular E2E ASR architectures are reviewed in Sec.. Additional techniques used to improve the performance of E2E ASR models are discussed in Sec.. Techniques used for compression and quantization of the all-neural E2E ASR models are covered in Sec.. Sec. gives a summary of the paper. % % %# Data augmentation and overfitting 
","   In this paper, we review various end-to-end automatic speech recognition   algorithms and their optimization techniques for on-device applications.   Conventional speech recognition systems comprise a large number of discrete   components such as an acoustic model, a language model, a pronunciation model,    a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted Finite State   Transducer , and so on. To obtain sufficiently high speech recognition   accuracy  with such conventional speech recognition systems, a very large   language model  is usually needed. Hence, the corresponding   WFST size becomes enormous, which prohibits their on-device implementation. Recently, fully neural network end-to-end speech recognition algorithms have been   proposed. Examples include speech recognition systems based on  Connectionist Temporal Classification , Recurrent Neural Network Transducer , Attention-based Encoder-Decoder models , Monotonic   Chunk-wise Attention ,    transformer-based speech recognition systems, and so on. These fully neural   network-based systems require much smaller memory footprints compared to   conventional algorithms, therefore their on-device implementation has become   feasible. In this paper, we review such end-to-end speech recognition models.   We extensively discuss their structures, performance, and advantages compared   to conventional algorithms.",429
" %  %     God, grant me the serenity to accept the things I cannot change, % courage to change the things I can, and wisdom to know the difference.\\ %  -Serenity Prayer %     Solving math word problems  poses unique challenges for understanding natural-language problems and performing arithmetic reasoning over quantities with commonsense knowledge. As shown in \autoref{fig:example}, a typical MWP consists of a short narrative describing a situation in the world and asking a question about an unknown quantity. To solve the MWP in \autoref{fig:example}, a machine needs to extract key quantities from the text, such as ""100 kilometers"" and ""2 hours"", and understand the relationships between them. General mathematical knowledge like ""distance = velocity  time"" is then used to calculate the solution.   %The task of automatically solving Math Word Problems  requires mapping the human-readable natural language into machine-understandable logic forms, e.g., expressions, followed by execution process that calculates the numeric answer. \autoref{fig:example} an example of a math word problem, the ground truth answer and the expression that derives the answer when executed. Recently, researchers have focused on solving MWPs using neural models. The advantage of these neural models is that they do not rely on hand-crafted features.  Researchers have recently focused on solving MWPs using neural-symbolic models. These models usually consist of a neural perception module  \times 100 + 100/2 \times 3.5 if we want to calculate the speed first and then multiply it by total hours. Or, we can solve it by , if we first compute the length of the second part of the journey given the ratio of time spans, and add it to the first part. However, only the first expression is given as the ground-truth expression in the dataset, and thus neural models tend to ``punish'' the second expression. In this way, fully-supervised learning fails to generate more diverse and correct expressions. The second problem is ``train-test discrepancy''. It means that MLE uses a surrogate objective of maximizing equation likelihood during training, while the evaluation metric of the task is solution accuracy, which is non-differentiable.  proposes to solve this via reinforcement learning but still use pre-trained MLE model. Last but not least, there's the problem of lack of fully annotated data online. Recruiting crowd-workers to provide the correct equations is time consuming. However, thousands of MWPs have been posted in online forums, where the final answers can be easily mined. These data can be useful if we can train our model without the supervision of expressions.   %To address these issues, we propose to solve the MWPs with weak supervision, where only the problem texts and the final answers are required for learning. % We adopt the goal-driven tree-structured  model proposed by~ as the base model. %Since the execution process of arithmetic expressions in previous deep learning models is non-differentiable, it is infeasible to use back-propagation to compute gradients. A straightforward approach is to employ policy gradient methods like REINFORCE. In weakly-supervised MWP, policy gradient methods explore the solution space and update the policy based on generated solutions that happen to hit the right answers, while incorrect solutions are totally abandoned. Since the solution space is quite large, policy gradients methods usually converge slowly or sometimes even fail to converge.  To improve the efficiency of weakly-supervised learning, we propose a novel fixing mechanism to learn from incorrect predictions, which is inspired by the human ability to learn from failures via abductive reasoning. The fixing mechanism propagates the error from the root node to the leaf nodes in the solution tree and finds the most probable fix that can generate the desired answer. The fixed solution tree is further used as a pseudo label to train the neural model. \autoref{fig:framework} shows how the fixing mechanism corrects the wrong solution tree by tracing the error in a top-down manner.  Furthermore, we design two practical techniques to traverse the solution space and discover possible solutions efficiently. First, we observe a positive correlation between the number of quantities in the text and the size of the solution tree , and propose a tree regularization technique based on this observation to limit the range of possible tree sizes and shrink the solution space. Second, we adopt a memory buffer to track and save the discovered fixes for each problem with the fixing mechanism. All memory buffer solutions are used as pseudo labels to train the model, encouraging the model to generate more diverse solutions for a single problem.   In summary, by combining the fixing mechanism and the above two techniques, the proposed learning-by-fixing  method contains an exploring stage and a learning stage in each iteration, as shown in \autoref{fig:framework}. We utilize the fixing mechanism and tree regularization to correct wrong answers in the exploring stage and generate fixed expressions as pseudo labels. In the learning stage, we train the neural model using these pseudo labels.  We conduct comprehensive experiments on the Math23K dataset. The proposed LBF method significantly outperforms the reinforcement learning baselines in weakly-supervised learning and achieves comparable performance with several fully-supervised methods. Furthermore, our proposed method achieves significantly better answer accuracies of all the top-3/5 answers than fully-supervised methods, illustrating its advantage in generating diverse solutions. The ablative experiments also demonstrate the efficacy of the designed algorithms, including the fixing mechanism, tree regularization, and memory buffer.  % This paper makes three major contribution: % [leftmargin=*,noitemsep,nolistsep] %    %Policy gradient methods like REINFORCE are frequently used in weakly-supervised tasks . Such methods suffer from sparse reward, cold start and inefficient exploration of solution space. This is because neural network make wrong perceptions and generate negative samples which are ``abandoned'' by REINFORCE. Human beings, like neural networks, tend to make inaccurate perceptions. However, they embody the skills to correct their misperceptions when reasoning about the wrong forms and guessing about correct patterns. What's more, they are able to approach the solutions in diverse ways. For example, in \autoref{fig:framework}, a child might provide a wrong expression  given the problem. Then he started to reason about where he did wrong and found out he could actually fix the expression by replacing the first """" with """". The fixed expression looks nothing like the ground truth expression *$) provided by the dataset.    % Therefore, policy gradients methods converge slowly or even fail to converge without MLE pre-training on fully-supervised data.  %Inspired by this, we propose a novel fixing mechanism which resembles human閳ユ獨 ability to diagnose and fix the expressions that cannot generate desired answers. The ground truth answer  is propagated through the expression tree in a top-down manner. In the meantime, we try depth-first-search for a possible fix. Similar to Memory-Augmented Policy Optimization which utilizes a memory buffer to save previous successful trajectories given by REINFORCE, we adopt a memory buffer to store all successful fixes. Different expressions in the buffer are all used to train the model, thus allowing us to generate more diverse answers.  % Our contributions are summarized as following: % [leftmargin=*,noitemsep,nolistsep] %  %      &  \makecell[l]{A truck travels 100 kilometers in 2 hours. At this\\ speed, if it travels for another 3.5 hours, how many\\ kilometers will it complete for the entire journey?}\\ %     & 275\\ %     \\ of Math23K} & *\\ %     \\ Our Model}&\makecell[l]{100/2*3.5+100, 100+100*3.5/2,\\ 100+100/, 100/), *}\\ %     \relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21}  % DO NOT CHANGE THIS \usepackage{times}  % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier}  % DO NOT CHANGE THIS \usepackage[hyphens]{url}  % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm}  % DO NOT CHANGE THIS \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing  % DO NOT CHANGE THIS {8.5in}  % DO NOT CHANGE THIS {11in}  % DO NOT CHANGE THIS % additional packages \usepackage{latexsym} \usepackage{makecell} \usepackage{amsmath,amssymb,mathtools,bm,etoolbox} \usepackage{algorithm} \usepackage[noend]{algorithmic} \usepackage{enumitem} \usepackage{xcolor} \usepackage{pifont} \usepackage{multirow} \usepackage{diagbox} \usepackage[switch]{lineno} \usepackage[autostyle=false, style=english]{csquotes} \MakeOuterQuote{""} \usepackage[draft]{hyperref} %  Disable links - according to AAAI format guide  \DeclarePairedDelimiter{\rceil} \DeclarePairedDelimiter\floor{\lfloor}{\rfloor}  {Algorithm}     \renewcommand{
"," % Most previous solvers of math word problems  are learned with full supervision and fail to generate diverse solutions for each problem. In this paper, we address this issue by introducing a weakly-supervised paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel learning-by-fixing  framework that mimics the human ability to learn from incorrect predictions. Specifically, the fixing mechanism propagates the error from the root node to the leaf nodes of a solution tree and infers the most probable fix that can be executed to the desired answer. To generate more diverse solutions, tree regularization is applied to guide the efficient shrinkage and exploration of the solution space, and a memory buffer is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions. Previous neural solvers of math word problems  are learned with full supervision and fail to generate diverse solutions. In this paper, we address this issue by introducing a weakly-supervised paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel learning-by-fixing  framework, which corrects the misperceptions of the neural network via symbolic reasoning. Specifically, for an incorrect solution tree generated by the neural network, the fixing mechanism propagates the error from the root node to the leaf nodes and infers the most probable fix that can be executed to get the desired answer. To generate more diverse solutions, tree regularization is applied to guide the efficient shrinkage and exploration of the solution space, and a memory buffer is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions.",430
"  % Hate speech, its extensiveness, effect     % --> humans can't inspect every sample  % Memes, their use & hateful memes % HM data set % challenge       % --> not all dataset is 'truly' requires multi-modality     % --> HM dataset proposes 'benign confounders' to make sure multimodality is required    Memes have gained huge popularity over the past years, resulting in over 180m posts on different social media platforms until 2018. Although memes are oftentimes harmless and generated especially for humorous purposes, they have also been used to produce and disseminate hate speech in toxic communities. Hate Speech  is a direct attack on people based on race, ethnicity, national origin, religious affiliation, sexual orientation, sex, gender, and serious disease or disability -- a growing problem in modern society. Giant tech companies, such as Facebook, own platforms where millions of users log in daily and they are obliged to remove a tremendous amount of HS to protect their users. According to Mike Schroepfer, Facebook CTO, they took an action on  pieces of content for violating their HS policies in the first quarter of 2020. This amount of malicious content cannot be tackled by having humans inspect every sample. Consequently, machine learning and in particular deep learning techniques are required to alleviate the extensiveness of online hate speech. Detecting hate speech in memes is challenging due to the multimodal nature of memes . Therefore, these techniques have to process the content the way humans do: holistically. When viewing a meme, a human would not think about the words and the picture independently; but understand the combined meaning. Moreover, while the visual and linguistic information of a meme is typically neutral or funny individually, their combination may result in a hateful meme.      A recent study shows that state-of-the-art methods for hate speech detection in multimodal memes perform poorly compared to humans:  vs.  accuracy. To catalyze sophisticated research in this area, Facebook AI launched the Hateful Memes Challenge and published a dataset containing more than 10,000 newly created multimodal memes. Multimodal tasks reflect many real-world problems, including how humans perceive and understand the world around them.       There has been a surge of interest in multimodal problems since 2015 in visual question answering, image captioning, speech recognition and beyond. But it is not always clear to what extent genuinely multimodal reasoning and understanding are needed to solve current challenges. For instance, for some datasets language can unintentionally impose strong priors, which might result in a remarkable performance, without any understanding of the visual content. The Hateful Memes challenge design and dataset are created to encourage and measure truly multimodal understanding and reasoning of the models. A key point to achieve this are the so-called ``benign confounders''  which addresses the risk of exploiting unimodal priors by models: for every hateful meme, there are alternative images or text that flip the label to not-hateful. Such image and text confounders require multimodal reasoning to classify the original meme and its confounders correctly. Thus, making the dataset challenging and appropriate for testing the true multimodality of a model.    In the following, we analyze the challenge dataset and describe our prize-winning solution that placed third among 3,173 participants in the Hateful Memes Challenge in detail. Our solution achieves  AUROC with an accuracy of  on the challenge test set, which improves all the benchmark models, including the state-of-the-art models at that time, such as ViLBERT  and VisualBERT . Nevertheless, the accuracy is still behind humans with a mentionable gap, highlighting the need for progress in multimodal research.   
","   Memes on the Internet are often harmless and sometimes amusing. However, by using certain types of images, text, or combinations of both, the seemingly harmless meme becomes a multimodal type of hate speech -- a hateful meme. The Hateful Memes Challenge\footnote{\url{https://www.drivendata.org/competitions/70/hateful-memes-phase-2/}} is a first-of-its-kind competition which focuses on detecting hate speech in multimodal memes and it proposes a new data set containing 10,000+ new examples of multimodal content. We utilize VisualBERT -- which meant to be the ``BERT of vision and language'' -- that was trained multimodally on images and captions and apply Ensemble Learning. Our approach achieves 0.811 AUROC with an accuracy of 0.765 on the challenge test set and placed third out of 3,173 participants in the Hateful Memes Challenge\footnote{HateDetectron at \url{https://www.drivendata.org/competitions/70/hateful-memes-phase-2/leaderboard/}}. The code is available at  %        \url{https://github.com/rizavelioglu/hateful_memes-hate_detectron}     %",431
"  Designing a robust spoken language identification  algorithm is very important for the wide usability of multi-lingual speech applications . With the resurgence of deep model learning, the SLID performance has been significantly improved by current supervised deep feature and classifier learning algorithms . In most algorithms, there is an implicit assumption that the training and testing data sets share a similar statistical distribution property. However, due to the complex acoustic and linguistic patterns, it is often the case that testing data set and training data set are from quite different domains . An intuitive solution is to do domain adaptation, i.e., to align the statistical distribution of testing data set to match that of training data set thus to improve the performance. Although with large collected labeled testing data set, it is not difficult to obtain a domain transfer function with supervised learning algorithms, in real applications, the label information of testing data set is often unknown. Therefore, in this study, we mainly focus on a more preferable and challenge situation, i.e., unsupervised domain adaptation.  Unsupervised domain adaptation algorithms have been proposed for speaker verification, e.g., probabilistic linear discriminant analysis  parameter adaptation  , feature-based correlation alignment  , and feature-distribution adaptor for different domain vectors . However, in these algorithms, most of them were proposed for speaker verification under the framework of the PLDA . As our experiments showed that the PLDA framework does not perform well for our SLID task due to the less of discriminative power of the modeling. Instead, in most SLID algorithms, a multiple mixture of logistic regression  model is used as a classifier model. Moreover, due to the complex shapes of the distributions in training and testing domains, it is difficult to guarantee the match between different domain distributions.  The purpose for domain adaptation is to reduce the domain discrepancy. Recently, optimal transport  has been intensively investigated for domain adaptation in machine learning field . The initial motivation for OT in machine learning is to find an optimal transport plan to convert one probability distribution shape to another shape with the least effort . By finding the optimal transport, it naturally defines a distance measure between different probability distributions. Based on this property, the OT is a promising tool for domain adaptation and shape matching in image processing, classification, and segmentation . In this paper, inspired by the OT based unsupervised adaptation , we propose an unsupervised neural adaptation framework for cross-domain SLID tasks. Our main contributions are:  We propose an unsupervised neural adaptation model for SLID to deal with domain mismatch problem. In the model, we explicitly formulate the adaptation in transformed feature space and classifier space in order to reduce the probability distribution discrepancy between source and target domains.  We coincide the OT distance metric in measuring the probability distribution discrepancy, and integrate it into the network optimization in order to learn the adaptation model parameters. Based on the adaptation model, significant improvements were obtained. %The remainder of the paper is organized as follows. Section  introduces the background and fundamental theory of . Section  describes the implementation details of . Section  presents the SLID experiments and results based on the proposed framework by analyzing the contribution of the CSA model in detail. Section  presents the discussion of the results and conclusion of the study. 
"," Due to the mismatch of statistical distributions of acoustic speech between training and testing sets, the performance of spoken language identification  could be drastically degraded. In this paper, we propose an unsupervised neural adaptation model to deal with the distribution mismatch problem for SLID. In our model, we explicitly formulate the adaptation as to reduce the distribution discrepancy on both feature and classifier for training and testing data sets. Moreover, inspired by the strong power of the optimal transport  to measure distribution discrepancy, a Wasserstein distance metric is designed in the adaptation loss. By minimizing the classification loss on the training data set with the adaptation loss on both training and testing data sets, the statistical distribution difference between training and testing domains is reduced. We carried out SLID experiments on the oriental language recognition  challenge data corpus where the training and testing data sets were collected from different conditions. Our results showed that significant improvements were achieved on the cross domain test tasks.",432
"  The internet is having a huge impact on all of our lives and our virtual presence reflects both our personalities and beliefs but also our biases and prejudices. Billions of people are interacting with various online content every day and while some of it is highly useful and enriches our knowledge and understanding of the world, an increasing portion of this content is also harmful. This includes hate speech, misinformation and other forms of online abuse. An increasing amount of effort is required to quickly detect this content, scale up the review work and make automatic decisions to take down the harmful media fast in order to minimize the inflicted harm to the readers.  Many of our interactions happen on social media platforms, which we use to share messages and pictures with our private community or general public audiences.  Facebook AI has launched a competition  to flag hateful memes consisting of both images and text. For this purpose they provide a unique labeled dataset of 10,000+ high quality new multimodal memes. The goal of the challenge is to create an algorithm that identifies multimodal hate speech in internet memes, while also being robust to their benign flip. A meme might be mean or hateful either because of the meme image itself, or the text or their combination. Benign flipping is an augmentation technique used by the competition organizers to flip a meme from hateful to non-hateful and viceversa. This requires changing either the meme text or the image to flip its label. Figure shows how this process works.  Since the problem is formulated as a binary classification task, the primary evaluation metric used to rank the results is the area under the receiver operating characteristic curve . This represents the area under the ROC curve, which in turn plots the True Positive Rate  vs. False Positive Rate  at different classification thresholds T. The goal is to maximize the AUROC.   Accuracy is the secondary tracked metric and it calculates the percentage of instances where the predicted class \^{y} matches the actual class, y in the test set.   Ideally, the model maximizes both these metrics.  In summary, our contribution is threefold:             
","   While significant progress has been made using machine learning algorithms to detect hate speech, important technical challenges still remain to be solved in order to bring their performance closer to human accuracy. We investigate several of the most recent visual-linguistic Transformer architectures and propose improvements to increase their performance for this task. The proposed model outperforms the baselines by a large margin and ranks 5\textsuperscript{th} on the leaderboard out of 3,100+ participants.    \footnote{Code is available at \url{https://github.com/vladsandulescu/hatefulmemes}.}",433
" In traditional ad-hoc retrieval, queries and documents are represented by variants of bag-of-words representations. This leads to the so called vocabulary mismatch problem: when a query contains words that do not exactly match words in a relevant document, the search engine may fail to retrieve this document. Query expansion and document expansion, the methods of adding additional terms to the original query or document, are two popular solution to alleviate the vocabulary mismatch problem.   Document expansion has been shown to be particularly effective for short text retrieval and language-model based retrieval . Most of the existing works in document expansion are unsupervised: using information from the corpus to augment document representation, e.g., retrieval based  and clustering based , or using external information to augment document representation .  Recently,  proposed a new approach to document expansion, which is based on a popular generative sequence-to-sequence model  in NLP, transformers . It leverages supervision to train the model to predict expansion terms conditional on each document. The paper has shown significant improvement on passage  datasets, when trained in-domain. In this paper, we follow this line of supervised neural document expansion approach and explore its performance on standard IR benchmarking dataset. Our main contributions are: 1. Adapting the method to unlabeled datasets by exploring transfer learning and weak-supervision approaches. 2. Adapting the method to traditional IR datasets, where a large number of long documents are present.    
","     Recently,  proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data.     In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.",434
"    Cognitive studies show that human infants develop object individuation skill from diverse sources of information: spatial-temporal information, object property information, and language~. Specifically, young infants develop object-based attention that disentangles the motion and location of objects from their visual appearance features. Later on, they can leverage the knowledge acquired through word learning to solve the problem of object individuation: words provide clues about object identity and type. The general picture from cognitive science is that object perception and language co-develop in support of one another .     Our long-term goal is to endow machines with similar abilities. In this paper, we focus on how language may support object segmentation. Recent work has studied the problem of unsupervised object representation learning, though without language. As an example, factorized, object-centric scene representations have been used in various kinds of prediction~, reasoning~, and planning tasks~, but they have not considered the role of language and how it may help object representation learning.  As a concrete example, consider the input images shown in \fig{fig:teaser} and the paired questions. From language, we can learn to associate concepts, such as {, with the referred object's visual appearance. Further, language provides cues about how an input scene should be segmented into individual objects: a wrong parsing of the input scene will lead to an incorrect answer to the question. We can learn from such failure that the handle belongs to the frying pan  and the chair has four legs .  Motivated by these observations, we propose a computational learning paradigm, \modelfull , associating learned object-centric representations to their visual appearance  in images, and to concepts---words for object properties such as color, shape, and material---as provided in language. Here the language input can be either descriptive sentences or question-answer pairs. \model requires no annotations on object masks, categories, or properties during the learning process.  In \model, four modules are jointly trained. The first is an image encoder, learning to encode an image into factorized, object-centric representations. The second is an image decoder, learning to reconstruct masks for individual objects from the learned representations by reconstructing the input. These two modules share the same formulation as recent unsupervised object segmentation research: learning to decompose the image into a series of { if it's a descriptive sentence.  The correctness of the executor's output and the quality of reconstructed images  are the two supervisory signals we use to jointly train Modules 1, 2, and 4.  %  % % %  We integrate the proposed \model with state-of-the-art unsupervised segmentation methods, MONet~ and Slot Attention~. The evaluation is based on two datasets: ShopVRB~ contains images of daily objects and question-answer pairs; PartNet~ contains images of furniture with hierarchical structure, supplemented by descriptive sentences we collected ourselves. We show that \model consistently improves existing methods on unsupervised object segmentation, % much more likely to group different parts of a single object into a single mask.  We further analyze the object-centric representations learned by \model. In \model, conceptually similar objects  appear to be clustered in the embedding space. Moreover, experiments demonstrate that the learned concepts can be used in new tasks, such as visual grounding of referring expressions, without any additional fine-tuning. %  %  %  % % % % % % % % %  % 
","     We present \modelfull , a paradigm for learning disentangled, object-centric scene representations from vision and language. \model builds upon recent advances in unsupervised object segmentation, notably MONet and Slot Attention. While these algorithms learn an object-centric representation just by reconstructing the input image, \model enables them to further learn to associate the learned representations to concepts, \ie, words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. \model can be integrated with various unsupervised segmentation algorithms that are language-agnostic. Experiments show that the integration of \model consistently improves the object segmentation performance of MONet and Slot Attention on two datasets via the help of language. We also show that concepts learned by \model, in conjunction with segmentation algorithms such as MONet, aid downstream tasks such as referring expression comprehension.",435
"  A speech signal can be considered as a variable-length temporal sequence, and many features have been used to characterize its pattern. Short-term spectral features are used extensively because of the quasi-stationary property of the speech signal. After short-term processing, the raw waveform is converted into a two-dimensional~ matrix of size , where  represents the frequential feature dimension related to the number of filter coefficients, and  denotes the temporal frame length related to the utterance duration.  For a text-independent speaker verification~ system, the main procedure is to extract the fixed-dimensional speaker representation from the variable-length spectral feature sequence. One of the widely used spectral features is the Mel-frequency cepstral coefficient ~. Typically, MFCC feature vectors from all the frames are assumed to be independent and identically distributed. They can be projected on the Gaussian components or phonetic units to accumulate statistics over the time axis and form a high-dimensional supervector. Then, a factor analysis-based dimension reduction is performed to generate a fixed-dimensional low rank i-vector representation. Recently, with the progress of deep learning, many approaches directly train a deep neural network~ to distinguish different speakers. Systems comprising of x-vector speaker embedding followed by a probabilistic linear discriminant analysis~ have shown state-of-the-art performances on multiple TISV tasks. In the x-vector system, a time-delay neural network~ followed by statistic pooling over the time axis is used for modeling the long-term temporal dependencies from the MFCC features.   {-0.3cm}          {0.cm}                                           For the i-vector, x-vector, and many other speech modeling methods, the feature matrix  is viewed as a multi-channel 1-D time series. Although the duration  may vary among the utterances, the feature dimension  must be a fixed value. In this paper, we consider the feature matrix as a single-channel 2-D image. From this new perspective, the spectral feature is viewed as a ``picture"" of the sound, and a 2-D CNN  is implemented in the same way as traditional image recognition paradigms. This kind of process brings a type of flexibility, i.e., the size of the input ``image,"" including the width  and the height , can be arbitrary numbers.  In other words, a 2-D CNN trained with a 64-dimensional spectrogram could potentially also process a spectrogram with 48 dimensions.   We aim to utilize the flexibility of the 2-D CNN to tackle the mixed-bandwidth~ joint modeling problem. Currently, there are many devices and equipment that capture speech data in different sampling rates, thus solving the sampling rate mismatch problem has become a research topic in the speech community. The traditional way to accomplish this goal is to train a specific model for every target bandwidth since the sampling rates are different . An alternative solution is to uniformly downsample the wideband~ speech data or extend the bandwidth of a narrowband~ data, so that they can be combined .  In this paper, we present a unified solution to solve the MB joint modeling problem. The key idea is to view the NB spectrogram as a sub-image of the WB spectrogram. The major contributions of this work are summarized as follows.  [noitemsep]       
"," This paper proposes a unified deep speaker embedding framework for modeling speech data with different sampling rates. Considering the narrowband spectrogram as a sub-image of the wideband spectrogram, we tackle the joint modeling problem of the mixed-bandwidth data in an image classification manner. From this perspective, we elaborate several mixed-bandwidth joint training strategies under different training and test data scenarios. The proposed systems are able to flexibly handle the mixed-bandwidth speech data in a single speaker embedding model without any additional downsampling, upsampling, bandwidth extension, or padding operations. We conduct extensive experimental studies on the VoxCeleb1 dataset. Furthermore, the effectiveness of the proposed approach is validated by the SITW and NIST SRE 2016 datasets.",436
"   % Automatic speech recognition  systems are typically trained on vast quantity of paired audio and text data to attain competitive performance. Obtaining these paired data requires substantial human annotation efforts and is often time-consuming, expensive and error-prone. With the emerging popularity of end-to-end ASR models, the need for large amounts of training data is more demanding than the conventional hybrid-based ASR systems. For this purpose, semi-supervised learning  is often investigated for speech recognition, where a model is trained using a finite amount of labeled data and a much larger amount of unlabeled data.   In the long history of semi-supervised learning  in speech recognition, self-training approach  and knowledge distillation , or known as teacher-student model training  are the two commonly used SSL methods. Recent success of representation learning enables a new approach towards leveraging unlabeled data. In natural language processing community,  BERT, ELMo, XLNet , GPT   and its follow-ups are classical examples of representation learning. The key philosophy of representation learning is based on using self-supervised learning, where we obtain `free' labels from unlabeled data and train them in a supervised manner via some proxy tasks. In the context of BERT, two proxy tasks are defined including masked language model task and two-sequence prediction task. These proxy tasks are designed to force the learning of a robust, meaningful representation.  After the representation has been learned, a downstream task model is then trained using labeled data with the learned representation. Optionally, the representation learning block and downstream task block can be fine-tuned together.   Learning efficient speech representation can be traced back to restricted Boltzmann machine , which allows pre-training on large amounts of unlabeled data before training the deep neural network speech models.  More recently, speech representation learning has drawn increasing attention in speech processing community and has shown promising results in semi-supervised speech recognition .  The design of proxy tasks in learning speech representation can be categorized into two types. The first type is based on contrastive loss and has been applied to speech representation such as wav2vec and its variants . The model is trained to learn representations containing information that most discriminates the future or masked frame from a set of negative samples via contrastive loss.  The second type is based on reconstructive loss. The proxy task for these representation learning methods is to reconstruct temporal slices of acoustic features based on contextual information. These reconstruction tasks can be defined as autoregressive reconstruction, or masked-based reconstruction. APC  and its follow-up  are examples to use autoregressive reconstruction loss.  In many state-of-the-art pretrained language model task, masked-based prediction is adopted in the proxy tasks such as BERT  and XLNet .  In speech, instead of prediction, we randomly mask temporal slices of acoustic features and attempt to reconstruct them .  Orthogonal to the contrastive-/reconstructive-loss based speech representation learning, vector-quantized speech representations have been proposed. One motivation to apply vector quantization  is that enforcing quantization can lead to better linguistic unit discovery  due to the discrete nature of phonetic units. In VQ-APC , the authors use VQ as a way to limit model capacity and control information needed in encoding representation. In VQ-wav2vec  and wav2vec 2.0 , the author use VQ to facilitate direct application of BERT and other NLP algorithms.  In this paper, we introduce DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We take inspirations from many recent advances in speech representation learning, and propose multiple improvements over vanilla DeCoAR. We summarize the contributions of this paper as follows: [leftmargin=*,itemsep=0pt, topsep=1pt]       % The rest of the paper is organized as follows. Section gives a brief overview of our previous DeCoAR method and related work in vector quantized speech representation learning. Section describes the proposed DeCoAR 2.0 approach. Experimental results on semi-supervised speech recognition are presented in Section followed by conclusion in Section.   % Learning robust speech representation has been exploited in recent years. Among these approaches, wav2vec 2.0  uses 10 minutes of labeled data with 53k hours of unlabeled data to achieve a word error rate  of 5.2/8.6 on LibriSpeech benchmark. The model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the contrastive loss formulation can result in several locally optimal codebooks, for exmaples, acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations.   %Furthermore, the codes at each time step the model select right after their feature encoder hardly contained meaningful phonetic information. So their contrastive approach might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.   % A simple workaround could be using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information in the codes, helping mitigatate the codebook learning problems in contrastive loss as discussed above. And compared to simple reconstruction where we utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. By utilizing the VQ layer, the model is able to keep the representation from those unwanted information flowing.     % Automatic speech recognition  systems are typically trained on vast quantity of paired audio and text data to attain competitive performance. Obtaining these paired data requires substantial human annotation efforts and is often time-consuming, expensive and error-prone. With the emerging popularity of end-to-end ASR models, the need for large amounts of training data is more demanding than the conventional hybrid-based ASR systems. For this purpose, semi-supervised learning  is often investigated for speech recognition, where a model is trained using a finite amount of labeled data and a much larger amount of unlabeled data.   % In the long history of SSL in speech recognition, self-training approach  is the most commonly used approach. In self-training methods, a `seed' ASR model is trained using paired audio/text data. The resulting model is then applied to transcribe the unlabeled audio data. The resulting hypotheses, combined with different data selection criteria, are treated as `pseudo-labels' and added to the original labeled dataset to retrain a new model. Simple in concept, self-training works well in practice with one major caveat - the pseudo-label injects systematic bias introduced by the seed model. To alleviate this, careful confidence calibration with system combinations are often used . Another family of SSL is based on knowledge distillation , or teacher-student model training , and is mostly applied to acoustic model training in hybrid-based ASR. In these setups, a teacher model  generates frame-wise soft label instead of hard label, and a student model is trained on the soft labels via KL divergence loss instead of a standard cross-entropy loss based on forced alignment. The knowledge distillation based SSL partially mitigates the systematic bias but is rarely being investigated towards sequence-level loss  or end-to-end ASR systems.    % Recent success of efficient representation learning, in particular in natural language processing , enables a new approach towards leveraging unlabeled data. Classical examples of representation learning for NLP include BERT, ELMo, XLNet , GPT and its follow-ups , to name but a few.  The key philosophy of representation learning is based on self-supervised learning, where we obtain `free' labels from unlabeled data and train them in a supervised manner via some proxy tasks. In the context of the well-known BERT, two proxy tasks are defined including masked language model task and two-sequence prediction task. These proxy tasks are defined in a way to force the learning of a robust, meaningful representation.  A downstream task is then trained on the labeled data with the learned representation. Optionally, the representation learning block and downstream task can be fine-tuned together.     % This paper presents DeCoAR 2.0, a follow-up on DeCoAR . We take inspiration from many recent advances in speech representation learning, and propose multiple improvements over vanilla DeCoAR. We summarize the contributions of this paper as follows: % [leftmargin=*,itemsep=0pt, topsep=1pt] %       % The rest of the paper is organized as follows. Section gives an overview on related work in speech representation learning, and a brief recap of our previous DeCoAR method. Section describes the proposed vector quantized DeCoAR approach. Experimental results on semi-supervised speech recognition are presented in Section followed by conclusion in Section.   % In this work, we propose an improved speech representation learning paradigms towards semi-supervised speech recognition based on our previous work .   % Current state-of-the-art models for speech recognition require vast amounts of transcribed audio data to attain good performance. In particular, end-to-end ASR models are more demanding in the amount of training data required when compared to traditional hybrid models. While obtaining a large amount of labeled data requires substantial effort and resources, it is much less costly to obtain abundant unlabeled data.   % For this reason, semi-supervised learning  is often used when training ASR systems. Recently, self-supervised learning閳ユ攣 paradigm that treats the input itself or modifications of the input as learning targets閳 has obtained promising results. Those self-supervised speech representation can be fall into main categories: Contrastive Predictive Coding  incorporates contrastive objective to learn representations containing information that  most discriminates the future or masked frame from a set of negative samples. Another approach is Autoregressive Predictive Coding  , which tries to directly predict or reconstruct the frame based on context.  % More recently, vector-quantized representations of audio data has drawn increasing attention in speech processing . The motivation is that enforcing the quantization leads to a better representation for acoustic unit discovery due to the discrete nature of phonetic units. VQ-APC  also try to exactly quantified of information , to control the capacity of the models. And the use of vector quantization limited capacity are forced to retain information to achieve maximal prediction.   % Despite the success of the wav2vec 2.0 model , the model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the codes at each time step the model select right after feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation can result in several locally optimal codebooks. A few highly probable optima observed were acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations to enable a good contrastive loss. Hence, the codebook learning methodology using contrastive loss might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.    % A simple solution could be to enforce the codes to explicitly carry information about the input features in the process. Using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information, helping mitigatate the codebook learning problems in contrastive loss as discussed above. Thus, we propose an novel self-supervised model that learns vector quantized deep transformer acoustic representations based on frames reconstruction. Since simple reconstruction utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. And by utilizing the VQ layer to limit those unwanted information flow into final representation, Vector Quantized Deep Contextualized Acoustic Representations  are able to achieve much better representation that's better suited for semi-supervised ASR tasks. By using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using deep transformer and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification  loss for the ASR task. We only train the small ASR model instead of fine-tuning for computing-efficiency. Our approach showed that supervision with 10 hours of labeled data on DeCoAR 2.0 achieves performance on par with training on all 960 hours directly.
","  Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR  and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.   % \yuzong{rewrite this} % We propose a novel approach for vector quantized deep contextualized acoustic representations. Following the same schema in DeCoAR, we first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from context frames. The new resulting deep contextualized acoustic vector quantized representations  are then used to train a small CTC-based ASR system using a small amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR 2.0 consistently outperform ones trained on other acoustic representations, giving the state-of-art and comparable results with wav2vec 2.0  on semi-supervised experiments on Librispeech. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 10 hours of labeled data achieves performance on par with training on all 960 hours directly.",437
" % % {A}{utomatic}  speech recognition , one of the core components in speech technology, has achieved significant advancements during the past decade . A key driving force behind these advancements is the rapid development of deep learning techniques .  % State-of-the-art  ASR systems    are usually trained with thousands of hours of transcribed speech data and a massive amount of text data. % % State-of-the-art  ASR systems    usually requires thousands of hours of transcribed speech data and a massive amount of text data to train a hybrid deep neural network-hidden Markov model  based acoustic model     and a recurrent neural network  language model . % Moreover, a hand-crafted pronunciation lexicon and a phoneme inventory based on linguistic expertise are often needed. Recently, end-to-end  ASR architectures, in which AM and LM training is integrated as a single pipeline, have gradually become the mainstream in ASR academic research , compared to   hybrid deep neural network-hidden Markov model  architectures . E2E architectures have the advantage of removing the need of a pronunciation lexicon and a phoneme inventory during system development. However, training an E2E ASR system tends to require even more transcribed speech data than for a hybrid DNN-HMM ASR system .  There are around  spoken languages in the world .  For most of them, the amount of transcribed speech data resources is very limited, or even non-existent . Many of these low-resource languages, such as ethnic minority languages in China and languages in Africa, may have never been formally studied. In addition to the lack of enough transcribed speech data, linguistic knowledge about such languages is incomplete, or may even be entirely lacking. Conventional supervised acoustic modeling  can therefore not be applied directly. This leads to the current situation that high-performance ASR systems are only available for a small number of major languages, e.g., English, Mandarin, French. To facilitate ASR technology for low-resource languages, investigation of unsupervised acoustic modeling  methods is necessary, which aims to find and model a set of basic speech units that represents all the sounds in the language of interest, i.e., the low-resource, target language.   Recently, there has been a growing research interest in UAM .  A strict assumption of UAM is that for the target language only raw speech data is available, while the transcriptions, phoneme inventory  and pronunciation lexicon are unknown. This is known as the zero-resource assumption .   %It is a challenging task, yet with significant research impact in a broad area of speech and language science and technology, e.g., query-by-example spoken term detection , text-to-speech without text , understanding the mechanisms underlying infant language acquisition , and the documentation  of endangered languages .  There are two main research strands in UAM. The first strand formulates the problem as discovering a finite set of phoneme-like speech units . This is often referred to as acoustic unit/model discovery  . The second strand formulates the problem as learning acoustic feature representations that can distinguish subword  units of the target language, and is robust to linguistically-irrelevant factors, such as speaker  . This is often referred to as unsupervised subword modeling . In essence, the second strand is focused on learning an intermediate representation towards the ultimate goal of UAM, while the first strand aims directly at the ultimate goal. These two strands are closely connected and can benefit from each other; for instance, a good subword-discriminative feature representation  % good feature representation that is discriminative to subword units and is robust to speaker variation  has been shown beneficial to AUD , while conversely,  discovered speech units with good consistency with true phonemes are helpful to % could provide phoneme-like pseudo transcriptions to assist the  learning   subword-discriminative acoustic feature representations .   This study addresses unsupervised subword modeling in UAM. Learning subword-discriminative feature representations in the zero-resource scenario has been shown to be a non-trivial task . The major difficulty is the separation of linguistic information   from non-linguistic information .   For instance, a speech sound such as [\ae]\footnote{International Phonetic Alphabet  symbol.} produced by different speakers  might be mistakenly modeled as different speech units .    There are many interesting attempts to unsupervised subword modeling . One typical research direction is to leverage purely unsupervised learning techniques. One method is the clustering of speech sounds that have acoustically similar patterns and that potentially correspond to the same subword units  , which results in phoneme-like pseudo transcriptions that can be used to facilitate subword-discriminative feature learning . % , e.g. cluster posteriorgrams  or DNN bottleneck features  .  Unsupervised and self-supervised representation learning algorithms are applied to learn, without using external supervision, speech features that retain the linguistic content in the original data while ignoring linguistically-irrelevant information, particularly speaker variation  .    A second research direction to unsupervised subword modeling is to exploit cross-lingual knowledge . Speech and text resources from out-of-domain  resource-rich languages have been shown beneficial to modeling subword units of in-domain low-resource languages. For instance,  used an OOD AM to extract cross-lingual bottleneck features , while  used an OOD ASR to generate cross-lingual phone labels. % by past studies .   % One idea is to utilize a pre-trained DNN AM from an OOD language to generate phoneme-discriminative representations of target speech, such as bottleneck features  . % The second idea would be to leverage an OOD ASR system to decode speech utterances in the target language and obtain cross-lingual phone labels as supervision for subsequent subword modeling  .  % These two ideas realize cross-lingual knowledge transfer at the AM level and phone label level respectively.  % Cross-lingual knowledge transfer can be done at AM level, i.e., an OOD pretrained AM used to generate  for speech of the  target language.  % It can also be done at  phone label level, i.e., an OOD ASR system decoding target speech utterances to generate phone labels as cross-lingual supervision .  %  This study adopts a two-stage learning framework which combines both research directions within the area of unsupervised subword modeling.  % The  high-level overview  of  the  proposed  framework  is  shown  in Fig. .  %, and  At the first stage, the front-end, a self-supervised representation learning model named autoregressive predictive coding      is trained. APC preserves phonetic  and speaker information from the original speech signal, but makes the two information types more separable . %This makes APC a suitable method for unsupervised subword modeling.   At the second stage, the back-end, a cross-lingual, OOD DNN model with a bottleneck layer  is trained using the APC pretrained features as the input features to create the missing  frame labels. % , as seen in Fig. .  %Frame labels required for DNN-BNF model training are not directly available due to the zero-resource assumption. In our framework, the labels are obtained using an OOD ASR system.  %By doing so, cross-lingual phonetic knowledge is exploited.  This system framework was proposed in our recent study ,  and showed state-of-the-art performances on the subword discriminability task on two databases in UAM: ZeroSpeech 2017  and Libri-light .   In this work, we expand and extend the work in . Specifically, we  compare the proposed approach to a supervised topline system that is trained on transcribed data of the target language;  compare the proposed approach with another cross-lingual knowledge transfer method  ; % investigate which of the AM-level or phone label-level knowledge transfer methods is more effective;   %  investigate the effects of the recently proposed APC model architectures in front-end pretraining in detail;   investigate the potential of our approach in relation to the amount of unlabeled training material by varying the data between  hours   and  hours, and compare the models' performance to the topline model. Throughout our experiments, English is chosen as the target low-resource language. Its phoneme inventory and transcriptions are assumed unavailable during system development. Dutch and Mandarin are chosen as the two OOD languages for which phoneme inventories and transcriptions are available.  Unsupervised subword modeling is typically evaluated using overall performance measures, such as ABX  , purity , normalized mutual information  . These metrics, however, do not provide insights on the approaches閳 ability of modeling individual phonemes or phoneme categories. As the ultimate goal beyond unsupervised subword modeling is to discover basic speech units that have a good consistency with the true phonemes of the target language, we, to the best of our knowledge for the first time in the literature, additionally present detailed analyses that explore the question of the effectiveness of the proposed approach to capturing phoneme and articulatory feature  information of the target language. % To answer this question The analyses are based on the standard ABX error rate evaluation , which we adapted for this work , and consist of two parts, i.e., an analysis at the phoneme level and at the AF level. The analyses are aimed at investigating what phoneme and AF information is  captured by the learned subword-discriminative feature representation, which can be used to guide future research to improve unsupervised subword modeling as well as AUD. Moreover, we correlate the phoneme-level ABX error rates and the quality of the cross-lingual phone labels which are used to train our back-end DNN-BNF model in order to study why the proposed approach performs differently in capturing different target phonemes' information, and how the performance is affected by the quality of cross-lingual phone labels.    %The analysis at the AF level is carried out as we are interested in the  extent to which the AF information in the target language can be learned by our subword-discriminative feature representation.  % AFs describe the target of the articulators in the vocal tract when pronouncing a specific phone . The use of AFs has been shown beneficial to low-resource ASR    and acoustic unit discovery .  % {do we need a introduction to AF?} % The AFs describe the movement of the tongue, lips and other organs to produce speech sounds. % {state why do we do this}  % The AF is a  compact and universal representation of speech, and is   more language-independent than the phoneme inventory representation.  % We are interested in to which extent is the AF information in the target language  learned in our subword-discriminative feature representation. %In the AF-level analysis, a new evaluation metric is proposed to measure the efficacy of our approach in capturing AF information. This metric replaces the phoneme inventory in the ABX discriminability task with the AF category.  % Specifically, the task is to predict whether a test speech segment  belongs to the same AF attribute as  or  as , where  and  contain speech sounds belonging to different AF attributes.  %Several AFs are investigated in this study, including place of articulation  and manner of articulation  for consonants, tongue height and tongue backness for monophthong vowels. %The AF-level analysis could potentially provide guidance on future research to improve unsupervised subword modeling as well as AUD. To our knowledge there are very few previous studies on AF-level analysis to unsupervised subword modeling and AUD.  % For instance, two systems achieving the same overall subword modeling performance might vary greatly in linguistic implications.   % overall performance metrics, such as ABX subword discriminability  , purity , normalized mutual information  .    % , or used as the input to perform further subword-discriminative learning .      % ,  i.e., the unsupervised feature representation learning problem.  % { high-level review representative approaches.  purely unsupervised learning approaches 1-1. clustering; 1-2 unsupervised feature learning.  leveraging OOD resources.} % {Text to be colored} % to train the deep neural network -based acoustic model and massive amount of text data to train the  %   The remainder of this paper is organized as follows. Section  provides a review of related works on the unsupervised subword modeling task. In Section , we provide a detailed description of the proposed approach to unsupervised subword modeling, and introduce comparative approaches to compare against our approach. Section  describes the methodology  used for the phoneme-level and AF-level analyses. Section   introduces the experimental design of this study, while Section  reports the results. Section  describes the setup for conducting the phoneme- and AF-level analyses, and discusses the results of the analyses. Finally, Section  draws the conclusions.  
"," % This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding  as the front-end and a cross-lingual deep neural network  as the back-end.  This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding  as the front-end and a cross-lingual deep neural network  as the back-end.  % Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases show our approach is competitive or superior to state-of-the-art studies. APC pretraining brings improvement to the entire framework, and brings larger improvement with increased amount of training data. Our best performance achieved by using unlabeled training data without linguistic knowledge of the target language is very close to that of a supervised system trained with labeled data of that language. The back-end of our approach is found more effective than a cross-lingual AM based BNF in cross-lingual knowledge transfer. Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases showed that our approach is competitive or superior to state-of-the-art studies.  % A comprehensive and systematic analysis at the phoneme- and articulatory feature - level is carried out to investigate the type of information that is captured by our learned feature representation. New metrics are proposed for the phoneme-level ABX subword discriminability task and attribute-level ABX AF task. The phoneme-level analysis showed that compared to MFCC, our approach achieves larger improvement in capturing diphthong information than monophthong vowel information, and the improvement varies greatly to different consonants. Results found there is a positive correlation between the effectiveness of the back-end in capturing a phoneme's information and the quality of cross-lingual phone labels assigned to that phoneme. The AF-level analysis showed that the proposed approach is better than MFCC and APC features in capturing manner of articulation , place of articulation , vowel height and backness information. Results indicate MoA is better captured by the proposed approach than PoA, and both MoA and PoA are better captured than vowel height and backness. Results implies AF information is less language-dependent than phoneme information.   Comprehensive and systematic analyses at the phoneme- and articulatory feature -level showed that our approach was better at capturing diphthong than monophthong vowel information, while also differences in the amount of information captured for different types of consonants were observed. Moreover, a positive correlation was found between the effectiveness of the back-end in capturing a phoneme's information and the quality of the cross-lingual phone labels assigned to the phoneme. The AF-level analysis together with t-SNE visualization results showed that the proposed approach is better than MFCC and APC features in capturing manner and place of articulation information, vowel height, and backness information.  % Taking all the analyses together, the two stages in our approach are both effective in capturing phoneme information. Monophthong vowel information is much more difficult to be captured than consonant information, which suggests a future research direction to improve the effectiveness of capturing monophthong vowel information.  Taken together, the analyses showed that the two stages in our approach are both effective in capturing phoneme and AF information. Nevertheless, monophthong vowel information is less well captured than consonant information, which suggests that future research should focus on improving capturing monophthong vowel information.",438
