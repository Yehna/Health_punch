document,summary,id
"   Imagine subjects are told that the words , , , and  are good, and the words , , , and  are bad. If they are then asked whether  and  are good or bad, most will immediately say that  is good and  is bad. Humans will immediately note that the difference between the two sets of words is that the two letters are identical in the good words, and different in the second. The fact that  and  do not appear in the training data does not prevent them from making this judgement.   However, many machine learning algorithms would not make this same inference given the training set. Depending on how inputs are provided to the algorithm and the training procedure used, the algorithm may conclude that since neither  nor  appears in the training data,  it is impossible to distinguish two inputs containing them.      The ability or inability of neural networks to generalize learning outside of the training set has been controversial for many years.  has made strong claims in support of the inability of neural networks and other algorithms that do not instantiate variables to truly learn identity effects and other algebraic rules. The explosion of interest in deep neural networks since that book has not truly changed the landscape of the disagreement; see  for a more recent discussion. %  Here we hope to shed some light on the controversy by considering a single instance of an algebraic rule, specifically an identity effect, and providing a rigorous framework in which the ability of an algorithm  to generalize it outside the training set can be studied.   In our framework, we consider mappings that transform the set of inputs, and consider whether particular learning algorithms are invariant to these transformations, in a sense which we will define. We show that if both the learning algorithm and the training set are invariant to a transformation, then the predictor learned by the learning algorithm is also invariant to the transformation, meaning that it will assess inputs before and after transformation as equally well formed.       We then show that a broad class of algorithms, including deep feedforward neural networks trained via backpropagation, satisfy our criteria . Finally, we show with computational experiments {how this dependence on encoding plays out in practice}. In our example we will see that one-hot encoding  leads to a learner that is unable to generalize outside the training set, whereas distributed encoding allows partial generalization outside the training set.   %The connectionist/symbolist dispute has been going on for a long time now.  Interestingly, the recent success of deep neural networks has not changed the landscape very much.  %Many of the arguments made for why connectionism cannot be a satisfactory model of human cognition can be found in the book  .   %These results tend to be of the qualitative variety. Though compelling they have not been formalized. Here we attempt to formalize them.   %Our framework is based on the idea of . A learning algorithm takes a set of data we call the training set. For example, the training set may consist of a set of words each with a well-formedness rating.      This work is a refinement and extension of earlier work . We have simplified the main theory and shown that it is applicable to a broader range of situations.       We see agreement between our theoretical predications and our numerical experiments for our identity effect test problem. Our theory predicted that when the encoded letters for different vectors are orthogonal , then since the transformation   is an orthogonal transformation, the learner will not be able to distinguish between the inputs  and . The theory has nothing to say about the case of the 3-bit active encoding, because in that case  is not orthogonal, and our theorems do not apply. Accordingly, in this case, even though the network is not able to give the correct answer of  for  and  for , and so not be said to learn the generalization perfectly, it does give a higher rating on average to   than to . We leave it to the reader to decide if this constitutes an exception to the claim that learners need to instantiate variables in order to generalize algebraic rules outside the training set .  Our results hew closely to those of ; see also . There the authors train a variable-free neural network to perform reduplication, the process where a linguistic element is repeated from the input to the output. Following the experimental work of , they trained the network on many examples of the pattern ABB, where A and B are substituted with syllables. The network is then tested by seeing if it can predict that the third syllable of a string such as ``li na '' should be ``na'', even when not exposed to this input before. The authors found that their network could perform partial generalization when the novel inputs included new syllables or new segments, but could not generalize to new feature values. The reason for this is that feature values were encoded in their model via a localist representation, and introducing a new feature value was like expecting the network to learn a function depending on a bit that was always set to zero in the training data, just like the localist representation in our set-up. Since novel segments were composed of multiple novel feature values, this corresponds to our 3-bit active encoding, where apparently learning can be extended imperfectly to new combinations of already seen segments.   continue a theme that is well known in connectionist literature: when representations on novel inputs overlap with representations in training data, networks are able to generalize training to novel inputs. See  for a discussion of this point in the context of identity effects.}      The authors were supported by NSERC Discovery Grants.     {}{}.  {Keras.} . \PrintBackRefs{\CurrentBib}   Ba  }{  Kingma  \ \BBA {} Ba  }{  {\protect \APACyear {2014}}  }]{  kingma2014adam} \APACinsertmetastar {  kingma2014adam}    \unskip\ {}{}. \APACrefatitle {Adam: a method for stochastic optimization.} {Adam: a   method for stochastic optimization.}{\BBCQ} {}{}{}. \PrintBackRefs{\CurrentBib}  {  G.~Marcus  }{  {\protect \APACyear {1999}}  }]{  marcus1999} \APACinsertmetastar {  marcus1999}    \unskip\ {}{}. \APACrefatitle {Do infants learn grammar with algebra or statistics?   {R}esponse} {Do infants learn grammar with algebra or statistics?   {R}esponse}.{\BBCQ} {284}{5413}{436--437}. \PrintBackRefs{\CurrentBib}   Davis  }{  G.~Marcus  \ \BBA {} Davis  }{  {\protect \APACyear {2019}}  }]{  marcus2019} \APACinsertmetastar {  marcus2019}    \unskip\ . : building artificial intelligence we can trust}   {Rebooting {AI}: building artificial intelligence we can trust}. {Pantheon}. \PrintBackRefs{\CurrentBib}  {  G\BPBI F.~Marcus  }{  {\protect \APACyear {2003}}  }]{  marcusbook} \APACinsertmetastar {  marcusbook}    \unskip\ .  {The algebraic mind: Integrating connectionism and cognitive   science}. {MIT press}. \PrintBackRefs{\CurrentBib}   Plaut  }{  McClelland  \ \BBA {} Plaut  }{  {\protect \APACyear {1999}}  }]{  mcclelland1999} \APACinsertmetastar {  mcclelland1999}    \unskip\ {}{}. \APACrefatitle {Does generalization in infant learning implicate   abstract algebra-like rules?} {Does generalization in infant learning   implicate abstract algebra-like rules?}{\BBCQ} {3}{5}{166--168}. \PrintBackRefs{\CurrentBib}  {  Mezzadri  }{  {\protect \APACyear {2007}}  }]{  mezzadri2006generate} \APACinsertmetastar {  mezzadri2006generate}    \unskip\ {}{}. \APACrefatitle {How to generate random matrices from the classical   compact groups} {How to generate random matrices from the classical compact   groups}.{\BBCQ} {54}{5}{592-604}. \PrintBackRefs{\CurrentBib}  \ \BBA {} Pater  }{  Prickett  \ \protect \BOthers {.}}{  {\protect \APACyear {2018}}  }]{  prickett2018seq2seq} \APACinsertmetastar {  prickett2018seq2seq}    \unskip\ {}{}. \APACrefatitle {{S}eq2{S}eq models with dropout can learn generalizable   reduplication} {{S}eq2{S}eq models with dropout can learn generalizable   reduplication}.{\BBCQ}  \APACrefbtitle {Proceedings of the 15th {W}orkshop on {C}omputational   {R}esearch in {P}honetics, {P}honology, and {M}orphology} {Proceedings of the   15th {W}orkshop on {C}omputational {R}esearch in {P}honetics, {P}honology,   and {M}orphology}\ . \PrintBackRefs{\CurrentBib}  \ \BBA {} Pater  }{  Prickett  \ \protect \BOthers {.}}{  {\protect \APACyear {2019}}  }]{  prickett2019learning} \APACinsertmetastar {  prickett2019learning}    \unskip\ {}{}. \APACrefatitle {Learning Reduplication with a Variable-Free Neural   Network} {Learning reduplication with a variable-free neural network}.{\BBCQ} }. \PrintBackRefs{\CurrentBib}   Shahriari  }{  Tupper  \ \BBA {} Shahriari  }{  {\protect \APACyear {2016}}  }]{  tupper2016} \APACinsertmetastar {  tupper2016}    \unskip\ {}{}. \APACrefatitle {Which Learning Algorithms Can Generalize Identity-Based   Rules to Novel Inputs?} {Which learning algorithms can generalize   identity-based rules to novel inputs?}{\BBCQ}  \APACrefbtitle {Proceedings of the 38th {A}nnual {C}onference of the   {C}ognitive {S}cience {S}ociety} {Proceedings of the 38th {A}nnual   {C}onference of the {C}ognitive {S}cience {S}ociety}\ . \PrintBackRefs{\CurrentBib}    
"," Often in language and other areas of cognition, whether two components of an object are identical or not determine whether it is well formed. We call such constraints identity effects. When developing a system to learn well-formedness from examples, it is easy enough to build in an identify effect. But can identity effects be learned from the data without explicit guidance? We provide a simple framework in which we can rigorously prove that {algorithms satisfying simple criteria} cannot make the correct inference. We then show that a broad class of algorithms including deep neural networks with standard architecture and training with backpropagation satisfy our criteria, . Finally, we demonstrate our theory with computational experiments {in which we explore the effect of different input encodings on the ability of algorithms to generalize to novel inputs}.     Keywords:  identity effects, machine learning, neural networks, generalization",0
" Task-oriented dialogue systems complete tasks for users, such as making a restaurant reservation or finding attractions to visit, in multi-turn dialogues . Dialogue policy is a critical component in both the conventional pipeline approach  and recent end-to-end approaches . It decides the next action that a dialogue system should take at each turn. Considering its nature of sequential decision making, dialogue policy is usually learned via reinforcement learning . Specifically, dialogue policy is learned by maximizing accumulated rewards over interactions with an environment . Handcrafted rewards are commonly used for policy learning in earlier work , which assigns a small negative penalty at each turn and a large positive/negative reward when the task is successful/failed. However, such reward setting does not provide sufficient supervision signals in each turn other than the last turn, which causes the sparse reward issues and may result in poorly learned policies .    [!tbp]       % \tiny % {L{1.05cm}|L{6.3cm}} \toprule % \multicolumn{2}{c}{System utterances} \\ \multirow{4}{*}{\makecell[l]{User\\Side}  } & Utterance \\  & I would like moderate price range please. \\   & Dialogue State annotation \\   & }    \\ \midrule \multirow{4}{*}{\makecell[l]{System\\Side}  } & Utterance \\  &  I found de luca cucina and riverside brasserie. does either of them sound good for you?  \\   & System action annotation \\   & } \\  % \footnotesize %      To address this problem, reward function learning that relies on  has been introduced . Specifically, state-action sequences generated by an optimal policy  are collected, and a reward function is learned to give high rewards to state-action pairs that better resemble the behaviors of the optimal policy. In this way, turn-by-turn rewards estimated by the reward function can be provided to learn dialogue policy. Obtaining expert demonstrations is critical to reward function learning. Since it is impractical to assume that an optimal policy is always available, a common and reasonable approach is to treat the decision makings in human-human dialogues as optimal behaviors. To accommodate the learning of reward function, human-human dialogues need to be annotated in the form of  from textual utterances. Table  illustrates an example of human-human dialogue and its state-action annotation. However, obtaining such annotations require extensive efforts and costs.  Besides, a reward function based on state-action pair might cause an unstable policy learning, especially with a limited amount of annotated dialogues .   To address the above issues, we propose to learn dialogue policies in a semi-supervised setting where the system action of expert demonstrations only need to be partially annotated. We propose to use an implicitly trained  as the reward function to replace the conventional reward function that is restricted to state-action pairs. Dynamics models describe sequential progress using a combination of stochastic and deterministic states in a latent space, which promotes an effective tracking and forecasting . In our scenario, we train the dynamics model to describe dialogue progress of expert demonstrations. The main rationale is that the reward function should give high rewards to actions that lead to dialogue progress similar to those in expert demonstrations. This is because dialogue progress at the early stage highly influences subsequent progress, and the latter directly determines whether the task can be completed. Since the learning of dynamics model maps observations to latent states and further reason over the latent states, we are no longer restricted to fully annotated dialogues. Using dynamics model as reward function also promotes a more stable policy learning.    Learning the dynamics model in the text space is, however, prone to compounding errors due to complexities and diversities of languages. We tackle this challenge by learning the dynamics model in an  space that encodes the effect of system utterances on dialogue progress. We achieve action embedding learning by incorporating an embedding function into a generative models framework for semi-supervised learning . We observe that system utterances with comparable effects on dialogue progress will lead to similar state transitions .  Therefore, we formulate the generative model to describe the state transition process. Using the generative model, we enrich the expert dialogues  with action embedding to learn the dynamics model.   Moreover, we also consider the scenarios where both state and action annotations are absent in most expert dialogues, referred to as unlabeled dialogues.  To expand the proposed approach to such scenarios, we further propose to model dialogue progress using action sequences and reformulate the generative model accordingly.     % Contribution bullet points Our contributions are summarized as follows: [topsep=0pt,leftmargin=*,noitemsep,wide=0pt]        We study the problem of semi-supervised policy learning and propose Act-VRNN to provide more effective and stable rewards estimations.  We formulate a generative model to jointly infer action labels and learn action embeddings. We design a novel reward function to first model dialogue progress, and estimate action rewards by determining whether the action leads to similar progress as expert dialogues. The experimental results confirm that Act-VRNN achieves better task completion compared with the state-of-the-art in two settings that consider partially labeled or unlabeled dialogues.   potential of policy learning for For future work, we will explore the scenarios that annotations are absent for all expert dialogues.      
"," Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems.  This is insufficient for training intermediate dialogue turns since supervision signals  are only provided at the end of dialogues. To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards.  This approach requires complete state-action annotations of human-to-human dialogues , which is labor intensive.  To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning. The proposed approach learns a dynamics model as the reward function which models dialogue progress  based on expert demonstrations, either with or without annotations. The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations. We further propose to learn action embeddings for a better generalization of the reward function. The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.",1
"  Comments of online articles provide a form of discussion and improve user's engagement. Automatic generation of article comments has a huge value in online forums and intelligent chatbots, etc. However, automatic generation of article comments is a new, challenging and not well-studied task in Natural Language Generation , it needs to understand the meaning of articles and generate multiple valuable comments.  Machine translation models  such as Sequence-to-sequence  with attention  tend to generate trival samples like 閳ユ泛emph{I am speechless}閳 or 閳ユ泛emph{I don't know}閳. In addition, for comment generation task, the models are hard to converge when we train models on articles with all comments about various topics. To generate pertinent comments, Lin et al.  select the most related comment in an article, then generate one article-comment pair for training model. However, the model discards most of the article-comment pairs and are hard to generate diversified comments.   In this paper, we focus on the generation of pertinent and diversified comments for news articles. Given a news article, we understand topics covered in the article and generate comments toward these topics. The idea is motivated by our observation on comment written by human, people often associate a news article with topics in their mind. For example, when reading a news article about 閳ユ泛emph{Chinese Basketball Association}閳, people may think the core leader 閳ユ泛emph{Yao Ming}閳. Based on this knowledge, they may give a pertinent comment like 閳ユ泛emph{we expect the institution reformation by Yao Ming}閳. We consider simulating the way people write comments with topics and propose a topic-aware network to introduce the topic information as prior knowledge in comment generation. The topic words trained by LDA  topic model and the keywords extracted by Textrank  are encoded to the topic information by our proposed encoder attention mechnism. Finally, we leverage the topic information to guide comment generation. % Then we use the topic information to obtain the relevant context information in article by our proposed encoder attention mechanism. used as the topic information in people's mind. Finally, we leverage the topic information and the relevant context information to guide comment generation.   % Firstly, the model respectively obtains the topic words and keywords of the news article using a pre-trained LDA  model and Textrank . The topic words and keywords are used as the topic information in people's mind. Then we use the topic information to obtain the relevant context information in article by keyword-level and topic-level encoder attention mechanism. Finally, we integrate the topic information and the relevant context information into pointer-generator networks to guide comment generation.  [t!]         In this work, we propose a guiding generation model for comment generation. Firstly, we design a word-level and topic-level encoder attention mechanism to capture topic information in the articles. Next, we integrate the topic information into Pointer-Generator Networks to guide pertinent and diversified comment generation. Experiments on a large scale of comments generation dataset show that our model produces the multiple high-quality comments and achieves the state-of-the-art performance. For future work, we plan to construct a hierarchical structure to learn article presentation and combine it with keyword information to generate a more related guiding information.   In this work, we propose a topic-aware generation model for comment generation. Firstly, we design a encoder attention mechanism to capture the topic information in the articles. Then, we leverage the topic information to guide comment generation. Experiments show that our model produces the pertinent and diversified comments and achieves the state-of-the-art performance.    Min: no longer used as of ACL 2018, following ACL exec's decision to   remove this extra workflow that was not executed much.   BEGIN: remove    
"," % \footnote{email: huangjunheng@baidu.com, panlu01@baidu.com} Comment generation, a new and challenging task in Natural Language Generation , attracts a lot of attention in recent years. However, comments generated by previous work tend to lack pertinence and diversity. In this paper, we propose a novel generation model based on Topic-aware Pointer-Generator Networks , which can utilize the topic information hidden in the articles to guide the generation of pertinent and diversified comments. Firstly, we design a keyword-level and topic-level encoder attention mechanism to capture topic information in the articles. Next, we integrate the topic information into pointer-generator networks to guide comment generation. Experiments on a large scale of comment generation dataset show that our model produces the valuable comments and outperforms competitive baseline models significantly.  % we synthesize topic information by a LDA model and an extractive model. Next, we integrate the topic information into pointer-generator networks to guide comment generation. Experiments on a large scale of comment generation dataset show that our model produces the multiple valuable comments and outperforms competitive baseline models significantly.   % Comment generation, a new and challenging task in Natural Language Generation , attracts a lot of attention in recent years. Comments generated by previous works tend to lack informativeness and diversity. In this work, we propose a novel generation model based on Topic-aware Pointer-Generator Networks , which focuses on constructing multiple topic-aware information in articles to guide informative and diversified comments generation. In detail, we design a method for building training dataset. Then, we propose a word-level and a topic-level encoder attention mechanisms to capture the topic-aware information in the article. Finally, we leverage the topic-aware information to guide comment generation. Experiments on a large scale comments dataset show that our model produces the multiple high-quality comments and achieves the state-of-the-art performance.",2
"  State-of-the-art machine learning approaches require huge amounts of training data. But for many NLP applications, there is little to no training data available. % yet only %at all.    are a viable solution to alleviate the cost of creating large training datasets before a new application can be used. Such systems start with no or few labeled instances and acquire additional training data based on user feedback for their predictions.   is a frequently used technique to quickly maximize the prediction performance, as the system acquires user feedback in each iteration for those instances that likely yield the highest performance improvement . Active learning has been shown to reduce the amount of user feedback required while improving system performance for interactive NLP systems  and to reduce the annotation costs in crowdsourcing scenarios . However, outside the typical annotation setup, it can be boring or frustrating for users to provide feedback on ill-predicted instances that hardly solve their needs. Consider a newly launched web application for learning a foreign language, which aims at suggesting exercises that match the user's proficiency according to Vygotsky's  . The underlying machine learning system starts without any data, but employs active learning to select an exercise the system cannot confidently predict.  Then, it adjusts its model interactively based on the user's feedback. While the system is still uncertain, the users often receive inappropriate   exercises. Thus, they get the impression that the system does not work properly, which is especially harmful during the inception phase of an application, as the community opinion largely defines its success.              In this paper, we distinguish the  of maximizing the prediction performance with minimal labeled instances and the  of providing useful instances for the user's current needs. For the first time, we propose an active learning approach that jointly optimizes these seemingly counteracting objectives and thus trades off the demands of system and user.  The users of educational applications can particularly benefit from this, as they can learn most if they receive appropriate learning material while the underlying system requires considerable training to reach acceptable performance. We employ our new approach in a language learning platform for C-tests . Our system successfully learns how to predict the difficulty of a C-test gap  and how to provide a C-test that is neither too easy  for the current user, which would cause boredom, nor too hard, which would create frustration . Predicting the difficulty of an exercise and correspondingly selecting exercises that match a user's proficiency are important steps towards self-directed language learning and massive open online courses  on language learning. Though we focus on this educational use case in this paper, our approach may also yield new insights for other problems that suffer from seemingly counteracting system and user objectives, for example, interactively trained recommender systems for books, movies, or restaurants.     In this work, we investigated how we can incorporate user feedback into existing active learning approaches without hurting the user's actual needs. We formalize both system  and user objectives and propose two novel sampling strategies which aim to maximize both objectives jointly. We evaluate our sampling strategies for the task of selecting suited C-tests, a type of fill-the-gap exercise, which fit the current proficiency of a human learner. We create simulated learners for five different proficiency levels from real-world data and use them to define different learning behaviors. Our experiments show that both our novel sampling strategies are successfully selecting instances which lead to a better model training while not hurting a learner's progress by selecting too easy or too difficult C-tests. Although system and user objective at first seem counteracting, our experiments indicate that they complement each other as jointly optimizing them outperforms optimizing only one of the goals. Additional experiments with an adaptive  for our trade-off sampling strategy show that properly balancing system and user objective can lead to considerable improvements in performance for both objectives.  Our findings open up new opportunities for training models on low-resource scenarios with implicitly collected user feedback while jointly serving the user's actual needs. Additional use cases like the training of personalized recommendation models as well as the use of reinforcement learning to find a good trade-off between system and user objective remain to be investigated in future work.  
"," Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system  and the user .  We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.\footnote{Our code and simulated learner models are available on Github: \url{https://github.com/UKPLab/acl2020-empowering-active-learning}}",3
" Image captioning  aims at generating a natural language description of an image.  Recent methods typically follow the  encoder/decoder paradigm where a convolutional neural network  encodes the input image,  and a sequence decoder,  decoders that require sequential execution: % they generate each word conditioned on the sequence of previously generated words.  However, this process is not parallelizable and thus results in high inference latency,  which is sometimes unaffordable for real-time industrial applications.   Recently, { .  The counterfactual baseline of an agent is the expected reward when marginalizing out a single agent閳ユ獨 action, while keeping the other agents閳 actions fixed.  As a result, only actions from an agent that outperform the counterfactual baseline are given positive weight,  and inferior actions are suppressed.  CMAL fully exploits the distinctive features of the multi-agent NAIC system: extremely short episode and large action space.    To further boost captioning performance, we propose to utilize massive unlabeled images as additional data for training,  which could be more easily obtained without costly human annotations.  We evaluate the proposed method on the challenging MSCOCO  image captioning benchmark.  Experimental results show that our method brings  decoding speedup relative to the autoregressive counterpart,  while achieving comparable performance to state-of-the-art autoregressive models.    To summarize, the main contributions of this paper are three-fold:    	     We have proposed a non-autoregressive image captioning model and a novel counterfactuals-critical multi-agent learning algorithm.  The decoding inconsistency problem in non-autoregressive models  is well addressed by the combined effect of the cooperative agents, sentence-level team-reward, and agent-specific counterfactual baseline.  The caption quality is further boosted by using unlabeled images.   Results on MSCOCO image captioning benchmark show that our non-autoregressive model can achieve a performance comparable to state-of-the-art autoregressive counterparts,  while at the same time enjoy  inference speedup.    This work was supported by Beijing Natural Science Foundation  and National Natural Science Foundation of China .   
"," 	Most image captioning models are autoregressive, \ie they generate each word by conditioning on previously generated words, which leads to heavy latency during inference. Recently, non-autoregressive decoding has been proposed in machine translation to speed up the inference time by generating all words in parallel. Typically, these models use the word-level cross-entropy loss to optimize each word independently. However, such a learning process fails to consider the sentence-level consistency, thus resulting in inferior generation quality of these non-autoregressive models. In this paper, we propose a Non-Autoregressive Image Captioning  model with a novel training paradigm: Counterfactuals-critical Multi-Agent Learning . CMAL formulates NAIC as a multi-agent reinforcement learning system where positions in the target sequence are viewed as agents that learn to cooperatively maximize a sentence-level reward. Besides, we propose to utilize massive unlabeled images to boost captioning performance. Extensive experiments on MSCOCO image captioning benchmark show that our NAIC model achieves a performance comparable to state-of-the-art autoregressive models, while brings $13.9\times$ decoding speedup.",4
" Researchers from Cognitive Science have established a long time ago that humans learn better and more effectively in an incrementive learning setting . Tasks like playing a piano or solving an equation, are learnt by humans in a strategy where they are first provided easier variants of the main challenge, followed by gradual variation in difficulty. This idea of incremental human learning has been studied for machines as well, specifically in machine learning. Curriculum Learning  as defined by  introduce and formulate this concept from Cognitive Science to a machine learning setting. They observe that on shape recognition problem , training the model first on a synthetically created dataset with less variability in shape, generalizes faster as compared to directly training on the target dataset. Furthermore, other experiments by  demonstrate performance improvements on a perceptron classifier when incremental learning is done based on the margin in support vector machines  and a language model task where  growth in vocabulary size was chosen as the curriculum strategy. These examples indicate that while curriculum learning is effective, the choice of the curriculum strategy, the basis for ordering of samples is not clear cut and often task specific. Furthermore some recent works like  have suggested that anti curriculum strategies perform better, raising more doubts over choice of strategy. In recent years Self-Paced Learning   has been proposed as a reformulation of curriculum learning by modeling the curriculum strategy and the main task in a single optimization problem.  \\ Sentiment Analysis  is a major challenge in Natural Language Processing. It involves classifying text segments into two or more polarities or sentiments. Prior to the success of Deep Learning , text classification was dealt using lexicon based features. However sentiment level information is realized at more levels than just lexicon or word based. For a model to realize a negative sentiment for ``not good'', it has to incorporate sequential information as well. Since the advent of DL, the field has been revolutionized. Long Short Term Memory  , Convolutional Neural Networks   and Attention based architectures  have achieved state-of-art results in text classification and continue to be strong baselines for text classification and by extension, Sentiment Analysis. Sentiment Analysis, further aids other domains of NLP such as Opinion mining and Emoji Prediction. \\ Curriculum Learning has been explored in the domain of Computer Vision  extensively  and has gained traction in Natural Language Processing  in tasks like Question Answering , Natural Answer Generation  and Domain Adaptation. In Sentiment Analysis,  propose a strategy derived from sentence length, where smaller sentences are considered easier and are provided first.  provide a tree-structured curriculum based on semantic similarity between new samples and samples already trained on.  suggest a curriculum based on hand crafted semantic, linguistic, syntactic features for word representation learning. However, these CL strategies pose the easiness or difficulty of a sample irrespective of sentiment. While their strategies are for sentiment analysis, they do not utilize sentiment level information directly in building the order of samples. Utilizing SentiWordNet, we can build strategies that are derived from sentiment level knowledge.  SentiWordNet  is a highly popular word level sentiment annotation resource. It has been used in sentiment analysis and related fields such as opinion mining and emotion recognition. This resource was first created for English and due to its success it has been extended to many other languages as well . This lexical resource assigns positivity, negativity and derived from the two, an objectivity score to each WordNet synset . The contributions of the paper can be summarized as follow:          %IF RESOURC SCARC SETTING Given that we have less data for to train the model,  states that with a smaller sample size, a network may not discover the generalization that characterizes the larger population. Curriculum Learning , solves it by providing the right amount of data at the right time. The network learns in phases where it is trained iteratively on each grade of difficulty. At the end of each training phase, the network is provided with a test set comprising of all of the  pairs from each grade. The network is also tested on a separate test of only that grade as well. The neural network learns basic generalizations from the easiest grade presented it to initially due to simpler sentences being present in the early phases. It the learning progresses, the weights are more stable and are less sensitive to small changes which leads to a more stable behaviour.  % %IF LINGUISTIC SETTING - This becomes important for deep learning. neural networks which are trained on a very large number of samples can start with random haywire weights initially but they still learn the task as the large number of samples allows them to distinguish between linguistic features and noise however, for tasks with a limited dataset such as those of resource scare languages, the network has a limited set of samples to learn the correct. using curriculum learning allows the network to initially learn simple sentences which contain less noise the basic linguistic features. in the later phase, the network is trained on gradually increasing complex samples which include more noise than the previously simpler sentences but the initial weights allows the network to learn only the important features.     While Curriculum Learning as defined by  is not constrained by a strict description, later related works  make distinction between Baby Steps and One-Pass methods for enabling curriculum learning. In our experiments we observe better performance with Baby Steps over One Pass hence we use the same for proposed SentiWordNet driven strategy.  also show that Baby Steps outperforms One Pass. For every sentence  \footnote{Our dataset has 5 labels.}, where    is then added to the new training set or     \footnote{Adding one sample at a time can be a very slow process, hence we add in batches. For our experiments, we take a batch size of  samples with lowest  to add at once.} and the process continues until training is done on all the sentences in . The process starts with first training on a small subset of , which have least  score. In this way incremental learning is done in Baby Steps.   In this paper, we define a SentiWordNet driven strategy for curriculum learning on sentiment analysis task. The proposed approach's performance is evident on multiple architectures, namely recurrent, convolution and attention based proving the robustness of the strategy. This approach also shows the effectiveness of simple lexicon based annotations such as SentiWordNet and how they can be used to further sentiment analysis. Future works could include strategies that consecutively enrich SentiWordNet as well and also those that can refine the resource by pointing out anomalies in the annotation. 
"," Curriculum Learning  is the idea that learning on a training set sequenced or ordered in a manner where samples range from easy to difficult, results in an increment in performance over otherwise random ordering. The idea parallels cognitive science's theory of how human brains learn, and that learning a difficult task can be made easier by phrasing it as a sequence of easy to difficult tasks. This idea has gained a lot of traction in machine learning and image processing for a while and recently in Natural Language Processing . In this paper, we apply the ideas of curriculum learning, driven by SentiWordNet in a sentiment analysis setting. In this setting, given a text segment, our aim is to extract its sentiment or polarity. SentiWordNet is a lexical resource with sentiment polarity annotations. By comparing performance with other curriculum strategies and with no curriculum, the effectiveness of the proposed strategy is presented. Convolutional, Recurrence, and Attention-based architectures are employed to assess this improvement. The models are evaluated on a standard sentiment dataset, Stanford Sentiment Treebank. These authors have contributed equally to this work}",5
" Recent work has demonstrated the efficacy of multilingual neural machine translation  on improving the translation quality of low-resource languages  as well as zero-shot translation . The success of multilingual NMT on low-resource languages relies heavily on transfer learning from high-resource languages for which copious amounts of parallel data is easily accessible. However, existing multilingual NMT approaches often do not effectively utilize the abundance of monolingual data, especially in low-resource languages. On the other end of the spectrum, self-supervised learning methods, consuming only monolingual data, have achieved great success on transfer learning  and unsupervised NMT  without fully benefiting from the rich learning signals offered by the bilingual data of multiple languages.   In this work, we propose to combine the beneficial effects of multilingual NMT with the self-supervision from monolingual data. Compared with multilingual models trained without any monolingual data, our approach shows consistent improvements in the translation quality of all languages, with greater than 10 BLEU points improvements on certain low-resource languages. We further demonstrate improvements in zero-shot translation, where our method has almost on-par quality with pivoting-based approaches, without using any alignment or adversarial losses. The most interesting aspect of this work, however, is that we introduce a path towards effectively adding new unseen languages to a multilingual NMT model, showing strong translation quality on several language pairs by leveraging only monolingual data with self-supervised learning, without the need for any parallel data for the new languages.     [t] {!}{ {c|ccccccccccccccc} \toprule xx                  & cs   & fr   & ru   & zh   & es   & fi   & de   & et   & lv   & lt   & ro   & hi  & kk  & tr   & gu  \\ \midrule Any-to-English  & 31.3 & 37.2 & 36.0 & 21.7 & 32.7 & 27.3 & 31.7 & 23.1 & 15.0 & 21.3 & 30.1 & 8.5 & 11.5 & 15.9 & 1.0 \\ English-to-Any  & 23.8 & 41.3 & 26.4 & 31.3 & 31.1 & 18.1 & 29.9 & 18.2 & 14.2 & 11.5 & 23.4 & 4.5 & 1.9  & 13.6 & 0.6 \\ }         We present a simple framework to combine multilingual NMT with self-supervised learning, in an effort to jointly exploit the learning signals from multilingual parallel data and monolingual data. We demonstrate that combining multilingual NMT with monolingual data and self-supervision  improves the translation quality for both low and high-resource languages in a multilingual setting,  leads to on-par zero-shot capability compared with competitive bridging-based approaches and  is an effective way to extend multilingual models to new unseen languages.  Future work should explore techniques like iterative back-translation  for further improvement and scaling to larger model capacities and more languages  to maximize transfer across languages and across data sources.                       
"," Over the last few years two promising research directions in low-resource neural machine translation  have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results:  Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models.  Self-supervision improves zero-shot translation quality in multilingual models.  Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on WMT ro-en translation without any parallel data or back-translation.",6
" In video captioning, the task is to generate a natural language description capturing the content of a video. Recently, dense video captioning has emerged as an important task in this field, where systems first generate a list of temporal event segments from a video, then decode a coherent paragraph  description from the generated segments.  simplifies this task as generating a coherent paragraph from a provided list of segments, removing the requirements for generating the event segments, and focusing on decoding better paragraph captions from the segments. As noted by , generating paragraph descriptions for videos can be very challenging due to the difficulties of having relevant, less redundant, as well as coherent generated sentences.   Towards this goal,  proposed a variant of the LSTM network that generates a new sentence conditioned on previously generated sentences by passing the LSTM hidden states throughout the entire decoding process.   further augmented the above LSTM caption generator with a set of three discriminators that score generated sentences based on defined metrics, i.e., relevance, linguistic diversity, and inter-sentence coherence. Though different, both these methods use LSTMs as the language decoder.   Recently, transformers have proven to be more effective than RNNs , demonstrating superior performance in many sequential modeling tasks.   first introduced the transformer model to the video paragraph captioning task, with a transformer captioning module decoding natural language sentences from encoded video segment representations.  This transformer captioning model is essentially the same as the original transformer for machine translation, except that it takes a video representation rather than a source sentence representation as its encoder input. However, in such design, each video segment caption is decoded individually without knowing the context , thus often leading to inconsistent and redundant sentences w.r.t. previously generated sentences .  recognize this problem as context fragmentation in the task of language modeling, where the transformers are operating on separated fixed-length segments, without any information flow across segments. Therefore, to generate more coherent video paragraphs, it is imperative to build a model that can span over multiple video segments and capture longer range dependencies.  Hence, in this work, we propose the Memory-Augmented Recurrent Transformer  model , a transformer-based model that uses a shared encoder-decoder architecture augmented with an external memory module to enable the modeling of the previous history of video segments and  sentences.  Compared to the vanilla transformer video paragraph captioning model, our first architecture change is the unified encoder-decoder design, i.e., the encoder and decoder in MART use shared transformer layers rather than separated as in~. This unified encoder-decoder design is inspired by recent transformer language models to prevent overfitting and reduce memory usage.  Additionally, the memory module works as a memory updater that updates its memory state using both the current inputs and previous memory state. The memory state can be interpreted as a container of the highly summarized video segments and caption history information. At the encoding stage, the current video segment representation is enhanced with the memory state from the previous step using cross-attention. Hence, when generating a new sentence, MART is aware of the previous contextual information and can generate paragraph captions with higher coherence and lower repetition.   Transformer-XL is a recently proposed transformer language model that also uses recurrence, and is able to resolve context fragmentation for language modeling.  Different from MART that uses a highly-summarized memory to remember history information, Transformer-XL directly uses hidden states from previous segments.  We modify the Transformer-XL framework for video paragraph captioning and present it as an additional comparison.  We benchmark MART on two standard datasets: ActivityNet Captions and YouCookII.  Both automatic evaluation and human evaluation show that MART generates more satisfying results than previous LSTM-based approaches and transformer-based approaches. In particular, MART can generate more coherent , less redundant paragraphs without losing paragraph accuracy .   In this work, we present a new approach -- Memory-Augmented Recurrent Transformer  for video paragraph captioning, where we designed an auxiliary memory module to enable recurrence in transformers. Experimental results on two standard datasets show that MART has better overall performance than the baseline methods.  In particular, MART can generate more coherent, less redundant paragraphs without any degradation in relevance.   
"," Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph.  Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer , which uses a memory module to augment the transformer architecture.  The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence , thus encouraging coherent paragraph generation.  Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.\footnote{All code is available open-source at \url{https://github.com/jayleicn/recurrent-transformer}}",7
" The abstract of a research paper is a short, succinct description of the content of the paper. In a few lines, it conveys the information that is subsequently revealed in detail over multiple pages supplemented with figures,  tables, and references to existing works. In biomedical literature, including research papers, review articles, and clinical practice guidelines, it is a common practice to have structured abstracts . They typically follow the IMRaD format, i.e., INTRODUCTION, METHODS, RESULTS, and  DISCUSSION.  Most medical journals indexed in PubMed conform to this style . Structured abstracts can help researchers to refer to their regions of interest quickly, label documents more effectively, assist the indexing process, and help in data mining . Recently researchers have designed deep network models to automatically segment unstructured abstracts in PubMed leveraging the large volume of structured abstracts already available .  However, structured abstracts are uncommon in other disciplines like computer science, although, arguably, the same benefits may be reaped if structured abstracts were available .    \definecolor{backgG}{RGB}{255, 255, 153} \definecolor{tagtxtG}{RGB}{102, 102, 0} \definecolor{backgPc}{RGB}{179, 255, 179} \definecolor{tagtxtPc}{RGB}{0, 102, 0} \definecolor{backgPw}{RGB}{255, 179, 179} \definecolor{tagtxtPw}{RGB}{102, 0, 0}  % } % }   [!htb] 	|} 		 \\  \\ 		Thanks to the wide adoption of personal mobile devices, it is possible to blend digital and face-to-face interaction and integrate co-located social media applications in the classroom. \goldentag{BACKGROUND} \\ 		To better understand how such applications can interweave digital and f2f interaction, we performed a detailed analysis of real-world use cases of a particular co-located social media app: SpeakUp.\goldentag{TECHNIQUE} \\ 		In a nutshell, SpeakUp allows the creation of temporary location-bound chat rooms that are accessible by nearby users who can post and rate messages anonymously. \goldentag{TECHNIQUE} \wrongtag{OBSERVATION}\\ 		We find that the use of co-located social media is associated with an increase in content-related interaction in the class.  \goldentag{OBSERVATION} \\ 		Furthermore, it is associated with an increase in the perceived learning outcomes of students compared to a control group.  \goldentag{OBSERVATION} \\ 		We further provide design guidelines to blend digital and f2f interaction using co-located social media in the classroom based on 11 case studies covering over 2,000 students. \goldentag{OBSERVATION} \\  and predicted  labels.} 	  %      In this paper, we investigate if sentences in an abstract can be labeled with discourse categories using machine learning methods even if the labeled data is sparse. We take abstracts in computer science  as a case study.  We categorize sentences in a CS abstract into three classes: BACKGROUND, TECHNIQUE, and OBSERVATION.  We adopt a deep learning model for sequential sentence classification pretrained on structured abstracts from PubMed. We prepare a small corpus of hand-labeled abstracts in CS. We fine-tune the model on a subset of the corpus and test it on the remainder. Fig.  shows an abstract from IEEE Transactions on Learning Technologies in which each sentence is labeled with one of the three classes. Both hand-annotated golden labels and the predictions done by the model are indicated.  We observe an accuracy of  on our test corpus, which is quite promising, given the limited amount of golden data. In brief, our contributions are     	 The code and the datasets are available publicly\footnote{https://github.com/soumyaxyz/abstractAnalysis}.  The rest of the paper is structured as follows.  Section  describes the discourse categories for CS abstracts and the rationale behind choosing this structure, Section  introduces the datasets, Section  describes the machine learning model, Section   evaluates of the model, and finally Section  concludes the paper.      In this paper, we proposed a method for automatic discourse classification of sentences in computer science abstracts. We demonstrated that transfer learning with fine-tuning can provide remarkable results even on a sparsely labeled dataset.  We observed that due to the difference in presentation style, the nature of the discourse classification of CS abstracts vary across sub-fields. Nevertheless, the results on cs.combined demonstrate that the proposed model generalizes fairly well across sub-fields of CS.               The next two lines define the bibliography style to be used, and    the bibliography file. 
"," The abstract of a scientific paper distills the contents of the paper into a short paragraph. In the biomedical literature, it is customary to structure an abstract into discourse categories like BACKGROUND, OBJECTIVE, METHOD, RESULT, and CONCLUSION, but this segmentation is uncommon in other fields like computer science. Explicit categories could be helpful for more granular, that is, discourse-level search and recommendation. The sparsity of labeled data makes it challenging to construct supervised machine learning solutions for automatic discourse-level segmentation of abstracts in non-bio domains. In this paper, we address this problem using transfer learning. In particular, we define three discourse categories -- BACKGROUND, TECHNIQUE, OBSERVATION -- for an abstract because these three categories are the most common. We train a deep neural network on structured abstracts from PubMed, then fine-tune it on a small hand-labeled corpus of computer science papers. We observe an accuracy of $75\%$ on the test corpus. We perform an ablation study to highlight the roles of the different parts of the model. Our method appears to be a promising solution to the automatic segmentation of abstracts, where the labeled data is sparse.",8
"  Recent developments in ANNs have drastically improved various tasks in natural language processing. However, these developments are based on a large amount of annotated data, which are not available with respect to many minority languages and for real language-learning children. Accordingly, unsupervised learning of languages based on raw speech data is required for industrial and academic purposes. This study reports our exploration of TTS without T in the Zero Resource Speech Challenge 2020, which intended to develop an end-to-end, unsupervised system that can learn speech recognition and TTS together. Even though the participants in the previous challenge mainly investigated mechanically sophisticated systems , we addressed the challenge using biologically/psychologically motivated ANN modules, by considering the unsupervised learning of human language as a biological/psychological problem. One of the adopted modules is our original discrete VAE that implements the Dirichlet-based Bayesian clustering within the end-to-end system.    This study explored the TTS-without-T task using biologically/psychologically motivated modules of neural networks: the ESN for the auditory module, the ABCD-VAE for the symbolic module, and the neural source-filter model for the articulatory module. Our technical contribution is the ABCD-VAE. It implemented the Dirichlet-based clustering in neural networks and made the end-to-end system possible. Specifically, it automatically detects the statistically optimal number of frame categories  and enables more non-parametric learning than other discrete VAEs .  The ABCD-VAE yielded linguistically informative representation in the posteriorgrams, but the MAP representation missed some of this information. This failure is likely not because of the limited capacity of the ESN encoder, which did not have any learnable parameters, since the canonical CNN-based encoder, adopted in previous work, yielded similar scores. Similar problems were reported in last year's Zero Resource Speech Challenge; most of the proposed discrete representations---particularly those with low bitrates---exhibited higher ABX error rates than the baseline, indicating a general difficulty in unsupervised learning of such representations .  A bigger problem is found in the articulatory module---failure to learn the F0 feature used in the source submodule. The naive learning of F0 improved this particular feature, but degraded the discrete representation of the ABCD-VAE and did not make the synthesized voice robustly more natural.  Thus, successful training of this articulatory module---in the end-to-end setting---will be the central issue for the next Zero Resource Speech Challenge in our framework.   We gratefully acknowledge the support of MEXT Grant-in-aid for Scientific Research on Innovative Areas \#4903  and the JST Core Research for Evolutional Science and Technology 17941861 .            Generated by IEEEtran.bst, version: 1.13     
","     In this study, we reported our exploration of Text-To-Speech without Text  in the Zero Resource Speech Challenge 2020, in which participants proposed an end-to-end, unsupervised system that learned speech recognition and TTS together.     We addressed the challenge using biologically/psychologically motivated modules of Artificial Neural Networks , with a particular interest in unsupervised learning of human language as a biological/psychological problem.     The system first processes Mel Frequency Cepstral Coefficient  frames with an Echo-State Network , and simulates computations in cortical microcircuits.     The outcome is discretized by our original Variational Autoencoder  that implements the Dirichlet-based Bayesian clustering widely accepted in computational linguistics and cognitive science.     The discretized signal is then reverted into sound waveform via a neural-network implementation of the source-filter model for speech production.",9
"  By aligning entities from different knowledge graphs  to the same real-world identity,  is a powerful technique for knowledge integration. Unfortunately, entity alignment is non-trivial because real-life KGs are often incomplete and different KGs typically have heterogeneous schemas. Consequently, equivalent entities from two KGs could have distinct surface forms or dissimilar neighborhood structures.     In recent years, embedding-based methods have become the dominated approach for entity alignment~. Such approaches have the advantage of not relying on manually constructed features or rules~. Using a set of seed alignments, an embedding-based method models the KG structures to  automatically learn how to map the equivalent entities among different KGs into a unified vector space where entity alignment can be performed by measuring the distance between the embeddings of two entities.   The vast majority of prior works in this direction build upon an important assumption - entities and their counterparts from other KGs have similar neighborhood structures, and therefore, similar embeddings will be generated for equivalent entities. Unfortunately, the assumption does not always hold for real-life scenarios due to the incompleteness and heterogeneities of KGs. As an example, consider Figure~, which shows two equivalent entities from the Chinese and English versions of Wikipedia. Here, both central entities refer to the same real-world identity, , a borough of New York City. However, the two entities have different sizes of neighborhoods and distinct topological structures. The problem of dissimilar neighborhoods between equivalent entities is ubiquitous.  reports that the majority of equivalent entity pairs have different neighbors in the benchmark datasets DBP15K, and the proportions of such entity pairs are over 86\%  in different language versions of DBP15K. Particularly, we find that the alignment accuracy of existing embedding-based methods decreases significantly as the gap of equivalent entities' neighborhood sizes increases. For instance, RDGCN , a state-of-the-art, delivers an accuracy of 59\% on the Hits@1 score on entity pairs whose number of neighbors differs by no more than 10 on DBP15K. However, its performance drops to 42\% when the difference for the number of neighbors increases to 20 and to 35\% when the difference increases to be above 30. The disparity of the neighborhood size and topological structures pose a significant challenge for entity alignment methods.   Even if we were able to set aside the difference in the neighborhood size, we still have another issue. Since most of the common neighbors would be popular entities, they will be neighbors of many other entities. As a result, it is still challenging to align such entities. To elaborate on this point, let us now consider Figure~. Here, the two central entities  have similar sizes of neighborhoods and three common neighbors. However, the three common neighbors  are not discriminative enough. This is because there are many  entities for England which also have the three entities in their neighborhoods -- e.g., the entity . For such entity pairs, in addition to common neighbors, other informative neighbors -- like those closely contextually related to the central entities -- must be considered. Because existing embedding-based methods are unable to choose the right neighbors, we need a better approach.   We present Neighborhood Matching Network , a novel sampling-based entity alignment framework. NMN aims to capture the most informative neighbors and accurately estimate the similarities of neighborhoods between entities in different KGs. NMN achieves these by leveraging the recent development in Graph Neural Networks . It first utilizes the Graph Convolutional Networks   to model the topological connection information, and then selectively samples each entity's neighborhood, aiming at retaining the most informative neighbors towards entity alignment. One of the key challenges here is how to accurately estimate the similarity of any two entities' sampled neighborhood. NMN addresses this challenge by designing a discriminative neighbor matching module to jointly compute the neighbor differences between the sampled subgraph pairs through a cross-graph attention mechanism. Note that we mainly focus on the neighbor relevance in the neighborhood sampling and matching modules, while the neighbor connections are modeled by GCNs. We show that, by integrating the neighbor connection information and the neighbor relevance information, NMN can effectively align entities from real-world KGs with neighborhood heterogeneity.  We evaluate NMN by applying it to benchmark datasets DBP15K  and DWY100K , and a sparse variant of DBP15K.  Experimental results show that NMN achieves the best and more robust performance over state-of-the-arts. This paper makes the following technical contributions. It is the first to:      We have presented NMN, a novel embedded-based framework for entity alignment. NMN tackles the ubiquitous neighborhood heterogeneity in KGs. We achieve this by using a new sampling-based approach to choose the most informative neighbors for each entity. As a departure from prior works, NMN simultaneously estimates the similarity of two entities, by considering both topological structure and neighborhood similarity. We perform extensive experiments on real-world datasets and compare NMN against 12 recent embedded-based methods.  Experimental results show that NMN achieves the best and more robust performance,   consistently outperforming competitive methods across datasets and evaluation metrics.   
"," Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. This paper presents Neighborhood Matching Network , a novel entity alignment framework for tackling the structural heterogeneity challenge. NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference. It provides two innovative components for better learning representations for entity alignment. It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity. It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair. Such strategies allow NMN to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task. Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods.",10
" Few-shot text classification, which requires models to perform classification with a limited number of training instances, is important for many applications but yet remains to be a challenging task.  Early studies on few-shot learning  employ data augmentation and regularization techniques to alleviate overfitting caused by data sparseness.  More recent research leverages meta-learning  to extract transferable knowledge among meta-tasks in meta episodes.   A key challenge for few-shot text classification is inducing class-level representation from support sets , in which key information is often lost when switching between meta-tasks. Recent solutions  leverage a memory component to maintain models' learning experience, e.g., by finding from a supervised stage the content that is similar to the unseen classes, leading to the state-of-the-art performance. However, the memory weights are static during inference and the capability of the model is still limited when adapted to new classes. Another prominent challenge is the instance-level diversity caused by various reasons , resulting in the difficulty of finding a fixed prototype for a class~. Recent research has shown that models can benefit from query-aware methods .   %\xd{[mark!!]}   %In the few-shot scenario, exploiting the memory to find some similar concepts to the unseen class will help the model induce and generalize better.  %A state-of-the-art model a memory block to find some similar concepts to the unseen class helps the model induce and generalize better .  However, current memory mechanism is static and the generalization ability is limited by the learnt memory parameters.    %It has been proved that query-aware  attention method is effective for this problem. Instead, we apply the unified dynamic memory routing module to enhance the induction procedure with query information.  %Another challenge in the existing few-shot text classification models is the instance-level diversity . Recent evidence has shown that query-aware  attention method is effective for this problem.   %Instead, we apply the unified dynamic memory routing module to enhance the induction procedure with query information.   %In this work, following the spirit of dynamic routing  method, we expand the memory reading process with a query guided routing method and propose a dynamic memory routing algorithm. Our method gives rise to flexibility in prior memory models since it can adapt to given inputs even after training, and can automatically adjust the coupling coefficients according to the inputs, which is more suitable for the few-shot learning task.  In this paper we propose Dynamic Memory Induction Networks  to further tackle the above challenges. DMIN utilizes dynamic routing  to render more flexibility to memory-based few-shot learning  in order to better adapt the support sets, by leveraging the routing component's capacity in automatically adjusting the coupling coefficients during and after training. Based on that, we further develop induction models with query information to identify, among diverse instances in support sets, the sample vectors that are more relevant to the query. These two modules are jointly learned in DMIN.   %, motivated by both  and . But unlike the former, where dynamic routing is used for constructing class-level representation from support sets, here we render flexibility in memory models to enable it to adapt to unseen inputs even after training. Unlike the latter, our model can automatically adjust the coupling coefficients in according to the inputs, which we expect to be more suitable for the few-shot learning task. Based on that, DMIN expands previous induction models with the supervised learning weights and query information, which effectively enhances the generalization ability of typical meta-learning approaches. The two modules are jointly learned in two memory-based dynamic routing module and the end-to-end fashion.    %In this paper we propose Dynamic Memory Induction Networks , attempting to further tackle the above challenges. We develop dynamic memory, motivated by both  and . But unlike the former, where dynamic routing is used for constructing class-level representation from support sets, here we render flexibility in memory models to enable it to adapt to unseen inputs even after training. Unlike the latter, our model can automatically adjust the coupling coefficients in according to the inputs, which we expect to be more suitable for the few-shot learning task. Based on that, DMIN expands previous induction models with the supervised learning weights and query information, which effectively enhances the generalization ability of typical meta-learning approaches. The two modules are jointly learned in two memory-based dynamic routing module and the end-to-end fashion.   %\xd{confirm with ruiying.}  The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets, improving the best performance by 24 accuracy. We perform detailed analysis to further show how the proposed network achieves the improvement.    %and the class-level representations are induced with the dynamic memory module and query information.   %We present new state-of-the-art performance that outperforms the existing models by 24 improvement on two benchmark datasets used in existing work.   %which is trained from a supervised learning stage to a meta-learning stage. It maintains an external memory module abstracting the previous learning experience. With the proposed dynamic memory routing method, we first reconstruct the sample representations by interacting with the memory module. Then the class-level representations are induced by a consistent procedure with the query information. The final classification is performed, as in the few-shot scenario, by finding the nearest class for an encoded query text.   % \item stress the performance.    We propose Dynamic Memory Induction Networks  for few-shot text classification, which builds on external working memory with dynamic routing, leveraging the latter to track previous learning experience and the former to adapt and generalize better to support sets and hence to unseen classes. The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets. Since dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning, we will investigate this type of models in other learning problems.  
"," This paper proposes Dynamic Memory Induction Networks  for few-shot text classification. The model utilizes dynamic routing to provide more flexibility to memory-based few-shot learning in order to better adapt the support sets, which is a critical capacity of few-shot classification models. Based on that, we further develop  induction models with query information, aiming to enhance the generalization ability of meta-learning. The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC dataset, improving the best performance  by 2$\sim$4$\%$. Detailed analysis is further performed to show the effectiveness of each component.",11
" %\whycomments{ %1. introducing QA and NQ. %2. Existing approaches and their drawbacks. %3. Our approaches. 1) GNN - document - hierarchical structure representation. 2) joint training short answer extraction and long answer extraction.} Machine reading comprehension , a task that aims to answer questions based on a given document,  has been substantially advanced by recently released datasets and models . %\whycomments{many citations here}.  %Especially the models based on BERT, which pre-trains language representations with bidirectional encoder representations from transformers. Natural Questions , a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets and questions that are from real user queries. Besides, unlike conventional MRC tasks , in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure shows an example from NQ dataset.  %The long answer is selected in the corresponding long answer candidate set of example. Both long and short answer can be empty if there is no such answer at all.    %Document modeling is one of the foundations in Machine Reading Comprehension. A reasonable modeling strategy will introduce strong inductive bias and enhance the document representation.   %  builds a pipeline model using two separate models: the Decomposable Attention model  to select a long answer, and the Document Reader model  to extract the short answer. %  proposes a BERT-based model that directly predicts the short answer span on the whole document and then selects the predicted long answer that contains the short answer.  % However, we believe that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results. % According to , a valid long answer must contain all of the information required to answer the question. Besides, a valid short answer should be helpful to confirm the long answer.   % For instance, when humans try to find the two-grained answers in the given Wikipedia page in Figure, they will firstly try to retrieve paragraphs  describing the entity bowling hall of fame, then try to confirm if the location  of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer.  % This suggests that two-grained answers can provide evidence for each other.  % However, the existing approaches for NQ fail to jointly model this multi-grained information.  % Existing approaches on NQ treat the long and short answer extraction as two individual sub-tasks during training and fail to model this multi-grained characteristic of this benchmark.  builds a pipeline model using two separate models: the Decomposable Attention model  to select a long answer, and the Document Reader model  to extract the short answer from the selected long answer. %  proposes a BERT-based model that directly predicts the short answer span on the whole document and then selects the predicted long answer that contains the short answer. However, we argue that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results.   Existing approaches on NQ have obtained promising results. For example,  builds a pipeline model using two separate models: the Decomposable Attention model  to select a long answer, and the Document Reader model  to extract the short answer from the selected long answer. Despite the effectiveness of these approaches, they treat the long and short answer extraction as two individual sub-tasks during training and fail to model this multi-grained characteristic of this benchmark, while we argue that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results.   According to , a valid long answer must contain all of the information required to answer the question. Besides, an accurate short answer should be helpful to confirm the long answer.   For instance, when humans try to find the two-grained answers in the given Wikipedia page in Figure, they will first try to retrieve paragraphs  describing the entity bowling hall of fame, then try to confirm if the location  of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer.  In this way, the two-grained answers can provide evidence for each other.    % According to , a valid long answer must contain all of the information required to answer the question. Besides, an accurate short answer should be helpful to confirm the long answer.   % For instance, when humans try to find the two-grained answers in the given Wikipedia page in Figure, they will first try to retrieve paragraphs  describing the entity bowling hall of fame, then try to confirm if the location  of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer.  % In this way, the two-grained answers can provide evidence for each other.   % However, existing approaches for NQ fail to jointly model this multi-grained information. %  builds a pipeline model using two separate models: the Decomposable Attention model  to select a long answer, and the Document Reader model  to extract the short answer from the selected long answer. %  proposes a BERT-based model that directly predicts the short answer span on the whole document and then selects the predicted long answer that contains the depeshort answer.  % Nevertheless, we argue that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results.  % \yqy{You'd better describe the advantage here or explain why we should consider these two subtasks simultaneously.}  To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs ,  %we propose a multi-grained MRC model based on graph attention networks  and BERT , we propose to use graph attention networks  and BERT ,  directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents.  % \yqy{Why graph attention network? Should you describe the advantage and necessary of using graph nn first?} In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote each other.  % \yqy{You mentioned you use four granularities representations but only use two here, where are the left two?} % The structural features are modeled by corresponding relational embeddings between nodes in the graph. At inference time, we use a pipeline strategy similar to , where we first select long answers and then extract short answers from the selected long answers.  Experiments on NQ dataset show that our model significantly outperforms previous models at both long and short answer criteria. We also analyze the benefits of multi-granularity representations derived from the graph module in experiments. % At the time of writing this paper, % our model achieves the state-of-the-art performance on both short and long answer leaderboard of NQ.  To summarize, the main contributions of this work are as follows:        We will release our code and models at \url{https://github.com/DancingSoul/NQ_BERT-DM}.       In this work, we present a novel multi-grained MRC framework based on graph attention networks and BERT. We model documents at different levels of granularity to learn the hierarchical nature of the document. On the Natural Questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the two-grained answers. The experiments show that our proposed methods are effective and outperform the previously existing methods by a large margin.  Improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals. Besides, the currently existing methods actually cannot process a long document without truncating or slicing it into fragments. How to model long documents is still a problem that needs to be solved.  
"," Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer  and a short answer .  Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens. We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously. The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively. In this way, we can model the dependencies between the two-grained answers to provide evidence for each other. We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.  % At the time of submission , our ensemble model has achieved the state-of-the-art results on both long answer and short answer leaderboard of Natural Questions, which are F1 scores of 74.5\% and 58.7\% respectively.  % Our ensemble model also surpasses single human performance on the development dataset at both long and short answer criteria.  % We present a novel multi-grained machine reading comprehension framework that focuses on modeling long documents at different levels of granularities: documents, paragraphs, sentences, and tokens. % % Motivated by strong representation learning ability of graph neural networks,  % We utilize graph attention networks on top of BERT to jointly capture different levels of information and structural features of the document. % %for multi-granularity answers so that different levels of information can be considered. % To verify our method, we conduct our experiment on Natural Questions, a new challenging machine reading comprehension benchmark for its long evidence documents and two-level answers. % % We jointly train long answer selection and short answer extraction. % % At inference, a pipeline framework, which first select a long answer % % and then extract short answer within this long answer is applied.\whycomments{reconsideration}. % We jointly train the two sub-tasks and our experiment shows that our approach significantly outperforms baseline systems by a large margin. % At the time of submission , our ensemble model has achieved the state-of-the-art results on both long answer and short answer leaderboard of Natural Questions, which are F1 scores of 74.5\% and 58.7\% respectively. Our ensemble model also surpasses single human performance on the development dataset at both long and short answer criteria. % % To the best of our knowledge, our model is the first model that surpasses single human performance on the development dataset at both long and short answer criteria.",12
"  When neural networks first started being applied to natural language in the 1980s and 90s, they represented a radical departure from standard practice in computational linguistics.  Connectionists had vector representations and learning algorithms, and they didn't see any need for anything else.  Everything was a point in a vector space, and everything about the nature of language could be learned from data.  On the other hand, most computational linguists had linguistic theories and the poverty-of-the-stimulus argument.  Obviously some things were learned from data, but all the interesting things about the nature of language had to be innate.  A quarter century later, we can say two things with certainty: they were both wrong.  Vector-space representations and machine learning algorithms are much more powerful than was thought.  Much of the linguistic knowledge which computational linguists assumed needed to be innate can in fact be learned from data.  But the unbounded discrete structured representations they used have not been replaced by vector-space representations.  Instead, the successful uses of neural networks in computational linguistics have replaced specific pieces of computational-linguistic models with new neural network architectures which bring together continuous vector spaces with structured representations in ways which are novel for both machine learning and computational linguistics.  Thus, the great progress which we have made through the application of neural networks to natural language processing should not be viewed as a conquest, but as a compromise.  As well as the unquestionable impact of machine learning research on NLP, the nature of language has had a profound impact on progress in machine learning.  In this paper we trace this impact, and speculate on future progress and its limits.  We start with a sketch of the insights from grammar formalisms about the nature of language, with their multiple levels, structured representations and rules.  The rules were soon learned with statistical methods, followed by the use of neural networks to replace symbols with induced vectors, but the most effective models still kept structured representations, such as syntactic trees.  More recently, attention-based models have replaced hand-coded structures with induced structures.  The resulting models represent language with multiple levels of structured representations, much as has always been done.  Given this perspective, we identify remaining challenges in learning language from data, and its possible limitations.           We conclude that the nature of language has influenced the design of deep learning architectures in fundamental ways.  Vector space representations  are not adequate, nor are vector spaces which evolve over time .  Attention-based models are fundamentally different because they use bag-of-vector representations.  BoV representations are nonparametric representations, in that the number of vectors in the bag can grow arbitrarily large, and these vectors are exchangeable.  With BoV representations, attention-based neural network models like Transformer can model the kinds of unbounded structured representations that computational linguists have found to be necessary to capture the generalisations in natural language. And deep learning allows many aspects of these structured representations to be learned from data.  However, successful deep learning architectures for natural language currently still have many hand-coded aspects.  The levels of representation are hand-coded, based on linguistic theory or available resources.  Often deep learning models only address one level at a time, whereas a full model would involve levels ranging from the perceptual input to logical reasoning.  Even within a given level, the set of entities is a pre-defined function of the text.  This analysis suggests that an important next step in deep learning architectures for natural language understanding will be the induction of entities.  It is not clear what advances in deep learning methods will be necessary to improve over our current fixed entity definitions, nor whether the resulting entities will be any different from the ones postulated by linguistic theory.  If we can induce the entities at a given level, a more challenging task will be the induction of the levels themselves.  The presumably-innate nature of linguistic levels suggests that this might not even be possible.  But of one thing we can be certain: the immense success of adapting deep learning architectures to fit with our computational-linguistic understanding of the nature of language will doubtless continue, with greater insights for both natural language processing and machine learning.    
"," In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures.  We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model.  This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.",13
" Attention-based neural networks, such as Transformer , and pre-training methods, such as BERT , are established state-of-the-art methods for sequence modelling and translation tasks. However, these methods and their applications typically focus on European languages, with little focus towards African languages. Community driven efforts such as Masakhane have since been established, and have made great strides towards benchmarking neural machine translation  tasks for all African languages.  In this paper, we provide a benchmark BLEU score for translation tasks between English and the ten remaining official South African languages. We train ten Transformer models on the JW300 parallel corpus  and provide BLEU scores on a derived testing subset. We then compare our model to a previously generated benchmark  using the Autshumato evaluation set .     We have presented a benchmark NMT score between English and all remaining official South African languages using the JW300 dataset for training and testing. We also tested our models on the Autshumato evaluation set and found that our models do not perform as well as models that were trained within the domain of the Autshumato training data. This low performance could be the result of two cases: our models failing to generalise outside of its training domain or; the fact that the Autshumato test set proposes many hypothetical translations for a single English source sentence, leading to a low BLEU score. Future work shall investigate both cases.             \printbibliography       
"," Recent advances in neural machine translation  have led to state-of-the-art results for many European-based translation tasks. However, despite these advances, there is has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa.",14
" % Algorithmic bias, as embedded in search and recommendation systems, has the capacity to profoundly influence society. For instance, recommendation systems targeting employment-related advertisements were found to demonstrate gender bias . The gendering of personal assistant technologies as female is also being questioned as constituting indirect discrimination, potentially contravening international women`s rights law .  With the rise in the use of facial recognition in areas such as border control, along with the issues with variance in accuracy depending on gender and race , there is a risk that bias will be incorporated directly into the core public infrastructure of a country. Even legal systems are vulnerable to the influence of algorithmic bias through the use of systems such as , where recommendations around parole lengths have demonstrated evidence of racial bias .    %%Language generation is a rapidly growing field with algorithms being used to write articles for publications such as Forbes, The Washington Post, and Bloomberg News. OpenAI recently developed GPT-2\footnote{https://openai.com/blog/better-language-models/}, that generated news articles deemed which appeared to be so realistic that it was considered unethical to release the full version of the software. The data it was trained upon comprised of 8 million web pages sourced from links posted on Reddit which had received more than three `likes'. Given that the user base of Reddit is predominantly young and male, and that the platform is known to host openly misogynistic groups , the composition of the training set, and therefore the model itself, could be open to accusations of biased towards a male worldview.   The source of this kind of bias often lies in the way societal inequalities and latent discriminatory attitudes are captured in the data from which algorithms learn.  Given the ways in which sentiments regarding race and gender ideology can be deeply embedded in natural language, uncovering and preventing bias in systems trained on such unstructured text can be particularly difficult. This paper focuses on algorithmic gender bias, and proposes a framework whereby language based data may be systematically evaluated to assess levels of gender bias prevalent in training data for machine learning systems. The framework is developed by accessing potential bias prevalent in articles in a popular UK mainstream media outlet, The Guardian, over a decade from 2009 to 2018. This is contrasted with biases uncovered in a corpus of 16,426 digitised volumes of 19th-century fiction from the British Library. This paper demonstrates how bridging AI and research in gender and language can provide a framework for potentially gender-proofing AI, and contributes to ongoing work on the systematic mitigation of algorithmic gender bias.     % % %{|l|l|l|} %\hline %Heading level &  Example & Font size and style\\ %\hline %Title  &  {\Large & %12 point, bold\\ %2nd-level heading & { %Text follows & 10 point, bold\\ %4th-level heading & { %    The findings of this research demonstrate how methods from machine learning, used within a framework informed by feminist linguistics and gender theory, can be used to evaluate levels of gender bias within natural language training corpora. A corpus of 19th century fiction along with a contemporary data set comprising every article published online in The Guardian newspaper over the decade between 2009 and 2018 was examined. The methods developed in this research uncovered gendered patterns in the corpus of 19th-century fiction that reflected Victorian concepts of gender while analysis of The Guardian uncovered linguistic patterns that capture contemporary concepts of gender. The emergence of feminist discourse in the media is also evident through gendered associations captured in word embedding uncovering an intriguing finding concerning how critiques of gender stereotypes could in fact generate stereotypical associations in neural embedding model. The systematic approach for capturing gender bias outlined in this paper is scalable and may be applied to a broad range of corpora, presenting new pathways for automatically assessing levels of bias in training corpora for search and information extraction systems.  \vskip 2em 
"," Algorithmic bias has the capacity to amplify and perpetuate societal bias, and presents profound ethical implications for society. Gender bias in algorithms has been identified in the context of employment advertising and recruitment tools, due to their reliance on underlying language processing and recommendation algorithms. Attempts to address such issues have involved testing learned associations, integrating concepts of fairness to machine learning, and performing more rigorous analysis of training data. Mitigating bias when algorithms are trained on textual data is particularly challenging given the complex way gender ideology is embedded in language. This paper proposes a framework for the identification of gender bias in training data for machine learning. The work draws upon gender theory and sociolinguistics to systematically indicate levels of bias in textual training data and associated neural word embedding models, thus highlighting pathways for both removing bias from training data and critically assessing its impact in the context of search and recommender systems.   %historical data that conveys biases in terms of imbalances and inequalities. These hidden biases are unfortunately captured in the learned patterns, and often emphasized in the results these algorithms provide to users. When a bias affects a sensitive attribute of a user, such as their gender or religion, the inequalities that are reinforced by search and recommendation algorithms even lead to severe societal consequences, like users discrimination.  %taken from into %Machine learning has the capacity to profoundly influence gender equality in society. Increasingly, everyday applications are incorporating machine learning technologies that categorise individuals and form generalisations from large collections of data. If the data explicitly or inadvertently captures existing inequalities or a world-view that is misaligned with societal goals, then the resulting applications may perpetuate gender bias .",15
"   Sequence labeling systems are generally trained on clean text, although in real-world scenarios, they often follow an error-prone upstream component, such as Optical Character Recognition  or Automatic Speech Recognition .  Sequence labeling is also often performed on user-generated text, which may contain spelling mistakes or typos~. Errors introduced in an upstream task are propagated downstream, diminishing the performance of the end-to-end system~. % While humans can easily cope with typos, misspellings, and the complete omission of letters when reading~, most Natural Language Processing  systems fail when processing corrupted or noisy text~. Although this problem is not new to NLP, only a few works addressed it explicitly~. Other methods must rely on the noise that occurs naturally in the training data.    In this work, we are concerned with the performance difference of sequence labeling performed on clean and noisy input. { illustrates the problem and our approach.  Our contributions are as follows:  %  algorithm  that directly induces noise in the input data to perform training of the neural model using a mixture of noisy and clean samples. %  method , adapted to the sequence labeling scenario, which explicitly addresses the noisy input data problem by encouraging the model to produce a noise-invariant latent representation . %  and misspellings against state-of-the-art baseline models~ and demonstrate the effectiveness of our approach . % }\fi.        In this paper, we investigated the difference in accuracy between sequence labeling performed on clean and noisy text .    We formulated the noisy sequence labeling problem  and introduced a model that can be used to estimate the real noise distribution . We developed the noise induction procedure that simulates the real noisy input . We proposed two noise-aware training methods that boost sequence labeling accuracy on the perturbed text:      Our experiments confirmed that NAT consistently improved efficiency of popular sequence labeling models on data perturbed with different error distributions, preserving accuracy on the original input . Moreover, we avoided expensive re-training of embeddings on noisy data sources by employing existing text representations. We conclude that NAT makes existing models applicable beyond the idealized scenarios. It may support an automatic correction method that uses recognized entity types to narrow the list of feasible correction candidates. Another application is data anonymization~.  Future work will involve improvements in the proposed noise model to study the importance of fidelity to real-world error patterns. Moreover, we plan to evaluate NAT on other real noise distributions  and other sequence labeling tasks to support our claims further.     ========= ACKNOWLEDGMENT ===========  
"," Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs---as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and  propose two Noise-Aware Training  objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community.",16
" Data annotation is time-consuming, expensive, and often requires experts or at least a good understanding of the data to reach a qualitative annotation.  Scalable, fast, and cheap methods exist, such as crowdsourcing solutions, like Amazon Mechanical Turk . However, using AMT or third parties is impossible when data privacy matters and must require internal annotation. This is especially true in specific domains such as banking, insurance, or medical. Nevertheless, there are more and more available services with user interfaces, like spoken dialog systems. These systems must be trained on large datasets in order to achieve acceptable interactions with users. They rely on Speech or Natural Language Understanding  . In addition to user intents, annotations can include slot and optional domain information, making multi-task annotation complex, time-consuming, and repetitive.  Dialog systems can collect massive amounts of user data, but this data can rarely be used directly and very often is impossible to annotate. One way to take advantage of these amounts of data is to use semi-supervised approaches . These methods have yielded consistently reliable results in text classification  for datasets like IMDB, Rotten Tomatoes, DBpedia, or RC1. However, these studies usually focus on large labeled datasets.  % Active Learning On the other hand, Active Learning aims to minimize the number of manual annotations required to reach an acceptable performance level. Active learners query unlabeled samples to be annotated . These queries are designed to extract informative or diverse samples. Some popular methods are uncertainty-based queries. Furthermore, many studies  show that random query selection is a robust and consistent baseline. Usually, efficient methods are too complex  to be easily integrated into the annotation process, making it time-consuming and expensive to be deployed effectively. Additionally, these methods that sample data to be annotated do not contribute directly to the training.  Among all these methods, Virtual Adversarial Training  , a semi-supervised approach, proves its robustness in a large variety of contexts like computer vision, text classification, or speech . VAT regularizes the training by adding adversarial examples and avoids overfitting. It could be interesting to evaluate its impact on NLU in a low data regime. Nevertheless, recent advances in NLU  rely on joint optimization of intent detection and slot filling, while VAT has been designed for single tasks and mainly evaluated on large datasets. Besides, if VAT can correctly regularize the model, even in a low data regime, it will then be possible to obtain better confidence in the model posterior distribution, which would therefore make it possible to query unlabeled samples better using the improved model confidence.  This would occur without using additional complex Active Learning methods and would increase the NLU model generalization with fewer data while reducing the effort of human annotation by querying more informative data.  This paper tries to answer these questions by evaluating the effectiveness of VAT in low data regimes in Spoken Language Understanding and its impact on the posterior distribution in an Active Learning paradigm. This framework is called VirAAL for Virtual Adversarial Active Learning. VirAAL combines VAT with uncertainty-based Active Learning in an attempt to increase sample efficiency even more.  Section  presents VirAAL and its components and methods for NLU: a joint-NLU model, an adapted Virtual Adversarial Training and the Active Learning procedure. Section  shows the protocol, experiments and results for low data regimes, Active Learning and a comparison with existing data augmentation methods. The paper ends with a conclusion and some perspectives.    This work first demonstrates that Virtual Adversarial Training is a consistent method for training NLU models: intent detection, slot filling and joint training. This is a very effective way to reduce the amount of annotations to obtain accurate NLU components. Experiments show that computing a common adversarial noise from multiple loss functions allows to effectively regularize the model even if it was not originally designed for this purpose. The proposed Virtual Adversarial Active Learning framework, VirAAL, shows even better improvement using entropy-based Active Learning combined with VAT. This is an inexpensive method in terms of computation for efficiently querying samples to annotate, thanks to the smoothness of the posterior distribution. Additionally, VirAAL leads to further improvements especially for both intent detection and slot filling and can reduce the labeling effort by up to 80\ . VirAAL demonstrates a capacity to deal with low data regimes. When compared with other data augmentation methods, VirAAL outperforms them in most cases, even yielding improvements over augmentation using a large transformer pre-trained language model. Finally, VirAAL results on joint training suggest that querying samples from a common latent space could improve entropy-based Active Learning rather than combining entropies.  This could be a future research direction. 
"," This paper presents VirAAL, an Active Learning framework based on Adversarial Training. VirAAL aims to reduce the effort of annotation in Natural Language Understanding . VirAAL is based on Virtual Adversarial Training , a semi-supervised approach that regularizes the model through Local Distributional Smoothness.  With that, adversarial perturbations are added to the inputs making the posterior distribution more consistent.  Therefore, entropy-based Active Learning becomes robust by querying more informative samples without requiring additional components.  The first set of experiments studies the impact of an adapted VAT for joint-NLU tasks within low labeled data regimes.  The second set shows the effect of VirAAL in an Active Learning  process.  Results demonstrate that VAT is robust even on multi-task training, where the adversarial noise is computed from multiple loss functions. Substantial improvements are observed with entropy-based AL with VirAAL for querying data to annotate. VirAAL is an inexpensive method in terms of AL computation with a positive impact on data sampling.  Furthermore, VirAAL decreases annotations in AL up to 80\% and shows improvements over existing data augmentation methods. The code is publicly available.",17
"  % don't really need to introduce RNNLM, but talk about Contextual ASR  % Language model plays an important role in automatic speech recognition system.  % Recently, recurrent neural language model  has shown better performance than back-off -gram language model.  % However, RNNLM have infinite history words, which makes it infeasible be transformed into a static decoding graph and merge into ASR decoding process. % To incorporate the advantage of RNNLM, a two-stage decoding process is often adopted. % In the first stage, We decode with the static graph usually generated from n-gram language model and generate a set of hypotheses. % In the second stage, RNNLM is used to rescore the hypotheses, and we pick the hypothesis with highest score as the output. % These set of hypotheses can either be represented as several independent word sequences or a compact form ""lattice"". % For the former one, we call it N-best rescoring, while the latter one, we call it lattice rescoring. % In this paper, we will focus on lattice rescoring.  % An exact lattice rescoring is not feasible because the search space will grow exponentially with respect to the depth of the original lattice. % In , the n-gram approximation method is proposed. N-gram approximation merge nodes if their n last history words are the same, which avoid the exponential growth of search space. % In , a refined pruning technique is used to reduce the search space when rescoring with long short term memory. %  then proposed a pruned lattice rescoring method, which is similar to A* search.  % Pruned lattice rescoring method has shown better and faster performance than all previous methods.  Personalized or contextual automatic speech recognition, which aims to improve accuracy by leveraging additional information or external knowledge, has been an important research topic . These prior works usually assume that a set of word-level biasing phrases are known ahead of time, e.g. a user閳ユ獨 personal contact list, which are used to nudge the ASR model towards outputting these particular phrases.  In the conventional hybrid ASR framework, bias phrases can be compiled into a weighted finite state transducer , with vocabulary injection, and on-the-fly language model biasing techniques   have shown significant performance gains. % contextual phrases as particular special symbols while training the base language model , then these special tokens are injected  and biased with on-the-fly WFST generated from  contextual phrases when decoding.  also benefits from the contextual information. Similarly, for end-to-end ASR architectures like Listen, Attend and Spell   , the WFST representation of context -grams is traversed along with the outputs from the LAS network, and the beam search decoding is biased either in first pass decoding  or rescoring . Alternatively, each context -gram can also be embedded into a fixed dimensional representation, and such contextual information is summarized by an attention mechanism and further fed as an additional input to the decoder .  One main distinction in our work is that rather than using a list of named entities, such as user's contact lists or song names as in many prior approaches, here we aim to exploit contextual information from word sequences or paragraphs, as illustrated in Figure . Henceforth, we refer to such textual content as video metadata. Utilizing the video metadata effectively can be challenging,  since it not only contains potentially relevant information, but also irrelevant text.  % For example, if the metadata is ""the car is fancy"", then we may give the words ""drive"", ""speed"", which is semantically related to 'car', with higher probability. % While some previous methods  have focused on building an base LM, and only apply the additional contextual LM for biasing the decoding,   unlike previous methods,  To address this challenge, our neural LM training is explicitly conditioned on the video metadata, and selectively attends to the metadata via an attention mechanism. The resulting contextual neural LM is used to rescore the lattices generated from the first-pass hybrid ASR decoding.  Therefore, our proposed methods are similar to  with three important distinctions. First, we produce the ASR lattice via a conventional WFST-based hybrid ASR model, and contextual biasing is performed by jointly rescoring the lattice and attending to the metadata. Second, rather than tying the contextual biasing with the end-to-end LAS training, we build the contextual LM separately from the acoustic model training, which effectively allows for modular evaluation and improvements of the contextual LM component. In common with earlier generations of technology, language model changes can be made independently of the acoustic model. Third, based on  that utilized contextual information by an attention mechanism, we further propose the hybrid pointer network .   %Incorporating other accessible information, often visual information, has been an active area of research for a long time. %In , ""lip-reading"" information is used to improve the robustness of ASR. %In , a pretrained deep Convolutional Neural Networks  is used to extract semantic vector or detect objects.  %Then these information is used to bias acoustic model, language model or end-to-end ASR model. %In this work, instead of using visual information, we utilize text information comes with video, like in figure. % % In the following article, we will call this kind of text information as 'video metadata'.  While conventional sequence-to-sequence  models typically generate tokens from a predefined vocabulary ,  pointer networks  can be used to % a specialized type of seq2seq model which to  pointer network directly explicitly select and output tokens from the input  sequence. Recently, hybrid pointer-generator networks , combining seq2seq models with pointer networks, have been proposed and used in summarization tasks . In such models, an additional scalar variable is generated at each time step and serves as a soft switch to choose between generating the token from a predefined vocabulary or selecting from the input sequence. This has been shown to be particularly effective in generating rare words that have very few occurrences in training data .  Our main contributions can be summarized into three categories:       In this work, we propose the use of a hybrid pointer network LM for lattice rescoring, thus making use of text metadata accompanying social media videos. We analyze the conditions of its effectiveness, and demonstrate that it can provide improvements in both LM perplexity and ASR WER.   Note that, hybrid pointer network may only boost the probability of any word that occur in exactly the same form as in video metadata, other than inflectional forms.    semantically   For example, if there is a word 'John' in video metadata, the probability of `John's' will not be boosted even if 'John' and `John's' are similar in spelling. So considering boosting the probabilities of related words can be a possible future direction. Also, in the hybrid pointer network framework, we can replace the recurrent components of multi-layer LSTM with other neural models, such as neural transformers .    
"," Videos uploaded on social media are often accompanied with textual descriptions. In building automatic speech recognition  systems for videos, we can exploit the contextual information provided by such video metadata. In this paper, we explore ASR lattice rescoring by selectively attending to the video descriptions.  We first use an attention based method to extract contextual vector representations of video metadata, and use these representations as part of the inputs to a neural language model during lattice rescoring.  Secondly, we propose a hybrid pointer network approach to explicitly interpolate the word probabilities of the word occurrences in metadata.  We perform experimental evaluations on both language modeling and ASR tasks,  and demonstrate that both proposed methods provide performance improvements by selectively leveraging the video metadata.  % auto-captioning system.   `pointer mechanism' % which explicitly boost the word probability if the word occurs in the text description.  to improve language models %  English and Spanish video ASR,  can outperform the n-gram and LSTM baseline",18
"  % Learning about vision and language is crucial for embodied AI agents to solve complex tasks. Language models have proven to be useful in understanding language semantics, and its generation . Transformers  have been applied to a wide variety of language tasks and have achieved remarkable performance on most of them. BERT  is a bi-directional language model that has proven to work well for transfer learning and fine-tuning after performing extensive pre-training on a large language corpus. However, dealing with a single modality is not sufficient to solve complex tasks that require understanding language with a visual context. Learning richer representations from visual and text data is a central task to solve multi-modal learning. Attention-based methods have proven to be very useful in learning long term dependencies and forming richer representations of the input sequences. Numerous approaches  have been proposed for learning visiolinguistic representations with transformers. Although these approaches have provided us with significant improvement on various benchmarks , the architectures used are over-parameterized require extensive training lasting for several weeks using multiple objectives to form a generalized representation of the task to be addressed, which is then followed by fine-tuning on a downstream task. This workflow has become a concerning problem. It results in deep learning methodologies being inaccessible and increased carbon footprints . In this work, we specifically explore adaptive methods. We refer to Adaptive mechanisms as those methods that change their behavior during training/run time and adapt stochastically to the environment based on data heuristics  learned by encountering samples from the same data distribution optimized by an objective function. Alternative approaches such as pruning, distillation  and quantization are rigid to some extent and induce some form of permanent modifications to the model. Adaptive methods enforce the network to learn parameters such that their behavior changes as per the complexity of the input sequence as perceived by the neural network. The code to reproduce the results  in this work is publicly available at this link.  Current self-attention approaches assume that the attention span of a head is invariant to the complexity of an input sequence. Attention heads can learn their optimal context size , which results in a reduction of FLOPS. When an optimal attention span is learned, the amount of attention given to a particular input sequence by an attention head is determined by its context size. We show that the context size varies with the emergent complexity of the sequence, and spans can help us understand how much sensitive a layer is to an input sequence.  Training models with a quarter of a million parameters are not feasible and practical for most users. One effective way to facilitate neural network scaling is by making the weights of the network sparse. This configuration allows us to perform faster training of deeper networks with relatively less compute. To make attention distributions sparse, we use  entmax  to obtain probability distribution of weights. Normalized exponential functions like softmax cannot assign a zero attention weight. This property enforces the context vector to stay dense, resulting in non-relevant sequences to be considered even though the network has discarded them by putting a deficient weight. Adaptive sparsity can make an attention head to learn richer distributions by oscillating the behavior of distribution to stay between softmax and sparsemax. We show that this behavior can help us understand preferences for the density of attention weight distribution and how it varies amongst each head about different modality.  We also study a form of regularization method called Layerdrop  to understand its regularization impact for multi-modal features. If the network can learn to drop identical layers , then it can be regarded as an adaptive depth mechanism.  We specifically use the  pruning method where the user specifies the drop rate because it offers maximal gains as suggested compared to its counterpart pruning methods. This method has proven to be effective in reducing the number of parameters and pruning layers during inference.  The contribution of this work is as follows:        %   While attention-based approaches are becoming universal, computationally efficient ways must be favored for broader adoption of provided pre-trained models on low resource hardware. Adaptive methods can significantly reduce the cost incurred to train such models and carbon footprints. In this work, we extend adaptive approaches to Visiolinguistic tasks to understand more about attention and adaptive mechanisms. While the empirical results are encouraging, important future work includes explorations of higher efficient adaptive and sparse mechanisms that can significantly cause FLOPS and parameter reduction with minimal loss in performance.   
"," The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.",19
" Entity Linking refers to the challenge of matching entity mentions in text  with the corresponding entity they refer to . The task is an essential building block to various NLP tasks such as information extraction or question answering. While some challenges  can be resolved using heuristics, more complicated cases  must be resolved based on mentions' contexts. Here, recent neural models based on representation learning on large-scale text collections  have achieved impressive results and are commonly considered state-of-the-art.  These results have mostly been reported for web-based encyclopedic text  and knowledge graphs . On the other hand, information extraction is increasingly applied for knowledge management in business contexts. Even in vast sectors with a rather technical focus such as mechanical engineering, service and other knowledge based activities are becoming more and more important to revenue . Given the rapid development of technology paired with a scarce and increasingly fluctuating workforce, experience management becomes vital: domain experts and service technicians must be enabled on the job even when inexperienced or when facing outdated technology. Frequently, documentation is scarce and reduces to hastily collected error messages, service reports, chat protocols or CRM entries. Linking this information to leverage it for a highly efficient support is a vital challenge to knowledge centered businesses. It requires the semantic indexing of unstructured text collections, from the classification of terms and phrases over the automatic extraction of facts to dialogue-based interfaces for digital assistants.  In this paper, we address the question whether the results achieved with neural models on large web-based encyclopedic text and knowledge graphs can be transferred to entity linking ``in the wild'' . We present a study in cooperation with our business partner Empolis Information Management, where automated entity linking is of vital interest to facilitate a highly efficient knowledge engineering on a diverse landscape of customer data. Particular, our study focuses on entity linking in a mechanical engineering application scenario: Entities refer to machine parts such as ``Flansch''/``Luftanschluss''  or error symptoms such as ``Leck''/``鑴發austritt'' . About 1200 of such entities are to be linked to mentions in over 1.6 million tickets collected by service technicians in the field. This setting differs substantially from Wikipedia-based scenarios:       : While Wikipedia covers a breadth of topics, business contexts come with particular terminology, entities and relations. This raises the question how a domain-specific model compares to a generic one pre-trained on Wikipedia.     : The German Wikipedia contains over 2 million entities, each with concise textual descriptions and manually annotated mentions . Business domains are often less rich and come with little annotated data. Also, texts such as error diagnoses and repair descriptions --- particularly when entered via mobile devices --- tend to suffer from spelling errors and incorrect grammar.     : Finally, business texts come in arbitrary languages . This raises the question whether language-specific models or generic multi-lingual models are preferable.   We explore these aspects using different entity linkers based on the state-of-the-art transformer network BERT. The approach is based on context matching, i.e. a match likeliness is derived from {      sentences  or sentence {      In this paper, we have studied the application of state-of-the-art deep language models for entity linking in a business context. We show that an ensemble of symbolic transformations and a neural approach using BERT achieves impressive results --- both on Wikipedia excerpts and a very noisy real-world dataset from an industry partner. It can be highlighted that our Bi-Encoder approach, which offers the opportunity to cache a set of pre-computed reference samples, is performing comparably to an expensive Cross-Encoder approach. This enables a resource-efficient production use-case.  While a straightforward fine-tuning to the target domain fails so far, developing effective strategies of fine-tuning to limited noisy domains will be our main focus in the future.    This work was funded by German Federal Ministry of Education and Research ).        --- felix       --- war's davor      --- ???  
","   Entity linking, the task of mapping textual mentions to known   entities, has recently been tackled using contextualized neural   networks. We address the question whether these results --- reported   for large, high-quality datasets such as Wikipedia --- transfer to   practical business use cases, where labels are scarce, text is   low-quality, and terminology is highly domain-specific.    Using an entity linking model based on BERT, a popular transformer   network in natural language processing, we show that a neural   approach outperforms and complements hand-coded heuristics, with   improvements of about 20\% top-1 accuracy. Also, the benefits of   transfer learning on a large corpus are demonstrated, while   fine-tuning proves difficult. Finally, we compare different   BERT-based architectures and show that a simple sentence-wise   encoding  offers a fast yet efficient search in   practice.",20
" Named entity recognition  aims to recognize named entities in a given text by determining their boundaries and classifying them into predefined categories . NER is a crucial step in various natural language processing applications such as event extraction~ and question answering~ as well as in big data analytics~. Early studies have addressed the recognition of named entities as a sequence labeling problem and extensive research efforts have been devoted to developing solutions using machine learning techniques~, hidden markov models~, and conditional random fields~. Recently, neural models have been introduced to named entity task in well-formed and noisy texts~. In spite of recent advances, NER remains to be a challenging problem due to several reasons such as the recognition of overlapping or nested entities, infrequent entities in user generated noisy texts, and semantically ambiguous entities in different contexts.  In the current era, the amount of online content has exploded which makes it exhaustive to search from a vast distributed source of information. Search tools or expert systems might effectively alleviate the problem of accessing available content on the web. However, continuous alteration of natural languages due to heavy social media usage, social-cultural factors in society, daily events  has reflections in written texts and leads to constant evolution of words, expressions and importantly named entities. Correctly identified named entities from unstructured or semi-structured content form a basis for the development of more effective and intelligent information management, text mining, and relation extraction systems~. For instance, mining daily news content by digital media applications for extracting information about a person or a location  necessitates querying an astonishing amount of news articles which can be facilitated by automatic detection of named entities in written texts. Paving the road for interpretable and reusable information through semantically annotated online content can also be listed as a particular benefit of extracting named entities and their relations from raw texts.  NER is a well-studied task for several languages including Turkish and recent successes in neural architectures have greatly advanced achieved performances on recognizing Turkish named entities~. In these studies, Bidirectional Long Short Term Memory networks with different word representations were widely used and evaluated on a common dataset consisting of person, location, and organization names~. A conditional random field  was shown to positively contribute to these networks that minimize the need for feature engineering. There is a recent interest in applying deep bidirectional transformers~ and transfer learning~ to Turkish entity tagging. In this work, we present a comprehensive evaluation of two notable neural architectures, namely BiLSTM networks and Transformer-based networks and compare their performances in the same experimental setting. In BiLSTM models, we explore different combinations of four kinds of embeddings as input  and experiment with different pretrained embeddings as initializations of word embeddings. In transformer-based models, we benefit from three different transformer based language models, namely multilingual cased BERT , Turkish BERT , and XLM-RoBERTa , and study the effectiveness of both linear and CRF layers at the top of the network. As our second contribution, we propose a transformer-based neural architecture accompanied with a CRF as the top layer  which sets the new state-of-the-art f-measure of 95.95\%. Our study not only extends the current Turkish NER literature but also validates the usability of transfer learning on processing a morphologically rich language.   The rest of this article is organized as follows. Section discusses related research on named entity recognition with a particular focus on Turkish NER studies. Section describes neural architectures utilized in this work. Section presents our dataset and parameter initializations used for building neural architectures. Section discusses conducted experiments and the results that we obtained. Finally, Section concludes the article and presents our future work.     Recent years have witnessed a surge of interests in Turkish named entity recognition. This study presents our empirical evaluations of recent neural sequence tagging models on Turkish NER task by providing a high-level comparison of different model settings and design considerations. Our results provide insights into the importance of word representations  and their initializations  in BiLSTM networks. Our experiments also include a comprehensive evaluation of neural architectures that utilize popular multilingual transformer-based language models on Turkish entity tagging. Their comparisons with BiLSTM models reveal their superior performance on the evaluation set and highlight the positive impact of transfer learning. In this work, we also propose a state-of-the-art transformer-based architecture with a CRF layer that achieves the highest f-measure of 94.90\  and 95.95\  on the validation and test sets, respectively.   As our future work, we plan to aggregate character and morphological embeddings with transformer-based language models and assess their impact on the overall performance. We also intend to study other word embeddings in BiLSTM networks, especially those that were shown to be effective in other morphologically rich languages such as Flair . Finally, we plan to develop new subword tokenizers such as a tokenizer that returns morphemes attached to a word as produced by a morphological analyzer.      Loading bibliography style file  
"," Named entity recognition  is an extensively studied task that extracts and classifies named entities in a text. NER is crucial not only in downstream language processing applications such as relation extraction and question answering but also in large scale big data operations such as real-time analysis of online digital media content. Recent research efforts on Turkish, a less studied language with morphologically rich nature, have demonstrated the effectiveness of neural architectures on well-formed texts and yielded state-of-the art results by formulating the task as a sequence tagging problem. In this work, we empirically investigate the use of recent neural architectures  proposed for Turkish NER tagging in the same setting. Our results demonstrate that transformer-based networks which can model long-range context overcome the limitations of BiLSTM networks where different input features at the character, subword, and word levels are utilized. We also propose a transformer-based network with a conditional random field  layer that leads to the state-of-the-art result  on a common dataset. Our study contributes to the literature that quantifies the impact of transfer learning on processing morphologically rich languages.",21
"  The prevalence of gender bias in political news coverage has been consistently highlighted by both academics and politicians . Such bias has been shown to deter women from entering political life .  Given issues concerning the global imbalance of women in politics  it is important that systematic ways of identifying gender bias in the media are developed.  In studies of the representation of women in the media content analysis of text is the method predominantly used.  Given the time consuming nature of this approach, the volume of text studied can be limited .  This paper explores the use of new automated techniques, employing natural language processing and machine learning to analyse a large corpus of newspaper text. Text classification has been used in similar ways to examine ideological differences expressed in political speeches  and gender differences among authors . While these studies are primarily author focused, this research will build on these methodologies to analyse the language of journalists as a collective. In this sense, for the purposes of building on existing methodologies, the author is considered analogous to the media organisation and systematic gender bias is attributed to it rather than to individual journalists.   The newspaper articles analysed included 15 years of newspaper coverage of ministerial level Irish politicians between 1997 and 2011.  This date range was selected as it represented a period in Irish political history where the same political party remained the largest party in government and the political figures holding office remained largely unchanged. This consistency in political power presents an opportunity to study the representation of politicians over a long period of time thus mitigating the influence of particular events or personalities on the language in the media. This study focused on two newspapers in Ireland with the largest circulation, The Irish Times and The Irish Independent . Newspaper content was selected rather than digital sources, as this is the primary source of text-based news in Ireland. The Irish Times is traditionally considered a paper of record and therefore could be a powerful indicator of the prevalence of gender bias in the media. With the aim of facilitating the broadening of the scope of this research in the future, the methodology outlined in this research is designed with scalability in mind so that it could be applied to text from a different media source. [ht]        A range of natural language processing techniques were applied to extract information from the newspaper text and machine learning algorithms were then used to build models to automatically classify articles according to whether they featured male or female politicians. The accuracy in the predictive models was indicative of the extent of differences in coverage of male and female politicians. These models were then analysed to assess whether differences in the coverage were attributable to gender bias or not. The context that the gender differences occurred was examined using concordance analysis.    %   The findings of this research showed that coverage of politicians in Irish newspapers contains evidence of gender bias. The research also demonstrated how text classification could be used to analyse texts for evidence of gender bias. Machine learning algorithms highlighted differences in how Irish newspapers covered male and female politicians and analysis of these differences showed that many could be attributed to gender bias.   There were two key objectives of this research. The first objective was to propose and test a new methodological approach to analysing gender bias in text. The purpose of this was to contribute to the existing range of methods available to researchers using a corpus approach in analysing gender. The second objective was to examine whether there is bias in the coverage of politicians in Ireland and what the nature of it is. Ireland currently holds the 92nd position globally for the level of women閳ユ獨 participation in politics . Given that gender bias in the media has been shown to discourage women from entering politics , it is important to examine media coverage of politics for evidence of gender bias. To date there has been little analysis of bias in the Irish media  and this research has addressed this issue.  The 閾夸苟dings showed that there was evidence of gender bias in the coverage of female politicians in Ireland. In previous studies of gender bias in the media, bias tends to be analysed in terms of the quantity and the content of the coverage . The findings of this research showed a distinct positive bias towards female politicians in terms of the volume of coverage they received and their placement within newspaper sections. However, differences were evident in how male and female politicians were featured in articles that may be attributable to gender bias.   Differences in coverage included an increased focus on family relationships and roles, differences in how they were associated with policy, an excessive focus on the gender of female politicians, more negative portrayals of political style and more personalised coverage. This research highlighted the ways in which gender bias is expressed in the coverage of politicians in Irish newspapers. The kind of bias identified aligned with bias found in some related studies. However, unexpected forms of gender bias were also uncovered demonstrating the value of exploring data-driven approaches to text analysis enabled by machine learning.        ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.     
","  This paper presents research uncovering systematic gender bias in the representation of political leaders in the media, using artificial intelligence. Newspaper coverage of Irish ministers over a fifteen year period  was gathered and analysed with natural language processing techniques and machine learning. Findings demonstrate evidence of gender bias in the portrayal of female politicians, the kind of policies they were associated with and how they were evaluated in terms of their performance as political leaders. This paper also sets out a methodology whereby media content may be analysed on a large scale utilising techniques from artificial intelligence within a theoretical framework founded in gender theory and feminist linguistics.",22
" Researchers have shown great interests in open domain multi-turn chatbots. In general, there are two lines of approaches, the first one is retrieval-based~, and the second one is generation-based~. Retrieval-based approaches retrieve several candidates through searching a given database and then select the best response from these candidates using matching models. Generation-based approaches use an encoder-decoder framework to generate responses word by word. Generally, the former approaches are better in terms of the syntactic correctness, the diversity, and the length of the responses. The latter one tends to generate short and generic responses, which we call ``safe'' responses. Therefore, retrieval-based approaches have been widely used in the industry such as the E-commerce assistant AliMe~ serving on Taobao\footnote{https://www.taobao.com/} and the XiaoIce~ implemented by Microsoft \footnote{https://www.microsoft.com/}. We focus on retrieval-based chatbots in this study.  The early retrieval-based models treat the context as a whole to match responses~ while the recent ones use each utterance in the context to match the candidate response and then aggregate the matching results to choose a response~. The recent ones work better because they retain more information of each turn, without compressing it into a single highly abstract vector.  Specifically, state-of-the-art methods follow a representation-matching-aggregation framework~. In the matching stage they match each utterance with the candidate response by word-level or segment-level similarity matrix. However, these matrices cannot fully reflect the sentence-level semantic information, and thus these methods make simple mistakes when similar words from the utterances appear in the negative candidates. We show such a case in Table to demonstrate this problem.       \tiny  %% & Dialogue Context  & IOI & S2M \\           is correct. \\         Turn-3 & \makecell[l]{C: Yeah it's correct. Btw, I have saved you as one of \\         \quad my Saved Sellers. You said you would give free  \\         \quad {gift} for doing that. Do you still give now? } \\         Turn-4 & R: Yes. \\         Turn-5 & C: What {gift} do you give? \\         .  & 0.86 & 0.98 \\          is correct.          \\ } & 0.56 & 0.01 \\         . Please make sure your \\         {shipping address} is correct. } & 0.98 & 0.91 \\         \hline             To address this issue, we propose a sequential sentence matching network . Instead of relying solely on word or segment similarity in matching, we calculate the similarity between a given utterance and response based on sentence-level matching. It eliminates the kind of mistakes shown in Table . Moreover, we design an effective mechanism to integrate sentence-level matching and word-level matching, so that we can take full advantage of different levels of semantic information and further improve the model performance.  We conduct experiments on three data sets: the Ubuntu Dialogue Corpus~, the Douban Conversation Corpus~, and the E-commerce Dialogue Corpus~. The results show that the sentence matching mechanism reduces word matching errors and S2M significantly outperforms the state-of-the-art on the three corpora. Experiment results also show that S2M performs better on longer context  comparing to the models that match on word-level or segment-level.  Our contributions in this paper are four-folds:  Proposal of a sequential sentence matching network in the context-response matching problem;  Proposal of an effective integration method to incorporate the sentence matching mechanism and the word or segment matching mechanism;  Empirical results show that our model significantly outperforms the state-of-the-art baselines on benchmark data sets;  Empirical results also show that our model is more competent to handle long dialogues.  [ht]               In this paper, we propose a sequential sentence matching network , which reduces the word-level matching errors in multi-turn response selection using the sentence-level semantic information. We also find that integration of sentence-level matching with word-level matching enables our model to learn richer features. Evaluation results on benchmark datasets indicate that our model can significantly outperform the state-of-the-art models. Moreover, for longer dialogues, our model is more competent than the state-of-the-art models that are based on word-level or segment-level matching. This further demonstrates the advantage of using sentence-level semantic information on the multi-turn response selection task. In the future, we will look into the remaining bad cases of our model of different corpora and try to identify further improvement opportunities.  
"," Recently, open domain multi-turn chatbots have attracted much interest from lots of researchers in both academia and industry. The dominant retrieval-based methods use context-response matching mechanisms for multi-turn response selection. Specifically, the state-of-the-art methods perform the context-response matching by word or segment similarity. However, these models lack a full exploitation of the sentence-level semantic information, and make simple mistakes that humans can easily avoid. In this work, we propose a matching network, called sequential sentence matching network , to use the sentence-level semantic information to address the problem. Firstly and most importantly, we find that by using the sentence-level semantic information, the network successfully addresses the problem and gets a significant improvement on matching, resulting in a state-of-the-art performance. Furthermore, we integrate the sentence matching we introduced here and the usual word similarity matching reported in the current literature, to match at different semantic levels. Experiments on three public data sets show that such integration further improves the model performance.",23
"  Teaching machines to read, process, and comprehend natural language is a coveted goal of machine reading comprehension  problems . Many existing MRC datasets have a similar task definition: given a document and a question, the goal is to extract a span from the document  or instead generate an abstractive answer to answer the question.  %Extractive machine reading comprehension tasks are : given a document and questions, the answers can be extracted as spans from the document.  % artificial intelligence: 缁嬪秴浜曢張澶屽仯婢堆嶇吹  %In this paper, we focus on conversational MRC tasks such as CoQA~ and QuAC~, in which a series of interconnected  questions are designed based on the given documents. In consequence, these questions together with their answers form conversations.  There is a growing trend of building MRC readers~ based on pre-trained language models~, such as GPT~ and BERT~. These models typically consist of a stack of transformer layers that only allow fixed-length .  Therefore, we argue that a good chunking policy should generate chunks that not only fully cover the correct answer span but also provide sufficient contexts around the correct answer. % within the chunk could not be well exploited, and such chunks are more likely to fail to cover complete answers.  %Second, besides the fixed-length chunking, most existing methods predict answers by only reading the local information within each chunk. However, in practice, information from different chunks is essential to answer questions that involve global contextual information such as coreferential name mentions within a document.   %\todo{To support this last claim, shall we add an analysis in our experiment/discussion section to see how baseline and our method perform on the test samples that involve coreferential name mentions?}  % our solution % propose   recurrent mechanism  % propose  learning to chunk %Considering the limitations mentioned above, we propose to let a machine reader learn how to chunk intelligently via reinforcement learning. Instead of using a fixed stride in one direction, we allow the model to decide the next chunk to be processed in either direction. Henceforth, the model is capable of making better predictions based on the carefully selected chunks . We also apply recurrent mechanisms to allow the information to flow between chunks . As a result, the model can have access to information beyond the current chunk.   % evaluation and experiments % %\footnote{The code is available at https://github.com/HongyuGong/ConversationalQA.} In the experiments, we evaluate the proposed RCM\footnote{The code is available at \url{https://github.com/HongyuGong/RCM-Question-Answering.git}.} on three MRC datasets: CoQA, QuAC, and TriviaQA. Experimental results demonstrate that RCM leads to consistent performance gains on these benchmarks. Furthermore, it also generates segments that are more likely to cover the entire answer spans and provide richer contextual information around the ground truth answers.   The primary contributions of this work are:               In this paper, we propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning. We also add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers. We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA. Our approach outperforms benchmark models across different datasets.           While we also notice that  while    Both chunk 2 and chunk 3 generated by our models move closer to the ground truth answer while the chunks of BERT-Large move farther.  The chunking model with LSTM recurrence is better at capturing the answer than the model with linear recurrence.   
"," In this paper, we study machine reading comprehension  on long texts, where a model takes as inputs a lengthy document and a question and then extracts a text span from the document as an answer. State-of-the-art models tend to use a pretrained transformer model  to encode the joint contextual information of document and question. However, these transformer-based models can only take a fixed-length  text as its input. To deal with even longer text inputs, previous approaches usually chunk them into  segments and predict answers based on each segment independently without considering the information from other segments. As a result, they may form segments that fail to cover the correct answer span or retain insufficient contexts around it, which significantly degrades the performance. Moreover, they are less capable of answering questions that need cross-segment information.  We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction. We also employ recurrent mechanisms to enable information to flow across segments. Experiments on three MRC datasets -- CoQA, QuAC, and TriviaQA -- demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.",24
"   In recent years, encoder-decoder based models  have become the fundamental instrument for sequence-to-sequence learning, especially in tasks that involves natural language.  The attention mechanism  proves essential for the encoder-decoder based models to efficiently draw useful source information from the encoder.   As illustrated in Figure, for those representations output by different encoder layers, the common practice for the decoder is to draw information only from the last encoder layer, which is regarded as a global and comprehensive view of the source sequence but short of precise and finer details. The impact of such architectural choice is demonstrated in Table. For this German-to-English translation example, the target sentence generated by the system reveals two common shortcomings in neural sequence-to-sequence models: 1) the generated text is unfaithful to the source ; and 2) repeated texts are generated  . They can both be attributed to the lack of detailed and accurate information.  Several studies have tried to narrow the information gap by introducing deep representations from the encoder layers. For example, fusing the representations from different encoder layers  or providing representations from different encoder layers to different decoder layers . However, mixed conclusions have been reached by such work, either with slight improvements  or obvious degradation .  In this work, we identify the hierarchy bypassing problem that affects all previous related efforts, hindering the efficient training of the encoder layer stack and weakening the hierarchy and diversity of representations from different encoder layers, which is the premise for sequence-to-sequence learning with deep representations.    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  	{0.3\linewidth} 		 \makeatother 		 		 	 	 	    \makeatother         {2pt}                  {|m{0.16\linewidth}|m{0.79\linewidth}|}             &  aber ich hatte das groe gl\""uck , ihn sehr fr\""uh , mit dreizehn jahren , kennenzulernen und so war ich schon zu meiner schulzeit auf seinen kursen .  \\     &but i had the good fortune to meet him at a very young age , when i was thirteen , and so i always  attended his courses while i was at school .             \\  & but i was very lucky to meet him at the age of three at the age of three , and so at the time , i was  on his classes .           \\  &  but i was very lucky to meet him at the age of thirteen, and so at the time while i was at school , i always on his courses .      \\ , where for each decoder layer, together with the global view from the last encoder layer, another purposeful view of the source sequence is also supplemented . Moreover, the layer-wise cross-view training is devised to continue the conventional training so that the representational ability of the multi-layer encoder is directly inherited . The effect of our approach is systematically investigated on typical strategies of routing source representations , which shows that the proposed approach successfully addresses the hierarchy bypassing problem, requires minimal parameter increase, and improves the performance of sequence-to-sequence learning with deep representations.   Specifically, through our experiments on the machine translation with two strong baselines, i.e., Transformer  and DynamicConv , which are the previous state-of-the-art models, we find that one of the cross-view variant, i.e., granularity consistent attention  ), promotes the performance substantially. We speculate the reason is that the GCA builds connections between the corresponding layers in the encoder and the decoder, so that the first decoder layer pays attention to the global information, i.e., coarse-grained representations, of the source sequence, which is instrumental in language modeling, while the last decoder layer pays attention to the fine-grained representations of the source sequence, which is helpful to generate words that are more precise. Notably, a similar connection pattern without the global view is used in computer vision for biomedical image segmentation with success . It is also considered by  in natural language processing but with opposite results, i.e., performance degradation. The reason is that a direct transfer of such pattern is not viable for sequence-to-sequence learning and further considerations are required for successful training as we show.  Overall, our contributions are as follows:               In this work, we focus on enhancing the information transfer between the encoder and the decoder for sequence-to-sequence learning, through injecting diverse source representations into the generation process.  Different from the single view approach in existing work, we propose the layer-wise cross-view decoding approach to route source representations of different granularity to different decoder layers, together with a global view preventing the regression of last encoder layers due to the hierarchy bypassing problem. The cross-view decoding also builds upon continued training to mine the expressive power of encoder layer stack in learned conventional single-view models. Out of several source-target routing strategies, we find that the granularity consistent attention  strategy for context attention shows the best improvements with the proposed layer-wise cross-view decoding. Experiments on the machine translation task verify the effectiveness of our approach. In particular, it outperforms the DynamicConv model which is the previous state-of-the-art in machine translation.        
"," In sequence-to-sequence learning, the decoder relies on the attention mechanism to efficiently extract information from the encoder. While it is common practice to draw information from only the last encoder layer, recent work has proposed to use representations from different encoder layers for diversified levels of information. Nonetheless, the decoder still obtains only a single view of the source sequences, which might lead to insufficient training of the encoder layer stack due to the hierarchy bypassing problem. In this work, we propose layer-wise cross-view decoding, where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source sequences. Systematic experiments show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks.",25
"  % Early notable works on the neural networks  such as Hebbian Learning  and the Perceptron  described biologically inspired learning of parameters. Another wave of Connectionism came with the invention of backpropagation, which is still the backbone of neural network training.  For the last couple of decades, neural networks have been approached primarily from an engineering perspective, with the key motivation being efficiency, consequently moving further away from biological plausibility. Recent developments  have however incorporated explicit constraints in neural networks to model specific parts of the brain and have found a correlation between the learned activation maps and actual neural activity recordings. Thus, these trained networks can perhaps act as a proxy for a theoretical investigation into biological circuits.  %Chomsky's theory of innate language structures in the human mind  and the claims on hierarchical bias in children's language acquisition have influenced the development of psycholinguistics and the way it aids in developing the representation of human cognition.  Recurrent Neural Networks  have been used to analyze the principles and dynamics of neural population responses by performing the same tasks as animals . However, these networks violate Dale's law , which states that the neurons have either a purely excitatory or inhibitory effect on other neurons in the mammalian brain. The decaying nature of the potential in the neuron membrane after receiving signals  from the surrounding neurons is also well-studied . The goal of our work is to incorporate these biological features into the RNN structure, which gives rise to a neuro-inspired and computationally inexpensive recurrent network for language modeling, which we call a {{The goal of our work is to introduce a neuro-inspired and computationally inexpensive recurrent network for language modeling, which we call a { and the associated verb must agree in number, is considered as evidence of hierarchical structure in English. This is exemplified using a sentence taken from the dataset made available by %Subject-verb agreement, where the main noun and the associated verb must agree in number, is considered as an evidence of hierarchical structure in English. natural language. This is exemplified using a sentence taken from the dataset made available by :        on the \underline{expressway} requires a toll.      on the \underline{expressway} require a toll.    The effect of agreement attractors  between the main noun and main verb of a sentence has been well-studied . Our work also highlights the influence of non-attractor intervening nouns. For example,       created by a \underline{hobbyist} as a \underline{gift} to \underline{someone} is not a commodity.\footnote{Sentence taken from the dataset made available by .}  % We assess our model's performance in the presence of such intervening nouns, by making it predict the number of the verb, and judge the grammaticality of a sentence in Section . % We compare our model to both LSTMs and SRNs on the verb number prediction and grammaticality tasks. In the number prediction task, if a model correctly predicts the grammatical number of the verb , it might be due to the  interference of non-attractor intervening nouns  rather than necessarily capturing its dependence the main noun . From our investigation in Section , we find that the linear recurrent models take cues present in the vicinity of the main verb to predict its number, apart from the agreement with the main noun.  In the subsequent sections, we investigate the performance of the Decay RNN and other recurrent networks, showing that no single sequential model generalizes well on all  phenomena, which include subject-verb agreements, reflexive anaphora, and negative polarity items as described in . %For a robust examination of our proposed model, in Section  we present our model閳ユ獨 performance on word-level language modeling task and will present that even with a minimalistic change in the number of parameters from SRN, our model achieved strong performance jump on an exponential scale. %In section , we show that there is no recurrent network that generalizes well on all types of sentences, which include subject-verb agreements, reflexive anaphoras, and negative polarity items as described in . This indicates the need for further research on the nature of the inductive bias conferred by certain classes of models and training procedures, which leads to variability in performance across sentence types. Our major outcomes are:        {biologically plausible}     recurrent model: the Decay RNN, which performs on-par with LSTMs for linguistic tasks such as subject-verb agreement and grammaticality judgement.        %    In this paper, we proposed the Decay RNN, a bio-inspired recurrent network that emulates the decaying nature of neuronal activations after receiving excitatory and inhibitory impulses from upstream neurons. We have found that the balance between the free term }\)) and the coupled term }\)) enabled the model to capture syntax-level dependencies. As shown by , explicitly modeling hierarchical structure helps to discover non-local structural dependencies. The contrast in the performance of the language models encourages us to look at the inductive biases, which might have led to better syntactic generalization in certain cases. Recently,  showed the existence of a line attractor in the dynamics of the hidden states for sentiment classification. Thus, similar dynamical-system-based analysis can be extended to our settings to further understand the working of the Decay RNN.  From the optimization point of view, it is yet not possible to incorporate Dale's principle for these artificial neural networks . Our work does provide a partial way to solve this problem; however, better ways to incorporate Dale's constraint with back-propagation is one of our future works.  One way is to investigate the locus of error in case of a number prediction task, by analyzing the patterns in the decision of the models at each instance.  From the cognitive neuroscience perspective, it would be interesting to investigate if the proposed Decay RNN can capture some aspects of actual neuronal behaviour and language cognition. Our results here do at least indicate that the complex gating mechanisms of LSTMs  may not be essential to their performance on many linguistic tasks, and that simpler and perhaps more cognitively plausible RNN architectures are worth exploring further as psycholinguistic models.  
"," Long short-term memory  networks and their variants are capable of encapsulating long-range dependencies, which is evident from their performance on a variety of linguistic tasks. On the other hand, simple recurrent networks , which appear more biologically grounded in terms of synaptic connections, have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. In this paper, we seek to develop models that bridge the gap between biological plausibility and linguistic competence. We propose a new architecture, the Decay RNN, which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons. Besides its biological inspiration, our model also shows competitive performance relative to LSTMs on subject-verb agreement, sentence grammaticality, and language modeling tasks. These results provide some pointers towards probing the nature of the inductive biases required for RNN architectures to model linguistic phenomena successfully. %Long short-term memory  and its variants, a group of recurrent networks, are capable of encapsulating long term dependencies, which is evident by their performance on the linguistic tasks. On the other hand, a simple recurrent network , which has more biological grounding in terms of synaptic connections, has consistently failed on capturing long term dependencies as well as the locus of grammatical errors in an unsupervised setting. In this paper, we use insights from psycholinguistics to develop a model that bridges the gap between neurological persuasiveness and commendable performance on linguistic tasks. We propose a new architecture-  which incorporates the decaying nature of voltage in a neuron membrane at the same time modeling the excitatory and inhibitory connections in the population of neurons. Besides its biological plausibility, our model also has a competitive performance to LSTM  on subject-verb agreement, sentence grammaticality, and language modeling tasks. The results on our model open the room to probe the nature of inductive biases responsible for the laudable or substandard performance of certain architectures.",26
"  Email services are one of the most popular communication media due to its efficiency and quickness. They allow to send and receive messages via the Internet, usually through free and anonymous registration. However, these features eases the bulk and unsolicited emails, best-know as spam. Users indiscriminately receive spam, whose content can include advertisements, digital marketing, or frauds such as malware distribution, leaked-data, and phishing . Spam emails produce a loss of work productivity and traffic congestion , and spam with a fraudulent aim also risk the security and privacy of who receive them.   The relatively low cost, straightforward creation and the user's difficulty of identification makes spam emails one of the most used attack vectors for cybercriminals. Considering the reports of Cisco Talos\footnote{\protect\url{https://talosintelligence.com/reputation_center/email_rep} Retrieved March 2020} and Kaspersky Lab\footnote{\protect\url{https://www.statista.com/statistics/420391/spam-email-traffic-share/} Retrieved March 2020}, spam emails represent approximately between  and  of the daily total volume of worldwide emails.   The main tools to detect spam are binary filters, i.e. algorithms that categorise emails in spam or not spam. Traditionally, anti-spam filters use manual analysis, pattern matching, and Artificial Intelligence  techniques . More recent approaches, mainly based on Automatic Text Classification, provides more accurate models . Nevertheless, despite the efforts of organisations and researchers to develop more efficient binary spam filters, spammers bypass them .   We propose for the first time to the best of our knowledge, to enhance the detection of spam by using an automatic multi-classification approach, instead of a binary model. The multi-classification of spam emails can improve the efficiency in cybersecurity incident handling, companies and citizens protection and early warning by detecting spam campaigns related to specific targets and patterns inside categories. Dada et al.  identified in their literature review that some researchers used the behavioural patterns of spammers as a critical aspect of spam detection. Besides, due to the high daily volume and variety of spam, its detection can be addressed as a Big Data problem that increases the need of solutions that offer an adequate protection service for citizens, companies and response teams. Redmiles et al.  proposed to help users by identifying what content of the email is suspicious to prevent the troubles occurred by spam messages.  To train ours multiclassification pipelines, we created a spam email multi-class dataset called Spam Email Classification  . We evaluated six pipelines based on Text Classification techniques that combine two feature extractors, Term Frequency - Inverse Document Frequency  and Bag of Words , and three Machine Learning algorithms, Support Vector Machine , Naive Bayes , and Logistic Regression . We followed the process shown in Figure .  [ht]             The rest of the paper is organised as follows: Section  presents the literature review about spam email challenges. Section  explains the methodology proposed to address the creation of SPEMC-K and the set of the designed classification pipelines. After that, in Section , we discuss the experiments and results. Finally, Section  presents our conclusion and our future work.        To the best of our knowledge, in this work, for the first time in the literature, we dealt with the problem of spam detection through the use of multi-classification, instead of a binary classifier.  spam emails instead of a binary model in order to identify spam classes patterns and detect cybercrime campaigns, to enhance the security and privacy of organisations and citizens.  We applied a hierarchical clustering on  English spam emails and discarded emails with less than five words, resulting in an unbalanced dataset of  emails divided into three categories: Health and Technology, Personal Scams, and Sexual Content, with , , and  respectively. We used this data for evaluating the combination of two feature extractors, TF-IDF and BOW, along with three Machine Learning classifiers, LR, SVM, and NB. The combination of TF-IDF with SVM achieved the best accuracy, . Instead, TF-IDF-NB achieved the fastest execution time per email, ms being the most suitable pipeline to use in real-time applications.   On the one hand, we have explored the content of spam emails as a multi-classification task for the first time. The results encourage us to divide the three spam classes into more classes and investigate the content of each one deeply to understand the spammers' behaviours. The research on the class Health and Technology, i.e., spam that contains products, could help to detect emerging products as Al Nabki et al.  did on the Darknet Tor. These can also help to anticipate or detect the concept drift in spam emails and define the use of spammers tricks over time. Moreover, considering another work of Al Nabki et al.  could lead to developing a model to rank and detect the most influential hidden services inside spam emails.  On the other hand, the high performance obtained by the pipelines suggested that the problems related to the concept drift in binary classification can occur with the multi-classification. Hence, we plan to assess the six pipelines on public datasets and other spam email time sets provided by INCIBE.   
"," %Email is one of the most popular collaborative communication media.  Spammers take advantage of email popularity to send indiscriminately unsolicited emails. Although researchers and organizations continuously develop anti-spam filters based on binary classification, spammers bypass them through new strategies, like word obfuscation or image-based spam. For the first time in literature, we propose to classify spam email in categories to improve the handle of already detected spam emails, instead of just using a binary model. First, we applied a hierarchical clustering algorithm to create SPEMC-$11$K , the first multi-class dataset, which contains three types of spam emails: Health and Technology, Personal Scams, and Sexual Content. Then, we used SPEMC-$11$K to evaluate the combination of TF-IDF and BOW encodings with Na閼煎俥 Bayes, Logistic Regression and SVM classifiers. Finally, we recommend for the task of multi-class spam classification the use of  TF-IDF combined with SVM for the best micro F1 score performance, $95.39\%$, and  TD-IDF along with NB for the fastest spam classification, analyzing an email in $2.13$ms.",27
"  Automatic dialogue act  recognition has reached near-human performance on standard corpora, such as the Switchboard Dialogue Act corpus. However, various application domains lead to different types of dialogues: this variability, which is illustrated in the French corpus described in Section, severely impacts the direct application of a model trained on a standard corpus to many other types of application tasks, as show next. Furthermore, developing DA-recognition models in other languages than English requires the costly annotation of large-enough corpora. In the last years, efforts have been dedicated to address this issue by proposing language-independent ISO standard, but variability in the types of dialogues makes the requirement to annotate dedicated training corpora still pressing. We propose to investigate cross-lingual transfer learning methods to reduce the amount of annotations required for each new domain and language.  %  Transfer learning aims at reusing knowledge gained from a large corpus to improve the performances of small models trained on a related task with low resources. We investigate in this work two sources of information from which we transfer knowledge: pre-trained English BERT sentence  embedding, and an English corpus annotated with dialogue acts. Two low-resource target tasks are considered: dialogue act recognition in German and in French, where the amount of annotated dialogue acts is limited to a few hundred samples that may be annotated by one application developer within a few hours. Two target conditions are also considered: either with the same set of dialogue acts in German and in English, or with a different set of dialogue acts in French.  In addition to the relatively large resources available in English, we further assume the availability of a relatively good machine translation system; we will use for this purpose Google's translation. Hence, our transfer learning strategy consists first in translating every target training and test material to English, and then fine-tuning our initial ""large"" English model to recognise these translated dialogues. Details of this method are given in Section.     We have investigated two types of transfer learning: pretrained word embeddings and classifier fine-tuning.  Three types of sentence representations, namely BERT, CNN and multi-head self-attention were utilized. The objective was to port a reference DA recognition model trained on English to another language and dialogue context with only a limited amount of annotated resources. We have validated these approaches on two target languages, German and French, and developed a dedicated French DA corpus with real-life dialogues recorded in quite different conditions than the existing standard DA corpora. The main conclusion is that all available sources of information are required to obtain the best results, but also that some data should be reserved for proper hyperparameter tuning. Another conclusion is that automatic translation introduces some bias and mistakes that need to be compensated with fine-tuning. The reduced success of transfer learning for French is likely due to the greater mismatch between the corresponding corpus types, and transfer learning does not totally compensate for the lack of training data in the target domain. Nevertheless, the proposed transfer learning architecture gives good enough results to open the way to future research in transferring rich English DA systems to other languages and less explored domains.  
","   This paper deals with cross-lingual transfer learning for dialogue act  recognition.   Besides generic contextual information gathered from pre-trained BERT embeddings, our objective is to   transfer models trained on a standard English DA corpus to two other languages,   German and French, and to potentially very different types of dialogue with different dialogue acts   than the standard well-known DA corpora. The proposed approach thus studies the applicability   of automatic DA recognition to specific tasks that may not benefit from a large enough number   of manual annotations. A key component of our architecture is the automatic translation module,   which limitations are addressed by stacking both foreign and translated words sequences into the same model.   We further compare both CNN and multi-head self-attention to compute the speaker turn embeddings   and show that in low-resource situations, the best results are obtained by combining all sources of   transferred information.",28
"  Developing machines that can follow natural human commands, particularly those pertaining to an environment shared by both machine and human, is a long-standing and elusive goal of AI~. Recent work has applied deep reinforcement learning  methods to this challenge, where a neural-network-based agent is optimized to process language input, perceive its surroundings and execute appropriate movements jointly . Deep RL promises a way to deal flexibly with the complexity of the physical, visual and linguistic world without relying on  hand-crafted features, rules or policies. Nevertheless, the cost of this flexibility is the large number of environment interactions  required for a network to learn behaviour policies from raw experience. To make the approach feasible, many studies thus employ a synthetic language that is generated on demand from templates by the environment simulator~. Attempts to integrate deep RL with natural language typically employ less realistic grid-like environments~ or grant agents privileged global observations and gold-standard action sequences to make learning more tractable~.     In the general case, our proposed recipe  involves the following steps:         , where  is the everyday name for object or relation .     In this work, we apply SHIFTT in a simulated room in the Unity\footnote{Unity. http://unity3d.com.} game engine. Note that there is no reason, in theory at least, why the recipe could not apply to robot-learning for object manipulation .     In this work, we have developed an agent that can follow natural human instructions requiring the identification, control and positioning of visually-realistic assets. Our method relies on zero-shot transfer from template language instructions to those given by human annotators when asked to refer and instruct in natural ways. The results show that, with powerful pretrained language encoders, this transfer effect is sufficiently strong to permit decoding extended language-dependent motor behaviours, despite the shift in distribution of the agent's input. More generally, we hope that this contribution serves to bring research on text-based and situated language learning closer together. To facilitate this, we make available our dataset of natural instructions and referring expressions aligned to ShapeNet models.   While this study can be considered an interesting proof of concept for the SHIFTT approach to training linguistic deep RL agents via transfer-learning, there are many ways in which it can be improved and extended. As agents become better able to learn a wide range of conditional policies covering a larger set of motor behaviours, it will be instructive to scale the linguistic scope of the agent via the proposed technique, for instance to language involving verbs, or to questions and dialogue. Moreover, an important long-term objective is to apply SHIFTT to the training of robotic agents. There are also many alternative possibilities effecting the knowledge transfer from language model to agent that are not considered here. For instance, our approach involves freezing rather than fine-tuning the BERT encoder weights to our desired behaviour policy, to avoid overfitting, but techniques such as knowledge distillation~ could point to more elegant ways to learn jointly from text and environmental experiences. In addition, we have focused on BERT, but improvements may be possible by applying alternative general-purpose language encoders, such as GPT-2~, Roberta~ and Transformer XL~.   In sum, we have presented a conceptually simple recipe for transferring knowledge from a text corpus to a reinforcement-learning agent, and shown that the method permits zero-shot transfer from simulated to  natural language with surprising efficacy. We hope this opens new channels for research combining unsupervised  representation-learning with reinforcement learning, particularly at the intersection of language, vision and behaviour.      
"," Recent work has described neural-network-based agents that are trained with reinforcement learning  to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates , which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model , on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",29
"  Recent findings in neural machine translation  suggest that modern translation systems have some serious flaws. % This is based on observations such as: \one translations produced via beam search typically under-estimate sequence length , the ;  \two translation quality generally deteriorates with better approximate search , the ;  \three the true most likely translation under the model  decoding, is in some sense the obvious decision rule for predictions. % While this assumption makes intuitive sense and works well in unstructured classification problems,  it is less justified in NMT, where oftentimes the most likely translations % together account for very little probability mass,  % a claim we shall defend conceptually and provide evidence for in experiments.  % % % % % Unless the translation distribution is extremely peaked about the mode for every plausible input, criticising the model in terms of properties of its mode can at best say something about the adequacy of MAP decoding. % Unfortunately, as previous research has pointed out, this is seldom the case . Thus, pathologies about the mode  cannot be unambiguously ascribed to NMT as a model nor to MLE, and inadequacies about the mode cannot rule out the possibility that the model captures important aspects of translation well in expectation.  %  % %  % % %  %  %  % % % % In this work, we criticise NMT models as probability distributions estimated via MLE in various settings: varying language pairs, amount of training data, and test domain.  % We observe that the induced probability distributions represent statistics of the data well in expectation, and that some length and lexical biases are introduced by approximate MAP decoding. % We demonstrate that beam search outputs are rare events, particularly so when test data stray from the training domain. % % % % The empty string, shown to often be the true mode  , too is an infrequent event. % Finally, we show that samples obtained by following the model's own generative story are of reasonable quality, % which suggests we should base decisions on statistics gathered from the distribution holistically.  One such decision rule is minimum Bayes risk  decoding. % We show that an approximation to MBR performs rather well, especially so when models are more uncertain. % % % % %  To summarise:  we argue that \one MAP decoding is not well-suited as a decision rule for MLE-trained NMT; we also show that \two pathologies and biases observed in NMT are not necessarily inherent to NMT as a model or its training objective, rather, MAP decoding is at least partially responsible for many of these pathologies and biases; finally, we demonstrate that \three a straight-forward approximation to a sampling-based decision rule known as minimum Bayes risk decoding gives good results, showing promise for research into decision rules that take into account the distribution holistically.  % % %     % % % %  %  % %  % % %  %   % % % % % %  % % % [2]{\mathbb{E}_{#1}\left[#2\right]} {\theta_{\text{MLE}}} \DeclareMathOperator{\ELBO}{ELBO} \DeclareMathOperator{\Cat}{Cat} \DeclareMathOperator{\KL}{KL}  \DeclareMathOperator{\Exp}{Exp} \DeclareMathOperator{\Poisson}{Poisson} \DeclareMathOperator{\Gamm}{Gamma} \DeclareMathOperator{\Dir}{Dir} \DeclareMathOperator{\Multi}{Multinomial} \DeclareMathOperator{\DM}{DM} \DeclareMathOperator{\GammaPoisson}{GammaPoisson} \DeclareMathOperator{\NegBin}{NegBin}  \DeclareMathOperator{ \DeclareMathOperator*{\argmax}{argmax} %  % \NewDocumentCommand{\todob}{m}{\textcolor{purple}{[TODO B: #1]}} \NewDocumentCommand{{\textcolor{purple}{[B: #1]}} [1]{\todo[size=\tiny,color=red!20]{[0]{Flores\xspace}   % {#2}}  % {i)\xspace} {ii)\xspace} {iii)\xspace} {iv)\xspace} {v)\xspace}    {ix)\xspace} {x)\xspace}    {w.r.t.\xspace}  [1]{\textcolor{orange}{#1}} [1]{\textcolor{blue}{[W: #1]}} [1]{\textcolor{blue}{[W: #1]}~}  [1]{\todo[size=\tiny,color=green!20]{ \usepackage{coling2020} \usepackage{times} \usepackage{url} \usepackage{latexsym} \usepackage[textsize=tiny]{todonotes} \usepackage{booktabs} \usepackage{amsmath,amssymb,amsfonts,latexsym,dsfont,xspace} \usepackage{physics,nicefrac} \usepackage{multirow} \usepackage{verbatim} \usepackage{xcolor} \usepackage{graphicx} \usepackage{subcaption} \usepackage{array} \usepackage{pgfplotstable} \usepackage{wrapfig} \usepackage{cleveref} %  \makeatletter \pgfplotstableset{     discard if not/.style 2 args={         row predicate/.code={             \def\pgfplotstable@loc@TMPd{\pgfplotstablegetelem{##1}{#1}\of}                                         \author{Bryan Eikema \\   University of Amsterdam \\   b.eikema@uva.nl \\\And   Wilker Aziz \\   University of Amsterdam \\   w.aziz@uva.nl \\}  \date{}       Recent studies have revealed a number of pathologies of neural machine translation  systems. Hypotheses explaining these mostly suggest there is something fundamentally wrong with NMT as a model or its training algorithm, maximum likelihood estimation . Most of this evidence was gathered using maximum   decoding, a decision rule aimed at identifying the highest-scoring translation,   % % % % % % .          %     %     %     %     %     %     % }          %                In this work, we discuss the inadequacy of the mode in NMT and question the appropriateness of MAP decoding. We show that for such a high dimensional problem as NMT, the probability distributions obtained with MLE are spread out over many translations, and that the mode often does not represent any significant amount of probability mass under the learnt distribution. We therefore argue that MAP decoding is not suitable as a decision rule for NMT systems. Whereas beam search performs well in practice, it suffers from biases of its own , it shifts statistics away from those of the data  , and in the limit of perfect search it falls victim to the inadequacy of the mode.        Instead, we advocate for research into decision rules that take into account the probability distribution more holistically.  Using ancestral sampling we can explore the learnt distribution in an unbiased way and devise sampling-based decision rules. Ancestral sampling does not suffer from non-admissibility, and, if the model fit is good, there is no distribution shift either.                We further argue that criticisms about properties of the mode of an NMT system are not representative of the probability distributions obtained from MLE training.   While this form of criticism is perfectly reasonable where approximations to MAP decoding   are the only viable alternative, there are scenarios where we ought to criticise models as probability distributions. For example, where we seek to correlate an observed pathology with a design decision, such as factorisation, or training algorithm. In fact, we argue that many of the observed pathologies and biases of NMT are at least partially due to the use of  MAP decoding, rather than inherent to the model or its training objective.    Even though NMT models spread mass over many translations, we find samples to be of decent quality and contain translations that outperform beam search outputs even for small sample sizes, further motivating the use of sampling-based decision rules. We show that an approximation to one such decision rule,   MBR decoding, shows competitive results. This confirms that while the set of likely translations under the model is large, the translations in it share many statistics that correlate well with the reference.  MLE-trained NMT models admit probabilistic interpretation and an advantage of the probabilistic framework is that a lot of methodology is already in place when it comes to model criticism as well as making predictions. We therefore advocate for criticising NMT models as probability distributions and making predictions using decision rules that take into account the distributions holistically. We hope that our work paves the way for research into scalable sampling-based decision rules and motivates researchers to assess model improvements to MLE-trained NMT systems from a probabilistic perspective.                                                                                     
"," Recent studies have revealed a number of pathologies of neural machine translation  systems. Hypotheses explaining these mostly suggest there is something fundamentally wrong with NMT as a model or its training algorithm, maximum likelihood estimation . Most of this evidence was gathered using maximum   decoding, a decision rule aimed at identifying the highest-scoring translation, \ie the mode. We argue that the evidence corroborates the inadequacy of MAP decoding more than casts doubt on the model and its training algorithm. In this work, we show that translation distributions do reproduce various statistics of the data well, but that beam search strays from such statistics. We show that some of the known pathologies and biases of NMT are due to MAP decoding and not to NMT's statistical assumptions nor MLE. In particular, we show that the most likely translations under the model accumulate so little probability mass that the mode can be considered essentially arbitrary. We therefore advocate for the use of decision rules that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.",30
" Spoken Dialogue Systems~ are widely used as assistants to help users in processing daily affairs such as booking tickets or reserving hotels. A typical dialogue system consists of three key components: spoken language understanding~, dialogue manager~, and natural language generation~.  Within the procedure above, dialogue state representation is crucial since DM needs a precise representation of the present dialogue state to select an appropriate action.  There are mainly two types of approaches for dialogue state representation: the state tracking approach and the hidden vector approach. The state tracking approach is to use a belief state tracker to extract the ontology from users' utterances. Those extracted ontology, known as slots, are used as the state representation.  The hidden vector approach, more popular utilized in end-to-end dialogue systems, is to use the hidden vector compressed from users' utterance as the state presentation.  %Spoken Dialogue Systems are widely used as assistants to help users in processing daily affairs. The architecture of a dialogue system usually consists of following key components: A Spoken Language Understanding that understands the users' intents, a Natural Language Generator that generates human-readable text responses, and a dialogue manager that captures the dialogue states and makes decisions for the response.  %Tasks of the dialogue system often vary from searching for a restaurant to booking several flight tickets. The demand for finishing tasks in diverse situations requires the SDS to have the ability to handle different domains of the dialogue. %Dialogue state representation is an essential part of building a successful dialogue system. A generally used method is to use the human-defined state representation, where the state records necessary information such as indispensable slot values the system needs. The dialogue policy then makes actions associating with the state.  In the real dialogue, a belief state tracker is usually adopted to recognize the ontology from the user's text and summarize the states by the ontology. %Another optional method is to use a hidden state representation. The text is compressed to hidden vectors from the raw utterance. The model summaries the context and dialogue acts are making from the hidden states. In this setting, the dialogue system is pure an end to end model with only text as the input.  The aforementioned approaches are almost satisfactory in a single domain setting dialogue task such as tickets booking since the number of the slots, and the entities are relatively small in a single domain setting.  Nevertheless, the performance of existing dialogue state representation approaches deteriorates rapidly when it comes to multi-domain setting. For the state tracking approach, the ontology space grows enormous in multi-domain dialogue systems. This growing ontology space leads to the accuracy degeneracy of dialogue state tracking, which limits the performance of dialogue systems. As for the hidden state representation approach, the human-labelled semantic information cannot be fully used. Besides, a hidden state representation is almost a black box which makes the dialogue system incomprehensible and hard to debug. The poor-quality and inaccurate multi-domain dialogue state representation severely limits the quality of multi-domain dialogue policy and further affects the overall performance of dialogue systems.   To build a satisfactory multi-domain dialogue system, we propose a model named Multiple Teachers Single Student~ to subtly circumvent the complex multi-domain dialogue state representation problem and learn a quality dialogue policy in a multi-domain setting. We use multiple teacher models~ to teach a student model to become a multi-domain dialogue expert.  Our intuition comes from a real-life scenario in which a student has to learn many subjects such as Math, History and Science~.  %It is demanding for a student to tackle hard problems from different subjects by directly learning from textbooks.  Usually, there is a full-time teacher regarding each subject. These teachers impart their professional knowledge of their respective subjects to a student. In other words, this student acquires a comprehensive understanding of all subjects by learning from these teachers. This well-educated student can achieve high performance in all subjects.  This MTSS learning pattern is well-suited for the multi-domain dialogue systems.  % In each domain, a specific teacher model learns more precisely about dialogue state and dialogue policy. By learning from these domain-specific teachers, a universal student model learns multi-domain knowledge and labelled semantic information.  More specifically, firstly, for each domain of a multi-domain dialogue corpus, an individual teacher model is employed to learn dispersed dialogue knowledge and semantic annotations as the extra information in this single domain. Each domain teacher takes dialogue history utterances and human-labelled semantic from its corresponding domain as the dialogue state. Based on these domain-specialized dialogue state representation, these customized teachers can acquire a high-quality dialogue policy.  Secondly, these well-trained domain-specific teachers in first step impart their learnt knowledge and dialogue policy to a universal student model through text-level guiding and policy-level guiding.  %\textcolor{red}{I SHOULD ADD SOMETHING TO ILLUSTRATE OUR CONTRIBUTION IN POLICY LEARNING;} We use knowledge distillation to implement this guiding process.   By learning from these domain-specific teachers, the universal student model acquires multi-domain knowledge and labelled semantic information and it finally becomes a multi-domain dialogue expert.  % The above methods work reasonably well in a single domain setting dialogue task. Things become complicated when it comes to the multi-domain setting. As the number of the domain increases, the user goal may involve several domains, and the ontology of the dialogue becomes vast. The amount of the slots remain to recognize grows several times than the origin. The accuracy of the state tracker decreases rapidly in such a situation. The accumulated errors in the recognized states affect bound the performance of a dialogue model. Even if you use hidden state representation, building a multi-domain dialogue model is still challenging. In a multi-domain setting, To learn the user intent with the latent states is more complicated than with the labelled semantic. Moreover, the latent states are always hard to understand and debug for humans.     %To successfully build a dialogue system in a multi-domain situation, we are inspired by the learning progress in real life. In school education, a student is requested to learn more than one subject at the same time. And at each subject, there is a teacher who has proficiency in his subject.  The student learns subjects from the respective teachers as is shown in Figure . The teacher tackle the hard problem in the subject and transfer the knowledge in an easy to understand way. Finally, the student reaches high achievements in all subjects. We find that such a pattern is suitable in a multi-domain setting to building a dialogue model. In each domain, a teacher learns the ontology, the state information and the dialogue methods in the limited range. A student model then learns all domains' knowledge directly from every domain teacher.   %Both two methods above have their restraints. An attached state tracking model bounds the dialogue model with states as input. The errors accumulate in the states tracking processing, especially in a multi-domain situation where the space of ontology is ample. On the other hand, training a dialogue model without using human-defined states leads to poor performance. Moreover, the latent states are always hard to understand and debug for humans.    %In this paper, we introduce a universal dialogue generation system dealing with multi-domain dialogues. Rather than using an external tracker to recognize the ontology, our model straightly generates hidden states from raw text. To make proper responses and benefit from well labelled semantic on training data, we brought a teacher-student framework. Multiple teachers are applied in every single domain to learn the dispersed dialogue knowledge and labelled extra semantic information. Then we extract and merge the well-learned knowledge and policy methods from individual teacher models into a universal student model. The framework ensures the student studies the well-learned responses during conversations. Our model takes full advantage of the labelled data and gets rid of the effects from the performance of an outside belief tracker.   To sum up, the contributions are summarized as follows:        %The rest of this paper is organized as follows: Section is a brief literature review. We detail our proposed multi-domain dialogue system in Section. Section is about how those multiple teacher models impart their knowledge to the student model. The experiment settings and results is presented in Section, after which we conclude in Section.   %   In this paper, we propose a novel approach to building a high-quality multi-domain dialogue system based on a teacher-student framework. We utilize multiple domain-specific teacher models to help a single student model become a multi-domain dialogue expert, which circumvent the knotty multi-domain dialogue state representation problem. To fully take advantage of the knowledge of the teacher models, we creatively make the teacher model impart their knowledge to the student in both text-level and policy-level.  To discover the potential of the teacher-student framework, we would focus on adopting the framework to the SOTA dialogue models in our future work.    In the paper, we propose a multi-domain dialogue generation model trained with a teacher-student framework. The model takes only raw text as input and takes full advantage of the human-labelled states during training. The model behaves better than the one using an external state tracker, with great improvements in the success rate during a conversation.   The problem exists in our model that it focuses on the text generation during the conversation, and takes no consideration of the knowledge base querying. So our model cannot be regarded as a complete dialogue system. However, we don't th ink it is unable to process. Adding an extra component, such as a memory network can solve the problem.  
"," %The Motivation of this paper is dialogue state representation? But after reading this paper, I am quite confused. It seems that teachers teach nothing about the dialogue state representation. They, nevertheless, impart their learnt dialogue policy by learning a better dialogue state representation in their domain.  \\ How to build a high-quality multi-domain dialogue system is a challenging work due to its  complicated and entangled dialogue state space among each domain, which seriously limits the quality of dialogue policy, and further affects the generated response. In this paper, we propose a novel method to acquire a satisfying policy and subtly circumvent the knotty dialogue state representation problem in the multi-domain setting. Inspired by real school teaching scenarios, our method is composed of multiple domain-specific teachers and a universal student. Each individual teacher only focuses on one specific domain and learns its corresponding domain knowledge and dialogue policy based on a precisely extracted single domain dialogue state representation. Then, these domain-specific teachers impart their domain knowledge and policies to a universal student model and collectively make this student model a multi-domain dialogue expert. Experiment results show that our method reaches competitive results with SOTAs in both multi-domain and single domain setting.   % How to build a high-quality multi-domain dialogue system is a challenging work due to its knotty state representation problem.  % Neither the state tracking approach nor the hidden vector approach can handle the complicated and entangled state space of multi-domain dialogue systems,  % Dialogue state representation is a bottleneck for multi-domain dialo which seriously affects the multi-domain dialogue system policy.  % In this paper, we propose a Multiple Teachers Single Student~ model to acquire a superior dialogue policy.  % Instead of directly tackling the complicated dialogue state representation problem in multi-domain setting, we acquire a superior multi-domain dialogue policy by using multiple teachers to learn a precise dialogue policy in each domain. These domain teachers impart their dialogue policy of their corresponding dialogue domain to a single student model and collectively make this student model a multi-domain dialogue expert.    % To our best knowledge, this is the first work to utilize MTSS model to cope with the challenging multi-domain dialogue system problem. Experiment results shows that our method outperform STOAs in both multi-domain and single domain setting.  % Dialogue systems dealing with multi-domain tasks are highly required. How to record the dialogue states remains a crucial problem in a task-oriented dialogue system. A general method is to use human-defined features as dialogue states and apply a state tracker to extract these features from users' utterances. However, in a multi-domain setting, the accuracy of a state tracker is low due to the vast amount of the features. The external state tracker limits the performance of the dialogue system. In this paper, we propose a multi-domain dialogue generation model that needs no external state trackers and still benefits from human-labelled semantic data. We use a teacher-student framework. Firstly, several teacher models are trained in their respective domains, learning dialogue policies from labelled states. Then the learned knowledge and experience are merged and transferred to a universal student model. The student model takes only raw utterance as its input. Experiments show that the dialogue system trained under our framework outperforms the one uses a belief tracker.",31
"  % { MT is used in an increasing number of applications, with more users relying on it for practical needs. However, the quality of MT is still not consistent across language pairs, domains and datasets. Furthermore, neural MT  systems can be deceptive as they are able to generate fluent translations that are  different in  meaning from the original  . In this context, having a mechanism to predict and inform the user on the extent to which a given MT output can be trusted or relied upon becomes crucial.  Quality estimation  is an active field of research where the goal is to predict MT quality at test time when no gold standard human translation is available to be used for comparison. Influential MT evaluation campaigns, such as the WMT conference, have been benchmarking QE system submissions every year since 2012. Current neural-based QE systems have  been shown to provide highly accurate predictions of MT quality . However, such systems %are resource-heavy and  require large amounts of parallel data for pretraining, as well as in-domain translations annotated with quality labels for training.  %In fact, neural-based QE systems and NMT use similar architectures and training procedures, and therefore current approaches to QE suffer from  some of the main issues in NMT, such as a drastic decrease in performance for out-of-domain data and in low-resource settings.  As an alternative, in this paper we explore unsupervised approaches to QE. We posit that NMT models offer a rich source of information on translation quality that has been so far disregarded in the work on QE for NMT outputs. % Translation probabilities and language model probabilities have been  used as  indicators of model confidence to predict translation quality for statistical MT systems  in feature-based QE approaches along with many other indicators. NMT systems have however been treated as a black box. Neural-based QE approaches ignore model-specific information. This is due, on the one hand, to the complexity of the underlying NMT models. On the other hand, as has been shown in the recent work , despite remarkable success in a wide variety of machine learning tasks, deep neural networks are not well calibrated and produce overconfident predictions.  We test these observations in the first attempt to use several model-internal NMT information as quality predictors. We study how the reliability of NMT model predictive probabilities changes depending on model architecture, search algorithm and the extent of training. Different from the few existing work on calibration in NMT , we collect human judgments of translation quality for this analysis. %We show that some of the models commonly used in practice produce reliable confidence estimates. % The models and the data produced for this study will be made publically available.  In addition to {. %which can be seen as a way of improving the reliability NMT model confidence estimates based on uncertainty quantification methods for deep Neural Networks .  %\todo{define confidence/uncertainty: model/epistemic uncertainty vs aleatoric or input/output uncertainty} Uncertainty in deep learning typically relies upon Bayesian NNs  that learn a distribution over model parameters and use it to represent model uncertainty ,  this will strongly correlate with the perceived quality of generated translations.   % To that end, we devise a set of unsupervised quality indicators that can be produced as a by-product of NMT decoding.   Finally, besides softmax probability distribution, we also use attention mechanism to extract information on model confidence \todo{confidence or uncertainty - maybe move earlier}.  %Attention weights have been used for confidence estimation in RNN-based NMT .  Previous studies have analysed the interpretability of attention in Transformer models , however, none of them considers attention as a source of information on model confidence    . %This is the first attempt to explore attention in state-of-the-art Transformer models, as it involves a more complex attention mechanism.   %Apart from few approaches that use attention weights to estimate word-level confidence for recurrent NNs , %\mf{Maybe we should formulate contributions in a bullet-point list?}  Our {  New uncertainty-aware indicators from NMT models and a thorough evaluation on how they correlate with human judgments of translation quality;  The first  attempt at analysing the attention distribution for the purposes of unsupervised QE in Transformer models;  A new dataset with 6 language pairs, machine translations by various NMT models and human judgements of quality. \textcolor{blue}{Our results suggest that unsupervised QE indicators computed from well-calibrated NMT internal model information can outperform state-of-the-art supervised QE approaches when the quality distribution of the NMT translations is not too skewed.}       We note that while the paper covers QE at sentence level, the extension of our unsupervised metrics to word-level QE would be straightforward.  % Neural MT systems generate a probability distribution over language vocabulary for each generated target word. We suggest that well calibrated predictive probabilities from the MT model are strongly correlated with the quality of generated translations.   % Due to non-linearity and complexity of the functions, NNs are poor at quantifying uncertainty.  % One such approach called Monte Carlo dropout  consists in using dropout at test time. Due to its simplicity, this method has been successfully applied to a variety of machine learning tasks .  % if a neural MT model is capable of representing uncertainty, its predictive probabilities will be a good indicator of translation quality.  % Very few studies have looked at model uncertainty in the MT task . Neural MT translation have achieved very good results, but its performance is not stable, as it can produce near human quality translations but also nonsensical results. Moreover, such models can generate well-formed sentences in the target language, which completely distort the meaning of the source. In this context, quality estimation gains particular importance, as it can inform the user to what extent they can trust a particular translation.   % In this paper, we propose to look at unsupervised quality indicators to improve on the above issues, as well as increase the interpretability of neural MT models. On the one hand, we leverage the information that can be extracted from the neural MT model itself in order to produce quality estimates. On the other hand, we look at cross-lingual representation thus addressing QE as a cross-lingual similarity task.       QE is crucial for NMT, given the increasing use of these systems in a wide variety of contexts. This poses new challenges for QE, as it needs to provide accurate and fast quality estimates for texts in all kinds of genres, styles, topics, etc. QE has been typically addressed as a supervised learning task with large resource-heavy systems providing highly accurate results. However, such systems are limited by access to annotated in-domain data.  for supervised training.   We have proposed to view QE task from a different perspective.  We have devised an unsupervised approach to QE where no training or access to any additional resources besides the MT system is required. Besides exploiting softmax output probability distribution and the entropy of attention weights from the NMT model, we leverage uncertainty quantification for unsupervised QE. We show that, if carefully designed, the indicators extracted from the NMT system constitute a rich source of information, competitive with supervised QE methods.  We analyzed how different MT architectures and training settings affect the relation between predictive probabilities and translation quality. We showed that improved translation quality does not necessarily imply a stronger correlation between translation quality and predictive probabilities. Model ensemble have been shown to achieve optimal results both in terms of translation quality and when using output probabilities as an unsupervised quality indicator.  Finally, we created a new multilingual dataset for QE covering various scenarios for MT development including low- and high-resource language pairs. Both the dataset and the MT models needed to reproduce the results of our experiments are available at .   This work can be extended in many directions. First, our sentence-level unsupervised metrics could be adapted for QE at other levels . Second, the proposed metrics can be combined as features in supervised QE approaches. Finally, other methods for uncertainty quantification, as well as other types of uncertainty, can be explored.   to achieve better performance.   We note that while the paper covers QE at sentence level, the extension of our unsupervised metrics to QE at other levels  would be straightforward.    By contrast to the few recent works that discuss calibration in NMT, we have analysed the relation between system predictive probabilities and human judgments of MT quality.    We study the calibration of neural MT probability estimates. We introduce a variety of unsupervised quality measurements. By contrast to the few existing works on the topic, we use human judgments to evaluate the accuracy of model confidence estimates.    We have approached QE from uncertainty quantification perspective. We have proposed various ways of quantifying uncertainty based on the softmax output, with softmax output entropy giving the best results. We further leveraged methods from approximate Bayesian inference showing that predictive probabilities obtained through model ensembling and MC dropout achieve the best results in predicting translation quality.
","  % Quality Estimation  is an important component in making Machine Translation  useful in real-world applications. Existing approaches require large amounts of expert annotated data, computation and time for training. As an alternative, we devise an unsupervised approach to QE where no training or access to additional resources besides the MT system itself is required. By exploiting methods for uncertainty quantification, we achieve substantial gains in correlation with human judgments of quality, \revised{rivalling state-of-the-art supervised QE models.} % significantly outperforming state-of-the-art in supervised QE. % system. % To evaluate our approach we collect the first dataset that enables further work on unsupervised QE.  Quality Estimation  is an important component in making Machine Translation  useful in real-world applications, as it is aimed to inform the user on the quality of the MT output at test time. Existing approaches require large amounts of expert annotated data, computation and time for training. As an alternative, we devise an unsupervised approach to QE where no training or access to additional resources besides the MT system itself is required. Different from most of the current work that treats the MT system as a black box, we explore useful information that can be extracted from the MT system as a by-product of translation. By employing methods for uncertainty quantification, we achieve very good correlation with human judgments of quality, rivalling state-of-the-art supervised QE models. To evaluate our approach we collect the first dataset that enables work on both black-box and glass-box approaches to QE.",32
"  Sanskrit is a fusional Indo-European language with rich morphology, both at the inflectional and derivational level. The language relies heavily on morphological markers to determine the syntactic, and to some extent the semantic roles, of words in a sentence. There exist limited and partly incompatible solutions  for morphological tagging of Sanskrit that heavily rely on lexicon driven shallow parsers and other linguistic knowledge. However recently, neural sequential labelling models have achieved competitive results in morphological tagging for multiple languages . We therefore explore the efficacy of such models in performing morphological tagging for Sanskrit without access to extensive linguistic information.  Most recent research treats morphological tagging as a structured prediction problem where the morphological class of a word is either treated as a monolithic label or as a composite label with multiple features .  model the morphological tags as a sequence of individual morphological features. Recently,  proposed to generate this sequence of morphological features using a neural encoder-decoder architecture.  shows a significant improvement in performance for morphological tagging in Sanskrit by using a monolithic tagset with recurrent neural network based tagging model. In systems using monolithic labels, multiple feature values pertaining to a word are combined to form a single label , which leads to data sparsity for morphologically rich languages such as Czech, Turkish and Sanskrit. The sparsity issue can be addressed by using composite labels which model the internal structure of a class as a set of individual features .  use a neural factorial CRF to model inter-dependence between individual categories of the composite morphological label.  % These systems are either designed as structured prediction models with factor graphs to capture the dependencies between the features, or as sequential models where hierarchically higher features are predicted conditioned on earlier ones. For composite labels, independent classifiers trained under a multi-task setting with shared parameters, were explored as well. %  show a significant improvement in performance for morphological tagging in Sanskrit  by using a monolithic tagset with recurrent neural network based tagging model.   % Morphological tagging is generally treated as a sequence labelling problem , and the morphological class of a word to be predicted is either treated as a monolithic label or as a composite label. In systems using monolithic labels, multiple feature values pertaining to a word are combined to form a single label , which obviously leads to data sparsity for morphologically rich languages such as Czech, Turkish and Sanskrit.    %To focus on the sparsity issue, systems which use composite labels were proposed. This enables the systems to capture  % The sparsity issue can be addressed by using composite labels which model the internal structure of a class as a set of individual features . These systems are either designed as structured prediction models with factor graphs to capture the dependencies between the features, or as sequential models where hierarchically higher features are predicted conditioned on earlier ones. For composite labels, independent classifiers trained under a multi-task setting with shared parameters, were explored as well.  % In this paper, we compare neural architectures The aforementioned approaches differ strongly on all system levels. However, as the decision for monolithic vs. composite labels is one of the central design choices when tagging morphologically rich languages, we use Sanskrit as a test case for a systematic evaluation for this choice. For this evaluation, we consider several neural architectures with different modelling principles. For the monolithic tag model, the neural architecture is based on a bi-directional LSTM with a linear CRF layer stacked on top of it . For composite labels, we explored a neural generation model that generates a sequence of morphological features for each word in the input sequence. In order to explicitly capture the inter-dependencies between the morphological features, we use a model based on a factorial Conditional Random Field  . Additionally, independent classifiers trained under a multi-task setting with sharing of parameters are also explored .  Our experiments specifically focus on the following problems and questions: %in their architectures, input representation, learning and inference. But, we find that, all the models we experimented with achieve competitive results when provided with sufficient training data. In fact, we can find uniformity in the mispredictions made by these systems. We specifically performed our experiments on the following three aspects:  [leftmargin=*]      We will show that syncretism, i.e., inflected forms of a lemma that share the same surface form for multiple morphological tags, is the major source of mispredictions. We evaluate if and how models with monolithic and composite labels deal with this phenomenon.          %What is the extent of misprediction due to syncretisms in the neural sequence labelling models as noted in ? Does breaking down the tag into component categories, reduce the extent of mispredictions due to syncretism?                     For models with composite labels, it should be possible to predict morphological classes which were not seen in the training data. Our experiments show that the performance of the systems remains more or less the same irrespective of the neural architecture.           This raises an important point: Models that perform marginally better in terms of evaluation metrics are supposedly not superior, since we see similar performances on special test sets targeting particular statistical phenomenon  and  linguistic phenomenon .         %   We evaluate how the position of the first system error affects subsequent labelling decisions, both on sentence and word level.%; this means if a system is able to recover from previously made errors.  %the performance of the systems based after which the first error is made. Again, the choice of architecture does not affect the results.      %Of recently, neural sequential models have achieved state of the art results in morphological tagging tasks for multiple languages . %All the aforementioned neural models rely on the Universal Dependency tagset. Hence these are language agnostic models and even facilitate cross lingual learning  for the task.    %In this work, for which there . In the process, we perform a detailed analyses of how well the systems be modified to accommodate the language specific nuances, which we explain below.% & {{Fin. verb}} & {{Participle}} & {{Compound}} & {{Others}} \\ \hline Tense   & 18 & &       In this work, we evaluated various neural models for morphological tagging of Sanskrit, concentrating on models that are capable of using composite labels. We find that all the composite label models outperform MonSeq by significant margins. These models, with an exception to MTL-Shared, achieve overall competitive results when enough training data is available. A major problem for all the sequence labelling models studied in this paper is syncretism of morphological categories, which should constitute the main focus of future research.     While the composite label models have an obvious advantage of predicting unseen labels in the training data, w, although the gains in Macro-F1 scores for the composite label models in comparison to the MonSeq model are statistically significant.      \fi    File acl2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{acl2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}ashim@cs.utah.edu, amrk@itu.dk, \usepackage{graphics} \usepackage{tabularx} \usepackage{url} \usepackage{enumitem} \usepackage{hyperref}   \usepackage{amssymb}  http://ctan.org/pkg/amssymb \usepackage{pifont}  http://ctan.org/pkg/pifont   {\ding{55}}      Enter the acl Paper ID here     \title{Evaluating Neural Morphological Taggers for Sanskrit}   \author{Ashim Gupta\textsuperscript{1}, Amrith Krishna\textsuperscript{2}, Pawan Goyal\textsuperscript{3}, Oliver Hellwig\textsuperscript{4} \\   \textsuperscript{1}School of Computing, University of Utah\\   \textsuperscript{2}ITU Copenhangen, \textsuperscript{3}Dept. of Computer Science and Engineering, IIT Kharagpur\\   \textsuperscript{4}University of Z鐪塺ich, IVS\\     pawang@cse.iitkgp.ac.in, oliver.hellwig@uzh.ch\\{\tt   pawang@cse.iitkgp.ac.in, oliver.hellwig@uzh.ch} \\   }  \date{}       Introduction        
","  Neural sequence labelling approaches have achieved state of the art results in morphological tagging. We evaluate the efficacy of four standard sequence labelling models on Sanskrit, a morphologically rich, fusional Indian language. As its label space can theoretically contain more than 40,000 labels, systems that explicitly model the internal structure of a label are more suited for the task, because of their ability to generalise to labels not seen during training. We find that although some neural models perform better than others, one of the common causes for error for all of these models is mispredictions due to syncretism.\footnote{Code and data available at \url{https://github.com/ashim95/sanskrit-morphological-taggers}}",33
"     The vast majority of existing named entity recognition  methods focus on a small set of prominent entity types, such as persons, organizations, diseases, and genes, for which labeled datasets are readily available. There is a marked lack of studies in many other domains, such as e-commerce, and for novel entity types, e.g.\ products and components. %          The lack of annotated datasets in the e-commerce domain makes it hard to apply supervised NER methods. An alternative approach is to use dictionaries, but freely available knowledge resources, e.g.\ Wikidata or YAGO, contain only very limited information about e-commerce entities. Manually creating a dictionary of sufficient quality and coverage would be prohibitively expensive.      This is amplified by the fact that in the e-commerce domain, entities are frequently expressed as complex noun phrases instead of proper names. Product and component category terms are often combined with brand names, model numbers, and attributes , which are almost impossible to enumerate exhaustively.      In such a low-coverage setting, employing a simple dictionary-based approach would result in very low recall, and yield very noisy labels when used as a source of labels for a supervised machine learning algorithm. To address the drawbacks of dictionary-based labeling,  propose a positive-unlabeled  NER approach that labels positive instances using a seed dictionary, but makes no label assumptions for the remaining tokens. %     The authors validate their approach on the CoNLL, MUC and Twitter datasets for standard entity types, but it is unclear how their approach transfers to the e-commerce domain and its entity types. %                                                                                 We adopt the PU algorithm of  to the domain of consumer electronic product descriptions, and evaluate its effectiveness on four entity types: , ,  and . Our algorithm bootstraps NER with a seed dictionary, iteratively labels more data and expands the dictionary, while accounting for accumulated errors from model predictions. %     During labeling, we utilize dependency parsing to efficiently expand dictionary matches in text.     Our experiments on a novel dataset of product descriptions show that this labeling mechanism, combined with a PU learning strategy, consistently improves F1 scores over a standard BiLSTM classifier. Iterative learning quickly expands the dictionary, and further improves model performance. The proposed approach exhibits much better recall than the baseline model, and generalizes better to unseen entities.                               In this work, we introduce a bootstrapped, iterative NER model that integrates a PU learning algorithm for recognizing named entities in a low-resource setting. Our approach combines dictionary-based labeling with syntactically-informed label expansion to efficiently enrich the seed dictionaries. Experimental results on a dataset of manually annotated e-commerce product descriptions demonstrate the effectiveness of the proposed framework.      
","     Named Entity Recognition  in domains like e-commerce is an understudied problem due to the lack of annotated datasets. Recognizing novel entity types in this domain, such as products, components, and attributes, is challenging because of their linguistic complexity and the low coverage of existing knowledge resources. To address this problem, we present a bootstrapped positive-unlabeled learning algorithm that integrates domain-specific linguistic features to quickly and efficiently expand the seed dictionary. The model achieves an average F1 score of $72.02\%$ on a novel dataset of product descriptions, an improvement of $3.63\%$ over a baseline BiLSTM classifier, and in particular exhibits better recall . %",34
" Recently spoken dialog systems have been a popular research topic in human language technology  area with various applications. Among these applications, dialog systems for clinical conversation  is a rising direction for its widespread and impactful use . Medical bot assists medical practitioners to converse with patients, collect information about their symptoms, physical and mental conditions, or even make suggestions on diagnosis. The bot has significant potential to make the diagnostic procedure more efficient. An example for automatic diagnosis dialog system is shown in Figure . Starting from a self-report, the medical bot collects and distills symptom information before it makes the disease prediction. % While clinical diagnosis are made by the doctors, according to their medical knowledge and symptom information of patients, collecting the information, sometimes via medical examinations, heavily relies on self-report of patients  and the follow-up dialog between doctor and patients. With the self-report and follow-up dialog, doctors distill information about the patient and make clinical decisions. A medical bot for automatic conversational diagnosis systems simulate such process and predict the disease based on collected information . An example of such dialog is shown in Figure . %  % % [] %  %  % ll@{}} % \toprule % \multicolumn{2}{c}{1. Self Report}                        \\ % \multicolumn{2}{l}{The baby has Sneeze and Runing Nose}         \\ \midrule % \multicolumn{2}{c}{2. Dialog}                             \\ % Agent: Does baby cough?               & User: Yes               \\ % Agent: Does baby have fever?          & User: Not sure          \\ \midrule % \multicolumn{2}{c}{3. Diagnosis}                          \\ % \multicolumn{2}{l}{Baby might have upper respiratory infection} \\  %   One of the core challenges of building such a dialog system is design and train a dialog policy manager that can reason and decide the action to take based on the understanding of user intentions and conversation context. It is more challenging for medical bot because of the need of integrating medical knowledge for reasoning and decision making.  proposed a reinforcement learning  framework with a multi-layer perceptron deep Q networks  .  extended the study by enhanceing the DQN with hand-crafted features among diseases and symptoms, generated from the training set. However, both models cannot directly learn from real doctor-patient  conversations.  For RL agent to fully explore the entire action space, the agent can only learn by interacting with rule-based user simulators, which can not learn from the dialog histories between real doctors and patients.  Another difficulty faced by RL-based manager is adapting trained policy to new tasks , since adaptation data is usually limited and hard to collect. On the other hand, Meta-learning algorithms are proposed to improve the model performance when training or adaptation data is limited. . Since both many- and few-shot learning heavily depend on the quality of learned representations, these studies encouraged us to combine meta-learning and deep reinforcement learning models to improve the dialog agents for automatic diagnosis in both scenarios by learning better representations of dialog actions and domain knowledge.   In this work, we propose prototypical Q networks  borrowing the ideas of prototypical networks  and matching networks . We evaluate the model in the medical dialog domain, since it is important and medical conversation usually suffers from data scarcity. The model makes fully use of real doctor-patient conversations by calculating prototype disease and symptom embeddings by encoding the dialog histories in the training set. Experiment results have shown that by learning a shared prototype embedding space, the ProtoQN outperforms MLP-DQN under both experiment settings. The experiments in this paper will focus on medial dialog to show benefit of proposed method, but we believe the conclusion can be generalized to other domains, since we do not use handcrafted features or external domain specific information. %     In this work, we propose a novel dialog management model, prototypical Q network, for supervised and few-shot dialog policy learning. We apply this model in the area of automatic conversational diagnosis. Experiments showed that the ProtoQNs outperforms the DQN model in both supervised and few-shot settings. In the supervised setting, ProtoQNs achieve results comparable to SOTA without using domain-specific features. As for the few-shot experiment, ProtoQN learns new diseases using few training samples without forgetting previously learned, and achieves SOTA. The model also shows less degradation as we injecting noise to conversation. Our study suggests that modeling real conversations directly reinforces simulator-based dialog policy learning. Embeddings of dialog actions are shareable among tasks  and benefits the fast adaptation to new ones. Here we show promising results in medical domain. In future, we will investigate more adaptive models as well as different domains and corpora toward the goal of modeling new dialog tasks better and with fewer examples.   
"," Spoken dialog systems have seen applications in many domains, including medical for automatic conversational diagnosis. State-of-the-art dialog managers are usually driven by deep reinforcement learning models, such as deep Q networks , which learn by interacting with a simulator to explore the entire action space since real conversations are limited. However, the DQN-based automatic diagnosis models do not achieve satisfying performances when adapted to new, unseen diseases with only a few training samples. In this work, we propose the Prototypical Q Networks  as the dialog manager for the automatic diagnosis systems. The model calculates prototype embeddings with real conversations between doctors and patients, learning from them and simulator-augmented dialogs more efficiently. We create both supervised and few-shot learning tasks with the Muzhi corpus. Experiments showed that the ProtoQN significantly outperformed the baseline DQN model in both supervised and few-shot learning scenarios, and achieves state-of-the-art few-shot learning performances.  % Deep reinforcement learning models, for example, deep Q networks  have been applied for automatic conversational diagnosis. The models learn dialog policies by interacting with a user simulator, which stores symptoms and ground-truth disease labels. However, the models cannot learn from real doctor-patient conversations, and the DQNs do not achieve satisfying performance when adapted to new, unseen diseases with only a few training samples, especially when the disease is hard to be diagnosed. In this work, we propose the Prototypical Q Networks as the dialog manager for the automatic diagnosis systems. The model calculates prototypes with real dialogs between doctors and patients, making better use of the training data. We create both supervised learning and few-shot learning tasks with the Muzhi corpus. Experiments showed that the Prototypical Q Networks significantly outperformed the baseline DQN models in both supervised and few-shot learning scenarios, and achieves state-of-the-art few-shot learning performances.",35
"  Sequence-to-sequence models are nowadays a mainstream approach in Neural Machine Translation . Such models are typically applied at the subword level based on byte-pair encoding , originally proposed by~. This algorithm mitigates the problem of rare and out-of-vocabulary words that present a significant issue for word-level models. BPE builds a vocabulary of the most frequent subword units of different lengths, starting from a single character. Then, the input sentence is divided into a sequence of the longest possible subword fragments matching the constructed vocabulary. This approach is appealing because of its strong empirical results and computational efficiency. However, the segmentation is language- and corpus-dependent and, hence, requires considerable hyperparameter tuning. The problem of finding an optimal subword segmentation is especially challenging for multilingual and zero-short translation.  Another recent direction in NMT focuses on character-level translation. This approach is conceptually attractive because it can help mitigate the previously mentioned shortcomings of subword-level models. Character-level models do not rely on an explicit segmentation of the input sentence  and resort to plain characters as a sentence's basic units. As such, models are implicitly enforced to learn the inner structure of complex words. Hence, such models are more robust in the face of out-of-vocabulary words and in translating noisy and out-of-domain text. In comparison to subword-level models, they should be able to model more accurately rare morphological variants of words . In addition, character-level models may work better in some fine-tuning scenarios, where the amount of available data is challengingly small .  In spite of its conceptual elegance, the character-level approach also presents considerable challenges, that help explain why this approach didn't receive much attention yet. Character sequences are significantly longer and, consequently, more challenging to model. Moreover, the level of semantics in character-level representation becomes even more abstract and, hence, larger models with a highly non-linear mapping function are required. Finally, the training and decoding time for such models is much longer. However, some of these issues can be tackled through resorting to new NMT architectures.  have shown that is possible to train a character-level model, within a reasonable time span, by reducing the length of the source representation. We utilize this publicly available model, henceforth: CharRNN, as a baseline in our experiments.  We base our work on the well-known Transformer architecture , which has shown state-of-the-art performance on several language pairs in NMT. The model is intrinsically very attractive for the character level due to the high training speed it enables and its strong modelling capacity with respect to longer-range dependencies. The Transformer relies on self-attention and does not include any recurrence in training. Therefore, the Transformer can be fully parallelized during training, leading to considerable speed-ups in comparison to recurrent networks.  We aim to stimulate further research in this direction, by demonstrating the computational feasibility of training fast character-level models, even on a single GPU. Below, we propose a new variant  of a publicly available, Transformer-based network and apply it at the character level. Our models applies the same source length reduction technique as  and introduces a six-layer Transformer at the encoder and decoder sides instead of recurrent layers as in CharRNN, making our network fully parallelizable. The main contribution of the paper is two-fold:  We demonstrate the feasibility of training high-quality and fast character-level translation models, even on a single GPU;  we propose a novel character-level Transformer-based architecture that is at least as accurate as the Transformer, yet is up to ~34\% faster.     In this work, we applied Transformer from OpenNMT-py at character level and proposed a new character-level Transformer-based NMT architecture, CharTransformer. We evaluated it on four languages from WMT閳15 corpora and compared these models to the character-level architecture previously proposed by . We showed that character-level Transformer and CharTransformer outperform this model in all tasks. We demonstrated that character-level translation does not require weeks of training and expensive multi GPU training scheme anymore to strong results. In addition, we showed that CharTransformer performs comparably with character-level Transformer and is 34 percent faster. CharTransformer outperforms the subword-level model in FI-EN and shows competitive results in CS-EN. We conclude that both models are promising for character-level translation and can stimulate further research in this field.    We provide the repository\footnote{The link will be provided later} that contains the source code of the implemented models. In future research, we would like to investigate multilingual character-level translation with Transformer and CharTransformer. In addition, we will research different properties of these models. Finally, we should emphasize that our results that we might close the gap between character-level and subword-level NMT in a very near future.     \clearpage 
"," Neural machine translation  is nowadays commonly applied at the subword level, using byte-pair encoding. A promising alternative approach focuses on character-level translation, which simplifies processing pipelines in NMT considerably. This approach, however, must consider relatively longer sequences, rendering the training process prohibitively expensive. In this paper, we discuss a novel, Transformer-based approach, that we compare, both in speed and in quality to the Transformer at subword and character levels, as well as previously developed character-level models. We evaluate our models on 4 language pairs from WMT闁15: DE-EN, CS-EN, FI-EN and RU-EN. The proposed novel architecture can be trained on a single GPU and is $\sim$34\% faster than the character-level Transformer; still, the obtained results are at least on par with it. In addition, our proposed model outperforms the subword-level model in FI-EN and shows close results in CS-EN. To stimulate further research in this area and close the gap with subword-level NMT, we make all our code and models publicly available.",36
"  As information in everyday life is increasing, it becomes difficult to retrieve a relevant piece of information efficiently. Thus, a Question-Answering  system can be used to efficiently present the requested information. QA is an important application of NLP in real life which is a specific type of information retrieval method. The QA system makes an attempt to automatically find out the contextually and semantically correct answer for the provided question in text. Generally, the three components associated with the QA system are question classification, information retrieval, and answer extraction/generation.    Though QA does not come without its challenges, one approach to get a machine to answer questions is Reading Comprehension .Reading a text and answering from it is a challenging task for machines, requiring both understanding of natural language and knowledge about the world. The first step in the process to create such a system is a Question Answering dataset. A popular benchmark QA dataset created by Stanford University is known as the Stanford Question Answering Dataset or SQuAD.    We can conclude that despite using InferSent for generating contextual sentence representations, simple ML models could not perform well . Comparing the results with a model like BERT, we can conclude that for comprehensive sentence representations larger and more complex models that employ DL techniques work better. Different versions of BERT such as BERT-Large, Multilingual-BERT, ALBERT ,RoBERTa, etc. can be studied to derive better inferences and results on SQuAD.    \onecolumn 
"," This study aims to provide a comparative analysis of performance of certain models popular in machine learning and the BERT model on the Stanford Question Answering Dataset . The analysis shows that the BERT model, which was once state-of-the-art on SQuAD, gives higher accuracy in comparison to other models. However, BERT requires a greater execution time even when only 100 samples are used. This shows that with increasing accuracy more amount of time is invested in training the data. Whereas in case of preliminary machine learning models, execution time for full data is lower but accuracy is compromised.",37
" Sentence embedding has attracted extensive attention for semantic text similarity since its wide usage in a broad range of NLP tasks, such as document organization and indexing, community question answering systems and large-scale information retrieval. For examples, in the platforms of cQA, such as Quora or Yahoo Answers, the community-driven nature of these platforms leads to a large amount of question duplication, therefore it is eager to have a way to identify similar paraphrase, which can reduce clutter and greatly improve the user experience. In general, there are two main model families to address semantic text similarity: 1) cross-encoder model that directly computed the similarity between a sentence pair without learning a sentence embedding explicitly. 2) a sentence embedding learner that tried to map a sentence into a real-value fixed-size representation, ensuring the similar sentences kept closer and dissimilar ones kept further.   %In computational argumentation, a number of arguments are located in a widespread set of articles and it is demanding for the system to organize these sentences together efficiently to help debater generate a compelling argumentative narrative.  The main advantage of sentence embedding learner over cross-encoder models lies on the high efficiency in computation. thus it is in favor of many practical industry applications. For cross-encoder models, a sentence pair was required as input and the model can directly predict the target score, while no independent sentence embedding was computed. Given a collection of  sentences to find the most similar sentence, cross-encoder models had to compute the score for all one million pairs for each query. If BERT was employed, it would took around 20 hours on a Titan-X GPU for a single probe, which made this method completely infeasible despite its promising results.   Sentence embedding remains an open yet challenging research problem. The extremely large number of sentences poses a great challenge to learn a discriminating sentence embedding. Assume vocabulary size is  and the sentence length is , the total number of entire sentence space is in the exponential magnitude of . Specifically, recent works have explored many learning techniques with different training objectives to learn fixed-length sentence representations. Some works aims to extending the success of word embedding trained on large amounts of text in an unsupervised manner and tries to exploit the sentence context to learn general-purpose sentence embedding that can directly be utilized in various downstream NLP tasks. These works, such as Skip-Thoughts, FastSent, proposed to utilize an encoder-decoder architecture  to predict the contextual sentence from large corpus of articles. Nevertheless, these methods have severely suffered from insufficient training samples, thus the learned sentence embeddings were not performed well in many tasks. Some researchers started to input individual sentences into popular language models and derived fixed-size sentence embedding directly. The most commonly used approach was to perform an average pooling of whole output sequences or directly take the output of the first token . Unfortunately, this method has been shown the worse performance compared with averaging of simple word embedding in many works.   The rapid development of metric learning in computer vision stimulates a new direction for the researcher in NLP.  A few works  have attempted to combine typical sentence encoder with pair-based metric learning, such as contrastive loss in Siamese network and triplet loss in Triplet Network, to learn sentence embedding and achieved quite promising results up to date. Nevertheless, these works have at least two limitations. Firstly, due to the difficulty of evaluate the dissimilarity of sentence pairs, existing works performed pair sampling randomly or utilized the distance of sentences in the article to perform pair sampling. Therefore, these works may neglect many informative pairs. Secondly, how the sampled sentence pairs affected the learned sentence embedding remain unknown, while researches in computer vision had shown that harder pairs were more informative and can drive better representation learning.   In this paper, from the perspective of pair-loss optimization, our theoretical analyse verified the finding in computer vision and showed that the only two key components to learn sentence embedding were pair sampling and instance weighting. As far we know, this was the first work in NLP to clearly stating the limitation of existing works on sentence embedding that sentence pairs shall be selected and weighted with meticulous efforts. Our model, SentPWNet, tried to overcome these limitations by iteratively incorporating a locality preserving and weighted pair-loss optimizer. The framework of SentPWNet was shown in Figure. The novelty of SentPWNet were in two fold. Firstly, SentPWNet utilized the locality to measure the informative level of a sentence pair, similar to. The locality weight was computed as the relative similarity between the similarity of each pair and the similarities to the others. Sentence pairs that have complex locality usually are hard to differentiate, thus they can get a larger weight and contribute more during the optimization, and vise versa. Secondly, the learning of SentPWNet was in an iterative learning manner, meaning that our model can benefit from the hardest pair sentences in each round until model convergence. To evaluate the performance, we conducted extensive experiments on three public benchmarks~ on semantic text similarity and thematic relatedness tasks. The experiment results showed that SentPWNet is superior to existing sentence embedding with a marginal improvement on all tasks. Furthermore, with the popularity of local life services, such as Yelp and Meituan, we collected a new place search dataset with total 1.4 million point of interest . As far as we know, this is the first POI dataset in million level that can provide a new benchmark for place search for the community, and we are going to make it public available in the near future. The experiment result on POI dataset indicated that our model was very effective in retrieval and consistently performed better than those baselines.  The paper is structured in the following way. Section gives a review of related work. Section explains the locality-weighting in theory and illustrates the details of the proposed model, SentPWnet. Experimental results on five datasets, including Quora, MRPC, Wikipedia Sentence, Wikipedia Title and POI search dataset are demonstrated in Section. Finally, the conclusion and future work are presented in Section.        In this paper, we are pushing the frontier of metric learning in NLP. Our theoretical analysis from the perspective of loss optimization provides a novel insight on the usage of pair-based that clearly indicates the importance of pair mining and instance weighting to learn sentence embedding. These two parts have been severely overlooked by most existing works. Our model, SentPWNet, incorporates the locality weighting schema and turns the conventional works with two isolated stages, sampling and learning, into a unified locality weighting and pair-based optimizing framework in an iterative manner. The experimental results clearly show the effectiveness of our model. Moreover, our self-collected POI dataset can provide the community a testbed for place retrieval task.   For future work, there are many works to be exploited. Despite our locality weighting scheme gives relatively good performance, it is still unknown whether it is the optimal way. Another interesting direction is the interpretability of the learned representation. Our model is in supervised manner and relies on human annotated training samples to a large extent. Therefore, how to explain the semantic meaning of learned representation still requires a lot of future efforts.        The acknowledgments section is defined using the ""acks"" environment    . This ensures the proper    identification of the section in the article metadata, and the    consistent spelling of the heading.         The next two lines define the bibliography style to be used, and    the bibliography file. 
"," %%Batch IS NOT Heavy: Learning Word Representations From All Samples Pair-based metric learning has been widely adopted to learn sentence embedding in many NLP tasks such as semantic text similarity due to its efficiency in computation. Most existing works employed a sequence encoder model and utilized limited sentence pairs with a pair-based loss to learn discriminating sentence representation. However, it is known that the sentence representation can be biased when the sampled sentence pairs deviate from the true distribution of all sentence pairs. In this paper, our theoretical analysis shows that existing works severely suffered from a good pair sampling and instance weighting strategy. Instead of one time pair selection and learning on equal weighted pairs, we propose a unified locality weighting and learning framework to learn task-specific sentence embedding. Our model, SentPWNet, exploits the neighboring spatial distribution of each sentence as locality weight to indicate the informative level of sentence pair. Such weight is updated along with pair-loss optimization in each round, ensuring the model keep learning the most informative sentence pairs. Extensive experiments on four public available datasets and a self-collected place search benchmark with 1.4 million places clearly demonstrate that our model consistently outperforms existing sentence embedding methods with comparable efficiency.",38
"   % introduction of SLU The spoken language understanding  module is a key component of spoken dialogue system , parsing user utterances into corresponding semantic representations . For example, the utterance ``'' can be parsed into a set of semantic tuples ``''. In this paper, we focus on SLU with semantic labels in the form of  triplets , which does not require word by word annotations. Both discriminative  and generative  methods have been developed to extract semantics from ASR hypotheses of the user utterance.   % ASR robustness on SLU SLU systems trained on manual transcripts would get a dramatic decrease in performance when applied to ASR hypotheses . To eliminate ambiguity caused by ASR errors, two kinds of input features can be exploited to enhance SLU models:  ASR hypotheses and  dialogue context information. 1) Considering the uncertainty of ASR hypotheses, previous works utilized ASR 1-best result , N-best lists , word lattices  and word confusion networks   for inputs to train an SLU model. Masumura et al.  proposed a fully neural network based method, , to encode WCNs. It first obtains bin  vectors by the weighted sum of all word embeddings in each bin separately, and then exploits a bidirectional long short-term memory recurrent neural network  to integrate all bin vectors into an utterance vector. Nevertheless, bin vectors are extracted locally, ignoring contextual features beyond certain bins. 2) Furthermore, the last system dialogue act can be utilized to track context, and provide some implications about the user intent under noisy conditions. However, utterance and context are independently encoded by different models to generate the final representation, resulting in a lack of interaction between them.  %Barahona et al.  improves the SLU performance under noisy conditions by enhancing the utterance representations with dialogue context representations, in which previous dialogue system acts are used for tracking context. However, utterance and context are independently encoded by different models  to generate the final representation, resulting in a lack of interaction between them.  % other method for ASR correction  % pretrained language models Recently, pre-trained language models, such as GPT  and BERT , have been successfully adopted in various NLP tasks. Huang et al.  adapted GPT for modeling word lattices, where lattices are represented as directed acyclic graphs. However, GPT is modeled as a unidirectional Transformer and neglects context in the future, thus less expressive than BERT. Although both word lattices and WCNs contain more information than N-best lists, WCNs have been proven more efficient in terms of size and structure.   % our work To these ends, we propose a novel BERT  based SLU model to jointly encode WCNs and system acts, which is named WCN-BERT SLU. It consists of three parts: a BERT encoder for jointly encoding, an utterance representation model, and an output layer for predicting semantic tuples. The BERT encoder exploits posterior probabilities of word candidates in WCNs to inject ASR confidences. Multi-head self-attention is applied over both WCNs and system acts to learn context-aware hidden states. The utterance representation model produces an utterance-level vector by aggregating final hidden vectors. Finally, we add both discriminative and generative output layers to predict semantic tuples. To the best of our knowledge, this is the first work to leverage the structure and probabilities of input tokens in BERT. Our method is evaluated on DSTC2 dataset , and the experimental results show that our method can outperform previous state-of-the-art models significantly.  %To the best of our knowledge, we are the first to utilize BERT to jointly model WCNs and dialogue context.  % The rest of the paper is organized as follows. The next section describes the framework of our proposed model. Experiments are presented in section , followed by conclusions.     In this paper, we propose to jointly encode WCN and dialogue context with BERT for SLU. To eliminate ambiguity caused by ASR errors, WCNs are utilized for involving ASR hypotheses uncertainties, and dialogue context implied by the last system act is exploited as auxiliary features. In addition, the pre-trained language model BERT is introduced to better encode WCNs and system acts with self-attention. Experimental results show that our method can beat all baselines and achieves new state-of-the-art performance on DSTC2 dataset. Except for encoding different ASR hypotheses, there are several studies about ASR error correction for SLU, which will be our future work.   \documentclass[a4paper]{article}  \usepackage{INTERSPEECH2020} \usepackage{multirow} \usepackage{multicol} \usepackage{makecell} \usepackage{amsmath,amssymb}  \usepackage{hyperref}   \title{Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding} }  The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.   \address{       MoE Key Lab of Artificial Intelligence \\       SpeechLab, Department of Computer Science and Engineering \\       AI Institute, Shanghai Jiao Tong University, China} \address{     MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University \\     SpeechLab, Department of Computer Science and Engineering \\     Shanghai Jiao Tong University, Shanghai, China}  @sjtu.edu.cn}   : spoken language understanding, word confusion network, dialogue context, Transformer, BERT       
"," Spoken Language Understanding  converts hypotheses from automatic speech recognizer  into structured semantic representations. ASR recognition errors can severely degenerate the performance of the subsequent SLU module. To address this issue, word confusion networks  have been used as the input for SLU, which contain richer information than 1-best or n-best hypotheses list. To further eliminate ambiguity, the last system act of dialogue context is also utilized as additional input. In this paper, a novel BERT based SLU model  is proposed to encode WCNs and the dialogue context jointly. It can integrate both structural information and ASR posterior probabilities of WCNs in the BERT architecture. Experiments on DSTC2, a benchmark of SLU, show that the proposed method is effective and can outperform previous state-of-the-art models significantly.  % Spoken Language Understanding  converts hypotheses from automatic speech recognizer  into structured semantic representations. Hypotheses errors can significantly degrade the subsequent SLU performance. To address the problem, word confusion networks , which contain richer information than 1-best or n-best hypotheses list, have been used as the input for SLU. The basic component of WCN is , containing multiple word candidates and their posterior probabilities. Previous work computes a feature vector for each bin separately without considering its context. In this paper, a novel WCN based Transformer model is proposed to include context information. The Transformer encodes each word candidate in all bins using a self-attention mechanism. To consider the posterior probability of each word candidate given by ASR, score embedding and re-weighted attention mechanism are proposed. Experiments on DSTC2, a benchmark SLU dataset, show that the proposed model not only outperforms previous state-of-the-art models, but also can be trained effectively and efficiently.  % kai.yu",39
"  were the  of  created ?  \\ Q2: who wrote  of the most  ,  , and  ?  \\ Q3: by the  which languages was  book  in ?  \\ Q4: in what  did ``  '' as an artistic concept arise ? \\ \hline      However, most of the previous works are devoted to deal with answer-aware question generation. That is, given a text and also an answer span, the system is required to generate questions. But in a real application for educational purpose, people or machines are often required to generate questions for natural sentences without explicitly annotated answer. Comparing with the answer-aware QG, the answer-agnostic QG  task is more challenging and attractive. Unfortunately, AG-QG has been much less studied. Du's work  is the first one to tackle this problem, and  achieve the state-of-the-art by employing an extended transformer network.  % with a sequence-to-sequence attention model, and now the state-of-the-art result in answer-agnostic QG task is achieved by the extended transformer network .  % There is much room for improvement in this task.   For the AG-QG task, where the input is only a sentence but without any answer, multiple questions might be asked from various perspectives. According to our statistics on SQuAD ,  %which is the most commonly used dataset for QG, we find that  nearly 34 of the source sentences are offered multiple gold reference questions, and nearly 20 of the source sentences are offered different types of questions. Table  gives an example, where one source sentence corresponds with four different types of questions. However, most existing approaches can only generate one question for one input sentence.    %To address the multi-types QG and to ask reasonable questions  %For a natural sentence that generally expresses the meaning of ""who does what to whom when where how"", the most important thing to ask reasonable questions is to determine what types of questions should be asked, i.e., which part of information should be focused on when asking questions.  To enable the model to ask different types of questions given the same input sentence, we propose a question type driven framework for AG-QG task. Specially, our model firstly predicts the probability of different question types distribution on the input sentence,  %via a multi-task learning with the main question generation task. The model  which allows us to choose the best  question types with higher possibility. Then these different question types will be embedded into different vectors, which will  %teach the model to learn the specific inner pattern of different types and  guide the decoder to pay attention to informative parts with respect to different questions.    % teach the model to learn different inner patterns of different question types.         % % In other words, for the same source sentences, people can ask different and even different types of questions, which matches the real situation.   % In this paper, we propose a multi-types model for the answer-agnostic QG task. Given a sentence and the number of questions need to be generated , our model predicts the probability distribution of generating different types of questions. Then it chooses the best K question types and generate K questions of these types .   % To achieve this, our model firstly predict the probability of different question types according to the given sentence and then select top K question types. % 鐏忔繆鐦弨閫涚啊娑撴稉,閹啿鎷版稉濠囨桨閻ㄥ墖op K閻ㄥ嫯銆冩潻鐗堟纯鐠愭潙鎮庢稉閻 % To achieve this, we propose a question type module which consists of two parts: question type embedding and question type prediction.   % The question type predict part, given the encoded representation, predicts the most proper type of question that would be generated when question type is not pre-defined. Then, given a question type, the question type embedding part embeds it into a vector, which is used for decoding at the first decoding step and is concatenated with decoder attention and hidden states at every decoding step to calculate the generate distribution. Different question types will be embedded into different vectors, and in this way, the question type vectors teach the model to learn the different inner patterns of different types of questions.  Meanwhile, according to the statistics on SQuAD, on average there are 3.09 non-stop words copying from the source sentence for each reference question. Those non-stop words appearing in both questions and sentences are regarded as keywords, since they act as the connection of these two parts. To increase the probability of copying keywords from source sentences, we design a new copy loss to enhance the traditional copy mechanism.  %Since the traditional loss only cares about whether the  generated word is exactly the same as in the golden question, but never pays attention to whether these keywords have been copied into the generated question.  In our model, by minimizing the new copy loss, the model will be forced to copy these keywords at least once during decoding.   % We design this copy loss to let the highest copy probability of key words to be as near to 1 as possible. That is,   We conduct experiments on SQuAD. Both the question type module and the new copy loss improve performance over the baseline model, and our full model combining two modules obtains a new state-of-the-art performance with a BLEU-4 of 13.9. Moreover, our model can ask different types of questions for a given sentence.  % just when the human-assigned parameter  is set to more than one.   % To improve the performance of our model, we enhance the traditional copy mechanism in order to make sure all these important key words are copied. Specially, we design a copy loss to increase the probability of copying key words from source sentences.   % Another benefit brought by question type embedding module is that, by assigning different question types, the model can generate different questions even though the input source sentence is the same. That is to say, the question type embedding module endows our model the ability to generate multiple questions for the same source sentence, which is more natural and conform to reality, since in real setting, different questions about one passage might be asked from various perspective. The point of our question type predict part is that although the model have the ability to generate all types of questions for one input, we still want it to find the most possible, the most proper and the best question to generate.  %In sum, this paper makes the following contributions: We conclude the contributions as follows:        In this paper, we propose two new strategies to deal with the answer-agnostic QG: question type module and copy loss mechanism.    modules: penalty loss mechanism and question type module, which actually consists of a question type embedding part and an optional question type predict part.  These proposed modules improve the performance over the baseline model, achieving the state-of-the-art. Moreover, our model has the ability and flexibility to generate multiple questions for one source sentence. Hopefully, the idea of question type module and copy loss mechanism can also be used to do answer-aware QG task or other similar text generation tasks.  However, our work still has limitations. First, the multiple questions generated for one source sentence are of low diversity, that means, although they are of different types, they actually look similar with each other. It is because our question type module is still simple, and we are seeking more complex alternative to address this problem. Secondly, the unsatisfying accuracy of the question type predict part prevents our model to achieve a better performance. Predicting question type without answer information is very hard but valuable since higher accuracy can definitely lead to performance that is closer to the upper bound. Hopefully, the idea of question type module and penalty loss mechanism can also be used to do answer-aware QG task.  
"," The answer-agnostic question generation is a significant and challenging task, which aims to automatically generate questions for a given sentence but without an answer. In this paper, we propose two new strategies to deal with this task: question type prediction and copy loss mechanism. The question type module is to predict the types of questions that should be asked, which allows our model to generate multiple types of questions for the same source sentence. The new copy loss enhances the original copy mechanism to make sure that every important word in the source sentence has been copied when generating questions. Our integrated model outperforms the state-of-the-art approach in answer-agnostic question generation, achieving a BLEU-4 score of 13.9 on SQuAD. Human evaluation further validates the high quality of our generated questions. We will make our code public available for further research.",40
" Content-enhanced network embedding, aims at learning continuous vectors for nodes with rich node contents such as texts. These node representations can be directly used in downstream tasks including classification, link prediction, recommendation, etc. To complete or improve these tasks, artificial labels are often added to the node. However, most of the time only a small number of labels could be accessed due to the costly human resources. Therefore, the necessity of making full use of precious human knowledge to automatically annotate unlabelled nodes and enrich their representations emerges.\par Most textual network embedding models  focus on learning unsupervised global node embeddings before classifying nodes in a supervised manner, which actually, is deemed as a waste of the valuable labelled information. TriDNR  learns vectors of node labels which are used to enhance representations of nodes with known labels. However, this approach fails to improve the representations of unlabelled nodes, which explains its weaker performance in the classification task in comparison to other unsupervised node embedding approaches, such as CENE . In addition, there are two common disadvantages of network embeddings according to  . First, graph embedding often initializes a vector to each node without shared parameters among different nodes in the embedding matrix, which requires a great level of computing resources in the training as the number of nodes grows. Second, it lacks a flexible way to generate representations of new nodes, and often requires the re-construction of a new network to re-train the node vectors.   To address these problems, this study first uses a shared node/text encoder, whose parameters for different nodes are shared via same words of the input text, to embed all nodes into continuous vectors. Subsequently, we train these node embeddings based on the structural information of the network and node labels. Finally, the trained node encoder, which not only preserves the structural and labelled information, but also extracts the semantic and syntactic features of texts. The node encoder could be used later to generate representations of nodes, including new nodes, by entering their node contents.\par Our embedding achieves state-of-the-art classification results in different training ratios on two the public textual networks. With the  training/labelled ratio, we push the benchmarks up by  and  respectively, indicating that even a low percentage of labels improves node representations. Additionally, a feasible solution that generalizes our model from textual networks to a broader range of networks is proposed.    This paper proposes a novel idea to embed networks with node contents. For the textual network, we first design an advanced text encoder to effectively extract semantic and syntactic features. In order to preserve both the structural and labelled information, the node encoder is jointly trained based on structure- and label-based objectives. By making the most of node labels, our experiments show that even a small proportion of node labels improve node representations significantly in the classification task. Further, A shared node encoder, whose inputs are node contents, not only highly saves the computational source in the learning of representations, but also enables the embedding model to generate the embeddings of new nodes. Finally, we discuss the solution to extent our model from textual networks to a wider range of networks. With the generalized framework, network embedding models are given with the capacity to incorporate manually annotated information and node attributes into node representations, at the same time are able to infer representations for new nodes.    This  paper  proposes  a  novel  idea  for  networks  with  node contents.  For  the  textual  network,  we  first  design  an  advanced  and  effective  text  encoder  to  extract  semantic  and syntactic features and convert them into vectors which are trained on labelled textual network, making the encoder preserve  both  structural  and  labelled  information.  Our  model makes  the  most  of  node  labels  and  our  experiments  also shows that only a small fraction of labels can improve node representations a lot. Besides, A shared node encoder, whose inputs are node contents, can not only save a lot of computational source when learning representations, but also enable the embedding model the ability of generalization for new nodes. Finally, we discuss how to generalize the frame work of  our  model  from  textual  networks  to  normal  networks.With  this  framework,  network  embedding  models  can  not only incorporate manually annotated information and node attributes into node representations, but also provides a flexible way to infer representations for new nodes.      
"," Voluminous works have been implemented to exploit content-enhanced network embedding models, with little focus on the labelled information of nodes. Although TriDNR  leverages node labels by treating them as node attributes, it fails to enrich unlabelled node vectors with the labelled information, which leads to the weaker classification result on the test set in comparison to existing unsupervised textual network embedding models. In this study, we design an integrated node encoder  for textual networks which is jointly trained on the structure-based and label-based objectives. As a result, the node encoder preserves the integrated knowledge of not only the network text and structure, but also the labelled information. Furthermore, INE allows the creation of label-enhanced vectors for unlabelled nodes by entering their node contents. Our node embedding achieves state-of-the-art performances in the classification task on two public citation networks, namely Cora and DBLP, pushing benchmarks up by 10.0\% and 12.1\%, respectively, with the 70\% training ratio. Additionally, a feasible solution that generalizes our model from textual networks to a broader range of networks is proposed.",41
" Meta-learning is emerging as a promising technical solution to low-resource Natural Language Processing  applications. Specially, Model-Agnostic Meta-Learning , one of the most popular meta-learning methods, trains on plenty of tasks to get a parameter initialization which is easy to adapt to target tasks with a few samples. As a model-agnostic framework, MAML is successfully employed in different NLP applications. %with different definition of ``task"". %meta-learning method for few-shot image classification, learn parameter initialization and adapt, model-agnostic framework, easy to apply in NLP.  Some works use MAML for few-shot text classification, such as relation classification and topic classification. %following the same setting of ``task"" as few-shot image classification.  Other works use MAML for multi-domain and low-resource language generation, such as few-shot dialogue system and low-resource machine translation. %by treating each domain as a ''task"". %our work, paml, yuzhou, low-resource translation, ijcai few-shot, 3 few-shot text classification papers.  %Existing studies applying MAML in NLP scenarios can be divided into 2 categories: xxx % 閸掑棛琚禒璇插閸滃瞼鏁撻幋鎰崲閸旓紕娈戝銉ょ稊 %They all successfully employ MAML framework for specific tasks. Few works, however, have a systematic investigation among different tasks on how to apply MAML in NLP applications.  In NLP, there are plenty of different applications and datasets have diverse properties even in one application. %For example, the datasets of dialogue system can be divided into 2 categories: Non-goal-driven datasets and goal-driven datasets.  For example, Persona-chat and DSTC are both datasets for dialogue systems.  The tasks for MAML in NLP datasets also have many specific characteristics. For example, PAML  regards each person's dialogues as a task and they have different personal profiles and dialogue quantity.  These characteristics have a great impact on the performance of MAML . Therefore, we need to consider many impacting factors when applying MAML on NLP, such as data quantity, similarity among tasks, and the balance between general language model and task-specific adaptation. Few works have thoroughly studied them. %However, MAML still suffers some problems in NLP applications. First, when applying MAML in NLP, the ``tasks"" we define may have different task profiles and data quantity. For example,  regard each person's dialogues as a task and people have different personal profiles and dialogue quantity. A general parameter initializaion cannot adapt to each specific task well. Second, MAML uses the same strategy to fine-tune to different tasks, which is not appropriate when tasks have dissimilar semantics and data quantity. Finally, there are plenty of datasets with diverse properties even in one NLP application, which brings difficulty to choose a suitable one to implement MAML.  %few-shot settings in NLP are different from CV: diverse datasets, hard to choose a suitable one. tasks have different semantics and data quantity, hard to balance general language model and task specific characteristic when training, even in one dataset, tasks may behave different when fine-tuning.  In this paper, we take an empirical approach to systematically investigating these impacting factors and finding when MAML works the best. We conduct extensive experiments over 4 datasets,  %focusing on the following impacting factors:  focusing on the following research questions:  RQ1. How can the general language model training affect the model's task-specific adaptation ability? RQ2. How do the task profile and data quantity affect our decision of fine-tuning epoch number? RQ3. How do the data quantity and similarity among tasks of the dataset affect the performance of MAML?  %1. The affect of the general language model training to the model's task-specific adaptation ability. 2. The affect of each task's semantics and data quantity to the decision of fine-tuning epoch number. 3. The affect of data quantity and similarity among tasks of the dataset to the performance of MAML.   The experimental results provide insights on MAML in NLP applications. Our conclusions can help researchers to better develop meta-learning methods in NLP.   {0pt} {0pt}  How can we balance the general language model and the ability of task-specific adaptation for the parameter initialization training in MAML? The parameter initialization fully trained via MAML has strong general language model, but has poor ability to adapt to specific tasks. %When do we get the best initilized model during the pre-training stage?   Can the task profile and data quantity affect our decision of fine-tuning epoch number? The best fine-tuning epoch number of each task is unrelated to its task profile and data quantity. So it is unnecessary to customize the fine-tuning epoch number for each task according to the semantic or data quantity information.  How the data quantity and similarity among tasks of the dataset affect the performance of MAML? MAML has significant advantages when the data quantity of each task is small and tasks are dissimilar with each other. %What is the best dataset for MAML in terms of the data quantity and similarity among task?  \fi  %By studying these questions, we provide insights on MAML in NLP applications. Our conclusions can help researchers to better develop meta-learning methods in NLP. %Based on our experimental results, we draw thefollowing conclusions:  % %{0pt} %{0pt} %     In this paper, we conduct an empirical study to investigate the impacting factors on the performance of MAML in NLP applications. We show that MAML works the best when the general language model is not fully trained by MAML, the data quantity of each task is small and tasks are dissimilar with each other. We also point out that it is unnecessary to customize the fine-tuning epoch number for each task according to the task profile or data quantity information. Our work sheds light on the applying  MAML in NLP.   include your own bib file like this: 
"," Model-Agnostic Meta-Learning , a model-agnostic meta-learning method, is successfully employed in NLP applications including few-shot text classification and multi-domain low-resource language generation. Many impacting factors, including data quantity, similarity among tasks, and the balance between general language model and task-specific adaptation, can affect the performance of MAML in NLP, but few works have thoroughly studied them. In this paper, we conduct an empirical study to investigate these impacting factors and conclude when MAML works the best based on the experimental results.",42
"   % Names are important issues for people. % A name is a term used for the identification of abstract , as well as tangible things .   In information systems, searching for information about a specific individual is a frequently performed activity; for example, retrieving a patient's electronic medical record from a medical records system and searching for a research paper online by the author's name or a news article by a journalist's name are daily tasks performed using individuals' names.  Names are also the focus of the online search, and individuals' reliance on names, as reflected in search engine queries, is steadily increasing.  For example, in 2004, 30\% of all search engine queries provided by users included personal names. A decade later, in 2014, one billion names were used in Google search engine queries each day.  While the use of personal names in online search has increased, the results retrieved from Web search engines has not kept pace.  Leading online search engines retrieve suboptimal results in response to searches for a person's name.  These poor results created a new customer need which has been fulfilled by companies, such as Pipl\footnote{https://pipl.com/} and ZoomInfo,\footnote{https://www.zoominfo.com/} which have dedicated their efforts towards providing information about specific people. Despite these new services, in many cases, users experience difficulty when selecting the exact name to search for or the correct form of a name when formulating a name-containing query.  Therefore, searching for people by name online remains a challenging problem.  There are several reasons for the poor search engine performance for queries containing names. First, unlike words, which, in most cases, have a single correct spelling, there are several legitimate variations for a given name.   Second, there are cases in which a name changes over time due to the use of a nickname, marriage, religious conversion , or gender reassignment. Third, many names are heavily influenced by a person's cultural background.  For example, the English forename of Anthony has several variations in other languages: Antoine , Antonius , Anton , and Antonio . The detection of aliases for people also poses a  challenge; for instance, the nickname of Kobe Bryant, the famous basketball player, is the ``Black Mamba.'' Therefore, finding a match for a name is more difficult than it is for general text.  Today, techniques used for name matching and the retrieval of similar names are mainly based on pattern matching and phonetic encoding. For example, in the context of names,  phonetic encoding algorithms  encode a given name into plain-text code that reflects the way people pronounce the name. This plain-text code assists in finding similar names in cases in which the code for two different names is identical . However, the performance of these algorithms has been poor. %In particular, those which base their suggestions on the sound produced by humans while pronouncing a given word.  %For example, the phonetic encoding algorithms, such as , , and  transform a given word into a plain-text code that resembles the way the word is pronounced.  In recent decades, there has been a data science revolution resulting in the development of products and services that utilize machine and deep learning algorithms to help people in various aspects of modern life, for example, searching for information on the Internet, filtering spam email, image recognition, etc. These advanced algorithms, which are capable of learning from a large set of examples, were found much more effective and robust than those that were designed using explicitly specifying rules.  For example,  is a deep learning-based model that utilizes large-scale text to transform words into continuous vector space representations .  These fixed-dimensional vector representations were found to have semantic meaning, which can be used for many natural learning processing  tasks, such as text classification, word similarity, and more. % The accelerated development of the data science in general and deep learning in particular has opened many new opportunities for improving many aspects in our lives. % One of the famous approaches   % These continuous vector representations of words computed by neural networks found promising for carrying semantic meanings. % In many cases, this is useful for various natural language processing  tasks, such as text classification, information retrieval, etc.  Inspired by , we propose a novel and generic approach that leverages the power of human speech and deep learning to address several issues associated with names, such as synonym suggestion and record linkage. The proposed , and accent;  the feature extraction phase, where we extract audio features which serves as a continuous vector space representation for each name;   the classification phase, in which a machine learning classifier is used to classify candidates that sound like the given name;   and  the last phase, in which candidates are filtered according to a predefined threshold .    In our evaluation, the performance of the proposed algorithm is compared with the performance of other the state-of-the-art machine and deep learning algorithms.  The performance was evaluated using the Behind the Name dataset with over 7,300 forenames and over 37,000 synonyms. We show that the proposed  than the well-known  algorithm  .  % Moreover, we demonstrate that our approach is language independent  % and can work in different language by ...   % we demonstrate the propsoed the problem of similar names suggestion.    %This means that audio and deep learning can be utilized together for providing significantly higher results compared to phonetic encoding algorithms.     %As opposed to encoding phonetic algorithms, which suggest similar names based on a simple plain-text code, the proposed representation is much more complex, therefore is capable to hold much more valuable information about a given name. % Using deep learning instead of plain-text suggested as previous will be much more precisie .     % In 2018, inspired by the , Chung and Glass proposed the , a speech version of the .     % Over the years, different algorithms reflecting the way a given word is pronounced, were suggested.    % For example, the , ,  and others are all phonetic encoding algorithms that transform a given word into a plain-text code that resembles how the word is pronounced by humans.  % Over the years, different algorithms, which transform a given word to other representations or forms, have been developed.   % Also, the  is a learning model technique that transform words into fixed dimensional vectors also known as . % In 2018, inspired by the , Chung and Glass proposed the , a speech version of the .  % Inspired by the , in this paper, we propose the , an innovative approach to confront different problems associated with names.  % This proposed framework suggests utilizing the information existing on the spoken names. % Here, we demonstrate the proposed framework on the task of suggesting similar names for a given first name. % We propose a novel framework employing the deep neural network architecture for learning fixed-length vector representations of audio segments excised from an automatic name speech.  % As opposed to previous approaches that suggest related names based on the same encoded representation or pattern, we propose a general approach that suggests similar first names based on the audio segment representation of a given name. % We show that our general algorithm provides significantly superior results compared to other existing methods that focus on encoding or detection of specific patterns. % For example, the average precision@1 obtained by our approach reached 1.8 times higher than the well-known  algorithm  . % This means that speech and deep learning can be utilized together for providing significantly higher results compared to phonetic encoding algorithms which retrieve names based on encoding sounds.     The remainder of this paper is organized as follows: In Section, we provide a brief overview of related work focused on issues similar to those addressed in this study. Section presents the \spokennametovec framework.  We provide a detailed description of the datasets used in this study in Section. In Section, we review the experimental setup, and in Section, we present the performance  of the proposed algorithm and the other algorithms evaluated. In Section, we discuss the results obtained, and our conclusions and future directions are provided in Section.           Upon analyzing the results presented in Section, we can conclude the following:  First, the proposed novel 's results.  Therefore, there is a room for better improvement handling audio feature extraction.   Our demonstration of this approach on forenames and surnames also demonstrated the approach's generality.   Third, unlike many algorithms, such as Soundex and Name2Vec which support only the English language, the 's configurations suggested the name Viktoria as a correct synonym for the given name of Victoria as demonstrated in Figure.  Sixth, it can also be seen that  that obtained an average F1 scores of 0.152, and 0.112, respectively.    Second, concerning precision performance, we estimated the  for the provided top 10 suggestions according to each algorithm when  .   Regarding small  , we can see that the algorithms suggesting similar names based name-based graph derived from family trees superior to all other algorithms.   For example, FTG , FTG , and FTG  obtained the highest average precision@1 of 0.272, 0.237, and 0.221, respectively.   The .     These results emphasize the effectiveness of our generic approach for suggesting similar names.  Seventh, for the recall performance, we can see that GRAFT, and the phonetic encoding algorithms outperformed all other algorithms, including the proposed , which obtained the highest recall score of 0.221.     The recall measure estimates the fraction of the total number of relevant names that were actually suggested.   According to the obtained results, we can notice that the G_N^{1,3}G_N^{1,j}j>3k$ candidates that are located the most close to the given name.  In the last phase, we filtered those candidates that their euclidean distance was too far from a predefined threshold.   The remaining candidates were retrieved in ascending order according to edit distance score between these candidates and the given name.   Using a threshold, we filtered candidates that sound different from the given name and used an ordering function to retrieved the remaining names. In this way, 's ability to support various tools for feature extraction; and  use any supervised machine learning algorithm for name classification.  The generality of this algorithm was also demonstrated in the suggestion of forenames and surnames.  This shows that the approach was effective for both forenames and surnames.     First, the  algorithm doesn't face.     In terms of precision, .  A possible future research direction is to examine other groups of names and datasets in order to understand the usefulness of the French configuration, as well as other configurations. Another avenue to pursue is combining the sound and the family tree approaches to improve similar name suggestion.     This study is reproducible research.  Therefore, the Spoken Name dataset, as well as the algorithm for suggesting synonyms for a given name is available.\footnote{https://github.com/aviade5/SpokenName2Vec}  Other datasets for evaluation are available upon request.    The authors would like to thank the icons8 website  for their beautiful icons.    The authors would like to thank  for proofreading this article, and the icons8 website  for their beautiful icons.    \clearpage       
","  Searching for information about a specific person is an online activity frequently performed by many users. In most cases, users are aided by queries containing a name in Web search engines for finding their will.  Typically, Web search engines provide just a few accurate results associated with a name-containing query. Most existing solutions for suggesting synonyms in online search are based on pattern matching and phonetic encoding, however very often, the performance of such solutions is less than optimal. %Underlying these poor results are the multiple legitimate spelling variations for a name, as opposed to regular text with a single way to be spelled correctly in most of the times.  %Alongside these poor solutions % In recent decades, the world has gone a data science revolution. % This revolution has resulted in offering many products involving machine and deep learning algorithms for assisting people in many daily problems. %, such as  filtering spam emails, image recognition, and more. %Their biggest advantage relies on their ability to study from a large set of examples, as opposed to previous solutions focusing on specifying rules explicitly. % One of the famous examples of this revolution was Word2Vec, a deep learning-based model transforming words into word embeddings. %These representations were found promising %thanks to their capability of carrying semantic meaning that utilizes  %for many natural language processing tasks. In this paper, we propose , a novel and generic algorithm which addresses the similar name suggestion problem by utilizing automated speech generation, and deep learning to produce spoken name embeddings. These sophisticated and innovative embeddings capture the way people pronounce names in any language and accent. Utilizing a name's pronunciation can be helpful for both differentiating and detecting names that sound alike, but are written differently. %A big advantage over over many well-used approaches focusing on text .   The proposed approach was demonstrated on a large-scale dataset consisting of 250,000 forenames and evaluated using a machine learning classifier and 7,399 names with their verified synonyms.%, we evaluated the proposed approach.  The performance of the proposed approach was found to be superior to 10 other algorithms evaluated in this study, including well used phonetic encoding and string similarity algorithms, and two recently proposed algorithms . The results obtained suggest that the proposed algorithm could serve as a useful and valuable tool for solving the problem of synonym suggestion. % Searching for a person闁炽儲鐛 name is a common online activity. % However, web search engines suffer from low numbers of accurate results to a query containing names. % Underlying these poor results are the multiple legitimate spelling variations for a given name, as opposed to regular text with a single way to be spelled correctly.   % Today, most of the techniques used to suggest related names in an online search are based on pattern matching and phonetic encoding.  % However, they frequently lead to poor performance. % Here, we propose a novel approach to tackle the problem of similar name suggestions. % Our algorithm utilizes historical data collected from genealogy websites, along with graph algorithms.  % In contrast to previous approaches, we propose a general method that suggests similar names based on the construction and analysis of digitized ancestral family trees. % %Using this valuable and historical information and combining it with network algorithms provides a large name-based graph that offers a great number of suggestions based on historical ancestors. % Similar names are extracted from the graph using generic ordering functions that outperform other algorithms that suggest names based on a single dimension, which limits their performance.       % Utilizing a large-scale online genealogy dataset with over 17 million profiles and more than 200,000 unique first names, we constructed a vast name-based graph.  % Using this graph along with 7,399 labeled given names with their true synonyms, we evaluated our proposed approach.  % The results showed that our approach gave superior performance in terms of accuracy, F1, and precision compared to other algorithms, including phonetic and string similarity algorithms.  % We propose our algorithm as a useful and valuable tool for suggesting similar names based on constructing a name-based graph using family trees.",43
"  The following instructions are directed to authors of papers submitted to AACL-IJCNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.       In this paper, we describe how learning gaze behaviour can help AEG in a multi-task learning setup. We explained how we created a resource by collecting gaze behaviour data, and using multi-task learning we are able to achieve better results over a state-of-the-art system developed by  for the essay sets which we collected gaze behaviour data from. We also analyze the transferability of gaze behaviour patterns across essay sets by training a multi-task learning model on unseen essay sets , thereby establishing that learning gaze behaviour  improves automatic essay grading.    We also perform an ablation test to validate which gaze behaviour attribute helps automatic essay grading the most in our multi-task learning setup.  In the future, we would like to look at using gaze behaviour to help in cross-domain AEG. This is done mainly when we don't have enough training examples in our essay set. We would also like to explore the possibility of generating textual feedback  based on the justifications that the annotators gave for their grades.    
","  % This document contains the instructions for preparing a manuscript for the proceedings of AACL-IJCNLP 2020. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document. % In this paper, we describe an approach to automatically grade essays using gaze behaviour TODO: Write it better.",44
" %sm: why weak supervision %  %  Email has continued to be a major tool for communication and collaboration over the past decades. The volume of email messages exchanged daily has also continued to grow and is projected to reach 306.4 billion messages and 319.6 billion messages a day by the end of 2020 and 2021 respectively. In addition to the significant volume of messages, email is one of the top time consuming activities for information workers. Recent studies show that communicating with colleagues and customers takes up to 28\% of information workers' time, second only to role-specific tasks at 39\% .   Such widespread use and significant amount of time spent on email have motivated researchers to study how people use email and how intelligent experiences could assist them to be more productive. One of the earliest works to characterize the main purpose email serves in work settings is that of Dabbish et al.. They conducted a survey of 124 participants to characterize different aspects of email usage. Based on this, they identified four distinct uses of email: task management, social communication, scheduling, and information exchange. More recent work conducted a large scale analysis of enterprise email identifying several use cases with many sub intents such as requesting an action, promising an action, updating a meeting, requesting information, social interaction, etc. Many other studies have focused on proposing methods for detecting intent of or suggesting actions in response to an email. Detecting intents in communications can integrate machine intelligence into email systems to build smart email clients that provide more value to email users. Several such applications have been studied including creating intelligent experiences that offer to assist users with scheduling a meeting, detecting action items, automatically populating to-do lists, creating alerts for high-priority messages, sharing documents, and answering questions.   Previous work posed email intent classification  as a supervised learning problem where human annotators were asked to annotate email messages given a predefined taxonomy and machine learning models were built to identify intents using the annotated dataset for training. Supervised models, especially those employing deep neural networks, rely on large scale annotated training data for learning. In many applications, manual annotation is either expensive and time-consuming to acquire, or infeasible due to privacy concerns for sensitive data. This is exactly the case for email data since its personal and private nature makes it hard to collect human annotations for. Even when annotations are collected, they are done on a limited amount of data that may not be sufficient or representative of the different domains.   Many application domains like recommendation, search, and email communication have rich user activities and interactions that can provide additional signals for learning. For example, leveraging user interaction  for web search ranking has been extensively studied. Most email clients also allow users to manage their calendars, task lists, etc. Users interact with these information in different ways including responding, forwarding, flagging emails, setting up appointments, etc. Many of these user actions are directly correlated with the intent of the email. For example, many scheduling intents could be correlated with taking an action on the user's calendar such as creating or updating a calendar item. These actions may correlate to a certain intent but are also noisy in nature. Refer to Figure for an example. Consider the scenario where we want to detect emails where the sender is requesting a document from the recipient. In the absence of enough annotated examples, we may heuristically label all emails that received a response email { %  to leverage cleanly annotated examples and weakly labeled ones {      We propose a learning mechanism for {\m} based on prior works in curriculum learning and self-paced learning     % \todo{GZ: these two appear here first. Either mention them earlier or provide citations}     to judiciously select informative weak instances to learn from.      % \ms{Again, is this the first joint-approach? If not, what's the difference compared to state-of-the-art?}      We conduct extensive experiments on real-world datasets to demonstrate the effectiveness of the proposed approach -- obtaining an accuracy improvement of  to  on average over state-of-the-art methods for different settings.   % The rest of the paper is organized as follows. In Section, we introduce the application of email intent classification and the process of getting clean and weak labels. In Section, we present the proposed framework {\m} for learning with clean and weak sources of supervision. We show empirical evaluation results in Section. In Section, we present related work. In Section, we present conclusions and discuss future work.  %In this work, we propose approaches to leverage manually annotated clean and interaction-based noisy examples { ). Similarly, distant supervision methods for relation extraction align sentences with knowledge bases  with heuristics like: . Most of these approaches rely on textual information and heuristics  or external sources  and ignore the rich meta-data surrounding the text that are available in many applications.   %Our work is similar to this line of work in that they both try to leverage user interaction data to improve a machine learning system. They are also different in many ways. First, we focus on intent classification in text as opposed to ranking. Additionally, we focus on methods that combine clean-labeled data and weak-labeled data as opposed to just using implicit feedback data like clicks. Finally while clicks were shown to be more accurate than other types of user interaction signals, they suffer from several types of biases . Understanding the difference between such user interaction signals and clicks is an interesting direction for future research.   % \todo{SM: insert a good example}  %sm: focusing on email communication %Many application domains like recommendation, search, and email communication have rich user activities and interactions that can provide additional signals for learning. Leveraging user interaction  for web search ranking has been extensively studied. Clicks have been used as additional features for web search ranking and also as a way to generate labeled query-document pairs without the need for relevance data. They were shown to be very accurate and show high correlation with user satisfaction, but they suffer from different forms of presentation bias. In this work, we focus on the email domain. Email has long been a popular tool for online communication and collaboration due to its easy access, fast interactions, and convenient management. It is estimated that around 169 billion emails are sent and received per day with the total number of emails expected to reach 319.6 billion in 2021. Most email clients also allow users to manage their calendars, task lists, etc. Users interact with these information in different ways including responding, forwarding, flagging emails, setting up appointment, etc. All these interactions provide useful information to build better systems to understand user's intents for email related tasks. These signals may not suffer from the some biases as clicks but, as we will show later, are noisier.    %Supervised learning has been successfully applied to the aforementioned tasks related to email data including spam detection, email prioritization, etc. Intent detection  is another task that has received significant attention over the years. Detecting intents in communications can integrate machine intelligence into email systems to build smart email clients that provide more value to email users. Several such applications have been studied including creating intelligent experiences that can offer to assist users with scheduling a meeting, detecting action items, automatically populating to-do lists, creating alerts for high-priority messages, sharing documents, and answering questions.  %Such actions could be recommended to users and performed on their behalf upon confirmation. They could be surfaced in email clients or offered by a digital assistant.   %sm: why email %\guoqing{This paragraph and the one before interwine, may need re-structure} %In this work, we select intent detection in emails as an application for our proposed framework. Existing approaches on email intent classification leverage content and surrounding contextual information but largely ignore the { as positive examples. However, there would be a lot of false positives since users may send attachments even without being requested for. Similarly, there will also be many false negatives; since users may not always send attachments even when requested to do so. Therefore, this heuristic rule serves only as a { signal for supervision. { for intent classification tasks. We propose an end-to-end trainable framework with deep neural networks to model these two sources of supervision simultaneously; in addition, we also explore techniques to integrate existing label correction approaches in our framework to further improve our performance. In summary, contributions from this paper are three-fold: % \ms{can we really claim the first one as a novel contribution?} %  %     by embedding them in a shared representation space. In addition, we incorporate prior work on label correction to further boost the model performance.      % \ms{Again, is this the first joint-approach? If not, what's the difference compared to state-of-the-art?} %      % \aha{we should end introduction after contribution. This kind of ends it on a weaker note. Should we move this up?} %  % There are indeed related scenarios in which weak and strong supervision have been combined for better results. There are several examples of such techniques in Web Search literature where weak signals based on clicks are combined with human labels to train better rankers . What makes our work different -- in addition to the scenario and methodology -- is the type of weak signals that are inferred from the conversation flow and email metadata, and used in joint optimization.  % \aha{should we add a paper structure paragraph given that we have space and to let the reader know when to expect the related work section} %The rest of the paper is organized as follows. In Section, we present the proposed framework {\m} for learning with clean and weak sources of supervision. In Section, we introduce the application of email intent classification and the process of getting clean and weak labels. We show empirical evaluation results in Section. In Section, we present related work. In Section, we give conclusion and future work.  %  %User Bob may reply to an Email with an attachment, in which we may view the original Email sent by Alice has the intent of requesting information.  In other words, replying with an attachment is a form of weak supervision,  as the original Email may not have a annotated label of content request. In addition, the reply with attachment behavior by the user provides us additional information to identify whether the original Email is a content request or not. Leveraging and modeling this additional source of weak supervision would greatly enhance our predictive model for Email intent detection.  %In essence, we investigate:  how to extract weak supervision from user interactions in email conversations; and  how to mathematically formulate weakly-supervised email intent detection and leverage weak supervision so as to improve intent detection performance. Our solutions to these challenges results in a novel framework {\m} for email intent detection. Our main contributions are summarized as follows:   % The rest of the paper is organized as follows. In section, we formally define the problem statement. We propose our framework and learning algorithms to leverage weak supervision in Section. In Section we outline several strategies to generate weak labels from user interactions followed by experimental evaluation in Section. In section we review the related work and finally conclude the paper in Section.   % TODO: Intent detection in digital assistant  %   In this paper, we leverage weak supervision signals from user interactions to improve intent detection for emails. We develop an end-to-end robust neural network model {\m} to jointly learn from a small amount of clean labels and a large amount of weakly labeled instances derived from user interactions. Extensive experiments on a real-world email dataset, Avocado, show {\m} to not only outperform state-of-the-art baselines but also its effectiveness in transferring the weak signals to another domain, namely Enron. 
","  Email remains one of the most frequently used means of online communication. People spend significant amount of time every day on emails to exchange information, manage tasks and schedule events. Previous work has studied different ways for improving email productivity by prioritizing emails, suggesting automatic replies or identifying intents to recommend appropriate actions. The problem has been mostly posed as a supervised learning problem where models of different complexities were proposed to classify an email message into a predefined taxonomy of intents or classes. The need for labeled data has always been one of the largest bottlenecks in training supervised models. This is especially the case for many real-world tasks, such as email intent classification, where large scale annotated examples are either hard to acquire or unavailable due to privacy or data access constraints. Email users often take actions in response to intents expressed in an email . Such actions can be inferred from user interaction logs. In this paper, we propose to leverage user actions as a source of weak supervision, in addition to a limited set of annotated examples, to detect intents in emails. We develop an end-to-end robust deep neural network model for email intent identification that leverages both clean annotated data and noisy weak supervision along with a self-paced learning mechanism. Extensive experiments on three different intent detection tasks show that our approach can effectively leverage the weakly supervised data to improve intent detection in emails.  %The need for labeled data is one of the largest bottlenecks in training supervised learning models. This is especially the case for many real-world tasks where large scale annotated examples are either too expensive to acquire or unavailable due to privacy or data access constraints. Many of these tasks involve users and as such allow access to a rich set of user {  %People nowadays spend a significantly amount of time in emails for online communication and collaboration. Identify email intent and further help users manage emails are important to enhance work productivity and task management efficiency. Existing approaches on email intent detection try to leverage deep learning to learn effective features from email contents, which often require large amount of annotated data. However, in practice, annotated data is usually limited in the scale, and rich information user  % interactions other than email contents are ignored. In addition, in email conversations, user interactions may not directly aligns \ms{Did you mean to comment out the last two sentences too?} %with the intent labels, however they could serve as sources for weak supervision. Therefore, in this paper, we study a novel problem of learning weak supervision from user interactions for email intent detection. We propose  a principled way to extract weak supervision from user interactions and a new framework named {\m} to model multi-source supervision for email intent detection. Experimental results on real-world datasets demonstrate the effectiveness of the proposed framework.",45
"  \par While Counterfactuals have been studied in different ways and in various domains, such as in Interpretable Machine learning, Philosophy,Mathematics, The research in this paper is concerned with the Detection of Counterfactuals as they occur in the domain of Natural Language and specifically, in English Language. We can express a description of Counterfactuals in this manner: They have a basic building block which we shall refer to as the antecedent,which denotes an event that did not occur, but which might have brought about another event, which we refer to as the consequent, if it did occur. More simply, .The dual task of detecting counterfactuals and locating the start and end indexes of the antecedent and consequent leverages on the data set and boundaries as defined by for the SemEval task 5. \par  Transfer learning is the transference of learner knowledge from one domain to the other, and if it is to be relevant, the domains need to be related. Intuitively, we do not fine-tune a model trained on programming language corpus for prediction with a movie reviews data set.Transfer learning becomes even more useful when there is inadequate data for training., and this is a situation that occurs all too often as preparing large data sets is an intensive task. compared classification model accuracy with a range of data sizes, and while the effect of data set size on model accuracy is dependent on the model in use as well as the specifics of the classification task, it was shown that using the BERT language model with the IMDB reviews data set, there was only a 5.3 percent degradation in accuracy when trained with 500 versus 22500 samples.There is however, such a thing as too little data, as a train size of 125 samples suffered a 24 percentage point drop in accuracy versus 500 samples.   \par Counterfactuals occur in only about 1\% of tweets, and that highlights their infrequency and thus the focus of our system in this two-track task is to introduce a pipeline that leverages transfer learning on extant pre-trained language models specifically to resolve the tasks of Counterfactual detection, which due to their characteristic of possessing implicit and explicit forms, limits the capability of rule-based or statistical methods to accurately detect some forms of counterfactuals.     With the conviction that the detection of Counterfactuals is a Sentence level classification task- by virtue of the provided data sets, and the identification of the antecedents and consequents a word level classification task, We introduce in this paper two pipelines for both tasks.The underlying language models used are base models of BERTand RoBERTa, so that following the trend of the large models outperforming the base models in sentence and token classification tasks, there is room for more qualitative optimization. All of the code used for the tasks as well as a basic guide are available on github. \footnote[1]{ https://github.com/Kc2fresh/Extracting-Counterfactual-data/}          In this work, we applied transfer learning using language models to Counterfactual detection, as well as the indexing of the antecedents and consequents, achieving an F1 score in sub-task 1, only slightly less than the top score, even while using base language models. It is demonstrated that the primary factor in the improved detection over previous probabilistic and rule-based methods is the underlying pre-trained language models. Thus higher accuracy will be achieved with improvements to the underlying models and In task 2, further development needs to be done on the sub-task pipeline, to handle output more efficiently, and achieve results that do not diminish the output of the language model prediction pipeline.         include your own bib file like this:  
","      We can consider Counterfactuals as belonging in the domain of Discourse structure and semantics, A core area in Natural Language Understanding and in this paper, we introduce an approach to resolving counterfactual detection as well as the indexing of the antecedents and consequents of Counterfactual statements. While Transfer learning is already being applied to several NLP tasks, It has the characteristics to excel in a novel number of tasks. We show that detecting Counterfactuals is a straightforward Binary Classification Task that can be implemented with minimal adaptation on already existing model Architectures, thanks to a well annotated training data set,and we introduce a new end to end pipeline to process antecedents and consequents as an entity recognition task, thus adapting them into Token Classification.",46
" In recent years, we have seen an astonishingly fast adoption of dialogue systems being utilized across many domains and we are interacting with them more and more in our everyday lives. From early such as chatbots ELIZA and ALICE to later ones like Siri and XiaoIce, the techniques have evolved from hand-crafted rule-based methods , retrieval-based methods  to learning-based methods . Recently, because of the advent of a series of deep learning models and the appearance of large-scale real dialogue corpora, end-to-end neural generative dialogue models emerge. Due to the simple implementation and the strong generalization ability of neural dialogue models, they are one of the most popular techniques to build practical chatbot services .  However, with the wide application of neural dialogue models, the ethical challenges they bring are attracting more and more attention. More recently it has been demonstrated that neural networks suffer from some problems including vulnerability due to their black-box nature and our lack of truly understanding their inner processing. Thus, as we are integrating these systems into more critical and sensitive domains, it raises some serious concerns, such as whether or not they can be manipulated to provide certain desired output. More specifically, if such systems can be manipulated it could potentially cause a drastic shift to the current paradigm of how we interact with these systems. For example, Tay, an AI chatbot developed by Microsoft, was shut down shortly after release due to its racism, sexist and obscene speech. Online troublemakers found its vulnerability and tried a variety of inputs to induce it to output inappropriate  responses . %This drawback of dialogue systems affects the user experience and evens cause negative social impacts. To that end, in this work, we set out to study this fundamental question of whether we can learn to manipulate state-of-the-art black-box dialogue models to produce target outputs by crafting inputs automatically. It goes without saying that if indeed we can manipulate these systems and if we are currently integrating them into our daily lives at such a rapid pace, then this opens the door to a plethora of potential harmful attacks that could be performed and potentially result in an almost unimaginable set of possible negative outcomes.  Nevertheless, even having now realized how critical this question is to being answered, the path to discovering an answer has numerous challenges. First, unlike many existing studies in other domains such as images, here the input search space is discrete and thus the traditional gradient-based optimization methodologies cannot be harnessed effectively. Furthermore, while seeking to discover if the current dialogue systems can be manipulated, we should not make the unreasonable assumption of having access to the full details/knowledge  of the system. Currently, most developed methodologies have focused on the continuous input space domain and furthermore assumed access to the model. Thus, since our problem is defined with a discrete input domain and our concern of whether these models can be manipulated is more realistic in the setting of a black-box dialogue model, then existing methods can not be applied %this leaves the existing methods unusable  and we require the development of novel frameworks to answer this indispensable fundamental question.  % To solve all the above-mentioned problems, we propose a reinforcement learning  based framework. % In the RL setting, a text generator  is set as the actor and the black-box dialogue model under study is set as the environment.  % The former interacts with the latter and gets fine-tuned according to the signals from the latter until it is able to generate desirable inputs that lead the dialogue model to provide expected outputs. % Extensive experiments are conducted on one well-trained state-of-the-art neural dialogue models.\purp{Note: I'll add maybe two sentences to the above paragraph mentioning how this is being used to solve the above-mentioned challenges.} Our main contributions are summarized as follows:  To address the above-mentioned challenges, in this paper, we regard the learning to craft input sentences as a sequential decision-making process. To this end, we propose the Target Dialogue Generation Policy Network , which serves as a reinforcement learning  agent to iteratively generate tokens guided towards specific objectives. The proposed policy networks are optimized by the REINFORCE-style estimators, eliminating the needs for standard gradient back-propagation which is largely impaired by the discrete nature of the sentence generation process and the assumption of no access to the dialogue model parameters. Our main contributions are summarized as follows: [leftmargin=0.5cm]    %The rest of the paper is organized as follows.First, our proposed framework is presented in detail in the next section.And then, we carry out our experimental setup and results with discussions.After that, we present related works. Finally, we conclude the work with possible future research directions.              %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Paragraph 1: A brief background introduction of dialogue systems.  % Paragraph 2: Given a dialogue model, why it is important to know whether we can find a crafted input that can lead the model to reply what we want.  % Paragraph 3: Major differences between our problem and traditional model attack problems. And corresponding challenges.  % Paragraph 4: Our proposed method and how it overcomes the challenges.  % Paragraph 5: Summarize the experimental results and list our contributions. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  %In recent years we have seen an astonishingly fast adoption of dialogue systems being utilized across many domains and we are interacting with them more and more in our everyday lives as they become increasingly human-like. This development and attention are primarily due to the recent advancements in deep learning techniques that are being harnessed for state-of-the-art dialogue systems.  %However, more recently it has been demonstrated that neural networks suffer from many problems including vulnerability due to their black-box nature and our lack of truly understanding their inner processing. Thus, as we are integrating these systems into more critical and sensitive domains, it raises some serious concerns, such as whether or not they can be manipulated to provide a certain desired output. More specifically, if such systems could be manipulated it could potentially cause a drastic shift to the current paradigm of how we interact with these systems. %For example, Tay, an AI chatbot developed by Microsoft, was shut down shortly after release due to its racism, sexist and obscene speech . Online troublemakers found its vulnerability and tried a variety of inputs to induce it to output inappropriate  responses . This drawback of dialogue systems affects the user experience and evens cause negative social impacts.  %To that end, in this work we set out to study this fundamental question of whether we can manipulate state-of-the-art black-box dialogue models to output an arbitrarily chosen target output. It goes without saying that if indeed we can manipulate these systems and if we are currently integrating them into our daily lives at such a rapid pace, then this opens the door to a plethora of potential harmful attacks that could be performed and potentially result in an almost unimaginable set of possible negative outcomes.  %Nevertheless, even having now realized how critical this question is to being answered, the path to discovering an answer is surrounded by numerous challenges. First off, unlike many existing studies in other domains such as images, here the input space is discrete and thus the traditional gradient-based  methodologies cannot be harnessed effectively. Furthermore, while seeking to discover if the current dialogue systems can be manipulated we should not make the unreasonable assumption of having access to the full details/knowledge  of the system. Currently, most developed methodologies have focused on the continuous input space domain and furthermore assumed access to the model. Thus, since by definition our problem is defined having a discrete input domain and our concern of whether they can be manipulated is much more realistic in the setting of a black-box dialogue model, then this leaves the existing methods unusable and requires the development of an entirely new framework to answer this indispensable fundamental question.  %In an attempt to solve the first challenge, we lend some ideas from the ...  %To solve all the above-mentioned problems, we propose a reinforcement learning  based framework. %In the RL setting, a text generator  is set as the actor and the black-box dialogue model under study is set as the environment. %The former interacts with the latter and gets fine-tuned according to the signals from the latter until it is able to generate desirable inputs that lead the dialogue model to provide expected outputs. %Extensive experiments are conducted on two well-trained state-of-the-art neural dialogue models. %We show that   %Our main contributions are summarized as follows: %%[leftmargin=*] %    % Building intelligent conversational agents, which converse with humans through natural language, is one of the most challenging problems in the field of artificial intelligence . % From early chatbots ELIZA , ALICE  to later ones Siri, XiaoIce , the techniques have evolved from hand-crafted rule-based methods, retrieval-based methods to learning-based methods. % Recently, because of the advent of a series of deep learning models and the appearance of large-scale real dialogue corpora, end-to-end neural generative models are widely applied in building dialogue systems .  % Although neural networks have shown significant effectiveness in extracting representations of data in higher and more abstract level and succeeded in many practical applications , researchers recently found they are sensitive to well-designed input samples . % Given an original input, imperceptible perturbations can often be made to it to lead the well-trained neural networks to respond with erroneous outputs. % This vulnerability brings a series of security issues to the applications of neural models.  % As an application of deep neural networks, end-to-end neural dialogue models are similarly threatened. % Unanticipated responses can be generated by dialogue agents to normal inputs, which degrades the performance of them and even causes additional troubles. % For example, Tay, an AI chatbot developed by Microsoft, was shut down shortly after release due to its racism, sexist and obscene speech . % Online troublemakers found its vulnerability and tried a variety of inputs to induce it to output inappropriate  responses . % This drawback hampers the widespread use of AI chatbots.  % To explore the possibility that well-trained end-to-end neural generative dialogue models can be attacked and inspire potential strategies to defend them, in the work, we focus on one problem: % Can we find a crafted input to lead a well-trained neural dialogue model to chat as we expect? % We try to execute two tasks: % 1) within a limited range of semantics, find a crafted input such that its corresponding response contains pre-defined target words and % 2) without any limitations, find a crafted input to which the dialogue model exactly replies a pre-defined target response.  % Compared with neural network models in routine tasks such as image classification, neural dialogue models are more challengeable to be attacked. % First, images come from a continuous input space, which means we can adjust the values in pixels arbitrarily based on the gradient information to craft an ideal input example. % Contrarily, in dialogue generation tasks, the inputs are discrete so gradients are not available anymore. % Besides, compared with classification problems where possible outputs  are finite and relatively few, dialogue models are able to generate any responses, which leads to infinitely many possible outputs. % In this paper, we propose a paraphrasing model    % % [t!] % %    % %   {|l|l|} % %    \\ % %  & \\ % %   {Five months without smoking ... and let me keep it up !} \\ % %   & \\ % %   {I just got a Free Pizza Coupon! Get yours before it comes out} \\ % %   & \\ % %    \usepackage[hyperref]{acl2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily \usepackage{multirow} \usepackage{booktabs} \usepackage{amsmath} \usepackage{amsfonts,amssymb} \usepackage{makecell} \usepackage{subfigure} \usepackage{graphicx} \usepackage{enumitem} \usepackage{amsfonts} \usepackage{array} \usepackage[ruled,linesnumbered]{algorithm2e}   %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \TeX}  \title{Chat as Expected: Learning to Manipulate \\Black-box Neural Dialogue Models}  \author{Haochen Liu \\   Michigan State University\\    \\   \\   Tyler Derr \\   Michigan State University\\    \\    \\\And   Zhiwei Wang \\   Michigan State University\\    \\   \\   Jiliang Tang \\   Michigan State University\\    \\}  % \author{First Author \\ %   Affiliation / Address line 1 \\ %   Affiliation / Address line 2 \\ %   Affiliation / Address line 3 \\ %    \\\And %   Second Author \\ %   Affiliation / Address line 1 \\ %   Affiliation / Address line 2 \\ %   Affiliation / Address line 3 \\ %    \\}  \date{}      Recently, neural network based dialogue systems have become ubiquitous in our increasingly digitalized society. However, due to their inherent opaqueness, some recently raised concerns about using neural models are starting to be taken seriously. In fact, intentional or unintentional behaviors could lead to a dialogue system to generate inappropriate responses. Thus, in this paper, we investigate whether we can learn to craft input sentences that result in a black-box neural dialogue model being manipulated into having its outputs contain target words or match target sentences. We propose a reinforcement learning based model that can generate such desired inputs automatically. Extensive experiments on a popular well-trained state-of-the-art neural dialogue model show that our method can successfully seek out desired inputs that lead to the target outputs in a considerable portion of cases. Consequently, our work reveals the potential of neural dialogue models to be manipulated, which inspires and opens the door towards developing strategies to defend them.                 Currently, the state-of-the-art dialogue systems are harnessing the power of deep neural models, and although they are proving to become more and more human-like, recent concerns have been raised for neural models across all domains if they can be manipulated . Thus, in this work, we investigate whether current neural dialogue models can be manipulated and develop a reinforcement learning based sentence generation framework that is able to craft the input sentences to lea dialogue system to output target responses. Our framework overcomes the tremendous challenges faced when attempting to optimize having a discrete input domain and in the realistic black-box setting where we assume no extra knowledge about the model . Our experiments are performed on two state-of-the-art dialogue neural models and we are able to find inputs that result in very similar output texts to the target output . We believe this result should perhaps at the very least raise awareness of this fact that indeed our dialogue models being used in our daily lives can be manipulated. Thus, our future work will be first designing more advanced methods for explicitly attacking these neural dialogue models and simultaneously developing strategies to defend against the very attacks we develop. Currently, the state-of-the-art dialogue systems are harnessing the power of deep neural models, and although they are proving to become more and more human-like, recent concerns have been raised for neural models across all domains as to whether they can be manipulated . Thus, in this work, we investigate whether current neural dialogue models can be manipulated and develop a reinforcement learning based sentence generation framework that can learn to craft the input sentences causing dialogue models to output target words and responses. We conduct extensive experiments on a state-of-the-art dialogue neural model and the results show that dialogue systems can indeed be manipulated. In addition, our proposed method is not only able to manipulate neural dialogue model, but it's also likely to be applied on black-box dialogue systems based on other methods , or even models for other natural language generation tasks . We will leave the investigations on these areas as future works.1. We show the potential of black-box dialogue models to be manipulated to produce target outputs. 2. We devise an RL-based framework TDGPN to effectively overcome the challenges associated with crafting inputs that enable black-box neural dialogue models to generate target outputs. 3. We conduct extensive experiments on a well-trained black-box neural dialogue model to verify the performance of the proposed framework.
"," Recently, neural network based dialogue systems have become ubiquitous in our increasingly digitalized society. However, due to their inherent opaqueness, some recently raised concerns about using neural models are starting to be taken seriously. In fact, intentional or unintentional behaviors could lead to a dialogue system to generate inappropriate responses. Thus, in this paper, we investigate whether we can learn to craft input sentences that result in a black-box neural dialogue model being manipulated into having its outputs contain target words or match target sentences. We propose a reinforcement learning based model that can generate such desired inputs automatically. Extensive experiments on a popular well-trained state-of-the-art neural dialogue model show that our method can successfully seek out desired inputs that lead to the target outputs in a considerable portion of cases. Consequently, our work reveals the potential of neural dialogue models to be manipulated, which inspires and opens the door towards developing strategies to defend them.",47
"          A variety of tasks use Voice Assistants  as their main user interface. VAs must overcome complex problems and hence they typically are formed of a number of components: one that transcribes the user speech , one that understands the transcribed utterances , one that makes decisions , and one that produces the output speech . Many VAs have a pipeline structure similar to that in Figure .  Our work is mainly focused on the DM sub-system and our primary contributions are: 1) proposing to decouple language understanding from information-state and modeling an affinity metric between them; 2) the identification of Multisource Denoising Autoencoder based pretraining and its application to learn robust fused representations; 3) quantifying robustness; 4) the introduction of a novel ranking algorithm using Energy-based models . In this work, we limit our scope to non-conversational utterances, i.e., utterances without followups containing anaphoric references and leave that for future work. We evaluate our approach on an internal dataset. Since our algorithm is primarily focused on leveraging inherent characteristics that are unique to large-scale real-world VAs, the exact algorithm may not be directly applicable to open-source Learning to Rank  datasets. But we hope our findings will encourage application and exploration of EBMs applied to LTR in both real-world VAs and other LTR settings.  The remainder of the paper is organized as follows: Section  discusses the task description while Section  covers the related work. Section  then describes the ranking algorithm, and Section  discusses the evaluation metrics, datasets, training procedure, and results.     We have presented a novel ranking algorithm based on EBM for learning complex affinity metrics between extracted meaning from user requests and user information-state to choose the best response in a voice assistant. We described a Multisource DAE pretraining approach to obtain robust fused representations of data from different sources. We illustrated how our model is also capable of performing zero-shot decision making for predicting and selecting intents. We further evaluated our model against other SOTA methods for robustness and show our approach improves relative-entropy.  
"," Voice Assistants aim to fulfill user requests by choosing the best intent from multiple options generated by its Automated Speech Recognition and Natural Language Understanding sub-systems. However, voice assistants do not always produce the expected results. This can happen because voice assistants choose from ambiguous intents --- user-specific or domain-specific contextual information reduces the ambiguity of the user request. Additionally the user information-state can be leveraged to understand how relevant/executable a specific intent is for a user request. In this work, we propose a novel Energy-based model for the intent ranking task, where we learn an affinity metric and model the trade-off between extracted meaning from speech utterances and relevance/executability aspects of the intent. Furthermore we present a Multisource Denoising Autoencoder based pretraining that is capable of learning fused representations of data from multiple sources. We empirically show our approach outperforms existing state of the art methods by reducing the error-rate by 3.8\%, which in turn reduces ambiguity and eliminates undesired dead-ends leading to better user experience. Finally, we evaluate the robustness of our algorithm on the intent ranking task and show our algorithm improves the robustness by 33.3\%.",48
"     Named entity recognition  aims to identify words or phrases that contain the names of pre-defined categories like location, organization or medical codes. Nested NER further deals with entities that can be nested with each other, such as the United States and third president of the United States shown in Figure , such phenomenon is quite common in natural language processing .   NER is commonly regarded as a sequence labeling task . These approaches only work for non-nested entities , but neglect nested entities. There have been efforts to deal with the nested structure.   introduced a layered sequence labeling model to first recognize innermost entities, and then feed them into the next layer to extract outer entities. However, this model suffers from obvious error propagation. The wrong entities extracted by the previous layer will affect the performance of the next layer. Also, such layered model suffers from the sparsity of entities at high levels. For instance, in the well-known ACE2005 training dataset, there are only two entities in the sixth level.   proposed a region-based method that enumerates all possible regions and classifies their entity types. However, this model may ignore explicit boundary information.    combined the layered sequence labeling model and region-based method to locate the entity boundary first, and then utilized the region classification model to predict entities. This model, however, cares less interaction among entities located in outer and inner layers.   In this paper, we propose a bipartite flat-graph network  for nested NER, which models a nested structure containing arbitrary many layers into two parts: outermost entities and inner entities in all remaining layers. For example, as shown in Figure , the outermost entity Thomas Jefferson, third president of the United States is considered as a flat  entity, while third president of the United States   and the United States  are taken as inner entities.  The outermost entities with the maximum coverage are usually identified in the flat NER module, which commonly adopts a sequence labeling model.  All the inner entities are extracted through the graph module, which iteratively propagates information between the start and end nodes of a span using graph convolutional network  . The benefits of our model are twofold:  Different from layered models such as , which suffers from the constraints of one-way propagation of information from lower to higher layers, our model fully captures the interaction between outermost and inner layers in a bidirectional way. Entities extracted from the flat module are used to construct entity graph for the graph module. Then, new representations learned from graph module are fed back to the flat module to  improve  outermost entity predictions. Also, merging all the entities located in inner layers into a graph module can effectively alleviate the sparsity of entities in high levels.  Compared with region-based models , our model makes full use of the sequence information of outermost entities, which take a large proportion in the corpus.  The main contributions of this paper can be summarized as follows:           This paper proposes a new bipartite flat-graph  model for nested NER which consists of two interacting subgraph modules. Applying the divide-and-conquer policy, the flat module is in charge of outermost entities, while the graph module focuses on inner entities. Our BiFlaG model also facilitates a full bidirectional interaction between the two modules, which let the nested NE structures jointly learned at most degree. As a general model, our BiFlaG model can also handle non-nested structures by simply removing the graph module. In terms of the same strict setting, empirical results show that our model generally outperforms previous state-of-the-art models.    
","  In this paper, we propose a novel bipartite flat-graph network  for nested named entity recognition , which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers. Bidirectional LSTM  and graph convolutional network  are adopted to jointly learn flat entities and their inner dependencies. Different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones , our model effectively captures the bidirectional interaction between them.  We first use the entities recognized by the flat NER module to construct an entity graph, which is fed to the next graph module.  The richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions. Experimental results on three standard nested NER datasets demonstrate that our BiFlaG outperforms previous state-of-the-art models.",49
"  Multi-task learning has recently emerged as a powerful paradigm in deep learning to obtain language  and visual representations  from large-scale data. By leveraging supervised data from related tasks, multi-task learning approaches reduce the expensive cost of curating the massive per-task training data sets needed by deep learning methods and provide a shared representation which is also more efficient for learning over multiple tasks. While in some cases, great improvements have been reported compared to single-task learning , practitioners have also observed problematic outcomes, where the performances of certain tasks have decreased due to task interference . Predicting when and for which tasks this occurs is a challenge exacerbated by the lack of analytic tools. In this work, we investigate key components to determine whether tasks interfere { from  theoretical and empirical perspectives. Based on these insights, we develop methods to improve the effectiveness and robustness of multi-task training.  There has been a large body of algorithmic and theoretical studies for kernel-based multi-task learning, but less is known for neural networks. The conceptual message from the earlier work  show that multi-task learning is effective over ``similar'' tasks, where the notion of similarity is based on the single-task models . The work on structural correspondence learning  uses alternating minimization to learn a shared parameter and separate task parameters.  use a parameter vector for each task and learn task relationships via  regularization, which implicitly controls the capacity of the model. These results are difficult to apply to neural networks: it is unclear how to reason about neural networks whose feature space is given by layer-wise embeddings.  To determine whether two tasks interfere constructively or destructively, we investigate an architecture with a shared module for all tasks and a separate output module for each task . See Figure  for an illustration. \todo{Our motivating observation is that in addition to model similarity which affects the type of interference, task data similarity plays a second-order effect after controlling model similarity.} To illustrate the idea, we consider three tasks with the same number of data samples where task 2 and 3 have the same decision boundary but different data distributions . We observe that training task 1 with task 2 or task 3 can either  improve or hurt task 1's performance, depending on the amount of contributing data along the decision boundary! \todo{This observation shows that by measuring the similarities of the task data and the models separately, we can analyze the interference of tasks and attribute the cause more precisely.}    Motivated by the above observation, we study the theory of multi-task learning through the shared module in linear and ReLU-activated settings. Our theoretical contribution involves three components: the {, and the { which can be used to measure the alignment of task data. By varying task covariances, we observe both positive and negative transfers from one task to another! We then provide sufficient conditions which guarantee that one task can transfer positively to another task, provided with sufficiently many data points from the contributor task. Finally, we study how to assign per-task weights for settings where different tasks share the same data but have different labels.  Experimental results. Our theory leads to the design of two algorithms with practical interest. First, we propose to align the covariances of the task embedding layers and present empirical evaluations on well-known benchmarks and tasks. On 5 tasks from the General Language Understanding Evaluation  benchmark  trained with the  average GLUE score, which is the standard metric for the benchmark. Further, we show that our method is applicable to transfer learning settings; we observe up to {2.5\%} higher accuracy by transferring between six sentiment analysis tasks using the LSTM model of .  Second, we propose an SVD-based task reweighting scheme to improve multi-task training for settings where different tasks have the same features but different labels. {On the ChestX-ray14 dataset, we compare our method to the unweighted scheme and observe an improvement of 0.4\% AUC score on average for all tasks .} In conclusion, these evaluations confirm that our theoretical insights are applicable to a broad range of  settings and applications.    \documentclass{article} % \usepackage{iclr2020_conference,times}  \usepackage{url,natbib} \usepackage{wrapfig} \usepackage{xspace}  \usepackage{multirow}   \usepackage{graphicx} \usepackage[us,12hr]{datetime} % \usepackage{float,caption,subcaption} {11pt}    \makeatletter [1]{% \textsuperscript{\@fnsymbol{#1}}% } \makeatother    \title{Understanding and Improving Information Transfer in Multi-Task Learning}  \author{Sen Wu\thanks{Equal contribution. Correspondence to \{senwu,hongyang,chrismre\}@cs.stanford.edu}\\ 	{Stanford University} 	\and Hongyang R. Zhang\printfnsymbol{1}\\ 	{University of Pennsylvania} 	\and Christopher R\'e\\ 	{Stanford University} }             \newpage \appendix         We studied the theory of multi-task learning in linear and ReLU-activated settings. We verified our theory and its practical implications through extensive synthetic and real world experiments.  Our work opens up many interesting future questions. First, could we extend the guarantees for choosing optimization schemes to non-linear settings? Second, a limitation of our SVD-based optimization scheduler is that it only applies to settings with the same data. Could we extend the method for heterogeneous task data? More broadly, we hope our work inspires further studies to better understand multi-task learning in neural networks and to guide its practice.  Acknowledgements. {Thanks to Sharon Y. Li and Avner May for stimulating discussions during early stages of this work. We are grateful to the Stanford StatsML group and the anonymous referees for providing helpful comments that improve the quality of this work. We gratefully acknowledge the support of DARPA under Nos. FA87501720095 , FA86501827865 , and FA86501827882 ; NIH under No. U54EB020405 , NSF under Nos. CCF1763315 , CCF1563078 , and 1937301 ; ONR under No. N000141712266 ; the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. H. Zhang is supported in part by Gregory Valiant's ONR YIP award . The experiments are partly run on Stanford's SOAL cluster. \footnote{https://5harad.com/soal-cluster/} The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.}
","   We investigate multi-task learning approaches that use a shared feature representation for all tasks.   To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task.   We study the theory of this setting on linear and ReLU-activated models.   Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning.   We show that misalignment between task data can cause negative transfer  and  provide sufficient conditions for positive transfer.   Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtain a 2.35\% GLUE score average improvement on 5 GLUE tasks over \bertlarge using our alignment method.   We also design an SVD-based task reweighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset. %",50
"  Neural models have shown remarkable performance improvements in a wide range of natural language processing  tasks. Systems of this kind can broadly be characterized as following a neural network design: we model the problem via a pre-defined neural architecture, and the resulting network is treated as a black-box family of functions for which we find parameters that can generalize well on test data. This paradigm leads to many successful NLP systems based on well-designed architectures. The earliest of these makes use of recurrent neural networks  for representation learning  , whereas recent systems have successfully incorporated fully attentive models into language generation and understanding .  In designing such models, careful engineering of the architecture plays a key role for the state-of-the-art though it is in general extremely difficult to find a good network structure. The next obvious step is toward automatic architecture design. A popular method to do this is neural architecture search . In NAS, the common practice is that we first define a search space of neural networks, and then find the most promising candidate in the space by some criteria. Previous efforts to make NAS more accurate have focused on improving search and network evaluation algorithms. But the search space is still restricted to a particular scope of neural networks. For example, most NAS methods are applied to learn the topology in a recurrent or convolutional cell, but the connections between cells are still made in a heuristic manner as usual .    Note that the organization of these sub-networks remains important as to the nature of architecture design. For example, the first-order connectivity of cells is essential to capture the recurrent dynamics in RNNs. More recently, it has been found that additional connections of RNN cells improve LSTM models by accessing longer history on language modeling tasks . Similar results appear in Transformer systems. Dense connections of distant layers help in learning a deep Transformer encoder for machine translation . A natural question that arises is: can we learn the connectivity of sub-networks for better architecture design?  In this paper, we address this issue by enlarging the scope of NAS and learning connections among sub-networks that are designed in either a handcrafted or automatic way . We call this the Extended Search Space method for NAS . Here, we choose differentiable architecture search as the basis of this work because it is efficient and gradient-friendly. We present a general model of differentiable architecture search to handle arbitrary search space of NAS, which offers a unified framework of describing intra-cell NAS and inter-cell NAS. Also, we develop a joint approach to learning both high-level and low-level connections simultaneously. This enables the interaction between intra-cell NAS and inter-cell NAS, and thus the ability of learning the full architecture of a neural network.  Our ESS method is simple for implementation. We experiment with it in an RNN-based system for language modeling. On the PTB and WikiText data, it outperforms a strong baseline significantly by 4.5 and 2.4 perplexity scores. Moreover, we test the transferability of the learned architecture on other tasks. Again, it shows promising improvements on both NER and chunking benchmarks, and yields new state-of-the-art results on NER tasks. This indicates a promising line of research on large-scale pre-learned architectures.  More interestingly, it is observed that the inter-cell NAS is helpful in modeling rare words. For example, it yields a bigger improvement on the rare entity recognition task  than that on the standard NER task .    We have proposed the Extended Search Space  method of NAS. It learns intra-cell and inter-cell architectures simultaneously. Moreover, we present a general model of differentiable architecture search to handle the arbitrary search space. Meanwhile, the high-level and low-level sub-networks can be learned in a joint fashion. Experiments on two language modeling tasks show that ESS yields improvements of 4.5 and 2.4 perplexity scores over a strong RNN-based baseline. More interestingly, it is observed that transferring the pre-learned architectures to other tasks also obtains a promising performance improvement.  
","  Neural architecture search  has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures . For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition  tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.",51
"  Lip reading aims to infer the speech content by using visual information like lip movements, and is robust to the ubiquitous acoustic noises  in our life.  %  This special property makes it important for automatic speech recognition in noisy or silent scenarios . % With the rapid development of deep learning technologies and the recent emergence of several large-scale lip reading datasets , there have been several appealing results in recent years . % However, almost all of the existing methods focus on the problem of monolingual lip reading. % In this paper, we try to make an exploration of multilingual lip reading, which has not been considered before to the best of our knowledge. %    Limited by the structure of our vocal organs, the number of distinguishable pronunciations we could make is finite. So the set of distinguishable pronunciations in each language is finite, leading to many common pronunciations shared among different languages. % For example, there are as many as 32 phonemes existing in both English and Mandarin words, as shown in Figure .. Figure . provide some example words with their corresponding phoneme-based representations. The same phonemes in different languages would generate the same or similar lip movements even though the speakers are of different languages, as shown by Figure .. % Besides, knowledge sharing and transfer among different languages could further help the unique model shared by different languages learn more easily than learning separately from every single language. % These factors make us think it possible to perform a synergize learning of  multilingual lip reading. %  Each language has its own rule to compose different units  into a valid word. If we could make the lip reading model master the composition rules for each language, it should be able to obtain good recognition results when meeting these languages. %   Based on this idea, we consider the learning process of the composition rule for each language as to learn a fill-in-the-blank problem according to the correct rules.  If the model could make correct predictions for any missing units, no matter which language the input is, as long as its previous and later context is given, then the decoder module should be also effective to compose correct phonemes into correct words in the multilingual lip reading setting. %  Therefore, a novel synchronous bidirectional learning  block is introduced to construct the decoder module to finish our prediction process for the multilingual lip reading problem.  Overall, the main contributions could be summarized as follows.  \setlength{            Inspired by the related multilingual study in the field of automatic speech recognition and NLP, we try to explore the possibility of multilingual synergized lip reading with large scale datasets for the first time. The phonemes are introduced as the modeling units to bridge different languages. And a new synchronous bidirectional learning manner is introduced to unify the two-directional context together in each block, to enhance the learning of each language. Both the proposed model and the learning process are not related to some specific properties of some single language, so it can be directly employed to three or more languages. Limited by the available large-scale lip reading datasets, we perform a thorough evaluation and analysis on the English and Mandarin datasets. Our work achieves new state-of-the-art performance on both the two challenging benchmarks, LRW  and LRW-1000 .   
"," 	% 闁搞儳鍋熼悺鐔哥▔閵堝嫰鍤嬮梻鍌ゅ櫍椤ｄ粙鏁 	%  濠㈣埖淇洪銏㈡嚊闁汇劌鍩恑p reading闁哄嫷鍨伴幆渚宕ｉ婵愭斀闁挎稒鑹鹃崹搴ｇ尵閼姐倖鐣遍柛姘椤帞绱掗崟顐ｅら柕鍡曟祰琚欓柣顔昏兌濞堟垿宕ラ崟顓ф綒缂備礁瀚幃 	%  闁圭粯鍔曢崵顓熺▔缂佸绉撮幃鎾愁潰閵夈儱钃熼柛姘灱琚欓柣顔昏兌濞堟垶寰勫鎰佸殧閻熺⿻ip reading 	%  閺夊牊鍎抽崺瀹籵ta 	%  % 闁哄牜鍓氶弸鍐礂閾忣偅鏆堥悗鐢殿攰閽栧嫰鏁嶅顒夋▼閻犲浂鍙閳诲牓鎳曢弬鎸庡ら柣銊ュ煇ip reading % 闁告垵鎼ぐ鍌炴倷閻у摜绐楁慨锝呯箳椤帞鎷犻鈾鏋呴柣銊ュ閻撳墎妲愰悩杈╊吅闂傚倸顕ù澶嬬閹烘洜绠鹃柟鎭掑劜濡叉悂寮垫径澶岊伇閻庤淇洪～澶婎嚗鐎ｎ剚鐣遍柨娑欑☉椤┭囧几濠婂媭渚宕圭ｎ偄绗侀柟鐑╄尙鍟婂☉鎾崇Т閹捇鎯冮崟顕呭殧閻熺兘骞嶉悗鐢垫嚀缁ㄦ煡鎯冮崟顔剧缂佸绉烽～澶婎嚗鐎ｅ墎绀夐梺顓ㄧ到閻ｇ姷浜搁崡鐐茶濞寸姰鍎辨禒娑欏緞濮樻剚鍤旈悷鐑芥儍閸掔灇p reading濞 %  % 闁糕晞妗ㄧ花顒顫㈤妶蹇曠闁瑰瓨鍨冲⿰鎴﹀箵閹邦剙姣夊ù婊冩憱BL婵℃妫欓悘锕傚Υ閸屾氨娈洪悹鍥跺弨閳诲牏鎲撮崟顐や紣闁汇劌瀚鐔哥▕閻樼粯锛栧Λ鐗堬公缁辨繃娼鐎靛弶绋夐搹鍦埗閻庤鐭粭鍌涚▔鐎ｎ偅鐎柣銊ュ閸庡繘宕橀崗鍝ョ憮閻庣數鎳撶紞瀣礈瀹ュ懎绀嬮柛蹇撳暟濞堟垿骞掗妸锔界劷闂傚偆鍣ｉ。浠嬪Υ %  Lip reading has received increasing attention in recent years. This paper focuses on the synergy of multilingual lip reading. There are about as many as 7000 languages in the world, which implies that it is impractical to train separate lip reading models with large-scale data for each language. Although each language has its own linguistic and pronunciation rules, the lip movements of all languages share similar patterns due to the common structures of human organs. Based on this idea,  we try to explore the synergized learning of multilingual lip reading in this paper, and further propose a synchronous bidirectional learning  framework for effective synergy of multilingual lip reading.  We firstly introduce phonemes as our modeling units for the multilingual setting here. Phonemes are more closely related with the lip movements than the alphabet letters. At the same time, similar phonemes always lead to similar visual patterns no matter which type the target language is. Then, a novel SBL block is proposed to learn the rules for each language in a fill-in-the-blank way. Specifically, the model has to learn to infer the target unit given its bidirectional context, which could represent the composition rules of phonemes for each language. To make the learning process more targeted at each particular language, an extra task of predicting the language identity is introduced in the learning process. Finally, a thorough comparison on LRW  and LRW-1000  is performed, which shows the promising benefits from the synergized learning of different languages and also reports a new state-of-the-art result on both datasets. %",52
" The rise of deep learning has made more complex sequence generation tasks feasible. Text-based generation of natural speech has been continuously investigated over the past decades. Concatenative synthesis with unit selection and statistical parametric speech synthesis were the state-of-the-art systems for many years. However, such systems require lots of human labour and are unsatisfactory for lacking naturalness. Recently, a sequence-to-sequence architecture, Tacotron, has greatly improved the naturalness and similarity of speech synthesis compared to traditional statistical parametric speech synthesis system. Tacotron, usually followed by a traditional or neural vocoder, takes linguistic feature and speaker identity as input and generates mel-spectrogram as output. Unfortunately, when dealing with out-of-domain or abnormal texts inputs, Tacotron-like attention based end-to-end structures could render unacceptable errors, including skipping, repeating, long unexpected pause and attention collapse. More recently, stepwise monotonic attention  method, which is based on monotonic attention, was proposed to enforce strict constraint to meet the demand of locality, monotonicity and completeness in the speech synthesis process.   As far as we know, building a naturally speaking TTS system requires at least ten hours of recording audio. Moreover, every audio utterance should be recorded in a professional recording studio and the transcribed phonemes should be evenly distributed. Preparing such a large amount of high-quality data with multiple speakers is impractical and extremely expensive. Typically, it's troublesome and unnecessary to let native Chinese speaker to say English if he knows little about English. Moreover, there is no chance to gather 10 hours training data for a specific person like a pop star. The only resources we can get are the limited talks or shows from TV. Therefore, utilizing a few minutes of audio and synthesizing arbitrary speech in target's voice remains a very important task.   However, building TTS system with limited data often sacrifices quality and reliability. To scale the capacity for new speakers, we can adapt existing pre-trained multi-speaker system to generate new speakers' voice, which is a well-studied subject of few-shot learning also known as speaker adaptation. There are mainly two approaches here: the first is just to update the new speaker embedding and combine it with linguistic feature as inputs to a TTS model, which may require a very strong speaker encoder network trained by thousands of speakers; the second is to fine-tune the entire multi-speaker network to select a optimal single-speaker model. Although fine-tuning can combine the advantages of multiple speakers and achieve a new speaker's better performance, as we described before, end-to-end attention models such as Tacotron-like models may meet unpredictable instability and bad cross-lingual speaking in few-shot learning settings. To achieve naturalness and robustness in speech synthesis, FastSpeech and duration informed attention network  have been recently proposed to overcome the unexpected errors of end-to-end systems by combining duration information of traditional statistical parametric speech synthesis system. The former FastSpeech is a non-autoregressive feed-forward framework without attention. The latter DurIAN, originally proposed for multi-modal speech synthesis, is an autoregressive framework which achieves robustness and naturalness by using skip state encoder and combining duration with windowed content-based attention.     To improve the scalability of TTS in few-shot speaker adaptation, we introduce AdaDurIAN, an adaptive neural TTS system based on DurIAN, with the ability to synthesize natural cross-lingual speech in a new speaker's voice with just few minutes of monolingual data. We investigate it in three different aspects that have not been fully explored in previous work. First, we employ sequences of phoneme and tone  to achieve a robust speaker-independent content encoder, and incorporate the concatenated representation of speaker characteristics into the output states of content encoder. Second, instead of fine-tuning weights of the whole architecture, we found a key aspect that only fine-tuning the speaker embedding and decoder network leads to fewer pronunciation errors. Last, to generate the smooth mel spectrograms in a streaming inference manner, we adopt a time-delayed LSTM post-net instead of a global CBHG-like module. Through various evaluations, our proposed AdaDurIAN significantly surpasses the Tacotron-like model in terms of naturalness, speaker similarity and cross-lingual speaking, and also shows its promising performance in few-shot emotion transfer tasks.  The rest of this paper is organized as follows. Section describes the detailed architecture of AdaDurIAN and the speaker adaptation strategy. The experiment setup and evaluations are presented in Section . Concluding remarks are summarized in the final section.    In summary, we proposed AdaDurIAN, a few-shot adaptive neural TTS system for higher naturalness and speaker similarity. We described the improvements of AdaDurIAN over original DurIAN and demonstrated the adaptation strategy when the speaker's data is very limited. Based on AdaDurIAN, we performed several few-shot speaker adaptation tasks to evaluate the stability, naturalness, speaker similarity and emotion transfer ability. The evaluations show that, compared with Tacotron-like model, AdaDurIAN has both higher MOS of naturalness, more preferences of speaker similarity and especially fluent cross-lingual speaking. Furthermore, we also applied AdaDurIAN in emotion transfer tasks and showed its promising performance.  \vfill\pagebreak      
"," This paper investigates how to leverage a DurIAN-based average model to enable a new speaker to have both accurate pronunciation and fluent cross-lingual speaking with very limited monolingual data. A weakness of the recently proposed end-to-end text-to-speech  systems is that robust alignment is hard to achieve, which hinders it to scale well with very limited data. To cope with this issue, we introduce AdaDurIAN by training an improved DurIAN-based average model and leverage it to few-shot learning with the shared speaker-independent content encoder across different speakers. Several few-shot learning tasks in our experiments show AdaDurIAN can outperform the baseline end-to-end system by a large margin. Subjective evaluations also show that AdaDurIAN yields higher mean opinion score  of naturalness and more preferences of speaker similarity. In addition, we also apply AdaDurIAN to emotion transfer tasks and demonstrate its promising performance.",53
" % 1 page  % Background Introduction of Commonsense Knowledge and Winograd Commonsense reasoning, as an important problem of natural language understanding, has attracted much more  attention in the NLP community recently.  Among all developed commonsense reasoning tasks, the Winograd Schema Challenge , which is a hard pronoun coreference resolution task, is one of the most influential ones. All questions in WSC are grouped into pairs such that paired questions have minor differences , but reversed answers.  For each question, we denote the other question in the same pair as its reverse question.  One pair of the WSC task is shown in Figure. Based on the design guideline of WSC, all commonly used features  do not have any effect. Human beings can solve these questions because of their shared commonsense knowledge. For example, ordinary people can know that the pronoun `it' in the first sentence refers to `fish' while the one in the second sentence refers to `worm' because `hungry' is a common property of something eating things while `tasty' is a common property of something being eaten.        % Limitation of existing approaches Conventionally, people tried to leverage crowd-sourced commonsense knowledge bases or search engines to solve the WSC task, but performances of these models are not satisfying. Recently, pre-trained language representation models have demonstrated significant improvements in both unsupervised and supervised settings. However, as these approaches treat the concept `commonsense knowledge' as a black box, we are not clear about why they can do better  and do not know how to further improve them. To answer these two questions, in this work, we present the first deep diagnosis of essential commonsense knowledge for answering WSC questions. Specifically, we invite annotators to first provide reasons for why they choose the answers when they answer the questions, and then group all the WSC questions by different types of used commonsense knowledge . By doing so, we can then analyze what kinds of commonsense knowledge can be well represented and understood by current models and more importantly, we can be clear about what kinds of commonsense knowledge are still challenging for current models, which could be an important future research direction for solving not only the WSC task but also the general commonsense reasoning problem.       % Creation of WinoWhy After the diagnosis, based on the collected reasons, we also create a new task WinoWhy, which aims at better evaluating models' abilities to understand commonsense knowledge.  For each question in the WSC task, we pair it with several reasons. Models are required to distinguish the correct reasons from all very similar but wrong candidates. From examples in Figure, we can see that even though all candidates are highly related to the original question, only one of them is the correct reason for resolving the coreference relation. Experimental results show that even though state-of-the-art models can achieve about 90\% accuracy on the original WSC task, they are still struggling on WinoWhy questions, which shows that current models are still far away from understanding the commonsense knowledge. Moreover, by conducting experiments on both WSC and WinoWhy tasks, we prove that even though supervised models can achieve better performance, these models can be sensitive to the dataset distribution, which indicates that the improvement is probably coming from better capturing the statistical bias of the dataset rather than better understanding the required commonsense knowledge.    % To conclude, the contributions of this paper are as follows:  We present the first comprehensive study of essential commonsense knowledge for answering the WSC task;  We create WinoWhy, which is a more challenging task to evaluate models' ability of understanding commonsense knowledge. The rest of the paper is organized as follows.  In Section, we present the diagnosis of essential commonsense knowledge for answering WSC questions, which includes the reason collection and categorization. After that, we show how we create WinoWhy in Section. In Sections and, we introduce the detailed experiments and analysis on both the original WSC and the proposed WinoWhy tasks. We introduce the related work about commonsense reasoning in Section.  In the end, we conclude this paper with Section.       In this paper, we presented the first deep diagnosis of essential commonsense knowledge for answering Winograd Schema Challenge questions.  By doing so, we better understand the strengths and limitations of current commonsense reasoning models.  More importantly, we better know about what kinds of commonsense knowledge are required to be acquired for better commonsense reasoning. On top of the collected reasons, we develop a new task called WinoWhy, which requires models to select the plausible reasons for answering WSC questions. Experiments show that even though current models have gained significant improvement over the original WSC task, they still cannot fully understand the reasons behind.   Both datasets and code are released.  
"," In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge . For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories. By doing so, we better understand the limitation of existing methods  and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning. Moreover, to investigate whether current WSC models can understand the commonsense or they simply solve the WSC questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called WinoWhy, which requires models to distinguish plausible reasons from very similar but wrong reasons for all WSC questions. Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original WSC dataset, they are still struggling at WinoWhy. Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution. WinoWhy and all codes are available at: \url{https://github.com/HKUST-KnowComp/WinoWhy}. % On top of that, we develop a new task called WinoWhy, which evaluates models' ability of understanding the commonsense knowledge by asking them to select the most  % In the end, by leveraging the existing commonsense knowledge resource, we propose the first explainable commonsense reasoning model, which outperforms all the previous models significantly on both the original Winograd schema challenge task and the new WinoWhy task.",54
"  %\preethi{intro is a all over the place. follow the flow in the abstract. start by saying no sota models for clinical QA, large scale emrQA enables exploring recent advances in neural, transformer based models and establish sota benchmarks for clinical qa. in doing this, we focus on improving certain abilities of these models that are critical in the clinical domain. physicians may phrase a question in a manner that has previously not been observed by the model at train time. yet we want the model to be able to generalize to these questions. hence, we propose ... clinical text has abundant domain specific terminology abbreviations. medical entities are used to describe conditions affecting the patient, tests, vitals to disgnose problems and meds and procs for treating problems. one way to capture clinical domain information is via fine tuning pretrained embeddings on clinical text. however there maybe benefits to also embedding meta information about the text such as entities. certain patterns may emerge that might help qa. hence we propose ... }  %Clinical question answering is quite an important task as it could potentially help the clinicians as well as patients in order to extract important patient's information from their electronic health records .   The field of question answering  has seen significant progress with several resources, models and benchmark datasets. Pre-trained neural language encoders like BERT  %, XLNet   and its variants  have achieved near-human or even better performance on popular open-domain QA tasks such as SQuAD 2.0 % as disease or syndrome  provides common information between such medical terminologies. Incorporating such entity information about tokens in the context and question can further improve the performance of QA models for the clinical domain. % Pre-trained neural models, such as BERT , are trained on open-domain corpus such as WikiText and Book corpus  and hence, do not have good representations for a wide range of medical terminologies. Fine-tuning on medical corpora like MIMIC-III  has helped in improving the performance of the these models .    %  Pre-trained neural models, such as BERT , are trained on open-domain corpus such as WikiText and Book corpus  and hence, do not have good representations for a wide range of medical terminologies. Fine-tuning on medical corpora like MIMIC-III  has helped in improving the performance of the these models . However, a challenge for developing QA models for clinical records is that different physicians can use different medical terminology to express the same entity; a physician may note down the condition as ``heart attack'' whereas the other could mention it as ``myocardial infarction''. \preethi{this may not be picked up by fine tuning as similar as there many not be enough instances of this disambiguation in similar contexts? if yes, then maybe edit the next line to convey that better.}  The limited amount of data in clinical domain makes it even harder for the models to learn efficiently. \preethi{this next line seems disconnected -- what will SNOMED do here? someone needs to annotate the 2 entities with the same code.} \weihung{The two concepts won't be the same code in SNOMED but they may have the relation of `is a'. While finetuning, it is possible for the model to learn the representation that the similar context can be much closer even though they are not exactly the same coding. But of course the amount of training data is the key.}  Though clinical ontologies such as SNOMED  can provide common information between these different medical terminologies in the form of same entity. \weihung{also treat the entity at the concept-level but not only at the word-level} Thus, embedding entity information along with word embeddings may help resolve these similar entities and enable better QA on clinical notes.    %In this work, we explore state-of-the-art model for the community-shared clinical question answering data, emrQA . We evaluate the performance of the models in a realistic setting where there is no overlap between the paraphrases of the questions observed in train and testing sets.   %The best performing model should be able to generalize well and not just memorize the questions. In order to model the intent we decided to use the logical forms of the questions, our model learns to predict the logical form of the question along with extracting the span of the answer from the context. We also incorporate the entity information via ERNIE  architecture to utilise the ontology information. This helps our proposed model in improving the overall performance by . %\preethi{how many of the now correctly predicted questions are unseen during train here? you can claim generalization ability here as well.}  %  %      as it provides explainability for the clinicians. %   %%%%%%%%%%%%%% Problem Formulation %%%%%%%%%%%%%%%%%%%%%%%%%%%%   The proposed entity-enriched QA models trained with an auxiliary task improve over the state-of-the-art models by about  across the large-scale clinical QA dataset, emrQA  . We also show that multitask learning for logical forms along with the answer results in better generalizing over unseen paraphrases for EMR QA. The predicted  logical forms also serve as an accompanying justification to the answer and help in adding credibility to the predicted answer for the physician.  
"," We explore state-of-the-art neural models for question answering on electronic medical records and improve their ability to generalize better on previously unseen  questions at test time. We enable this by learning to predict logical forms as an auxiliary task along with the main task of answer span detection. The predicted logical forms also serve as a rationale for the answer. Further, we also incorporate medical entity information in these models via the ERNIE  architecture. We train our models on the large-scale  emrQA dataset and observe that our multi-task entity-enriched models generalize to paraphrased questions $ % \preethi{on the same strict split right? yeah, we. can report that} %We propose a multi-task clinical question answering model that utilizes logical form, of the question, to generalize well over unobserved paraphrased questions. We incorporated the entity information, obtained from \verb+MetaMap+, in Bidirectional Encoder Representations from Transformers  based models by utilizing its entity enriched variant: Enhanced Language Representation with Informative Entities . We experiment with two clinical datasets: emrQA and MADE and observed that there is a significant improvement  over state-of-the-art model . We observed that adding an auxillary task that provides clinical domain-knowledge with the help of the logical forms of the questions, improves the performance of existing state-of-the-art models. Additionally, the predicted logical forms also serve as a rationale for the answer. % domain-knowledge via entities and logical forms that help model generalization to unobserved paraphrased questions at test time. We experiment with multiple clinical datasets  and show significant improvements  over models that do not use entity and logical form information in training. We observe that adding an auxiliary task  that provides domain-knowledge with the help of the logical forms of the questions, improves the performance of existing state-of-the-art models. Additionally, the predicted logical forms also serve as a rationale for the answer. %  improves the performance in both domain-specific and open-domain QA tasks. Additionally, the predicted logical forms also serve as a rationale for the answer.",55
"  {T}{h}e worldwide spread of the COVID-19 infectious disease resulted with a pandemic that has threatened millions of lives. Social media has been playing a major role in fighting the virus and its impact through a multitude of measures including the continuous transmission of local and global updates about the pandemic as well as issuing warnings and and guidelines for dealing with the pandemic and its aftermath. According to Statista , an estimated 2.95 billion people in 2019 used social media worldwide. The number is projected to increase to 3.43 billion in 2023. One remarkable statistic is around the continually changing demographic of new consumers and the increase in social media penetration reach. For example, while in 2018 the Pew Research Centre  reported that 閳ユ笗ost Americans continue to get news on social media, even though they may have concerns about its accuracy閳. Numerous surveys have been undertaken to capture the online behavior of news consumers worldwide, and the trend seems to be that social media platforms are highly influential when it comes to acquiring news stories, for the majority of people. In a large-scale study conducted in 2019 by Ofcom , the UK government閳ユ獨 regular for the communications services that are used by the public, it was shown that 閳ユ窏alf of the adults in the UK now use social media to keep up with the latest news"". Furthermore, governments and major centers for disease control, including the World Health Organization  and the Centers for Disease Control and Prevention , are relying on social networks as a mean for managing the evolving pandemic by regularly disseminating guidance and updates and by providing emergency responses.   The dark side of social media was exhibited in a tsunami of fake and unreliable news that ranged from selling fake cures to using the social media as a platform to launch cyberattacks on critical information systems. This led the {, according to WHO Director-General Tedros Adhanom Ghebreyesus at the Munich Security Conference on Feb 15, 2020 . Moreover, various researchers and news outlets  tackled the rising {. For example, malicious users have used social media platforms such as {, { and { resulted with organizations, governments and business leaders exercising excessive pressure on the social media platforms in order to curtail the flood of fake news and viral misinformation. This became a priority in order to ensure that people who are in lockdown would receive the appropriate information in order to do the appropriate thing, control the disease and mitigate its impact. Although social networks platforms have plans for mitigating and banning harmful content, it is apparent that they themselves were not well prepared and needed an emergency plan in order to respond to COVID-19 { approaches in the literature tackling the identification of credible content in social media, the topic was not of high priority for the research and industrial communities. In fact, there was no justification for investing in this research direction.  This paper aims to address the aforementioned problems while tackling the evolving challenges using a large dataset that was extracted from twitter targeting COVID-19. The study uses a data analytics approach based on tweets meta-data, text and context, as well as users meta-data and profiles. The paper explores extensively one million COVID-19-related tweets that were collected over a period of two months and belonging to 288K users. The analysis of the unique users' profiles, meta-data and context of the tweets allowed us to deduce various important findings and insights while providing guidance for potential solutions. To the best of our knowledge, except Li et al.  who characterized the propagation of situational information in social media during COVID-19, no computing-related work has yet empirically addressed the positive or negative impact of social networks { refers to a unique tweet excluding the retweets, { refers to total number of followers of the user who initiated the unique tweet and reflects the number of tweeters that may potentially see and interact with it. The initial results indicate that around 16.1\% of the tweets  are exploiting COVID-19 contexts for advertisement, redirecting users to out of scope topics or even maliciously misleading the community. A further ontology-based analysis on the context and users' meta-data confirms that only 3.5\% of the unique users initiating the tweets have medical profile while 2.8\% are virus specialists. Accordingly, at least of 93.7\% of the COVID-related tweets  may be transmitting misleading or unverified medical information. On the other hand and in order to highlight the importance of non-medical users in spreading important information in such situation, a deeper analysis was performed to identify unique users with key specialties. Results reaffirmed our initial findings and show that users with context-relevant occupations such as doctor, writer, reporter, journalist, editor and governor do not even constitute 1\% of the total reach count .  Accordingly, these insights illustrated the need to identify relevant influencers in specific contexts and seek their help in order to disseminate verified and reliable information. Finally, it is important to note that the infodemic that is impacting social media including Facebook, Instagram, Snapchat, etc. is by order of magnitude bigger .   The contributions of this work are three folds:   during COVID-19 pandemic. To the best of our knowledge, no computing-related work has yet addressed and studied through experiments either the positive or the negative impact of social networks on defeating COVID-19.   The remainder of this paper is organized as follows. In Section , we illustrate the study's research methodology while in Section  we provide an analysis of the impact of misleading twitter contexts. Section  provides empirical analysis of the impact of COVID-19 related posts per user specialty and occupation.  Section  details our research findings and directions while Section  concludes with comments.         This paper investigated the COVID-19 infodemic negative impact on the major efforts to defeat the pandemic through a novel large-scale Twitter-based study, which provided quantitative assessment using real-life experiments reflecting the actual environments. The empirical analysis of 1 million COVID-19-related tweets belonging to 288K unique users illustrated the severe impact of misleading people and spreading unreliable information. Inferred insights showed that  the potential reachability of the 16.1\  tweets that misled users by redirecting them to out of scope and/or malicious content is 5.6 billion, and  a minimum of 93.7\  of the remaining within-context 83.9\  tweets  were initiated by users with non-reliable medical and/or relevant speciality profiles, and consequently might be disseminating misleading non-credible medical information. Moreover, different insights highlighted the low reachability  of the unique users with key context-relevant specialties and occupations such as doctor, writer, reporter, journalist, editor and governor. The results shed the light on the importance of identifying non-medical key influencers for assisting in spreading legitimate information relevant in such situations. Finally, the paper elaborated on few computing and non-computing implications as well as future research directions to highlight the potential solutions and future work in such a promising field.  \ifCLASSOPTIONcaptionsoff    \fi  
"," News creation and consumption has been changing since the advent of social media. An estimated $2.95$ billion people in 2019 used social media worldwide. The widespread of the Coronavirus COVID-19 resulted with a tsunami of social media. Most platforms were used to transmit relevant news, guidelines and precautions to people. According WHO, uncontrolled conspiracy theories and propaganda are spreading faster than the COVID-19 pandemic itself, creating an {. Extensive analysis has been performed on approximately $1$ million COVID-19 related tweets collected over a period of two months. Furthermore, the profiles of $288,000$ users were analyzed including unique users' profiles, meta-data and tweets' context. The study noted various interesting conclusions including the critical impact of the  exploitation of the COVID-19 crisis to redirect readers to irrelevant topics and  widespread of unauthentic medical precautions and information. Further data analysis revealed the importance of using social networks in a global pandemic crisis by relying on credible users with variety of occupations, content developers and influencers in specific fields. In this context, several insights and findings have been provided while elaborating computing and non-computing implications and research directions for potential solutions and social networks management strategies during crisis periods.",56
" %1. Mention Where are tables used, why we study stuff about tables As an efficient way to organize and display data, tables are broadly used in different applications: researchers use tables to present their experimental results; companies store information about customers and products in spreadsheets;  flight information display systems in the airports show flight schedules to passengers in tables.  %2. Mention various applications based on Tables According to Cafarella et al., there are more than 14.1 billion tables on the Web.  Among those tables, many are very informative which means they include relations and attributes of real-world entities, and have been used for a variety of downstream tasks.  For example, tables like Wikipedia infoboxes have been used to construct knowledge bases since they are of high quality and consistent structure . %pivk2006automatic Data-to-text models take tables from specific domains as input and transform them into fluent natural language sentences such as sports news  %, weather reports  and product descriptions . % With structure information and metadata, tables store factual knowledge and therefore are also used to build question answering  systems  % .   %3. Specifically mention table retrieval,  Describing the current progress        We have addressed the problem of ad hoc table retrieval with the deep contextualized language model BERT. Considering the structure of a table, we propose three content selectors to rank table items in order to construct input for BERT which effectively utilize useful information from tables and overcome the input length limit of BERT to some extent.   We combine BERT features and other tables features to solve the table retrieval task as a pointwise regression problem. Our proposed Hybrid-BERT-Row-Max method outperforms the previous state-of-the-art and BERT baselines with a large margin on WikiTables dataset. Through empirical experiments, we find that using the max salience selector with row items is the best strategy to construct BERT input. Overall, we also find that sum salience selector is the best for cell items. While for column items, mean salience selector only seems to be the best when a feature-based approach is used. We further show that the feature-based approach of BERT is better than jointly training BERT with a feature fusion component.  We also conduct experiments on WebQueryTable dataset and demonstrate that our method generalizes to other domains.   Our analysis on fine-tuned BERT shows that various sequence-level features are captured by the self-attention of BERT and [CLS] embedding tends to aggregate sequence-level information, which could explain why using it as features is effective for the ad hoc table retrieval task.   We also find that [SEP] embeddings from the last layer of BERT are very close to query embeddings, which indicates that making use of [SEP] has the potential to further improve the performance.   Future work Though the motivation behind this paper is that different content selection strategies should be used for different queries, we do not explore how to design a model to choose the best selector. In fact, it is possible that for different types of queries, we should choose different content selector. Future work could design a framework that automatically chooses the strategy considering the query types.  Besides, designing pretraining tasks for tables and pretraining BERT on a large table collection could be promising to further improve the performance of BERT on table-related tasks such as table retrieval.  
"," Pretrained contextualized language models such as BERT have achieved impressive results on various natural language processing benchmarks. Benefiting from multiple pretraining tasks and large scale training corpora, pretrained models can capture complex syntactic word relations.  In this paper, we  %study how to  use the deep contextualized language model BERT for the task of ad hoc table retrieval. We investigate how to encode table content considering the table structure and input length limit of BERT. We also propose an approach that incorporates features from prior literature on table retrieval and jointly trains them with BERT. In experiments on public datasets, we show that our best approach can outperform the previous state-of-the-art method and BERT baselines with a large margin under different evaluation metrics.  %We also demonstrate that our methods  generalize by achieving similar improvements on the open domain WebQueryTable dataset.",57
" Intensive Care Units  are the last line of defense against critical conditions that require constant monitoring and advanced medical support. Their importance has been highlighted in recent times, when ICUs around the world have been overrun by the COVID-19 pandemic . It is in times like these when research into ways to adequately manage scarce critical care resources must be even more vigorously pursued, in order to offer additional tools that support medical decisions and allow for the effective benchmark of clinical practice.\\  The issue of mortality prediction in the ICU has been approached from a statistical standpoint by means of risk prediction models like APACHE, SAPS, MODS, among others . These models use a set of physiological predictors, demographic factors, and the occurrence of certain chronic conditions, to estimate a score that serves as a proxy for the likelihood of death of ICU patients. Because of the relatively straightforward way of interpreting results, simple statistical approaches such as logistic regression are the go-to modeling techniques used to estimate mortality probability and the importance of the predictors involved. On the other hand, the simplicity of the models also mean that their limited expressiveness may not accurately represent the possibly non-linear dynamics of mortality prediction. Given this, high-capacity machine learning models might be useful to increase predictive performance. Concretely, the relevant literature shows that the use of deep learning models trained on physiological time-series data can outperform these previously mentioned statistical models . \\  One of the advantages of deep learning over other techniques is its ability to use multiple modes of data to train predictive models. In the biomedical domain, health records, images, and time-series data, have been used for different tasks with success . This advantage is relevant for mortality prediction , as a substantial amount of data is generated inside ICUs as free-text notes which can be used as input to create Natural Language Processing  predictive models. The nature of NLP poses some challenges for which deep learning is uniquely suited via its ability to deal with high-dimensional data and its elegant way to take temporal and spatial patterns into account. Some works have used deep neural networks and free-text to predict mortality  and length of stay , showing that there is interesting potential for this type of models.\\  On the other hand, a particularly important downside of deep learning is that, compared to the simpler logistic regression based models, feature importance is not as readily available. This in turn makes these models hard to interpret, as internally the model may transform the original input features to high-dimensional spaces via non-linear transformations, making it hard to establish the impact of each predictor on the predicted outcome. It has been documented that given their large predictive capacity, deep learning models can easily fit spurious correlations in the datasets used for their training, leading to potential diagnostic issues . However some work has been done to interpret deep learning models in order to offer explanations intended to foster trust and further encourage their usage in the critical care setting. For instance, in our previous work we developed an interpretable deep learning mortality prediction model that uses physiological time-series data from the first 48 hours of patient ICU stay .\\  In this work, we present ISeeU2, a deep learning model that uses free-text medical notes from the first 48 hours of stay to predict patient mortality in the ICU. We use the MIMIC-III database  to train a convolutional neural network  that is able to use raw nursing notes with minimal pre-processing to efficiently generate a prediction, and we couple the prediction of mortality with word importance and sentence importance visualizations, in a way that annotates the original medical note to show what parts of it are more predictive for death or survival, according to the model.    In the past some works have used deep learning to predict ICU mortality using free text. Grnarova et al  proposed the use of a convolutional neural network for ICU mortality prediction using free-text medical notes from MIMIC-III. They used all medical notes from each patient stay to predict mortality, and trained their model using a custom loss function that included a cross-entropy term involving mortality prediction at the sentence as well, with promising results. Jo et al  used a hybrid Latent Dirichlet Allocation  + Long Short Term Memory  model for ICU mortality prediction trained on medical notes from MIMIC-III, in which the LSTM used the topic LDA features as input. Suchil et al  used stacked denoising autoencoders to create patient representations out of medical free-text notes, to be used for downstream tasks as mortality prediction. Si et al  proposed the use of a ConvNet for multitask prediction , using all available patient medical notes up until time of discharge. Jin et al  proposed a multimodal neural network architecture and a Named Entity Recognition  text pre-processing pipeline to predict in-hospital ICU mortality using all available types of free-text notes and a set of vital signs and lab results from the first 48 hours of patient stay, extracted from MIMIC-III.\\  Most of these works include some ad-hoc interpretability mechanism: Grnarova et al  included a sentence-based mortality prediction target which is then used to score individual words according to their associated predicted mortality probability, Jo et al  used LDA-computed weights to provide word importance, Suchil et al  used a gradient-based interpretability approach to compute the importance of words in the input notes.\\  Our work has key differences relative to those from the related literature. As opposed to , we only use notes from the first 48 hours of patient stay instead of all notes available up until the time of discharge/death, and as opposed to cite{jin2018improving} we only use nursing notes and not the whole spectrum of notes available in MIMIC-III. Also from an interpretability standpoint we rely on a theoretically sound concept from coalitional game theory, known as the Shapley Value , instead of explainability heuristics. Finally our visualization approach puts emphasis on presenting results in a way that can be easily understood and it is useful for users.\\  The contributions of our work are summarized in the following:      This paper is organized as follows: first we will show the overall distribution of our patient cohort dataset and its corresponding distribution of medical free-text notes. Then we will briefly describe our approach to interpretability using the Shapley Value, followed by a description of our convolutional architecture and experimental setting. Finally we will present and discuss our results and end with our conclusions and suggested future work.      In this paper we have presented ISeeU2, a convolutional neural network for the prediction of mortality using free-text nursing notes from MIMIC-III. We showed that our model is able to offer performance competitive with that of much more complex models with little text pre-processing, while at the same time providing visual explanations of feature importance based on coalitional game theory that allow users to gain insight on the reasons behind predicted outcomes. Our visualizations also provide a way to annotate free-text medical notes with markers to flag parts correlated with predictions of survival and death. We have also shown that nursing notes could be rich enough to capture the concepts needed for mortality prediction at a level of accuracy far higher than what is currently possible with traditional statistical techniques.  
"," Accurate mortality prediction allows Intensive Care Units  to adequately benchmark clinical practice and identify patients with unexpected outcomes. Traditionally, simple statistical models have been used to assess patient death risk, many times with sub-optimal performance. On the other hand deep learning holds promise to positively impact clinical practice by leveraging medical data to assist diagnosis and prediction, including mortality prediction. However, as the question of whether powerful Deep Learning models attend correlations backed by sound medical knowledge when generating predictions remains open, additional interpretability tools are needed to foster trust and encourage the use of AI by clinicians. In this work we show a Deep Learning model trained on MIMIC-III to predict mortality using raw nursing notes, together with visual explanations for word importance. Our model reaches a ROC of 0.8629 , outperforming the traditional SAPS-II score and providing enhanced interpretability when compared with similar Deep Learning approaches.",58
"  With the rise in popularity of social media, the usage of internet meme has been increasing as well to convey messages or reactions in a unique way. Most of the automatic sentiment classifiers in the past has been designed either for text or images, but for memes, we see that there's both the textual and visual component, which introduces the challenge of multi-modal aspects. Majority of these memes are used for performative acts, which generally have some kind of a sentiment for the ongoing social discourse. A lot of times, malicious users post disturbing or hate memes, which are often flagged my moderators. A lot of companies hire people to do this job, but with the rapidly growing volume, it becomes an important task to detect this in an automated manner which is scalable. It is much more challenging than simple text or image classification as the intent is described by the combined effect of both, i.e. visual cue and language understanding. The two main challenges are identifying or extracting the text from the image, and secondly, connecting the text with the image to get the sentiment, which often required domain knowledge. For example, a meme which has the marvel character Captain America in it, often employs the domain knowledge that he is honorable and responsible to generate humour combined with the text.  The organizers of SemEval 20 Task 8 created a dataset containing of x internet memes and proposed three tasks. Task A - sentiment classification, where the goal is to classify it to positive, negative or neutral. Task B - humor classification, classify memes to humor categories like sarcastic, humorous, offensive and motivation. Task C - scales of semantic class, quantify extent of humors into scales, not, slightly. We propose a multi-task learning system which uses combined feature from ResNet based CNN Model for Image feature extractor block \& recurrent DNN Model consists of stacked layers of bidirectional LSTM and GRUs with contextual attention as text feature extractor to learn all the three tasks at once. Our model gets best Macro-F1 scores of 0.3488, 0.5112, 0.3240 and corresponding Micro - F1 Score of 0.5021, 0.3998 on Tasks A, B and C respectively.        In the present work, we have build a system which uses multi-task learning to learn all the three tasks from SemEval 2020 Task 8 - memotional analysis as a single unified problem. The best macro F1-scores on Tasks A, B and C are 0.3488, 0.5112 and 0.3240 respectively. While for first task, SE-ResNet18 visual backbone gave the best results, for Task B \& C, ResNet34 gave the best result.In case of Task C, model performance performed worst because of imbalance data \& high intraclass correlation among fine grained class labels. In the future, we would like to work on topic modelling of social images into our pipeline to incorporate the domain knowledge.      
"," In this article, we describe the system that we used for the memotion analysis challenge, which is Task 8 of SemEval-2020. This challenge had three subtasks where affect based sentiment classification of the memes was required along with intensities. The system we proposed combines the three tasks into a single one by representing it as  multi-label  hierarchical  classification problem.Here,Multi-Task learning or Joint learning Procedure is used to train our model.We have used dual channels to extract text and image based features from  separate Deep Neural Network Backbone and aggregate them to create task specific features. These task specific aggregated feature vectors ware then passed on to smaller networks with dense layers, each one assigned for predicting one type of fine grain sentiment label. Our Proposed method show the superiority of this system in few tasks to other best models from the challenge.",59
"  Recently, detecting and recognizing texts in natural scenes have been an area of prime interest, both in the industry as well as in the academic field. Extracting and understanding text from images have helped automate visa applications, loan processing systems, navigation and is a must for self-driven cars.  In the current literature, text detection has been treated separately to text recognition , while both are important for text extraction. In this paper, we focus on the more challenging scenario, which is the scene text detection.   Scene text detection and extraction have seen a lot of advancements. Some of the earlier approaches,  predicted word bounding boxes by assuming the text to be horizontal and machine printed. As a result those methods perform sub-optimally on natural images since natural images contain a lot of oriented and irregularly shaped text. To address these limitations,  relaxed the assumption by predicting oriented bounding boxes. Further relaxations were introduced in some recent methods which can predict irregularly shaped text as in .  The proposed method is inspired by  in which we estimate the character bounding boxes but instead of linking them using affinity boxes, we construct a graph where each character is a node and we use a graph neural network for edge prediction to join two adjacent characters. Affinity heat maps computed in  fails to join characters that are spatially separated, as well as characters that have a high aspect ratio, refer to Figure  for illustration. The proposed framework with the help of a novel graph neural network operation can handle those aforementioned challenges efficiently.  Existing graph convolution methods  focus on training and propagating node features, where edges serve the purpose of defining the adjacency matrix. For example, part segmentation of point clouds, , assumes fixed classes and each point/node predicts its class affiliation. This method restricts the number of unique objects to an upper threshold and does not apply to our task in which there can be a large number of words in a single image. To eliminate this problem, we propose to use link prediction which can effectively create any number of groups of characters. We also propose a novel method of using multi-dimension features for edges which are also trained and propagated and used to predict link between adjacent characters, see Figure  for illustration. We also show that our method performs better than existing graph convolution methods for the task of linking character bounding boxes to form words.   Our main contributions are as follows:        Since the proposed network learns both Node and Edge parameters, we call it NENET.    In this paper we propose a novel approach of Graph Neural Network which uses edge propagation in conjugation with node propagation to link predicted characters. In spite of a high variance of word lengths, the proposed method can connect characters into words quite accurately. Currently we are working towards combining character recognition engine to the proposed NENET architecture so that we can leverage on the use of lexicon to further improve link prediction.   
","   Text detection in scenes based on deep neural networks have shown promising results. Instead of using word bounding box regression, recent state-of-the-art methods have started focusing on character bounding box and pixel-level prediction. This necessitates the need to link adjacent characters, which we propose in this paper using a novel Graph Neural Network  architecture that allows us to learn both node and edge features as opposed to only the node features under the typical GNN. The main advantage of using GNN for link prediction lies in its ability to connect characters which are spatially separated and have an arbitrary orientation. We show our concept on the well known SynthText dataset, achieving top results as compared to state-of-the-art methods.",60
"  Open-domain dialogue systems have typically been modeled using end-to-end approaches, more specifically encoder-decoder  architectures~. These seq2seq models are commonly trained on a maximum likelihood objective, which leads to repetitive and uninformative responses. As seen in Figure { acknowledges Speaker 2's previous statement and follows up with a question introducing a new topic and statement, in contrast with  candidate B which abruptly transitions into the new topic.   According to human conversations  are sequentially organized units. Turns and actions realized within them are related to what came before and affect what  comes next. Inspired by the previous studies, we propose a policy-driven neural response generation  approach for open-domain,  knowledge-grounded dialogue systems. Our motivation for this work is to have a mechanism for open domain conversational systems, i.e., a dialogue policy, that can enable such higher-level control of generated responses. The dialogue policy provides a sequential organization plan or { specifies the order and relationship of sentences within a turn targeting engaging responses to users throughout the interaction. %conversation is critical to planning an interaction to keep users engaged. This form of control is similar to dialogue management  and natural language generation  in task-oriented systems  where a meaning representation determined by the DM is realized as a response during NLG. To further control the content and order of sentences within the generated response, previous work on task-oriented systems proposed explicit content and sentence planning.  Previous work for open-domain dialogue systems also follow a similar method for sentence  planning and design dialogue policies to predict a set of discrete attributes such as topic and dialogue acts~. However, these studies rely on a set of templates for NLG, resulting in repetitive response structures. %which are not flexible for open-domain conversations.   We design a set of dialogue policy models that adapt to the dialogue context to appropriately control the responses at both the turn and sentence-levels.  We extend the end-to-end-approach of: we take in as input both the dialogue context and an action plan to predict the next response. We train our PD-NRG model by fine-tuning on the Generative Pre-trained Transformer  model in a TransferTransfo fashion. The TransferTransfo model is a state-of-the-art neural open-domain dialogue system that won 1st place in automated evaluation and 2nd place in human evaluation at the NeurIPS ConvAI2 conversational Intelligence Challenge. Our approach differs from previous works that condition on discrete attributes independently by conditioning on these attributes jointly.    Our contributions include:       %We conducted experiments using the recently released Topical-Chat dataset\footnote{https://github.com/alexa/alexa-prize-topical-chat-dataset} of knowledge grounded conversations. We show through automated and human evaluation that the proposed methods generate relevant responses while  %maintaining controllabilty. Furthermore, we demonstrate that the generated responses do indeed realize the action plans they were conditioned on.  %We also investigate different dialogue policy models to predict an action plan given the dialogue context. We observe that a basic dialogue policy that operates at sentence-level generates better responses than turn-level generation as well as simpler, no-policy baselines.  % Our method can be used for developers who want to develop conversational systems without hand-crafting dialogue flows.    %In order to improve the informativeness of generated responses,~,  and  proposed methods to ground generated responses on knowledge, in the form of knowledge snippets or entities and related facts.~ used end-to-end memory networks to condition the generated responses on knowledge, where attention over the knowledge relevant to the conversation context is estimated, and multiple knowledge representations are included as input during response decoding. ~ added a copy mechanism to attend and copy from relevant knowledge along with the dialogue history.~ retrieve relevant knowledge graphs given the conversation context and encode the graphs with a static graph attention mechanism. The decoder attentively reads the retrieved knowledge graphs and the knowledge triples within each graph.  %In human-human conversations, turns usually include a mixed set of dialogue act units some of which present the provided knowledge, while other either provide feedback to the previous discourse [will write more here, I need to find related work on discourse and argumentation studies] For example,  %This form of control is similar to natural language generation in task-oriented systems where a response is realized based on the meaning representation that is typically specified by dialog acts and arguments of the system action.     %These systems typically follow a pipeline, including natural language understanding and dialogue state tracking to estimate a user's intent and associated arguments, followed by a dialogue policy to determine the next system action, in the form of a meaning representation which is then used to realize a response. Among other things, the system action depends on the meaning representation which controls the generated response as specified by dialogue acts and arguments of the system action.  %This form of control is similar to natural language generation in task-oriented systems which aim to help users to accomplish a specific task. Due to their interaction with back-end knowledge or action provider tools, task-oriented dialogue systems typically follow a pipeline, including natural language understanding and dialogue state tracking to estimate a user's intent and associated arguments, followed by back-end lookup and a dialogue policy to determine the next system action, in the form of a meaning representation which is then used to realize a response. Among other things, the system action depends on the output from the back-end, hence the meaning representation includes controls over the generated response. For example, the output response can include the availability of what the user requested or can ask for more attributes to further specify user's intent, as specified by dialogue acts and arguments of the system action.    %Following along similar lines, in this work, we propose neural response generation for open domain dialogue systems that is conditioned on a meaning representation that includes knowledge, dialogue acts, and topics for the response to be generated. Similar to task-oriented systems, the form of the output for open-domain systems depends on the availability of knowledge and its relationship to the dialogue context. Our approach differs, as we both aggregate the discrete attributes and jointly condition our model on it rather than independently. The goal of the response generation model is to then realize the meaning representation in the context of the dialogue history. We aim to have controllability over our generated responses in addition to generating engaging and interesting responses. We propose using an action plan for the meaning representation and investigate action plans that represent knowledge at the sentence  level, and compare them with previous work that represents knowledge at the turn level.   %Following, we extend the end-to-end approach by taking in as input our action plan along with the dialogue history and then predict the next turn. We take the approach of predicting the individual sentences inside a turn in a sequential manner conditioning on previously generated sentences as well as an individual sentence's action plan. This contrasts previous approaches which predict a whole turn at once. We use the recently released Topical Chat dataset of knowledge grounded conversations, which includes multi-turn conversations from two Turkers, where each turn could be related to one of the knowledge sentences provided.      In this work, we propose a policy-driven neural response generation approach for knowledge grounded open-domain dialogue systems. We estimate an action plan that consists of a set of attributes that control the content and style of the generated responses at the turn and sentence levels. We investigate both manual and machine learning based policies. Through human evaluation, we empirically demonstrate that a basic dialogue policy that does sentence level generation outperforms turn level generation, as well as knowledge-grounded response generation baselines. Furthermore, the generated responses realize their respective action plans. This allows builders of dialogue systems control over the model's responses allowing for more consistent user experiences. Our future work includes investigation of better approaches for learning such dialogue policy models along with adding other attributes such as sentiment.      
"," Open-domain dialogue systems aim to generate relevant, informative and engaging responses. Seq2seq neural response generation approaches do not have explicit mechanisms to control the content or style of the generated response, and frequently result in uninformative utterances. In this paper, we propose using a dialogue policy to plan the content and style of target responses in the form of an action plan, which includes knowledge sentences related to the dialogue context, targeted dialogue acts, topic information, etc. The attributes within the action plan are obtained by automatically annotating the publicly released Topical-Chat dataset. We condition neural response generators on the action plan which is then realized as target utterances at the turn and sentence levels. We also investigate different dialogue policy models to predict an action plan given the dialogue context. Through automated and human evaluation, we measure the appropriateness of the generated responses and check if the generation models indeed learn to realize the given action plans. We demonstrate that a basic dialogue policy that operates at the sentence level generates better responses in comparison to turn level generation as well as baseline models with no action plan. Additionally the basic dialogue policy has the added effect of controllability.     % In experimental results, we demonstrate that the proposed models that operate at sentence level result in generation of better responses with the added effect of controllability in comparison to turn level generation. % we need one more sentence here describing the latest experiments.   % The next sentence is a place holder, we can include and extend it or fully exclude it, depending on completion of experiments. %Finally, we also investigate the feasibility of predicting such action plans.  %The goal of open-domain dialogue systems is to generate relevant and informative responses. However, common end-to-end approaches have been shown to produce repetitive and uninformative responses and do not have explicit mechanisms to control the content or style of the generated response. Recent work propose grounding responses on knowledge and investigated attributes, such as dialogue acts, to improve the diversity of responses. In contrast, the common approach in deployed task-oriented dialogue systems is to learn a meaning representation to control the generated response.    %The proposed models with sentence-level control for knowledge result in better responses.",61
"  The growing complexity of deep neural networks has given rise to the desire for self-explaining models. % In text classification, for instance, one popular method is to design models that can perform classification using only a rationale, which is a subset of the text selected from the model input that fully explains the model's prediction. % This selective rationalization method, often trained to choose a small yet sufficient number of text spans, makes it easy to interpret the model's prediction by examining the selected text. %  In contrast to classification, very little progress has been made toward rationalization for text matching models. The task of text matching encompasses a wide range of downstream applications, such as similar document recommendation, question answering, and fact checking. Many of these applications can benefit from selecting and comparing information present in the provided documents. % % % For instance, consider a similar post suggestion in a tech support forum as shown in Figure. The extracted rationales could provide deeper insights for forum users while also helping human experts validate and improve the model.   % % %  In this work, we extend selective rationalization for text matching and focus on two new challenges that are not addressed in previous rationalization work. % % First, since text matching is fundamentally about comparing two text documents, rationale selection should be jointly modeled and optimized for matching. % % Second, the method should produce an interpretable alignment between the selected rationales showcasing their relations for the downstream prediction. This is very different from rationalization for text classification, where the selection is performed independently on each input text and an alignment between rationales is unnecessary.  One popular method for aligning inputs is attention-based models. % However, a limitation of neural attention is that the alignment is rarely sparse, thus making it difficult to interpret how the numerous relations among the text spans lead to the model閳ユ獨 prediction. Recent work has explored sparse variants of attention, but the number of non-zero alignments can still be large. % Additionally, because of the heavy non-linearity following most attention layers, it is difficult to truly attribute the model's predictions to the alignment, which means that attention-based models lack fidelity. % % %  We propose to address these challenges by directly learning sparse yet sufficient alignments using optimal transport . % % We use OT as a building block within neural networks for determining the alignment, providing a deeper mathematical justification for the rationale selection. In order to produce more interpretable rationales, we construct novel variants of OT that have provable and controllable bounds on the sparsity of the alignments. Selecting and aligning text spans can be jointly optimized within this framework, resulting in optimal text matchings. % Our model is fully end-to-end differentiable using the Sinkhorn algorithm for OT and can be used with any neural network architecture.  % % % % %  We evaluate our proposed methods on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets, with tasks ranging from similar document identification to reading comprehension. % Compared to other neural baselines, our methods show comparable task performance while selecting only a fraction of the number of alignments. % We further illustrate the effectiveness of our method by analyzing how faithful the model's predictions are to the selected rationales and by comparing the rationales to human-selected rationales provided by   on the e-SNLI and MultiRC datasets. %   Balancing performance and interpretability in deep learning models has become an increasingly important aspect of model design. In this work, we propose jointly learning interpretable alignments as part of the downstream prediction to reveal how neural network models operate for text matching applications.  Our method extends vanilla optimal transport by adding various constraints that produce alignments with highly controllable sparsity patterns, making them particularly interpretable.  Our models show superiority by selecting very few alignments while achieving text matching performance on par with alternative methods. As an added benefit, our method is very general in nature and can be used as a differentiable hard-alignment module in larger NLP models that compare two pieces of text, such as sequence-to-sequence models. Furthermore, our method is agnostic to the underlying nature of the two objects being aligned and can therefore align disparate objects such as images and captions, enabling a wide range of future applications within NLP and beyond.   
","  Selecting input features of top relevance has become a popular method for building self-explaining models.  In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. Our approach employs optimal transport  to find a minimal cost alignment between the inputs. However, directly applying OT often produces dense and therefore uninterpretable alignments. To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity. Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations. % We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models.\textsuperscript{\textdagger} Denotes equal contribution.} Our code is publicly available at \url{https://github.com/asappresearch/rationale-alignment}.}",62
" Social media platforms allow users to express their opinions online towards various subject matters. Despite much progress in sentiment analysis in social media, the prediction of opinions, however, remains challenging. Opinion formation is a complex process. An individual's opinion could be influenced by their own prior belief, their social circles and external factors. Existing studies often assume that socially connected users hold similar opinions. Social network information is integrated with user representations via weighted links and encoded using neural networks with attentions or more recently Graphical Convolutional Networks  . This strand of work, including , leverages both the chronological tweet sequence and social networks to predict users' opinions.  The majority of previous work requires a manual segmentation of a tweet sequence into equally-spaced intervals based on either tweet counts or time duration. Models trained on the current interval are used to predict users' opinions in the next interval. However, we argue that such a manual segmentation may not be appropriate since users post tweets at different frequency. Also, the time interval between two consecutively published tweets by a user is important to study the underlying opinion dynamics system and hence should be treated as a random variable.  Inspired by the multivariate Hawkes process , we propose to model a user's posting behaviour by a temporal point process that when user  posts a tweet  at time , they need to decide on whether they want to post a new topic/opinion, or post a topic/opinion influenced by past tweets either posted by other users or by themselves. We thus propose a neural temporal opinion model to jointly predict the time when the new post will be published and its associated stance. Instead of using the fixed formulation of the multivariate Hawkes process, the intensity function of the point process is automatically learned by a gated recurrent neural network. In addition, one's neighbourhood context and the topics of their previously published tweets are also taken into account for the prediction of both the posting time and stance of the next tweet.  To the best of our knowledge, this is the first work to exploit the temporal point process for opinion prediction on Twitter. Experimental results on the two Twitter datasets relating to Brexit and US general election show that our proposed model outperforms existing approaches on both stance and posting time prediction.  [tp!]        In this paper, we propose a novel Neural Temporal Opinion Model  to address users' changing interest and dynamic social context. We model users' tweet posting behaviour based on a temporal point process  impose a  Recurrent Marked  temporal point process on the users' time series of tweets to leverage the time intervals for the joint prediction of the posting time and stance label of the next tweet.  We develop a topical attention mechanism based on a VAE and a context-level RNN to differentiate the user's attention towards neighborhood tweets. Experimental results verify the effectiveness of the model. Furthermore, visualisation of the topics and attention signals shows that NTOM captures the dynamics in the focused topics and contextual attention.   effacy, prominently  
"," Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context. In this paper, we model users' tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user's historical tweet sequence and tweets posted by their neighbours. We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context. Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.",63
" The volume and velocity of online news has increased dramatically in recent years. For news analysts in various domains  this creates a need for automated methods to detect and summarize news events in real time, since doing so with human effort alone is rapidly becoming intractable. News is now consumed online directly from news platforms and aggregators, but also socially via social media, creating a complex media ecosystem.  Automated methods can assist human analysts by providing alerts to emerging news events, generating brief descriptions of the detected event, and directing the analyst towards relevant documents. Methods that can be applied to different sources  are especially useful.  Here we present a methodology that utilizes social network analysis techniques for trending topic detection and characterization in news streams. We make the assumption that news events link multiple entities  at a particular point in time. We hypothesize that news events may therefore be effectively detected by studying the temporal evolution of a knowledge graph that links entities based on their co-occurrence in news documents. In this paper, we explore this approach by developing a software pipeline that creates and analyses complex networks in which the nodes are entities and the edges represent entity co-occurrence in news articles.  Changes in node degree are used as an indicator of possible news events, which are then characterized using an extended knowledge graph that incorporates noun phrases alongside entities.  To evaluate our method we use two data sets. The first data set consists of articles from the  data set from the Kaggle website\footnote{https://www.kaggle.com/snapcrack/all-the-news}. This data set contains  articles from major news sites of the United States of America. The second dataset is Twitter data relating to the FA Cup Final football match between Chelsea and Liverpool in 2012, which consists of tweets collected during the match by Aiello et al . We evaluate our method quantitatively and qualitative against a variety of methods tested on that data set in that study.  Our main findings in this paper can be summarized as follows:       This paper presents a prototype news event detection methodology, based on natural language processing and network analysis. Events are located by finding peaks in the prominence of named entities, based on node-level time series in the entity knowledge graph and characterized by community detection in KeyGraphs linking entities and noun-phrases.  Our evaluation suggests that NED provides a significant improvement against other state of the art methods. This is supported on two qualitatively different datasets . The combination of named entities and social network analysis techniques, such as community detection, seems to be very effective in detecting and tracking topics in document streams, and provides a more comprehensive description of each detected event, compared with the rest of the evaluated methods.   We suggest that NED performs better than other methods for detecting events in document news streams because it focuses on named entities  and, in particular, it identifies `peaking entities' which show an increased level of prominence within the dynamic knowledge graph. This helps to remove irrelevant articles from further processing; here such articles constitute `noise' in the document stream and do not contribute anything to the detection and description of the trending topics or events. The approach removes a substantive proportion of articles in this way, in some cases  around 50\  of them. Another advantage of our method is the use of named entities and noun phrases to form communities of n-grams that describe an event.   By utilizing the entity co-appearance network, the NED method captures relational information about entity interactions. Thereby it can efficiently detect peaking entities based on an approach where an entity is important not only because it appears frequently itself, but also because it co-occurs with other important entities. The use of relational information allows event detection to access holistic patterns in articles. While here we have focused on weighted degree as a node-level indicator of structural change, future work will consider other network statistics including macro-/meso-level attributes such as community structure, core-periphery, and backbone topology.    The authors acknowledge funding from a commercial entity, Adarga Ltd . The funder had no input or editorial influence over the manuscript.               Remove comment to use the external .bib file .     and comment out the ``thebibliography'' section.       Comment out this section when you  is enabled.     
"," Detecting important events in high volume news streams is an important task for a variety of purposes. The volume and rate of online news increases the need for automated event detection methods that can operate in real time. In this paper we develop a network-based approach that makes the working assumption that important news events always involve named entities  that are linked in news articles. Our approach uses natural language processing techniques to detect these entities in a stream of news articles and then creates a time-stamped series of networks in which the detected entities are linked by co-occurrence in articles and sentences. In this prototype, weighted node degree is tracked over time and change-point detection used to locate important events. Potential events are characterized and distinguished using community detection on KeyGraphs that relate named entities and informative noun-phrases from related articles. This methodology already produces promising results and will be extended in future to include a wider variety of complex network analysis techniques.",64
"  {R}{ecurrent} neural networks  are usually used for processing sequence-related problems as a memory model. They widely appear in deep learning models to solve problems such as sequence learning , language modeling , etc. These neural networks are designed to approach a dynamical system to get a complex time relationship, which can also be called as sequential dependence. A RNN structure has the connections between hidden units which can be described as the following ,  	_{t}=_{t}+_{rec}_{t-1}+),  where  and , are the input and the hidden state at time , respectively. , , and  are the variables to be trained  which are the input weight matrix, the recurrent weight matrix and the bias, respectively.  is the element-wise activation function.  and  are the dimension of the input feature and the number of hidden neurons, respectively.  RNNs cannot process long-term sequences because of the gradient vanishing and exploding problems due to the improper activation function and the uncontrolled repeated multiplications of the recurrent weight matrix . The existing methods proposed to solve the problem of learning long-term dependence can be roughly divided into two categories: the gated RNNs and the gate-free RNNs. The first category of methods enhance the original recurrent neurons with a set of gates to preserve long-term memory, for example, the famous long short-term memory  and GRU models .  Although the gated RNNs get notable performance and have become the popular configuration for many machine learning tasks, the generation of gates takes up considerable amount of computing resources and the forget gate also limits the learning of very long sequences while the uncontrolled recurrent weight matrix can cause gradient exploding . The second category of methods are gate-free RNNs proposed to solve the gradient problems without using gates. While the gate-free RNNs perform better on learning the long-term dependence, less efforts have been devoted to reducing the short-term disturbances.  In the study , long-term dependence is expected to be learned by the deep layers of a multi-layer HMLSTM network while the shallow layers of it are used for the short-term dependence. This rule is usually utilized unconsciously when constructing multi-layer RNN model to learn multi-scale dependence. However, simply using the same kind of LSTM cells is hard to adapt to different functional requirements. On the contrary, using different models for long-term and short-term dependence may be more adaptive. Based on this motivation, a Dual Recurrent Neural Networks  is proposed in this paper. Consisting of two parts and a transition connection with selection mechanism between them, DuRNN progressively learns the short and long-term dependence. The first part is a conventional recurrent neural network with full connected neurons, where the singular values of the recurrent weight matrix are constrained to learn short-term memory and avoid gradient exploding. The second part consists of independent recurrent connected neurons  with numerical restriction on the recurrent weights to learn long-term dependence. The non-saturated activation function relu  is used in both parts. A selection mechanism is added to transfer the different dependence learned by the two parts in order to complete the divide-and-conquer strategy of learning separate dependence. Similar to the macroscopic human memory generative processes, i.e., humans first have the short-term memory and then transfer it to the long-term memory , we are trying to adapt a divide-and-conquer strategy to learn long-term information. Similar to the state-of-the-art CNN and RNN methods for generating deep models, our dual recurrent neural networks can also be considered as a layer to be stacked to form a multi-layer DuRNN for better performance.  Our contributions can be summarized as follows.  1) A new framework is proposed to assist the capture of long-term dependence. It uses two kinds of recurrent connections, i.e, local full recurrent connections and long-term independent recurrent connections to progressively learn the short-term and long-term dependence.  2) We propose a selection mechanism to connect these different recurrent networks such that the long-term memory can be better separated and utilized.  3) Extensive experiments are conducted on the adding problem,  MNIST classification, language modeling and action recognition. The results confirm that DuRNN improves the performance in both long and short sequences.     In this paper we proposed a new sequential model named Dual Recurrent Neural Network . It can capture various length dependence by a pair of different recurrent connections.  A selection mechanism is in charge of information transition between the short-term memory and long-term memory, which helps to improve the accuracy and the utilization of neurons. The short-term memory is first learned and then selectively transferred to the long-term memory. This mechanism functions like the macroscopical human memory in some sense, in which the short-term information are also selected and reinforced. Its efficiency and rationality are validated and our model is applicable on both long- and short-term sequences. Experimental results on multiple tasks show good performance of our model.   
"," Recurrent neural networks  are widely used as a memory model for sequence-related problems. Many variants of RNN have been proposed to solve the gradient problems of training RNNs and process long sequences. Although some classical models have been proposed, capturing long-term dependence while responding to short-term changes remains a challenge. To this problem, we propose a new model named Dual Recurrent Neural Networks . The DuRNN consists of two parts to learn the short-term dependence and progressively learn the long-term dependence. The first part is a recurrent neural network with constrained full recurrent connections to deal with short-term dependence in sequence and generate short-term memory. Another part is a recurrent neural network with independent recurrent connections which helps to learn long-term dependence and generate long-term memory. A selection mechanism is added between two parts to help the needed long-term information transfer to the independent neurons. Multiple modules can be stacked to form a multi-layer model for better performance. Our contributions are: 1) a new recurrent model developed based on the divide-and-conquer strategy to learn long and short-term dependence separately, and 2) a selection mechanism to enhance the separating and learning of different temporal scales of dependence. Both theoretical analysis and extensive experiments are conducted to validate the performance of our model, and we also conduct simple visualization experiments and ablation analyses for the model interpretability. Experimental results indicate that the proposed DuRNN model can handle not only very long sequences , but also short sequences very well. Compared with many state-of-the-art RNN models, our model has demonstrated efficient and better performance.",65
"} {D}{eep} neural networks trained in a supervised manner are a popular contemporary choice for various speech related tasks such as automatic speech recognition , emotion recognition and age/gender recognition. However they are a double-edged sword by virtue of providing extremely good performance given that large scale annotated data is available, which is usually expensive. For problems like emotion recognition, reliably annotated data is also extremely scarce and even modern datasets are very limited in size. Transfer learning approaches attempt to solve this problem by domain adaptation , but even they need a large amount of annotated data for the primary supervised task and generalization is not guaranteed. Self-supervised learning is a recent and rapidly developing area of machine learning which might offer a potential solution to this problem. % Recent self-supervised approaches like MoCo  and GDT  are extremely interesting ways to learn representations from unlabeled data. However, they present results that are inferior to fully supervised pretraining. PIRL  is an extension of MoCo that outperforms fully supervised pretraining on ImageNet for object detection. In this work, we present a method for visually guided self-supervised learning of speech features that outperforms baseline self-supervised methods and also outperforms fully supervised pretraining on the evaluated downstream tasks.  Self-supervision is an interesting way to attempt to combat the paucity of labeled data by capturing the intrinsic structure of the unlabelled data. The idea behind self-supervision is to find a `pretext task / proxy task' for the network to learn that does not require any explicit labeling, but instead the data's inherent structure provides the labels. During training, the network is tasked with predicting these implicit labels, which could be of various kinds. For instance, predicting the next element or a randomly masked element of a known sequence given the history/context is a popular pretext task. The key idea is that the whole sequence is already available as an unlabeled data sample, and we are just choosing an intrinsic property  as the label for the proxy supervised learning problem. This `label' is provided to us for free by the data and does not require any sort of external annotation. These pretext tasks may also model and span across multiple modalities . This is especially relevant in the context of affective computing where we are interested in modeling complementary multimodal information, especially in audio and video.  [t]             In this work, we investigate self-supervised learning for audio representations. Audio representations are a cornerstone of speech and affect recognition. Most affective computing applications involve the analysis of a speech signal using either handcrafted low level descriptors or through a supervised  neural network which directly predicts the labels of interest. However, self-supervised learning may offer better representations for these applications, especially in cases where labeled data is hard to come by and unlabeled audio data is readily available. We look into how self-supervision can be used to produce robust audio features that contain emotional information.  First, we examine the state-of-the-art in self-supervised audio feature learning which we use as baselines. We then propose a novel visual self-supervised method and a novel audio-only self-supervised method for learning audio features. We also show how visual self-supervision helps encode emotional information into the audio features.  Most existing self-supervised learning approaches are unimodal. The few existing cross-modal approaches typically have some interaction between the modalities in the latent space by pretext tasks like clustering  but they do not produce an intuitive interaction between the two modalities. By contrast, our work proposes audio features that are explicitly guided by lip movements and facial expressions' reconstruction . We implicitly capture visual information related to lip movements and facial expressions in the audio features . The visual modality is needed only during training and our audio features can be evaluated on audio-only datasets. % Our work highlights the ability to use visual self-supervision using any audiovisual speech dataset to improve the performance of any audio-only method on a variety of problems and under various levels of noise. \\  We summarize our research contributions as follows:       and facial expressions due to being driven by video generation, outperform existing audio-only self-supervision approaches for speech and emotion recognition.       % The paper is organized as follows. Section  studies relevant related work in self-supervised learning and multimodal learning, and we position our work with respect to existing literature in terms of its novelty and utility. Section  discusses our proposed method for visual-supervision to guide learning of speech representations by face reconstruction. Section  introduces our proposed methods for audio-only self-supervision for speech representation learning. Section  introduces the datasets and baselines used in the work. Section  discusses the results of the various experiments we perform to validate our proposed features. Section  discusses the implications of our results, the limitations of our work, and possible future directions of research. Section  concludes the paper.   }   This work investigates visual self-supervision by facial reconstruction for the learning of improved audio features. Facial reconstruction for an entire video using only a single frame for speaker identity along with the audio information drives the audio encoder to produce features that contain information related to lip movements and facial expressions. The trained audio encoder can then be used for audio-only downstream tasks like speech recognition and emotion recognition. These particular tasks benefit due to correlation between lip movements and speech, and facial expressions and emotion. We further propose two audio-only self-supervised methods which we combine with the visual self-supervised method. This helps us encode complementary information to yield even richer features. This result could have interesting implications in cross modal self-supervised learning. Our proposed features deliver state-of-the-art performance for self-supervised methods for all the tested datasets and experimental settings. Our features are also fairly robust under various levels of noise for speech recognition. Our pretrained self-supervised model is also a promising starting point for solving tasks on smaller datasets which are common in areas like affective computing.    use section* for acknowledgment \ifCLASSOPTIONcompsoc     The Computer Society usually uses the plural form   
"," Self-supervised learning has attracted plenty of recent research interest. However, most works for self-supervision in speech are typically unimodal and there has been limited work that studies the interaction between audio and visual modalities for cross-modal self-supervision. This work  investigates visual self-supervision via face reconstruction to guide the learning of audio representations;  proposes an audio-only self-supervision approach for speech representation learning;  shows that a multi-task combination of the proposed visual and audio self-supervision is beneficial for learning richer features that are more robust in noisy conditions;  shows that self-supervised pretraining can outperform fully supervised training and is especially useful to prevent overfitting on smaller sized datasets. We evaluate our learned audio representations for discrete emotion recognition, continuous affect recognition and automatic speech recognition. We outperform existing self-supervised methods for all tested downstream tasks. Our results demonstrate the potential of visual self-supervision for audio feature learning and suggest that joint visual and audio self-supervision leads to more informative audio representations for speech and emotion recognition.",66
"  The widely adopted perspective for judging the sustainability of equity investments is along three pillars, E S G, which stand for Environmental, Social and Governance. In this paper we propose a novel approach of volatility forecasting based on ESG newsflow, an original integration of ESG into the investment process. Recently, integrating sustainability into investment strategies are receiving exponentially increasing attention in finance .  Environmental metrics cover all aspects of the firm's interaction with the environment, such as its CO2 emissions, its approach to the climate change transition or its broad strategy in the use of natural resources. The Social dimension encompasses all standards set by companies as they build relationships with employees, suppliers and the communities in which they operate  while Governance would cover leadership elements like executive compensation, diversity of the board and controversies. These ESG inputs are vital to assess the sustainability and the relevant risks of an investment position . Conventionally, ESG related factors are formatted as structured data to facilitate the integration of ESG aspects into quantitative models and the building of expertise in systematic ESG investing.  In our research, we study the predictive power of ESG news on volatility.  We develop a pipeline for ESG news extraction and a state-of-the-art transformer based language model to predict stock volatility . This pipeline can be generalized to other predicting targets with ease. As a measure of price fluctuations and market risk, volatility plays an important role in trading strategies, investment decisions and position scaling. In this paper, we focus on predicting Equity realized volatility, which is empirically calculated by the variance of observed returns of an asset. Volatility predictions often rely on predictive models based purely on price/return time-series, from standard statistical models of the GARCH family up to more recent deep-learning model based predictions .  The input to our models is an alternative source of ESG information: textual financial news-flow. Compared to structured ESG data provided by analysts or data vendors, ESG information from news-flow reflects more timely events of companies, and offers an alternative channel of capturing the relation of ESG events to market dynamics in a timely manner. Numerous research demonstrated that financial news is closely related to market and is becoming a gold mine to analyze market participants' behaviour .  An intuitive example is illustrated in Fig..   Though bearing rich information, ESG news is challenging to process for predicting models. Raw textual data is categorical and symbolic represented, which is a hindrance for quantitative models. Financial news is sparse in the sense that it moves in-parallel with real-world events in irregular timings. This is in contrast to the structured and well-formatted market and factor data typically used in conventional quantitative models. Although there are a variety of work studying predicting market behaviours with different data sources, how to exploit the predictive power of ESG news on volatility is rarely researched.  To this end, we resort to natural language processing  and deep learning techniques to explore the predictive power of ESG news.  In particular, one key NLP technique, which helps hurdle the challenges above, is language representation  . Deep neural networks, e.g. recurrent neural networks and transformer, trained on large scale text corpus exhibit remarkable success in a variety of NLP applications, such as sentiment analysis, text matching, dialogue systems and so on. This technique transforms text symbols into a numerically high-dimensional dense vectors, while importantly still preserving context and semantic relations.  Contributions. Specifically, the contribution of this paper is as follows:         % with our environmental, social, and governance  data and services, covering thousands of companies. % 閳 a concept that has gained currency in the financial industry as a basis for judging the sustainability of investments.  % Integration of ESG criteria can potentially improve the risk/return attributes of investment portfolios. % ESG investments can curb risk and exert positive impacts at the same time. % ESG investments tend to exhibit better risk/return attributes than equivalent traditional investments. % ESG criteria are a key sign of quality.  % Meanwhile, in recent years, the proliferation of ESG data reported by companies has enabled quantitative investment to build their expertise in systematic ESG investing, allowing them to develop a deep understanding of the fundamental mechanics behind best practices.   % We now target a full ESG integration in our Strategies; i.e. a systematic and explicit inclusion of ESG risks and opportunities across our quantitative engines.  % Historically, as the availability of data was relatively scarce, the reflection of our Environmental, Social and Governance  philosophy was expressed through exclusionary screening.  % This consisted of avoiding stocks which failed to meet our moral and ethical values.  % Thanks to large scale text corpus and Transformer architecture,   % Even more, the advancing development of Natural Language Processing techniques has inspired increasing efforts on stock trend prediction by automatically analyzing stock-related article % Language representation learning  % Language modeling is the task of predicting the next word in a given % piece of text.  % One of the most important recent developments in natural language processing is the realization that a model trained for language modeling can be successfully fine-tuned for most down-stream NLP tasks with small modifications.  % These models are usually trained on very large corpora, and then with addition of suitable task-specific layers fine-tuned on the target dataset [6]. % Text classification, which is the focus of this thesis, is one of the obvious use-cases for this approach  % Traditional efforts on predicting stock volatility have been carried out based on structured information, e.g. price history, volume, fundamental factors and so on.  % Deep learning techniques have demonstrated superior performance in a variety of applications.  % In quantitative finance, neural networks are exploited to model structured data, e.g. price time series, fundamental factors, etc, for predicting price, return and so on.  % Neural networks also boost the rapid development of Natural Language Processing  technologies, which enable various advanced solutions for language representations, text classification, sentiment analysis, text summarization, language understanding, etc.  % \documentclass[conference]{IEEEtran} \documentclass{article} \usepackage[preprint, nonatbib]{nips}  % \usepackage{natbib} %  % \usepackage{cite}  \usepackage{amsmath,amssymb,amsfonts} \usepackage{algorithmic}  \usepackage{graphicx} \usepackage{epstopdf}  \usepackage{textcomp} \usepackage{xcolor} \usepackage{mathtools} % \usepackage{bbm} \usepackage{comment} % \usepackage{booktabs} % For formal tables % \usepackage{dsfont} \usepackage{caption} \usepackage{subcaption} \usepackage{tabularx,booktabs} \usepackage{slashbox} \usepackage{booktabs,caption} \usepackage[flushleft]{threeparttable} \usepackage{multirow}  \usepackage{amsmath} \usepackage{color} \usepackage{comment}  \usepackage{mathtools}  \usepackage{overpic} [2]{ [width=0.48\linewidth]{#1}  \put  {\large #2}  }  \usepackage{tabularx}   [1]{{[TIAN: #1]}} : #1]}}  [1]{>{}  \author{%   Tian Guo\thanks{Correspondence to }   \And   Nicolas Jamet \\   \And   Valentin Betrix \\   \And   Louis-Alexandre Piquet \\   \And   Emmanuel Hauptmann \\   \\    Systematic Equity Research \\    RAM Active Investments \\ }    \title{ESG2Risk: A Deep Learning Framework from ESG News to Stock Volatility Prediction}     Incorporating environmental, social, and governance  considerations into systematic investments has drawn numerous attention recently.  In this paper, we focus on the ESG events in financial news flow and exploring the predictive power of ESG related financial news on stock volatility. In particular, we develop a pipeline of ESG news extraction, news representations, and Bayesian inference of deep learning models. Experimental evaluation on real data and different markets demonstrates the superior predicting performance as well as the relation of high volatility prediction to stocks with potential high risk and low return. It also shows the prospect of the proposed pipeline as a flexible predicting framework for various textual data and target variables.    %        %    %   \newpage Disclaimer  This document is not intended for persons who are citizens of, domiciled or resident in, or entities registered in a country or a jurisdiction in which its distribution, publication, provision or use would violate current laws and regulations.  This publication has been prepared for general guidance on matters of interest only, and does not constitute professional advice. You should not act upon the information contained in this publication without obtaining specific professional advice. No representation or warranty  is given as to the accuracy or completeness of the information contained in this publication, and, to the extent permitted by law, RAM Active Investments SA  does not accept or assume any liability, responsibility or duty of care for any consequences of you or anyone else acting, or refraining to act, in reliance on the information contained in this publication or for any decision based on it. Furthermore, the information, opinions and estimates in this document reflect an evaluation as of the date of initial publication and may be changed without notice. Past performance must not be considered an indicator or guarantee of future performance, and the addressees of this document are fully responsible for any investments they make. The content of this document is confidential and can only be read and/or used by its addressee. RAM is not liable for the use, transmission or exploitation of the content of this document. Therefore, any form of reproduction, copying, disclosure, modification and/or publication of the content is under the sole liability of the addressee of this document, and no liability whatsoever will be incurred by RAM. The addressee of this document agrees to comply with the applicable laws and regulations in the jurisdictions where they use the information reproduced in this document. This document is issued by RAM. This publication and its content may be cited provided that the source is indicated. All rights reserved. Copyright 2020.     In this paper, we implement a novel deep learning framework, ESG2Risk, to predict future volatility of stock prices.  We show that a transformer-based language model successfully manages to extract information from ESG newsflow to predict future volatility of stock returns. Predictions of volatility in our model is more accurate when attempting to identify the stocks with the highest volatility risk in the market, hence the worst potential risk contributors to an Equity selection.  Our research gives evidence that ESG newsflow does significantly impact future return and risk of companies and is a relevant factor for investors to consider when investing. Our findings in different geographies confirm that ESG newsflow integration can contribute to build profitable investment strategies, on top of improving the ESG profile of an Equity selection.  
"," Incorporating environmental, social, and governance  considerations into systematic investments has drawn numerous attention recently.  In this paper, we focus on the ESG events in financial news flow and exploring the predictive power of ESG related financial news on stock volatility. In particular, we develop a pipeline of ESG news extraction, news representations, and Bayesian inference of deep learning models. Experimental evaluation on real data and different markets demonstrates the superior predicting performance as well as the relation of high volatility prediction to stocks with potential high risk and low return. It also shows the prospect of the proposed pipeline as a flexible predicting framework for various textual data and target variables.",67
"  Convolution Neural Network  based models for end-to-end  speech recognition is attracting an increasing amount of attention. Among them, the Jasper model recently achieves close to the state-of-the-art word error rate  2.95\%  on LibriSpeech test-clean with an external neural language model. The main feature of the Jasper model is a deep convolution based encoder with stacked layers of 1D convolutions and skip connections. Depthwise separable convolutions  have been utilized to further increase the speed and accuracy of CNN models. The key advantage of a CNN based model is its parameter efficiency; however, the WER achieved by the best CNN model, QuartzNet, is still behind the RNN/transformer based models.   A major difference between the RNN/Transformer based models and a CNN model is the length of the context. In a bidirectional RNN model, a cell in theory has access to the information of the whole sequence; in a Transformer model, the attention mechanism explicitly allows the nodes at two distant time stamps to attend each other. However, a naive convolution with a limited kernel size only covers a small window in the time domain; hence the context is small and the global information is not incorporated. In this paper, we argue that the lack of global context is the main cause of the gap of WER between the CNN based ASR model and the RNN/Transformer based models.   To enhance the global context in the CNN model, we draw inspirations from the squeeze-and-excitation  layer introduced in, and propose a novel CNN model for ASR, which  we call  is also inspired by the design choices of QuartzNet, such as the usage of depthwise separable 1D convolution in the encoder. However, there are some key differences in the architectures in addition to the incorporation of the SE layer. For instance, we use a RNN-T decoder instead of the CTC decoder. Moreover, we use the Swish activation function, which contributes a slight but consistent reduction in WER. Overall,  achieves the WER of 1.9\%/4.1\% on LibriSpeech test-clean/test-other. This is a big improvement over previous CNN based architectures such as QuartzNet, and it outperforms transformer and LSTM based models.  This paper also studies how to reduce the computation cost of  for faster training and inference. First, we adopt a progressive downsampling scheme that is commonly used in vision models. Specifically, we progressively reduce the length of the encoded sequence eight times, significantly lower the computation while maintaining the encoder's representation power and the overall model accuracy. As a benefit, this downsampling scheme allows us to reduce the kernel size of all the convolution layers to five without significantly reducing the effective receptive field of an encoder output node.    We can scale  by globally changing the number of channels in convolutional filters. Figure illustrates the trade-off of {\netname} between model size and WER, as well as its comparison against other methods. Clearly, our scaled model achieves the best trade-offs among all.  In summary, the main contributions of this paper are:  an improved CNN architecture with global context for ASR,  a progressive downsampling and model scaling scheme to achieve superior accuracy and model size trade-off.      In this work, we proposed and evaluated a CNN based architecture for end-to-end speech recognition. A couple of modeling choices are discussed and compared. This model achieves a better accuracy on the LibriSpeech benchmark with much fewer parameters compared to previously published CNN models. The proposed architecture can easily be used to search for small ASR models by limiting the width of the network. Initial study on a much larger and more challenging dataset also confirms our findings.   
"," Convolutional neural networks  have shown promising results for end-to-end speech recognition, albeit still behind RNN/transformer based models in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call  that achieves good trade-off between computation and accuracy.   We demonstrate that on the widely used Librispeech benchmark,  achieves a word error rate  of 2.1\%/4.6\% without external language model , 1.9\%/4.1\% with LM and 2.9\%/7.0\% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the best previously published model of 2.0\%/4.6\% with LM and 3.9\%/11.3\% with 20M parameters. The superiority of the proposed  model is  also verified on a much larger internal dataset. %\jiahuiyu{Do we want to mention youtube results in abstract?}",68
"  Progress in speech processing such as speech recognition and text-to-speech enables users to interact with smart devices through a voice user interface  rather than directly controlling it. But these techniques mainly focus on spontaneousness during the interaction. Before that, it is practically important that the interaction starts well. Among various criteria where the device recognizes the start of the interaction, two typical approaches are keyword spotting and speaker verification.  Keyword spotting  is the task of detecting the prescribed spoken term in the input utterance. Many studies and applications have focused on pre-defining the keyword as their product name for wake-up or command words for specific actions . Meanwhile, according to users' convenience and customizing needs, some research on open-vocabulary KWS has attracted interest since the users can define any keywords. A typical way to handle arbitrary keywords is to express any words as acoustic word embeddings which are fixed-dimensional vector representations of arbitrary-length words. These embeddings learn the acoustic similarity between pronunciations of words pair so that they can encode acoustic information. In training, some approaches use cross-entropy loss , but triplet loss is mainly used because it can directly map the similarity to the relative distance in embedding space . Recently, an approach that considers phonetic information based on connectionist temporal classification   together showed good results . Still, open-vocabulary KWS has a lot of room for improvement due to its challenging nature.  Speaker verification  is the task of verifying the current speaker is a valid user. Here, we only deal with text-independent SV that does not have any restrictions on speech contents. SV requires an enrollment which is a process of registering the user's speaker identity. Then, speaker information is extracted from each input utterance and compared with the enrolled data. For successful SV, this speaker information must be expressed as a speaker discriminative representation. Recent the most powerful approaches based on deep neural networks are encoding speaker information as a fixed-dimensional vector representation, so-called speaker embedding. For learning discriminative embeddings, the networks are trained to classify speakers using cross-entropy loss  or to group speakers in embedding space using triplet loss . The criticized problem of these systems is that a long utterance must be used for the input as well as the enrollment to extract speaker information reliably. It is because the amount of accumulated information increases as the speech lengthens under the assumption that there is one speaker for one utterance. To cover the problem, several approaches with pooling methods  have been proposed to weight the relevant speech frames. However, if the input length is not long enough, their performances are still degraded. Accordingly, many short-duration SV studies are being conducted to have high performance even with a short utterance .  Even though acoustic and speaker information considers each other as a marginal feature that should be suppressed for robust discriminative learning, both KWS and SV have been handled independently. The ideal situation we think of is that the device can detect the keyword and verify the user at the same time using a short word-level utterance defined by the user. %In other words, open-vocabulary KWS and short-duration SV must be able to operate under the same conditions. In other words, open-vocabulary KWS and short-duration SV will eventually operate with the same input in the same conditions.  So in this paper, we propose a multi-task network that performs both KWS and SV simultaneously by fully utilizing acoustic, speaker, and phonetic information. The multi-task network consists of an enhancement network, acoustic feature extraction network, speaker feature extraction network, and pooling network. The sub-networks are trained by being shared or contributed to each other. In this process, we also introduce novel techniques of CTC-based soft voice activity detection  and global query attention. We evaluate our proposed approach on discrimination tasks for KWS and SV, respectively. Experimental results demonstrate that acoustic, speaker, and phonetic domains are interrelated and it is effective to integrate them for learning discriminative embeddings even in noisy environments, open-vocabulary, and short-duration conditions. Also, we present a visualization example to intuitively understand the proposed methods, and results of ablation experiments to show the effectiveness of the multi-task network. % % -------------------------------------------------------------------------- %           Muti-task Network % --------------------------------------------------------------------------    In this paper, we propose a multi-task network comprising multiple sub-networks. Also we introduce novel techniques of CTC-based soft VAD and global query attention to tightly utilize interrelated domain information even in challenging conditions of noisy environments, open-vocabulary KWS, and short-duration SV. Each sub-network is originally designed for individual purposes of enhancement, KWS, and SV, but great performance improvement can be achieved by effectively shared and mutually contributed. Experimental results demonstrate that the proposed approach outperforms the baselines.     --------------------------------------------------------------------------             Acknowledgement   -------------------------------------------------------------------------- 
"," Keyword spotting  and speaker verification  have been studied independently although it is known that acoustic and speaker domains are complementary. In this paper, we propose a multi-task network that performs KWS and SV simultaneously to fully utilize the interrelated domain information. The multi-task network tightly combines sub-networks aiming at performance improvement in challenging conditions such as noisy environments, open-vocabulary KWS, and short-duration SV, by introducing novel techniques of connectionist temporal classification -based soft voice activity detection  and global query attention. Frame-level acoustic and speaker information is integrated with phonetically originated weights so that forms a word-level global representation. Then it is used for the aggregation of feature vectors to generate discriminative embeddings. Our proposed approach shows 4.06\% and 26.71\% relative improvements in equal error rate  compared to the baselines for both tasks. We also present a visualization example and results of ablation experiments.",69
"  Speaker identification using deep neural networks becomes an active research area in recent years .  In traditional supervised speaker identification training, the data used for training needs hand labelling, where the segments and the corresponding speaker labels are manually annotated . It might be expensive to process a large dataset with a large number of speakers using hand annotation .      Instead of hand annotating speaker labels in supervised training, weakly supervised training only relies on the set of speaker labels that occur in the corresponding utterance . This kind of weakly labelled large data collections are available online . Making use of such data collections would be helpful for training with a large amount of data.   Weakly supervised training has been widely used in speech technology. In , Karu et.al  proposed a DNN based weakly supervised speaker identification training technique. In their work, speaker diarization is firstly applied, and i-vectors are then extracted for each segments. A DNN is trained to predict the set of speaker labels without the true mapping from the i-vectors to the speaker labels.  In , Xu et,at. proposed a DNN based approach for multi-label audio tagging. In their work, an auto-encoder is trained to predict multiple labels using one input utterance. In , Xu et al. proposed to use a gated convolutional neural network for audio classification. In their work, the model is trained to predict one or more classes from an audio without time stamp labels.   Except for speech technology, weakly supervised learning has been widely used in other domains. In , Liu et,al. proposed a weakly supervised transfer learning approach to classify multi-temporal remote-sensing images using one labelled image. In , Xu et,al. proposed a weakly supervised training approach for image semantic segmentation using image-level labels.     text classification , image semantic analysis  and image classification . , object detection , image semantic segmentation , image to text translation  and biomedical information analysis  . In  Meng et, al. uses weakly supervised training for text classification. In their approach, pseudo-labels are generated for pre-training, and a self-training module is used for refinement.      Identifying speaker identities using recording-level speaker labels could also be viewed as a multi-instance multi-label  problem , where an utterance consists of a bag of instances and are associated with multiple labels .    In this work, a hierarchical attention network  based weakly supervised speaker identification approach is proposed. In the training and test data, each utterance contains multiple speakers and only the utterance-level labels are available. Different speakers might occur in different part of the input utterance, and some segments might contain multiple overlapped speakers. The model is trained to predict the set of all of the speaker labels from one input utterance . The proposed hierarchical attention network contains a frame-level encoder with attention, and a segment-level encoder with attention, which capture speaker information locally and globally . The frame-level encoder with attention tries to find the important frames within a segment, and the segment-level encoder tries to find the most important parts in the input utterance for speaker identities. Finally, the whole input utterance is compressed into a single vector and input to a DNN classifier. The score vector for each speaker is obtained using a sigmoid function. The proposed hierarchical attention network  enables the model to highlight and pay attention to the most important parts of input utterance relates the speaker identities.          The rest of the paper is organized as follow: Section   presents the architecture of our approach.  Section  depicts the data and the data construction process, the experimental setup, the baselines to be compared and implementation details. The results are obtained and shown in Section , and a conclusion is in Section .              In this work, a hierarchical attention network is proposed to solve the weakly labelled speaker identification problem. The input utterance is split into each local segments using a sliding window. Frame-level and segment-level encoder and attention capture speaker information locally and globally. The experiments are done with different test conditions and different amount of training data. The obtained results show that the proposed hierarchical attention network with sliding window performs better than X-vector and Attentive Xvector baselines, as well as hierarchical attention network with static window.  In the future work, more complex network architectures and larger dataset such as Voxceleb2 will be investigated.     This work was in part supported by Innovate UK Grant number 104264.   
"," Identifying multiple speakers without knowing where a speaker's voice is in a recording is a challenging task.  In this paper, a hierarchical attention network is proposed to solve a weakly labelled speaker identification problem.  The use of a hierarchical structure, consisting of a frame-level encoder and a segment-level encoder, aims to learn speaker related information locally and globally. Speech streams are segmented into fragments. The frame-level encoder with attention learns features and highlights the target related frames locally, and output a fragment based embedding. The segment-level encoder works with a second attention layer to emphasize the fragments probably related to target speakers. The global information is finally collected from segment-level module to predict speakers via a classifier. To evaluate the effectiveness of the proposed approach, artificial datasets based on Switchboard Cellular part1  and Voxceleb1 are constructed in two conditions, where speakers' voices are overlapped and not overlapped. Comparing to two baselines the obtained results show that the proposed approach can achieve better performances. Moreover, further experiments are conducted to evaluate the impact of utterance segmentation. The results show that a reasonable segmentation can slightly improve identification performances.  	 - speaker identification is more and used in real world situations where several speakers are present  - several speakers are present  in odmain triaing data is requied - however labelling of that kind of data is prohibitive.  - therefore we want to look int weak supervision  - This work proposes a network archttecture that encompasses lcoal and global selection fo data, therebey allowing to focus on speicfic speakers present in short segments  Locallly embedded information is then summarised in a global conext.  - The prposed method is evaluated on data where several speakers are present and speakers also overlap.  -  artificial datasets based on Switchboard  Cellular part1  and Voxceleb1 are constructed in two conditions.  - Compared to XX the propsed method shows significnat performance gain is YY % relative.  Experiments are conducted to evaluate the impact of utterance segmentation.",70
" Recent advances in speech recognition systems have enabled successful large scale deployments of various customer-facing speech applications, e.g. personal conversational agents, automatic transcriptions for accessibility, and multi-modal video understanding. This trend has increased the need for developing accurate automatic speech recognition  models for many languages and domains. However, there are significant challenges to achieving this.   First, even though current ASR systems are arguably within striking distance of human performance for broadcast news and telephone-speech domains , more challenging real-world scenarios involving unconstrained, natural speech that is filled with background music and noise, various speaking styles and emotions, disfluencies, heavy accents, un-cued speaker and language switching, is still an open problem for speech recognition systems . In this paper, we focus on the domain of public social media videos which involve all these challenges while representing an interesting benchmark for evaluating the effectiveness of different learning methods due to their ever increasing amount, multi-modal nature, and the availability of related metadata, e.g. video title, post text, and comments.  Second, it can be prohibitively expensive and difficult to collect sufficient amounts of supervised training labels to feed data hungry neural speech recognition models for each language. Therefore, we'd like to minimize the amount of supervised data used in training these models. With increased access to large computational resources, three families of methods have emerged in this direction:  Using large volumes of unpaired audio and text data.   Augmenting audio data with contextual metadata as distant labels .  Data augmentation through reverberation, structured noise, speed perturbation, time and frequency masking . Given the various existing approaches for combining unlabeled audio with unpaired text and for incorporating distant labels, it is unclear, however, which of these methods are complementary, how they compare to each other, and how they can be combined effectively into a scalable recipe to maximize speech recognition performance for a complex domain like social media videos under low-resource constraints.   In this paper, we take a step towards answering some of these questions by conducting large scale experiments to compare and combine methods drawn from these families of techniques, focusing on low-resource setups for transcribing public social media videos in two languages: Dutch and Romanian, using 27,000 and 58,000 hours of unlabeled data respectively. Applying data augmentation for all models , we compare a recently proposed weakly-supervised approach   and two commonly used self-labeling methods: frame-level distillation  and sequence-level distillation  for hybrid, CTC-based, and encoder-decoder ASR setups .    %Our experiment setup and results are presented in Section . We discuss related work in Section  and conclude in Section .  %{elaborate on used methods}.   %may be add couple sentences for prior work on multi-modal weak-supervision  % There is a growing body of research focusing on semi-, and weakly-supervised learning methods for speech recognition which formed the basis and motivation for this work. Early work benefiting from audio metadata for ASR model training  utilized closed caption data for TV shows and news, which are very close to the audio content but aren't a word-for-word transcript, as proxies labels either to bias language models  during decoding or as a direct supervision signal. Self-labeling, an effective method for semi-supervised learning, uses an initial bootstrap supervised model to provide proxy labels of a large volume of unlabeled audio for another round of model training . It showed strong results for maximum likelihood trained models  but fell short when a discriminative loss is incorporated . Motivated by work on model distillation ,  recently benefited from self-labeling on 1 million hours of unlabeled audio, while maintaining the superior performance by using actual transcripts for the discriminative training phase. For encoder-decoder models,  used self-labeling to improve upon the best supervised models both for the personal assistant and audio book domains. Since data augmentation is becoming ubiquitous regardless of task, data set size, and ASR training paradigms  given their provided gains, we use them in all of our experiments.   % Focusing on low-resource setups for transcribing public social media videos, we conduct systematic experiments on weakly-supervised and self-labeling methods, and their combination, over two languages; Dutch and Romanian. We investigate two approaches for self-labeling within couple ASR training paradigms. {elaborate on used methods}  % We conduct experiments for two types of approaches of self-labeling:   %  Matching the teacher's distribution at frame-level %  Using 1-best hypothesis generated from decoding using a word language model  % Given the effectiveness of data augmentation, all baselines, self-labeled, and weakly-supervised are using three types of of augmentation; speed perturbation, additive noise, and time and frequency masking. It was shown to significantly improve the quality of the teacher model used for self-labeling.   % discuss fine-tuning strategies vs distillation as a way of going from seq2seq into encoder-only       On 27,000 hours and 58,000 hours of unlabeled Dutch and Romanian public social media video data respectively, self-labeled encoder-decoder speech recognition models with sequence-level distillation showed WER reduction of more than 20\  relative to the strongest data-augmented supervised baseline. These gains are the biggest compared to other frame-level self-labeling and weak-supervision using video metadata as distant labels. Moving beyond our current minimum of 150 hours of initial supervision for self-labeling teachers, future work will focus on methods that preserve the same level of WER while using one or two orders of magnitude lower volumes of initial labeled data.  In this paper, we investigated self- and weakly-supervised training with hybrid, encoder-decoder and CTC-based models for low-resource speech recognition on Dutch and Romanian public social media videos with about 300 and 150 hours of supervised data respectively.   Using 27,000 hours and 58,000 hours of unlabeled Dutch and Romanian data, self-labeled encoder-decoder speech recognition models with sequence-level distillation achieved WER reduction of more than 20\  relative to the strongest data-augmented baseline.   These gains are the biggest compared to frame-level and sequence-level distillation on hybrid and CTC models.  While weakly-supervised trained models using video meta-data as distant labels brought 10\  relative improvement over the encoder-decoder baseline, they didn't come close to the self-labeling gains. Combining the distant and self-labels barely made any difference, showing that the combined system didn't utilize the contextual metadata information. We additionally observed that on hybrid models, frame-level distillation using Top-3 frame-level labels provided the same level of gains as the sequence-level distillation which utilizes a language model during label generation. While the encoder-decoder models seem to benefit from the implicit sub-word language model of the  self-labels, further research is needed to realize these benefits on other systems. Moving beyond our current minimum of 150 hours of supervised data, future work will focus on methods that preserve the same level of WER while using one or two orders of magnitude lower volumes of supervised data.   This scenario is also more likely to present a greater importance on language model and contextual metadata.   We believe the encoder-decoder model learns a longer range language model on the sub-word units of the self-labels and therefore provides the largest gains of 20\  on average compared the best baseline supervised system.   Confirming its sensitivity to transcription quality , using self-labels for LFMMI training, even with the large amount of audio data, is not as useful as using the much smaller supervised data in the discriminative training stage of hybrid models.   Interestingly, using Top-3 frame-level labels for distillation provided the same level of gains as the hybrid sequence-level distillation which utilizes a language model during label generation.   Combining weak-supervision on data-ws and self-labeling on data-large barely makes any noticeable change compared to the self-labeling alone, showing that the model has achieved all its gains from diverse audio conditions without utilizing much of the information in the textual meta-stream.    Each iteration of self-labeling added about 3\  to 4\  reduction in WER compared to a single iteration, pushing the overall gain from 10\  to 16\  on average compared to the baseline supervised system.  
"," Many semi- and weakly-supervised approaches have been investigated for overcoming the labeling cost of building high-quality speech recognition systems. On the challenging task of transcribing social media videos in low-resource conditions, we conduct a large scale systematic comparison between two self-labeling methods on one hand, and weakly-supervised pre-training using contextual metadata on the other. We investigate distillation methods at the frame level and the sequence level for hybrid, encoder-only Connectionist Temporal Classification  based, and encoder-decoder speech recognition systems on Dutch and Romanian languages using 27,000 and 58,000 hours of unlabeled audio respectively. Although all approaches improved upon their respective baseline word error rates  by more than 8\%, sequence-level distillation for encoder-decoder models provided the largest relative WER reduction of 20\% compared to the strongest data-augmented supervised baseline.",71
"   Recently, pre-trained models, especially BERT,  dominate Natural Language Processing  world.  The models learn powerful and universal representation by utilizing self-supervised learning at the pre-training stage to encode the contextual information.  The representation is beneficial to performance, especially when the data of the downstream task is limited. As of late, BERT-like models are also applied to the speech processing domain. The pre-trained model learns the robust speech representations for speech processing tasks, such as  Automatic Speech Recognition  and speaker recognition, with the self-supervised learning. approaches.  However, since the size of these BERT-like pre-trained models is usually prohibitively large, these models require a significant amount of memory for computation, even at the fine-tuning stage.  The requirement hinders the application of pre-trained models from different downstream tasks.    ALBERT addresses the challenge of efficiency. ALBERT is a lite version of BERT for text by sharing one layer parameters across all layers and factorizing the embedding matrix to reduce most parameters. Although the number of parameters is reduced, the representations learned in ALBERT are still robust and task agnostic, such that ALBERT can achieve similar performance in the same downstream tasks comparing to BERT.   In this paper, we first examine the knowledge encoding in each layer of Mockingjay, a pre-trained model utilizing BERT architecture to encode speech information. We found the learned parameters are redundant across layers. Thus, we bring the idea of sharing parameters from ALBERT to the speech processing domain and propose a novel self-supervised model, Audio ALBERT , for parameter-efficient representation learning.  We show that AALBERT yields comparable performance to other pre-trained models in downstream tasks, but with much smaller networks. To understand how to use the pre-trained networks properly in downstream tasks, we also analyze representations extracted from different layers of AALBERT. We use a simple classifier to probe each layer, and we find that the representations of the intermediate layers contain more phonetic and speaker information than that of the last layer. The finding indicates that the representations from the last layer fit the pre-training task too much, and the intermediate layers may be more suitable for adapting to downstream tasks. To our best knowledge, this is the first study to bring the idea of model compression in ALBERT to speech processing, to show the benefits in the efficiency of the novel architecture, AALBERT, for speech-related tasks, and to analyze learned latent representations for better usage of pre-trained networks in downstream tasks. The code will be available soon      In this paper, we present a novel model, Audio ALBERT . AALBERT is a pre-trained model for extracting latent representations that encode the audio information. The model is learned by reconstructing the masked input acoustic features to the linear spectrogram. We show that AALBERT can achieve comparable performances against Mockingjay, a BERT-like pre-trained audio model, yet with much fewer parameters. Besides, we show promising results in encoding audio information with much smaller pre-trained models.   For our future work, we will investigate various model architectures to improve further the efficiency of pre-trained models in computation and parameter usage.          Template for Blind SLT-2021 paper; to be used with:            spconf.sty  - ICASSP/ICIP LaTeX style file, and            IEEEbib.bst - IEEE bibliography style file.   -------------------------------------------------------------------------- \documentclass{article} \usepackage{spconf,amsmath,graphicx}    Example definitions.   -------------------- \def\x{{\mathbf x}} \def\L{{     Single address.   ---------------  \address{BLIND}     For example:   ------------  \address{School\\  	Department\\  	Address}     Two addresses .   ----------------------------------------------------------  \twoauthors    {A. Author-one, B. Author-two}  	{School A-B\\  	Department A-B\\  	Address A-B}    {C. Author-three, D. Author-four}  	{School C-D\\  	Department C-D\\  	Address C-D}         
","  Self-supervised speech models are powerful speech representation extractors for downstream applications. Recently, larger models have been utilized in acoustic model training to achieve better performance. We propose Audio ALBERT, a lite version of the self-supervised speech representation model. We apply the light-weight representation extractor to two downstream tasks, speaker classification and phoneme classification. We show that Audio ALBERT achieves performance comparable with massive pre-trained networks in the downstream tasks while having 91\% fewer parameters. Moreover, we design probing models to measure how much the latent representations can encode the speaker闁炽儲鐛 and phoneme闁炽儲鐛 information. We find that the representations encoded in internal layers of Audio ALBERT contain more information for both phoneme and speaker than the last layer, which is generally used for downstream tasks. Our findings provide a new avenue for using self-supervised networks to achieve better performance and efficiency.",72
"  % Traditional ASR systems separate acoustic modeling and language modeling.   % Furthermore, Using the language model to rescore N-best sentence hypotheses can improve the performance of ASR systems dramatically. The language model is an important component of automatic speech recognition  systems , and perplexity  is known to be closely correlated with word error rate  . Nowadays, state-of-the-art language models are commonly modeled using neural networks . The language model aims to learn the probability of word sequences, which are normally decomposed in an auto-regressive manner. To capture long contextual dependencies, the recurrent neural network  can be applied, which often uses the cross entropy training criterion along with softmax .  The idea of applying large-margin to the softmax layer is used to encourage intra-class compactness and inter-class separability among learned features.  % As a simple illustration, Figure  shows the result of applying the large-margin softmax in the MNIST hand-written digit classification task . It shows that the separability among classes improves as the margin term  increases.  In the field of face recognition there exists a line of work   that studies large-margin in the softmax layer, providing significant improvements in performance. Considering that the vectors in the projection matrix before the last softmax layer in neural language models  are essentially feature vectors of the words, which resemble the feature vectors of images in face recognition, we are thus curious to examine the performances of the aforementioned margins in NLM.  %   Large-margin in NLM is not an unfamiliar concept. In , a global-level margin that discriminates sentences is introduced. In contrast, this paper focuses on the margin between atomic-level word vectors. We apply different types of large-margins from face recognition to NLM. Our initial experiments show that using the largin-margin softmax from face recognition out-of-the-box for NLM deteriorates the PPL dramatically. We assume that this is due to the fundamental differences between words and faces in their class distributions. It is important to note that unlike in face recognition, the posterior probability of words in NLM is highly unbalanced. Zipf's law  is a common approximation of word frequency versus word rank in natural languages. In , the authors observe that NLM learn word vectors whose norms are closely related to word frequencies. Therefore, we conduct a series of experiments to compare various norm-scaling techniques for the word vectors. In addition, we implement a heuristic to scale the norms of the context vectors. It turns out that one of the norm-scaling methods slightly improves the PPL. When it is used along with the margin techniques, comparable WER to the baseline is achieved. Finally, to figure out the effects of margin techniques in NLM, we visualize the word vectors and observe that word vectors trained with large-margin softmax exhibit expected behaviors and ``stretch"" the word vectors to more evenly populate the embedding space.     In this work, we investigate the use of large-margin softmax in neural language models. We first apply margins from face recognition out-of-the-box, which evidently deteriorates perplexity. Considering the unbalanced nature of word distributions, we further conduct experiments to find good norm-scaling settings for neural language models and tune the margin parameters. Then we apply the models trained with large-margin softmax in rescoring experiments, where we can reach the same word error rate performance as the standard softmax baseline. Finally, to figure out the effects of large-margin in neural language models, we visualize the word vectors. It is interesting to note that the expected margin are found among the word vectors trained with large-margin softmax, which makes them more evenly populate the embedding space. At the same time, the semantic and syntactic relations among words are also preserved.  
","   To encourage intra-class compactness and inter-class separability among trainable feature vectors, large-margin softmax methods are developed and widely applied in the face recognition community. The introduction of the large-margin concept into the softmax is reported to have good properties such as enhanced discriminative power, less overfitting and well-defined geometric intuitions. Nowadays, language modeling is commonly approached with neural networks using softmax and cross entropy. In this work, we are curious to see if introducing large-margins to neural language models would improve the perplexity and consequently word error rate in automatic speech recognition. Specifically, we first implement and test various types of conventional margins following the previous works in face recognition. To address the distribution of natural language data, we then compare different strategies for word vector norm-scaling. After that, we apply the best norm-scaling setup in combination with various margins and conduct neural language models rescoring experiments in automatic speech recognition. We find that although perplexity is slightly deteriorated, neural language models with large-margin softmax can yield word error rate similar to that of the standard softmax baseline. Finally, expected margins are analyzed through visualization of word vectors, showing that the syntactic and semantic relationships are also preserved.",73
"  Traditional text-to-speech synthesis  such as the deep neural network -based statistical parametric speech synthesis  framework  converts input text into output waveforms by using modules in a pipeline: a text analyzer to derive linguistic features such as syntactic and prosodic tags from text, a duration model to predict the phoneme duration, an acoustic model to predict the acoustic features such as mel-cepstral coefficients and , and a vocoder to produce the waveform from the acoustic features. Such a TTS system can produce reasonably good waveforms, but training the modules in the pipeline can be laborious. For example, the text analyzer requires manually annotated prosodic tags; the duration and acoustic models need alignment between the linguistic and acoustic feature sequences.   Sequence-to-sequence neural TTS is a recently developed framework that uses a single model to conduct the task of all or most modules in the SPSS-based TTS pipeline.  For example, an ideal end-to-end sequence-to-sequence TTS uses a single neural network to directly convert the text into the waveform. Because such a TTS system is expected to implicitly learn the word pronunciation, prosodic realization, speaking styles, and the alignment between text and speech, it can be trained from many TTS databases with only the waveform and the text transcription.  In practice, a sequence-to-sequence TTS system may still leverage a separate neural waveform generator and a grapheme-to-phoneme converter for ideographic languages such as Japanese and Chinese. Although it is not fully end-to-end, such a TTS system requires no alignment and simplifies the training process.  %Sequence-to-sequence neural network based text-to-speech synthesis  is a recently developed approach that converts linguistic features into acoustic features directly with a single model. This is different from pipeline neural TTS systems, which converts text into speech with several steps. The pipeline TTS methods consists of dedicated models for each step. Major steps include XXX, aligner, XXX, and XXX. On the other hand, the neural sequence-to-sequence based TTS uses the single model only and this can be trained from TTS databases directly. %%without an external aligner, unlike traditional TTS methods that consists of dedicated models for each task.  %In addition, sequence-to-sequence based TTS can take graphemes as input. In this setting, a sequence-to-sequence based TTS model learns pronunciation of words and alignment between text and speech implicitly without relying on either a pronunciation lexicon dictionary or alignment labels. This feature of sequence-to-sequence based TTS allows us to use huge amounts of unlabeled speech corpora from various languages, to which applying traditional methods would require prohibitive costs.  Several sequence-to-sequence based TTS methods have been proposed so far . Some of them have been evaluated against the DNN-based SPSS pipeline and unit-selection-based TTS systems, and it is widely agreed now that sequence-to-sequence based TTS can generally generate more natural synthetic speech. Particularly, Tacotron2 , which is a successor to Tacotron , and a Transformer-based TTS system  have advanced the sequence-to-sequence based TTS methods to the human-level naturalness.  %As sequence-to-sequence based methods and traditional DNN-based pipeline methods are making progress, it is becoming clear that both TTS approaches have indispensable functionalities in common. Abrasion studies show that autoregressive probabilistic modeling is crucial for both approaches . Additionally, an adopted waveform generation method determines the upper bound of synthetic speech quality . Nevertheless, many sequence-to-sequence based methods have been compared with pipeline systems using a non-autoregressive acoustic model, a conventional vocoder for waveform generation, or both . Because recent pipeline systems adopt both autoregressive modeling and a neural vocoder as recent sequence-to-sequence based methods, to compare the sequence-to-sequence and pipeline approaches with respect to the core of their framework, it is necessary to select comparable pipeline methods using autoregressive probabilistic model and equivalent waveform generation technique as sequence-to-sequence based methods.  We applied Tacotron  to Japanese in our previous research . Unlike English, Japanese is not a language to which sequence-to-sequence based TTS methods can be applied straightforwardly, mainly due to two issues: character diversity and pitch accent. Our modified Tacotron systems successfully produced synthetic speech with correct pitch accent by using accentual type labels along with phonemes as inputs. However, the synthetic speech of our proposed systems did not match the naturalness of those from comparable pipeline systems using an AR acoustic model and neural vocoder.   One possible way to  %The results suggested that phonemes and accentual type labels were not sufficient for fill the gap between our proposed systems and the pipeline systems is to introduce richer full-context label: the super set of phonemes and accentual type labels containing more complex features . However, this strategy is the exact opposite of  end-to-end sequence-to-sequence TTS strategies.  %Instead of enriching features to improve synthetic speech,  Another approach is empowering sequence-to-sequence TTS models themselves to compensate for their lack of complex features by implicitly extracting more from simple inputs like texts. However, little known about what kind of linguistic features can be compensated for by a powerful model and which changes  contribute to the improvement to what extent. This is what we want to investigate in this paper.   %This is what was expected to happen in the change from Tacotron  to Tacotron2 . The details of the effects of the change still are poorly known even now, partially because many changes happened at once in Tacotron2. For example,  achieved high quality of synthetic speech in Japanese by using Tacotron2 with full-context labels as input, but it is still unclear that whether their achievement came from rich linguistic information from full-context labels or improved configuration in Tacotron2. %The effects such as model parameter size, neural network structure, and input linguistic feature are poorly known. %Therefore, there is room to investigate how much a particular neural network structure can improve synthetic speech, what kind of linguistic feature can be compensated for by a powerful model, and which changes contribute to the improvement to what extent.  In this paper, we improve our sequence-to-sequence TTS models %in Japanese without enriching linguistic information. We take the approach to empower TTS models  by following the changes made by Tacotron2  and compare the effect of each group of the changes. Concretely, the configuration changes include increasing the model parameter size and simplifying the encoder's network structure. Furthermore, we apply the best configurations to English TTS systems using character input and analyze if the systems can implicitly learn linguistic features such as phone and lexical stress.  %in an end-to-end manner by applying them to  %instead of Japanese, in which letter-level TTS can barely be achieved.   We also compared our sequence-to-sequence TTS models with the new configuration against strong pipeline systems with autoregressive  probabilistic models and WaveNet-based waveform generation. Note that, in many studies, the sequence-to-sequence based methods have been compared with pipeline systems using a non-AR acoustic model, a conventional vocoder for waveform generation, or both . Our comparison may be fairer because recent studies have shown that AR modeling is crucial for not only sequence-to-sequence  but also the DNN-based pipeline TTS systems .    This paper is organized as follows. In section  we review the background of the pipeline based TTS method, sequence-to-sequence TTS methods, and our previous work about Tacotron based Japanese TTS methods. In addition, we summarize the transition from Tacotron to Tacotron2 to provide background for our new experimental conditions. In section , we describe our TTS systems used for this investigation. In section  we explain the new experimental conditions and their results. Section  concludes our findings\footnote{This paper is partially based on our previous work published in . The main focus of this journal paper is an analysis of implicit learning abilities on linguistic features in sequence-to-sequence text-to-speech synthesis in Japanese and English and differs from that of previous work, where we proposed a new neural network architecture for Japanese sequence-to-sequence text-to-speech synthesis.}.  %The additional materials are two experiments for our proposed methods with new configurations in Japanese and English. The contributions of this paper are as follows: 1) we improve Japanese TTS models of our proposed methods by increasing model parameter size and show their synthetic speech is now more natural than that of comparable pipeline systems. 2) We provide comparison for each group of changes made by Tacotron2, namely increased model parameter size and simplified encoder structure, and show that increasing parameter size can significantly improve naturalness of synthetic speech. 3) We compare grapheme and phoneme input in English, and show that an encoder with a complex structure can make synthetic speech given raw text as natural as synthetic speech given phonemes. 4) All experiments include comparable pipeline systems using autoregressive probabilistic modeling and a neural vocoder as baseline and show performance and limitations of our proposed systems compared with the pipeline systems.}        In this paper, we investigated under what conditions sequence-to-sequence based text-to-speech  could work well given simple input such as text or phonemes in Japanese and English, along with comparing them with comparable deep neural network  based pipeline TTS systems using complex full-context labels. We empowered models of our sequence-to-sequence based TTS methods instead of enriching linguistic features to see how much enforced models could overcome the linguistic feature limitation.   We upgraded configurations for our Tacotron based methods from our previous research.   Our previous work revealed that our Tacotron based methods underperformed the DNN-based pipeline methods due to linguistic feature limitation.  We increased the parameter size of models for our Tacotron methods. In addition, we tested a convolutional neural network  based encoder from Tacotron2, along with the CBHL  encoder from the original Tacotron for our TTS systems. Unlike other studies, our baseline pipeline systems used autoregressive probabilistic modeling and a neural vocoder as the sequence-to-sequence based methods do, so the differences in the two methods were mainly about framework approaches.  , not about probabilistic modeling assumption or vocoder performance.   Our experiment showed that increasing parameter size enabled the sequence-to-sequence based methods using phonemes and accentual-type labels as inputs to outperform the comparable pipeline systems in Japanese. This suggested that a powerful sequence-to-sequence TTS model could learn to compensate for a lack of rich linguistic features. We further investigated the upgraded TTS systems using characters as input in English. We found that the CBHL encoder could learn to disambiguate pronunciation ambiguities given characters as well as phone input better than the CNN encoder.   We concluded that powerful models with enough model capacity and proper network structures may compensate for a lack of complex linguistic features that had been used for traditional TTS methods and can synthesize highly natural speech from Japanese experiments.  However, we also observe that the CBHL encoder could not learn English stressed syllables from characters perfectly and hence resulted in flatter fundamental frequency.  Our future work includes improvements of network architectures and exploring a new way for learning supra-segmental features more appropriately .  
","  Neural sequence-to-sequence text-to-speech synthesis  can produce high-quality speech directly from text or simple linguistic features such as phonemes. Unlike traditional pipeline TTS, the neural sequence-to-sequence TTS does not require manually annotated and complicated linguistic features such as part-of-speech tags and syntactic structures for system training. However, it must be carefully designed and well optimized so that it can implicitly extract useful linguistic features from the input features. %The inputs are much simpler than traditional pipeline TTS methods that  %%conventional statistical parametric speech synthesis  %typically rely on complex manually-annotated linguistic features  %%full-context labels  %such as part-of-speech tags, syntactic structures and ToBI labels. Therefore an encoder part of the neural sequence-to-sequence TTS approach needs to be carefully designed and optimized so that it has capabilities to implicitly extract linguistic features required for predicting target acoustic features.   In this paper we investigate under what conditions the neural sequence-to-sequence TTS can work well in Japanese and English along with comparisons with deep neural network  based pipeline TTS systems. Unlike past comparative studies, the pipeline systems also use neural autoregressive  probabilistic modeling and a neural vocoder in the same way as the sequence-to-sequence systems do for a fair and deep analysis in this paper.  %which allows a fair comparison with the neural sequence-to-sequence systems.  %This allows us to find out fundamental differences between the two approaches. %, not about the probabilistic modeling assumption or vocoder.  We investigated systems from three aspects: a) model architecture, b) model parameter size, and c) language. For the model architecture aspect, we adopt modified Tacotron systems that we previously proposed and their variants using an encoder from Tacotron or Tacotron2. For the model parameter size aspect, we investigate two model parameter sizes. For the language aspect, we conduct listening tests in both Japanese and English to see if our findings can be generalized across languages.   Our experiments on Japanese demonstrated that the Tacotron TTS systems with increased parameter size and input of phonemes and accentual type labels outperformed the DNN-based pipeline systems using the complicated linguistic features and that its encoder could learn to compensate for a lack of rich linguistic features. Our experiments on English demonstrated that, when using a suitable encoder, the Tacotron TTS system with characters as input can disambiguate pronunciations and produce natural speech as good as those of the systems using phonemes. However, we also found that the encoder could not learn English stressed syllables from characters perfectly and hence resulted in flatter fundamental frequency. %a rich encoder used in Tacotron can disambiguate pronunciation given characters as good as phonemes input cases.  In summary, these experimental results suggest that a) a neural sequence-to-sequence TTS system should have a sufficient number of model parameters to produce high quality speech, b) it should also use a powerful encoder when it takes characters as inputs, and c) the encoder still has a room for improvement and needs to have an improved architecture to learn supra-segmental features more appropriately.   %the model parameter capacity of the neural sequence-to-sequence TTS needs to be sufficiently large for producing high quality speech and also a strong and rich encoder is recommended when it uses characters as inputs. %the model parameter capacity of the neural sequence-to-sequence TTS needs to be sufficiently large for producing high quality speech and also a strong and rich encoder is recommended when it uses characters as inputs.",74
"   % Keyword spotting Keyword spotting has become an essential access point for virtual assistants. Vocalized keywords such as Alexa, Hey Google, or Hey Siri can be used to initiate search queries and issue commands to mobile phones and smart speakers. The underlying algorithms must process streaming audio---the majority of which must be ignored---and trigger quickly and reliably when needed.  % Neural networks in speech Neural networks have achieved state-of-the-art performance in automatic speech recognition tasks. Applications of neural networks to keyword spotting have also been explored, particularly within the contexts of quality improvement and latency reduction for low-resource environments and end-to-end model training.  % Federated learning Federated Learning  is a decentralized computation paradigm that can be used to train neural networks directly on-device. In FL, all model updates shared by devices with the server are ephemeral , focused , and aggregated . In conjunction with techniques such as differential privacy and secure aggregation, FL can integrate strong anonymity and privacy guarantees into the neural network training process.  % FL for keyword spotting Federated learning provides a path to train keyword models at the edge, on real user data, as opposed to proxy data. In contrast, centrally-trained keyword-spotting models use proxy data, since false accepts  are not logged.  % Prior FL examples Multiple production models have been trained with federated learning, including next-word prediction, emoji prediction, n-gram language models, and query suggestions for mobile keyboards. Many of these models achieve better performance as a result of the additional signals and unbiased data available on-device. Recently, the feasibility of training keyword-spotting algorithms with FL has been explored with smaller datasets.  % Non-IID data On-device training comes with challenges, including the fact that the quantities and characteristics of training examples vary considerably from device to device. Centrally-trained models benefit from the ability to sample data in a controlled, independent and identically distributed  manner, resulting in gradient updates that are unbiased estimates of the total gradient for the dataset. This is not true on-device, where client updates are biased representations of the gradient across the entire population.  % Fitting non-IID data Non-IID data adversely affect convergence, and have been identified as a fundamental challenge to FL. Proposals to fit non-IID data better include optimizers that account for client drift, data sharing between client devices, and adaptive server optimizers with client learning rate decay, among others.  % Contributions in this work The primary contribution of this paper is to demonstrate that keyword-spotting models can be trained on large-scale datasets using FL, and can achieve false accept and false reject rates that rival those of centralized training. Using simulated federated learning experiments on large-scale datasets consisting of thousands of speakers and millions of utterances, we address the algorithmic challenges associated with training on non-IID data, the visibility challenges associated with labeling on-device data, and the physical constraints that limit augmentation capabilities on-device.     Empirical studies were conducted to train a keyword-spotting model using FL on non-IID data. Adaptive server optimizers like FedYogi helped train a model with a lower false reject rate in fewer training rounds. We also demonstrated the necessity and utility of replacing MTR with SpecAugment for on-device training. Ablation studies revealed the importance of multiple client epochs and reduced client clipping. And we provided strong empirical evidence in favor of client learning rate decay for training with non-IID data. Finally, we overcome the visiblity limitations of on-device training by demonstrating that, in the absence of high-quality on-device labels, teacher-student training can achieve comparable performance. 
","   We demonstrate that a production-quality keyword-spotting model can be trained   on-device using federated learning and achieve comparable false accept and   false reject rates to a centrally-trained model. To overcome the algorithmic   constraints associated with fitting on-device data , we conduct thorough empirical   studies of optimization algorithms and hyperparameter configurations using   large-scale federated simulations. To overcome resource constraints, we   replace memory-intensive MTR data augmentation with SpecAugment, which reduces   the false reject rate by 56\%. Finally, to label examples , we explore teacher-student training.",75
"  Traditionally, network pruning methods have been employed to obtain sparse neural network models to support edge devices with limited resources. However, machine learning production models today often target a variety of consumer hardware capabilities. The wide spectrum of mobile devices alone differ in latency by orders of magnitude for the same machine learning model. Systems such as home speakers and cars further increase this disparity. Additionally, different software applications can have distinct latency requirements. For example, the speech recognizer for real-time video conference captioning requires higher synchronicity than one for online video website subtitle generation.  Ideally, different-sized models with varying sparsity levels should be trained to target every single device type. However, this is impractical given the myriad of existing devices. Alternatively, one could train a few sparse models only targeting typical hardware configurations, but it necessitates the maintenance overhead of a device sparsity table. Moreover, even on a single device, resource availability fluctuates as concurrent activities vary. Models with static sparsity levels hence likely lead to sub-optimal resource usage.  To support such diversity of scenarios, we propose Dynamic Sparsity Neural Networks . A single trained DSNN model can execute at any predefined sparsity configuration at inference time with no or insignificant loss in accuracy compared to regular individually trained single sparsity networks. DSNN enables dynamic sparsity adjustment according to device capability, resource availability, and application requirements, thereby achieving an optimal accuracy-latency trade-off with minimal memory footprint.  DSNN was inspired by recent work which showed that even for untrained random networks, there exist arbitrarily sparse sub-networks that achieve very high quality. Therefore, it is likely that trained networks also simultaneously contain powerful sub-networks at multiple sparsity levels.  Methodologically, DSNN builds upon slimmable neural networks  that similarly tackle model deployment across heterogeneous devices. However, SNN was only designed for convolutional neural networks, restricting their applicability to many domains and tasks. We demonstrate in \S that a naive generalization of SNN to automatic speech recognition  models shows poor performance.  DSNN, on the other hand, is a sparsity-based extension of SNN that is applicable to any weight-based neural network. Since modern specialized hardware allows such sparse models to have comparable speedup to SNN that prunes entire convolutional channels, this generalization comes at little inference time cost. In this paper, we choose to focus on the task of ASR due to an increasing demand for on-device ASR. We show that a single DSNN model generally matches the quality of individually trained single sparsity networks across multiple sparsity configurations . DSNN hence contributes to practical machine learning systems through its ability to dynamically adjust to multiple hardware types with different resource and energy constraints. This greatly reduces both the training overhead and the management complexity of deployment processes.      We presented a training scheme that allows one single trained model to optimally switch its sparsity level at inference time. Given that its performance is on par with individually trained single sparsity networks, such a model can simultaneously support a variety of devices with different hardware capabilities and applications with diverse latency requirements. Future work can attempt to close the marginal gap between DSNN and single sparsity networks, especially at high sparsity levels.  In this work, we only considered models with all parameters of the same layer type pruned by the same fraction. However, components of a machine learning model are sometimes not equally important and setting different sparsity levels for different weights may yield a higher quality model. As each weight matrix is independently pruned in the DSNN training algorithm, DSNN is able to approximate the performance of individually trained networks with arbitrary sparsity configurations across weights. Combined with a greedy search algorithm, DSNN can be used to search for an optimal per-weight sparsity configuration, similar to. This can be an interesting future exploration.  
"," In automatic speech recognition , model pruning is a widely adopted technique that reduces model size and latency to deploy neural network models on edge devices with resource constraints. However, multiple models with different sparsity levels usually need to be separately trained and deployed to heterogeneous target hardware with different resource specifications and for applications that have various latency requirements. In this paper, we present Dynamic Sparsity Neural Networks  that, once trained, can instantly switch to any predefined sparsity configuration at run-time. We demonstrate the effectiveness and flexibility of  using experiments on internal production datasets with Google Voice Search data, and show that the performance of a  model is on par with that of individually trained single sparsity networks. Our trained  model, therefore, can greatly ease the training process and simplify deployment in diverse scenarios with resource constraints.",76
"  Lexical substitution is the task of generating words which can replace a given word in a given textual context. For instance, in the sentence ``My daughter purchased a new car'' the word car can be substituted by its synonym vehicle keeping the same meaning, but also with the co-hyponym bike, or even the hypernym means of transport while keeping the original sentence grammatical. Lexical substitution can be useful in various applications, such as word sense induction, lexical relation extraction, paraphrase generation, semantic spelling correction, text simplification, textual data augmentation, etc.    The new generation of language models  based on deep neural networks, such as  ELMo, BERT, and XLNet enabled a profound breakthrough in many NLP tasks, ranging from sentiment analysis to named entity recognition. Commonly these models are used to perform pre-training of deep neural networks which are finally fine-tuned to perform some task different from language modelling. In this paper we provide the first large-scale comparison and analysis of various neural LMs/MLMs applied to the task of lexical substitution and two tasks which exploit lexical substitution, namely word sense induction and text data augmentation. More specifically, the main contributions of the paper are as follows:    of neural language models and masked language models  applied for lexical substitution based on both intrinsic and extrinsic evaluations.    produced by substitution models and human annotators.   for improvement of lexical substitution quality.        In this paper, we presented the first large-scale computational study of three state-of-the-art neural language models  and their variant on the task of lexical substitution in the context. In addition to extensive experimental comparisons on several intrinsic lexical substitution benchmarks, we present a comparison of the models in the context of two applications: word sense induction and text data augmentation.   Our finding suggests that  the simple unsupervised approaches based on large pre-trained neural language models yield results comparable to sophisticated traditional supervised baseline approaches;  integration of the information about the target substantially boosts the quality of lexical substitution and shall be used whenever possible.   In addition to comparison on the benchmarks, we also show which models tend to produce semantic relations of which types  providing valuable guidelines to practitioners aiming to use lexical substitution in applications. Indeed, depending on the type of semantic relations required in an NLP application one or another type of neural LM shall be used.                
"," Lexical substitution in context is an extremely powerful technology that can be used as a backbone of various NLP applications, such as word sense induction, lexical relation extraction, data augmentation, etc. In this paper we present a large-scale comparative study of popular neural language and masked language models , such as context2vec, ELMo, BERT, XLNet, applied to the task of lexical substitution. We show that already competitive results achieved by SOTA LMs/MLMs can be further improved if information about the target word is injected properly, and compare several target injection methods. In addition, we provide analysis of the types of semantic relations between the target and substitutes generated by different models providing insights into what kind of words are really generated or given by annotators as substitutes.",77
"} % Computer Society journal  papers do something unusual % with the very first section heading . % They place it ABOVE the main text! IEEEtran.cls does not automatically do % this for you, but you can achieve this effect with the provided % \IEEEraisesectionheading{} command. Note the need to keep any \label that % is to refer to the section immediately after    In this work, we aim to develop a deep learning model that achieves a good trade-off between accuracy and interpretability on text classification tasks. Based on our insights on capsules, we propose the interpretable capsule network~ by employing attention mechanism and adapting CapsNets from computer vision tasks to text classification tasks. We provide novel, simple yet effective way to interpret our iCapsNets. In particular, iCapsNets achieve the local and global interpretability at the same time. Experimental results show that our iCapsNets yield human-understandable interpretation results, without suffering from significant performance loss compared to non-interpretable models.    use section* for acknowledgment \ifCLASSOPTIONcompsoc     The Computer Society usually uses the plural form   
"," Many text classification applications require models with satisfying performance as well as good interpretability. Traditional machine learning methods are easy to interpret but have low accuracies. The development of deep learning models boosts the performance significantly. However, deep learning models are typically hard to interpret. In this work, we propose interpretable capsule networks~ to bridge this gap. iCapsNets use capsules to model semantic meanings and explore novel methods to increase interpretability. The design of iCapsNets is consistent with human intuition and enables it to produce human-understandable interpretation results. Notably, iCapsNets can be interpreted both locally and globally. In terms of local interpretability, iCapsNets offer a simple yet effective method to explain the predictions for each data sample. On the other hand, iCapsNets explore a novel way to explain the model's general behavior, achieving global interpretability. Experimental studies show that our iCapsNets yield meaningful local and global interpretation results, without suffering from significant performance loss compared to non-interpretable methods.",78
"  {S}{entiment} analysis and emotion recognition are of vital importance in dialogue systems and have recently gained increasing attention. They can be applied to a lot of scenarios such as mining the opinions of  speakers in conversations, improving the feedback of robot agents, and so on. Moreover, sentiment analysis in live conversations can be used in generating talks with certain sentiments to improve human-machine interaction. Existing approaches to conversational sentiment analysis can be divided into party-dependent approaches, like DialogueRNN, and party-ignorant approaches, such as AGHMN. Party-dependent methods distinguish different parties in a conversation while party-ignorant methods do not.  Both party-dependent and party-ignorant models are not limited to dyadic conversations.  Nevertheless, party-ignorant models can be easily applied to multi-party scenarios without any adjustment. In this paper, we propose a fast, compact and parameter-efficient party-ignorant framework based on emotional recurrent unit , a recurrent neural network that contains a generalized neural tensor block  and a two-channel feature extractor  to tackle conversational sentiment analysis.  Context information is the main difference between dialogue sentiment analysis and single sentence sentiment analysis tasks. It sometimes enhances, weakens, or reverses the raw sentiment of an utterance . There are three main steps for sentiment analysis in a conversation: obtaining the context information, capturing the influence of the context information for an utterance, and extracting emotional features for classification. Existing dialogue sentiment analysis methods like c-LSTM, CMN, DialogueRNN, and DialogueGCN make use of complicated deep neural network structures to capture context information and describe the influence of context information for an utterance.  We redefine the formulation of conversational sentiment analysis and provide a compact structure to better encode the context information, capture the influence of context information for an utterance, and extract features for sentiment classification. According to Mitchell and Lapata, the meaning of a complete sentence must be explained in terms of the meanings of its subsentential parts, including those of its singular elements. Compositionality allows language to construct complicated meanings from its simpler terms. This property is often expressed in a manner of principle: the meaning of a whole is a function of the meaning of the components. For conversation, the context of an utterance is composed of its historical utterances information. Similarly, context is a function of the meaning of its historical utterances. Therefore, inspired by the composition function in, we design GNTB to perform context compositionality in conversation, which obtains context information and incorporates the context into utterance representation simultaneously, then employ TFE to extract emotional features. In this case, we convert the previous three-step task into a two-step task. Meanwhile, the compact structure reduces computational cost. To the best of our knowledge, our proposed model is the first to perform context compositionality in conversational sentiment analysis.  The GNTB takes the context and current utterance as inputs, and is capable of modeling conversations with arbitrary turns. It outputs a new representation of current utterance with context information incorporated . Then, the contextual utterance vector is further fed into TFE to extract emotional features. Here, we employ a simple two-channel model for emotion feature extraction.  The long short-term memory  unit and one-dimensional convolutional neural network  are utilized for extracting features from the contextual utterance vector. Extensive experiments on three standard datasets demonstrate that our model outperforms state-of-the-art methods with fewer parameters. To summarize, the main contributions of this paper are as follows:         The remainder of the paper is organized as follows: related work is introduced in Section; the mechanism of our model is explained in Section; results of the experiments are discussed in Section; finally, concluding remarks are provided in Section.       In this paper, we proposed a fast, compact and parameter-efficient party-ignorant framework bidirectional emotional recurrent unit  for sentiment analysis in conversations. Our proposed generalized neural tensor block , skilled at context compositionality, reduced the number of parameters and was suitable for different structures. Additionally, our TFE is capable of extracting high-quality emotion features for sentiment analysis. We proved that it is feasible to both simplify the model structure and improve performance simultaneously.  Our model outperforms current state-of-the-art models on three standard datasets in most cases. In addition, our method has the ability to model conversations with arbitrary turns and speakers, which we plan to study further in the future. Finally, we also plan to adopt more recent emotion categorization models, e.g., the Hourglass of Emotions, to better distinguish between similar yet different emotions.   
"," Sentiment analysis in conversations has gained increasing attention in recent years for the growing amount of applications it can serve, e.g., sentiment analysis, recommender systems, and human-robot interaction. The main difference between conversational sentiment analysis and single sentence sentiment analysis is the existence of context information which may influence the sentiment of an utterance in a dialogue. How to effectively encode contextual information in dialogues, however, remains a challenge. Existing approaches employ complicated deep learning structures to distinguish different parties in a conversation and then model the context information. In this paper, we propose a fast, compact and parameter-efficient party-ignorant framework named bidirectional emotional recurrent unit for conversational sentiment analysis. In our system, a generalized neural tensor block followed by a two-channel classifier is designed to perform context compositionality and sentiment classification, respectively. Extensive experiments on three standard datasets demonstrate that our model outperforms the state of the art in most cases.",79
"  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %   % FF - BEGIN % . %     %  %     % % final paper: en-us version  %     % %     %   % space normally used by the marker %     % This work is licensed under a Creative Commons  %     % Attribution 4.0 International License. %     % License details: %     % \url{http://creativecommons.org/licenses/by/4.0/}. % } % FF - END        Knowledge Bases , such as Freebase , DBpedia , and Wikidata, contain rich and precise information about entities of all kinds, such as persons, locations, organizations, movies, scientific theories to name a few. Each entity has a set of carefully defined relations and attributes, e.g. ``was born in'' or ``play for''. This wealth of structured information gives rise and facilitates the development of semantic processing algorithms as they can directly operate on and benefit from such entity representations. For instance, imagine a search engine that is able to retrieve mentions in the news during the last month of all retired NBA players with a net income of more than 1 billion US dollars. The list of players together with their income and retirement information may be available in a knowledge base. Equipped with this information, it appears to be straightforward to look up mentions of such retired basketball players in the newswire. However, the main obstacle for such a direct counting algorithm is the lexical ambiguity of entities. In the context of this application, one would want to only retrieve all mentions of ``Michael Jordan '' and exclude mentions of other persons with the same name such as ``Michael Jordan ''.   This is why Entity Linking  -- the process of matching a mention, e.g. ``Michael Jordan'', in a textual context to a KB record  fitting the context --  is the key technology enabling various semantic applications. Thus, EL is the task of identifying an entity mention in text and establishing a link to an entry in a  knowledge base .   Entity linking is an essential component of many information extraction and Natural Language Understanding  pipelines since it resolves the lexical ambiguity of entity mentions and determines their meanings. A link between a textual mention and an entity in a knowledge base also allows to take advantage of the information encompassed in a semantic graph, which is shown to be useful in such NLU tasks as information extraction , biomedical text processing , or semantic parsing and question answering . This wide range of direct applications is the reason why entity linking is enjoying a great interest from both academy and industry  for more than two decades.   Recently, a new generation of approaches for entity linking based on the neural models and deep learning  emerged  pushing the state-of-the-art to the new level. The goal of this survey is to provide an overview of this latest wave of models, emerging from 2015 until now.     Models based on neural networks have managed to excel in EL as in many other natural language processing tasks due to their ability to learn useful deep distributed representations of linguistic data. The state-of-the-art neural entity linking models have shown significant improvements over ``classical'' machine learning approaches  % %. Classical machine learning systems for entity linking   that are based on shallow architectures, e.g. Logistic Regression, and/or depend mostly on hand-engineered features. Such models often cannot capture all relevant statistical dependencies and interactions . In contrast, deep neural networks are able to learn sophisticated representations within their deep layered architectures reducing the burden of manual feature engineering. This capability enabled  improvements on  various tasks, including EL as will be discussed in detail in Section .  In this survey, we systemize recently proposed neural models, distilling one generic architecture commonly used by the popular neural EL models . We categorize and summarize the models used in each component of this architecture, e.g. candidate generation or ranking. The prominent variations of this generic architecture, e.g. end-to-end EL or global models, are also categorized and discussed. To better structure the sheer amount of available models, various types of methods are illustrated in the form of taxonomies  while notable features of each model are carefully assembled in tabular form .  % FF   An important component of neural entity linking systems is entity vector representations and entity encoding methods. It has been shown that encoding in low-dimensional vectors the KB structure , entity definitions, as well as textual information in large annotated corpora, helps to improve the generalization capabilities of EL models significantly. We summarize novel methods for entity encoding, as well as context/mention encoding techniques.   Many natural language processing systems take advantage of deep pre-trained language models like ELMo , BERT , and their modifications. EL made its path into these models as a way of introducing information stored in KBs, which helps to adopt word representations to some text processing tasks. We discuss this novel application of EL and how it can be further developed.        . EL model takes a raw textual input and enriches it with entity mention links in a KB. Commonly the task is split into entity recognition and entity disambiguation sub-tasks.      }          %In this section, we contrast our survey to previous surveys on EL systems .  One of the first surveys on EL is provided by , in 2015. They aim at providing  a standard problem definition to reduce a confusion that appears due to existence of variant similar tasks related to EL , and  a clear comparison of models and their various aspects. %to show, which aspect of a method makes better. In the same year,  published a survey covering the main approaches to entity linking, its applications, evaluation methods, and future directions.  % From the time that these papers were published, various models have been developed, which are required to be further analysed and discussed  There are also other surveys, which address a wider scope. The work of , published in 2020, %covering the studies published before 2019,   involves information extraction models and semantic web technologies.  %They include the studies based on the criteria of making non-trival use of an ontology, knowledge-base, tool or language founded on some standards, in Semantic Web technologies part. In Information Extraction part, they include systems which provide extraction and/or linking model in terms of three elements: entities, concepts, and relation.  Namely, they consider named entity recognition, entity linking, terminology extraction, keyphrase extraction, topic modeling, topic labeling, and relation extraction tasks for information extraction side.  %Therefore, their survey broadly covers numerous topics in both subjects.  %Rather, we concentrate specifically on subset methods of entity linking task, i.e. neural EL models, to discover their success.  In a similar vein, , released in 2020, overview the research in named entity recognition and named entity disambiguation/linking published between 2014-2019. % %Instead, we focus on rapidly developing neural models presented since 2015.   Another recent survey paper by , published in 2020, analyses and summarizes EL approaches that exhibit some holism. This viewpoint limits the survey to the works that exploit various peculiarities of the EL task: additional metadata stored in specific input like microblogs, specific features that can be extracted from this input like geographic coordinates in tweets, timestamps, interests of users posted these tweets, and specific disambiguation methods that take advantage of these additional features.   %According to the authors, holism in the EL task is to focus on  different types of input, %e.g. text documents, KGs,  % the features, which can be extracted from input, and  the methods that can be used in the data processing for EL. Based on this, they classify EL models that exhibit some holism and analyse them. %according to their inputs and data features, usage of NLP tasks for information extraction, and their collective disambiguation of mentions, e.g. embeddings. %Therefore, their specific focus is different from our purpose.       The previous surveys  do not cover many recent publications ,  broadly cover numerous topics , or  are focused on the specific types of methods . There is not yet, to our knowledge, a detailed survey specifically devoted to recent neural entity linking models. %to discover their success. The previous surveys also do not address the topics of entity and context/mention encoding, applications of EL to deep pre-trained language models, and cross-lingual EL. We also the first to summarize the domain-independent approaches to EL, several of which are based on zero-shot techniques.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    More specifically, this paper makes the following contributions:  % a survey of state-of-the-art neural entity linking models;  a discussion of recent zero-shot and cross-lingual EL approaches; % a survey of EL applications to modeling word representations.  [itemsep=1mm, parsep=0pt]      % FF The structure of this survey is the following. We start with defining the task of EL in Section . In Section , the common architecture of neural entity linking systems is presented. Modifications and variations of this basic pipeline are discussed in Section . In Section , we summarize the evaluation results for EL and entity representation models. Section  is dedicated to the application of EL by highlighting recently emerged applications for improving neural language models. Finally, Section  summarizes the survey and suggests a prominent direction of future work in neural EL.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  [t]   % FF:  EL contains two main steps: Entity Recognition, mentions in a plain text are distinguished, and Entity Disambiguation, a corresponding entity is predicted for the given mention. Entity Disambiguation is further divided into two steps: Candidate Generation, possible entities are produced for the mention, and Entity Ranking, a score between context/mention and a candidate is computed through the representations.}        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      In this survey, we have analyzed recently proposed neural entity linking models, which generally perform the task with higher accuracy than classical methods scores. We provide a generic neural entity linking architecture, which are applicable for most of the neural EL systems, including components e.g. candidate generation, entity ranking, mention and entity encoding. The various modifications of general architecture are grouped into four common directions:  joint entity recognition and linking models,  global entity linking models,  domain-independent approaches including zero-shot and distant supervision methods, and  cross-lingual techniques. The taxonomy figures and feature tables are provided to explain categorization and to show which prominent features are used in each method.   The majority of studies still rely on external knowledge for the candidate generation step. The mention encoders have made a shift from convolutional and recurrent models to self-attention architectures and start using pre-trained contextual language models like BERT. There is a surge of models that tackle the problem of adapting a model trained on one domain to another domain in a zero-shot fashion. These approaches do not need any annotated data in the target domain, but only descriptions of entities from this domain to make such adaptation. It is shown in several works that the cross-encoder architecture is superior as compared to models with separate mention and entity encoders. Many approaches rely on pre-trained entity representations, only few take advantage of a trainable entity encoder inside an EL model. The global context is widely used, but there are few recent studies that focus only on local EL.   Among the joint recognition and disambiguation solutions, the leadership is still owned by . Among published local models for disambiguation, the best result is reported by  Sil et al.  and   . It is worth noting that this model can be used in a zero-shot setting. The global models outperform the local ones. The work of  reports results that are consistently better in comparison to other solutions. The performance improvements are attributed to the masked entity prediction mechanism for entity embedding and to the usage of the pre-trained model based on BERT with a multi-step global scoring function.   FF :Several studies have demonstrated benefits for deep transfer learning models of using information stored in KBs by incorporating EL into these models.   
"," In this survey, we provide a comprehensive description of recent neural entity linking  systems developed since 2015 as a result of the ``deep learning revolution'' in NLP. Our goal is to systemize design features of neural entity linking systems and compare their performances to the best classic methods on the common benchmarks. We distill generic architectural components of a neural EL system, like candidate generation and entity ranking summarizing the prominent methods for each of them, such as approaches to mention encoding based on the self-attention architecture. The vast variety of modifications of this general neural entity linking architecture are grouped by several common themes: joint entity recognition and linking, models for global linking, domain-independent techniques including zero-shot and distant supervision methods, and cross-lingual approaches. Since many neural models take advantage of pre-trained entity embeddings to improve their generalization capabilities, we provide an overview of popular entity embedding techniques. Finally, we briefly discuss applications of entity linking, focusing on the recently emerged use-case of enhancing deep pre-trained masked language models such as BERT. %Previous surveys on entity linking do not provide a discussion for many of these topics including entity embeddings, applications of EL to improve deep pre-trained language models, cross-lingual linking, and zero-shot models.",80
" . }  Propaganda is biased information that deliberately propagates a particular ideology or political orientation . Propaganda aims to influence the public's mentality and emotions, targeting their reciprocation due to their personal beliefs . News propaganda is a sub-type of propaganda that manipulates lies, semi-truths, and rumors in the disguise of credible news . The emphasis on this manipulation differentiates propaganda and its various classes from each other and free verbalization . News propaganda can lead to the mass circulation of misleading information, shared agenda, conflicts, religious or ethnic reasons, and can further even lead to violence and terrorism. Due to massive size, high velocity, rich online user interaction, and diversity, the manual identification of propaganda techniques is overwhelmingly impractical. Hence, the development of a generalized system for propaganda detection in news articles is a vital task for security analysts and society . %Hence, robust propaganda detection in news articles is a very vital task not only for the security analysts but also for society.   In SemEval2020-Task11,  propose a corpus of  news articles for propaganda detection. Each article is annotated with propaganda spans belonging to  propaganda techniques. The annotation is performed at the fragment level. The task of propaganda detection is divided into two sub-tasks; Span identification  and technique classification . Span identification sub-task aims to detect the propagandist spans of text in the news articles. Whereas, the technique classification sub-task aims to classify the propaganda spans into various propaganda techniques. The work presented in this paper aims to provide independent approaches for both sub-tasks  and . Most recent approaches for propaganda detection task use pre-trained transformer models. This work propose a multi-granularity knowledge sharing model built on top of the embeddings extracted from these transformer models for the propaganda span detection sub-task. Further, we show the effectiveness of ensembling linguistic features-based machine learning classifiers with these transformer models covering the minority-classes for the technique classification sub-task.    In this paper, we proposed systems for the task of span identification and multi-class imbalanced technique classification of propaganda spans in news articles. We analyzed the performance of various machine learning and deep learning-based architectures for these high granularity tasks. On the span-identification sub-task test set, our multi-granularity knowledge sharing model gives a span-level F1 score of . For the technique classification task, our ensemble of pre-trained transformer model with logistic regression gives a micro F1 score of . We further infer the effectiveness of incorporating linguistic features and achieve non-zero F1 scores for all techniques and  gain in the macro-F1 score. Our results also unveil the limitations/ineffectiveness of deep learning models to capture the minority-class techniques.  we analyzed the performance of various machine learning and deep learning-based architectures for the high granularity task of span identification and multi-class imbalanced technique classification of propaganda spans through our tests. On test set our transformer based multi-granularity knowledge sharing system gives a span-level F1 score of  for the span-identification sub-task. For technique classification our ensemble of pre-trained transformer model with logistic regression gives a micro F1 score of . We further infer the effectiveness of incorporating linguistic features by the non-zero F1 scores for all techniques and the  gain in the macro-F1 score. Our results also unveil the limitations/ineffectiveness of deep learning models to capture the minority-class techniques.   We plan to extend our work by using trainable transformer-model embeddings and improve the performance of the span-identification sub-task. The work can further be enhanced by adding more granularities for knowledge-sharing. The proposed knowledge-sharing model may also be used for various closely-related tasks such as fake news and hate speech detection, given application-specific appropriate objective functions are defined across multiple granularities for these tasks.  
"," Propaganda spreads the ideology and beliefs of like-minded people, brainwashing their audiences, and sometimes leading to violence. SemEval $2020$ Task-11 aims to design automated systems for news propaganda detection. Task-11 consists of two sub-tasks, namely, Span Identification - given any news article, the system tags those specific fragments which contain at least one propaganda technique and Technique Classification - correctly classify a given propagandist statement amongst $14$ propaganda techniques. For sub-task $1$, we use contextual embeddings extracted from pre-trained transformer models to represent the text data at various granularities and propose a multi-granularity knowledge sharing approach. For sub-task $2$, we use an ensemble of BERT and logistic regression classifiers with linguistic features. Our results reveal that the linguistic features are the reliable indicators for covering minority classes in a highly imbalanced dataset.",81
"  Deep learning has undoubtedly pushed the frontier in Natural Language Processing . Particularly large pre-trained language  models have improved results for a wide range of NLP applications. However, the lack of portability of NLP models to new conditions remains a central issue in NLP. For many target applications, labeled data is lacking , and even for pre-training general models data might be scarce . This makes it even more pressing to revisit a particular type of transfer learning, namely domain adaptation . A default assumption in many  machine learning algorithms is that the training and test sets follow the same underlying distribution. When these distributions do not match, we face a dataset shift -- in NLP typically referred to as a domain shift. In this setup, the  domain and the source training data differ, they are not sampled from the same underlying distribution. Consequently, performance drops on the target, which  undermines the ability of models to truly generalize . Domain adaptation is closely tied to a fundamental bigger open issue in machine learning: generalization beyond the training distribution. Ultimately, intelligent systems should be able to adapt and robustly handle any test distribution, without having seen any data from it. This is the broader need for out-of-distribution generalization, and a more challenging setup targeted at handling unknown domains.   Work on domain adaptation focused largely on supervised domain adaptation. In such a classic supervised DA setup, a small amount of labeled target domain data is available, along with some larger amount of labeled source domain data. The task is to adapt from the source to the specific target domain in light of limited target domain data.  However, annotation is a substantial time-requiring and costly manual effort. While annotation directly mitigates the lack of labeled data, it does not  easily scale to new application targets.  In contrast, DA methods aim to shift the ability of models from the traditional interpolation of similar examples to models that extrapolate to examples outside the original training distribution . Unsupervised domain adaptation  mitigates the domain shift issue by learning only from unlabeled target data, which is typically available for both source and target domain. UDA fits the classical real-world scenario better, in which labeled data in the target domain is absent, but unlabeled data might be abundant. UDA thus provides an elegant and scalable solution. We believe these advances in UDA will help for out-of-distribution generalization.         We categorize research into model-centric, data-centric and hybrid approaches, as shown in Figure.  Model-centric methods target approaches to augment the feature space, alter the loss function, the architecture or model parameters. Data-centric methods focus on the data aspect and either involve pseudo-labeling  to bridge the domain gap, data selection and pre-training methods. As some approaches take elements of both, we include a hybrid category.\footnote{We take inspiration of the data-centric and model-centric terms from~ in MT, and add hybrid.} A comprehensive overview of UDA methods and the tasks each method is applied to is provided in Table. %We discuss DA setups  and related problems .   Comprehensive reviews on DA exist, each with a different focus: visual applications , machine translation  , pre-neural DA methods in NLP . Seminal surveys in machine learning on transfer learning include~,~, and~. %.   In this survey, we  comprehensively review neural approaches to unsupervised domain adaptation in NLP,\footnote{We disregard methods which are task-specific .}  we analyze and compare the strengths and weaknesses of the described approaches, and  we outline potential challenges and future directions in this field.        In this survey, we review  strands of unsupervised domain adaptation, summarized into model-centric, data-centric, and hybrid methods, including trends in pre-training. We also revisit the notion of domain and suggest to use the term variety instead, to better capture the multitude of dimensions of variation. Our survey identifies a limited focus on sentiment benchmarks and single-task evaluation for UDA. Lastly, we outline future directions, linking to the broader challenges related to learning beyond 1:1 scenarios and out-of-distribution generalization. This also calls for new directions on benchmarks and learning under scarce data.     anonymize for now  \clearpage 
","     Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge.      Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future NLP.\footnote{Accompanying repository: \url{https://github.com/bplank/awesome-neural-adaptation-in-NLP}}",82
" People express their opinions on blogs and other social media platforms. Automated ways to understand the opinions of users in such user-generated corpus are of immense value. It is especially essential to understand the stance of users, which involves finding people's opinions on controversial topics. Therefore, it's not surprising that many researchers have explored automated ways to learn stance given a text . While learning stance from users' individual posts have been explored by several researchers , there is an increased interest in learning stance from conversations. For example, as we show in Fig. , a user denies the claim made in the original tweet. This kind of stance learning has many applications, including insights into conversations on controversial topics  and finding potential rumor posts on social-media . However, the existing datasets used for training and evaluating the stance learning models limit the broader application of stance in conversations.    The existing research on stance in conversations has three significant limitations: 1) The existing datasets are built around rumor events to determine the veracity of a rumor post based on stance taken in replies . Though useful for rumor detection, this does not generalize to non-rumor events , 2) The existing datasets focus primarily in direct responses and do not take into account quotes. This is critical as quotes have been gaining prominence since their introduction by Twitter in 2015, especially in the context of political debates ,  3) The existing datasets have uneven class distributions, i.e., only a small fraction of the examples in the dataset have supporting and denying stances, and most other examples have no clear stance. These unbalanced classes lead to poor learning of denying stance  . The denying class is expected to be more useful for downstream tasks like finding an antagonistic relationship between users. Therefore there is a need to build a new dataset that has more denying stance examples.  To overcome the above limitations, in this research, we created a new dataset by labeling the stance in replies  to posts on Twitter. To construct this dataset, we developed a new collection methodology that is skewed towards responses that are more likely to have a denial stance. This methodology was applied across three different contentious events that transpired in the United States during 2018. We also collected an additional set of responses without regard to a specific event. We then labeled a representative sample of the response-target pairs for their stance. Focusing on the identification of denial in responses is an essential step for the identification of tweets that promote misinformation  and also to estimate community polarization . By leveraging these human-labeled examples, along with more unlabeled examples on social-media, we expect to build better systems for detecting misinformation and understanding of polarized communities.   % Several studies have identified the tendency of users to report or debunk false information , and l  To summarize, the contribution of this work is fourfold:         This paper is organized as follows. We first discuss the related work and then describe our approach to collect the potential tweets to label in `Dataset Collection Methodology'. As the sample that can be labeled is rather small  compared to the entire available dataset, we discuss the sample construction procedure for annotation. Then, we describe the annotation process and the statistics of the dataset that obtained as a result of annotation in section `Annotation Procedure and Statistics'. Next, we present some baseline models for stance learning  and present the result. Finally, we discuss our results and propose future directions.  % Building manually labeled datasets is of great value as it allows comparison of various algorithms with respect to human expectations. In this research, we build a new dataset to learn the language pattern that users' employ while taking a stance . This dataset could be used for automated ways to infer the stance in replies.   % In this paper, we create a new dataset by labeling stance in replies  to posts on Twitter on controversial issues. To the best of our knowledge, this is currently the largest human-labeled stance dataset for Twitter conversations with over 5000  stance labels. More importantly, we designed a way to pick the examples such that stance classes are more balanced. Moreover, we provide separate stance labels for `Replies' and `Quotes'. As we show in this paper, the two modalities of replies and quotes behave very differently when it comes to stance learning. Additionally, we include many baseline models for learning the stance in conversations, and show the benefit of having a larger dataset.  %  Moreover, because Quotes are more context-dependent, we observe that these tweets could be more challenging to label.  % We plan to explore this in future work.   In this research, we created a new dataset that has stance labels for replies  on Twitter posts on three controversial issues and on additional examples which do not belong to any specific topic. To overcome the limitations of prior research, we developed a collection methodology that is skewed toward non-neutral responses, and therefore has a more balanced class distribution as compared with prior datasets that have `Comment' as the majority class. We find that, when applied to contentious events, our methodology is effective at recovering contentious conversations and more non-neutral threads. Finally, our dataset also separates quotes and replies and is the first dataset to have stance labels for quotes. We envision that this dataset will allow other researchers to train and test models to automatically learn the stance taken by social-media users while replying to  posts on social media.    We also experimented with few machine learning models and evaluated their performance. We find that learning stance in conversations is still a challenging problem. Yet stance mining is important as  conversations are the only way to infer negative links between users of many platforms, and therefore inferring stance in conversations could be very valuable. We expect that our new dataset will allow the development of better stance learning models and enable a better understanding of community polarization and the detection of potential rumors.    how stance is used while discussing controversial topics.    , and perhaps lead to a better understanding of community polarization and detection of potential rumors.  
"," % Opinions are  expressed on blogs and social-media platforms.  Stance provides a natural way to articulate social interaction  thereby lending itself to model users' opinions. However,  most existing study on Stance takes a simplistic view assuming a `sentence'  holding a perspective that is independent of the context and the author. Stance should be approached in a broader context of social action wherein authors use stance to position themselves with respect to objects of interests, thereby, aligning with other stance takers.  % In this research we study attitudinal stance as a means to model conversations on Twitter. Using `The Stance Triangle' approach, we build a computational model that evaluates authors' position towards an object, and simultaneously aligns them with other authors. To evaluate the model, we collect Tweets on controversial topics and build a dataset that has opinion labels for each user. Unlike prior work, our model positions authors as central and their alignment as a crucial to learning stance. Thus, our work proposes a new direction to the stance learning problem which is grounded in theory and is more amenable to conversations on social-media. %",83
" Ethical NLP research has recently gained attention %, such as attempting to evaluate if gender biases exist in data and if so, how to avoid propagating potentially harmful biases among NLP systems  .  For example, the environmental cost of AI research has become a focus of the community, especially with regards to the development of deep neural networks . Beyond developing systems to be greener, increasing the efficiency of models makes them more cost-effective, which is a compelling argument even for people who might downplay the extent of anthropogenic climate change.  In conjunction with this push for greener AI, NLP practitioners have turned to the problem of developing models that are not only accurate but also efficient, so as to make them more readily deployable across different machines with varying computational capabilities . This is in contrast with the %Google recently popular principle of make it bigger, make it better .   Here we explore teacher-student distillation as a means of increasing the efficiency of neural network systems used to undertake a core task in NLP, dependency parsing. To do so, we take a state-of-the-art Biaffine parser from . The Biaffine parser is not only one of the most accurate parsers, it is the fastest implementation by almost an order of magnitude among state-of-the-art performing parsers.   We utilise teacher-student distillation to compress Biaffine parsers trained on a %linguistically  diverse subset of %the  Universal Dependency  treebanks. We find that distillation maintains accuracy performance close to that of the full model and obtains far better accuracy than simply implementing equivalent model size reductions by changing the parser's network size and training normally. %far surpasses simply training smaller parsers with same model sizes as the distilled models.  Furthermore, we can compress a parser to 20\% of its trainable parameters with minimal loss in accuracy and with a speed 2.30x  faster than that of the original model on CPU .          We have obtained results that suggest using teacher-student distillation for UD parsing is an effective means of increasing parsing efficiency. The baseline parser used for our experiments was not only accurate but already fast, meaning it was a strong baseline from which to see improvements. We obtained parsing speeds 2.30x  faster on CPU  while only losing 1 point for both UAS and LAS when compared to the original sized model. Furthermore, the smallest model which obtains these results only has 20\  of the original model's trainable parameters, vastly reducing its environmental impact. 
"," The carbon footprint of natural language processing research has been increasing in recent years due to its reliance on large and inefficient neural network implementations. Distillation is a network compression technique which attempts to impart knowledge from a large model to a smaller one. We use teacher-student distillation to improve the efficiency of the Biaffine dependency parser which obtains state-of-the-art performance with respect to accuracy and parsing speed . When distilling to 20\% of the original model's trainable parameters, we only observe an average decrease of $\sim$1 point for both UAS and LAS across a number of diverse Universal Dependency treebanks while being 2.30x  faster %increase in speed over the  than the baseline model %by 2.26   on CPU  %and 121\% on GPU   at inference time. We also observe a small increase in performance when compressing to 80\% for some treebanks. Finally, through distillation we attain a parser which is not only faster but also more accurate than the fastest modern parser on the Penn Treebank.",84
"  Upon hearing a spoken word, listeners selectively attend to an item that best matches the word's referent. For example, on seeing a display containing a hat and a bear, listeners selectively attend to the hat when they hear . Likewise, they selectively attend to a picture of a train upon hearing  when presented with a train and a fridge.     In more complex displays such as Figure, which contain both phonological and semantic foils to the referent of , listeners exhibit selective attention to both types of foil relative to the unrelated items. Furthermore, listeners selectively and briefly attend to the phonological foil  switching attention to the semantically related item. Figure  depicts early fixations to phonological foils by 30-month old toddlers within 400ms of word onset followed by a shift to semantic foils . Similar results are found with adults, though the initial phonological preference is conditioned by the picture preview time relative to word onset .  This pattern of findings is explained by assuming that the listener generates a phonological representation from the unfolding auditory signal and uses this representation to identify the best matching semantic and visual representation generated from the visual input provided by the images. The locus of the match could, in principle, occur at any of the representational levels linking the auditory and visual stimuli: phonological, semantic or visual. However, the early preference for the phonological foil suggests that the locus of the match resides at the phonological level\footnote{ also point out that removal of the picture preview phase in this task obliterates the early phonological preference, presumably because participants don't have time to generate the phonological codes for the images.}.  A recent computational model uses a hub-and-spoke architecture to capture the integration of phonological, semantic and visual information in driving attention in visual world tasks . The recurrent hub of the model  %  receives inputs from visual and phonological layers, and propagates activity to target semantic and eye layers which themselves feedback activity to the hub. Using an artificially constructed corpus, the model successfully replicates rhyme effects, e.g., hear  and look at boat .    argue that the close integration of visual, phonological and semantic information in the hub is central to the model's capacity to capture the phonological rhyme effect observed in visual world tasks. We would argue that a feature of the model also critical for obtaining a preference for rhyming over unrelated items is the persistence of all the discrete phonological segments at the input during processing. The rhyming segment of the word thereby comes to dominate the phonological input as the simulation of a visual world trial proceeds.   In this paper, we explore the hypothesis that incremental unfolding of the spoken word, one phonological segment at a time, is sufficient in itself to account for early phonological preferences of the type depicted in Figure, i.e., a transitory early preference for phonologically related items over  semantically and visually related items, as well as unrelated ones, followed by a preference for semantically and visually related items over  unrelated and phonologically related ones. We evaluate this hypothesis by constructing a neural network model that  processes  unfolding phonological representations of words at the input and learns to map these dynamic phonological sequences to corresponding static semantic and visual representations of the words' referents at the output. In essence, the model can be considered to implement a form of lexical comprehension. Particularly noteworthy aspects of the model include:      {} As a first step, we focus on phonological  effects with a view to extending the model eventually to encompass phonological  effects, \protect\`{a} la \protect. To anticipate the findings, our model successfully accommodates the early phonological over semantic/visual preference observed in visual world studies . However, we do  consider this model a complete account of language mediated attention in visual world settings, but rather a tool to explore the power of dynamic phonological representations in guiding our attention to semantic and visual items.     The research reported in this paper evaluates the proposal that incremental unfolding of a spoken word is in itself sufficient to account for the transient preference for phonological competitors over both unrelated and semantically/visually related ones in a visual world task. We evaluate this proposal with a neural network model designed to map  dynamic phonological inputs to static semantic-visual representations via gated recurrent units .   The 20 trained models each successfully learned the entire set of 200 vocabulary items. The trained models were tested in simulated `target-absent' visual world trials in which the model activations for the four competitor referents --- either unrelated to the referent of the unfolding word, or phonologically, semantically or visually related to it ---are continuously estimated. The activation is estimated by the distance between the current model output and the semantic-visual representations of all the candidate referents.  Figure depicts a clear early higher activation of the phonological competitor followed by a shift in favour of the semantic and visual competitors later in the trial. We interpret these activations as an early preference for the phonological competitor in a `target-absent' visual world trial, followed by a later preference for the semantic and visual competitors. These results confirm our proposal that a dynamic unfolding phonological input is sufficient to generate an initial preference for the phonological competitor over  semantic and visual competitors in a visual world task.  The models also have the desirable quality of exhibiting a rapid increase in vocabulary during the earlier stages of training, a phenomenon often reported in the child language literature as vocabulary spurt . The timing of the spurt is conditioned by the cohort size of vocabulary items. Although we are unaware of any studies specifically investigating the relation between  vocabulary growth and word cohort size, some studies of early lexical development report a deleterious effect of similar sounding words on vocabulary development and lexical processing .  We now turn to the issue of why our model exhibits an early phonological preference over a semantic-visual preference. Upon `hearing' the onset phone of a word, the model output migrates to the region of the semantic-visual space  consistent with the current phonological input. In a `target-absent' visual world trial this is bound to be towards the representation of the phonological competitor---if one is present---which is the only one consistent with the onset phone. Therefore, the phonological competitor has the highest activation. However, as the input word unfolds over time, the semantic-visual region consistent with the phonological input shifts. The model has been trained to associate  with corresponding semantic-visual representations: the more of the word the model `hears', the more its semantic-visual outputs shift towards the semantic-visual associates of the input word. Hence, the models favours phonological competitors before semantic-visual competitors in a `target-absent' visual world task. The model therefore predicts that in such a task where the scene also contains a phonological onset competitor, unambiguous identification of the target would be delayed relative to a scene that did not contain such a competitor. Evidence for such a delay has been reported in infant word recognition experiments. When 24-month-olds were presented with a display containing a phonological onset competitor , their target responses were delayed but not when the pictures閳 labels rhymed  .  It is worth noting that our model architecture does not permit feedback of activity from the semantic-visual representations to the phonological representations. In other words, there is no `implicit naming' of the stimuli in the visual world trial simulations reported: the model does not generate phonological representations from semantic-visual representations. A corollary of this feature is that the locus of the match between auditory and visual stimuli in a visual world task lies at the semantic-visual level, not at the phonological level. This built-in assumption of the model is at odds with the claim that reducing picture preview time in a visual world task can eliminate early phonological preferences . However, we note a growing body of empirical evidence that an extended picture preview time is not required to observe an early phonological preference effect in visual world tasks . These recent findings point to the possibility that other task demands that highlight semantic competitors may suppress phonological effects during referent identification.  Some forms of semantic feedback, such as that implemented in , may serve to eliminate  early phonological preferences in visual world tasks in certain circumstances, such as those reported by . In this case, identification of the neuro-computational mechanism responsible for controlling the presence/absence of the widely-reported phonological effects would be required. We speculate that growth in  connectivity from semantic representations, perhaps through the emergence and consolidation of the lexical-semantic system, may permit semantic-visual representations to modulate the  phonological processes as implemented in the current model.   We conclude that phonological representations mapped dynamically in a  fashion to semantic-visual representations are  to capture the early phonological preference effects reported in a visual world task. The semantic-visual preference observed later in such a trial does not require  feedback from a semantic or visual system.   We do not claim that such top-down connections do not exist. Indeed, we would expect a proper computational account of the visual world task to include such resources. Our strategy has been to seek to minimise the computational resources needed to account for the phenomenon at hand. We suppose that incremental development of these resources is the best way to achieve understanding of visual world processes.   \setlength{ \setlength{    
","  Visual world studies show that upon hearing a word in a target-absent visual context containing related and unrelated items, toddlers and adults briefly direct their gaze towards phonologically related items, before shifting towards semantically and visually related ones. We present a neural network model that processes dynamic unfolding phonological representations and maps them to static internal semantic and visual representations. The model, trained on representations derived from real corpora, simulates this early phonological over semantic/visual preference. Our results support the hypothesis that incremental unfolding of a spoken word is in itself sufficient to account for the transient preference for phonological competitors over both unrelated and semantically and visually related ones. Phonological representations mapped dynamically in a  fashion to semantic-visual representations capture the early phonological preference effects reported in a visual world task. The semantic-visual preference observed later in such a trial does not require  feedback from a semantic or visual system.  Keywords:  language; neuro-computational models; development; visual world task; phonology; semantics; cohort effects; machine learning; lexical competition; spoken word recognition; attention.",85
" In many natural language processing  tasks, the outputs are structures which can take the form of sequences, trees, or in general, labeled graphs. Predicting such output structures~ involves assigning values to multiple interdependent variables. Certain joint assignments may be prohibited by constraints designed by domain experts. As a simple example, in the problem of extracting entities and relations from text, a constraint could disallow the relation ``married to'' between two entities if one of the entity is not a ``person''. It has been shown that carefully designed constraints can substantially improve  model performance in various  applications~, especially when the number of training examples is limited.  Designing constraints often requires task-specific manual effort. In this paper, we ask the question:  We provide a general framework for discovering constraints in the form of a system of linear inequalities over the output variables in a problem. These constraints can improve an already trained model, or be integrated into the learning process for global training.  A system of linear inequalities represents a bounded or unbounded convex polytope. We observe that such a system can be expressed as a two-layer threshold network, i.e., a network with one hidden layer of linear threshold units and an output layer with a single threshold unit. This two-layer threshold network will predict  or  depending on whether the system of linear inequalities is satisfied or not. In principle, we could try to train such a threshold network to discover constraints. However, the zero-gradient nature of the threshold activation function prohibits using backpropagation for gradient-based learning.  Instead, in this paper, we show that a construction of a specific two-layer rectifier network represents linear inequality constraints. This network also contains a single linear threshold output unit, but in the hidden layer, it contains rectified linear units .  showed that a two-layer rectifier network constructed in such a way is equivalent to a threshold network, and represents the same set of linear inequalities as the threshold network with far fewer hidden units.   The linear constraints thus obtained can augment existing models in multiple ways. For example, if a problem is formulated as an integer program~, the learned constraints will become additional linear inequalities, which can be used directly. Alternatively, a structure can be constructed using graph search~, in which case the learned constraints can filter available actions during search-node expansions. Other inference techniques that extend Lagrangian Relaxation~ can also employ the learned constraints. Essentially, the learned constraints can be combined with various existing models and inference techniques and the framework proposed in this paper can be viewed as a general approach to improve structured prediction.  We report experiments on three NLP tasks to verify the proposed idea. The first one is an entity and relation extraction task, in which we aim to label the entity candidates and identify relations between them. In this task, we show that the learned constraints can be used while training the model to improve prediction. We also show that the learned constraints in this domain can be interpreted in a way that is comparable to manually designed constraints.  The second NLP task is to extract citation fields like authors, journals and date from a bibliography entry. We treat it as a sequence labeling problem and show that learned constraints can improve an existing first-order Markov model trained using a structured SVM method~. In the final experiment we consider chunking, i.e., shallow parsing, which is also a sequence labeling task. We train a BiLSTM-CRF model on the training set with different sizes, and we show that learned constraints are particularly helpful when the number of training examples is small.  In summary, the contributions of this paper are: [nosep] { https://github.com/utahnlp/learning-constraints}}   %%%%%%%%%%%%%%%%%    We presented a systematic way for discovering constraints as linear inequalities for structured prediction problems. The proposed approach is built upon a novel transformation from two layer rectifier networks to linear inequality constraints and does not rely on domain expertise for any specific problem. Instead, it only uses general constraint features as inputs to rectifier networks. Our approach is particularly suited to tasks where designing constraints manually is hard, and/or the number of training examples is small. The learned constraints can be used for structured prediction problems in two ways:  combining them with an existing model to improve prediction performance, or  incorporating them into the training process to train a better model. We demonstrated the effectiveness of our approach on three NLP tasks, each with different original models.  
","  Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.",86
"   Transfer learning using language models pre-trained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing  tasks. By contrast to earlier context-independent approaches such as word2vec  and GloVe , models such as ULMFiT , ELMo , GPT  and BERT  create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for longer text segments than words. Recent pre-trained language models has been rapidly advancing the state of the art in a range of natural language understanding tasks  as well as established NLP tasks such as named entity recognition and syntactic analysis .  The transformer architecture  and the BERT language model of  have been particularly influential, with transformer-based models in general and BERT in particular fuelling a broad range of advances in natural language processing tasks over the recent years. However, most recent work introducing new deep neural language models has focused on English, with models for other languages released later, if at all. For BERT, the original study introducing the model  addressed only English, and Google later released a Chinese model as well as a multilingual model, mBERT, , CamemBERT , FinBERT , and RuBERT , demonstrating substantial improvements over the multilingual model in various language-specific downstream task evaluations. However, these efforts have so far not added up to a broad-coverage collection of consistent-quality language-specific deep transfer learning models, and we are not aware of previous efforts to introduce readily executable pipelines for creating data for pre-training deep transfer learning models. Here, we take steps towards addressing these issues by introducing both a simple, fully automated pipeline for creating language-specific BERT models from Wikipedia data as well as 42 new such models.     This short manuscript has provided a first brief introduction to the WikiBERT models, a collection of dedicated language-specific BERT models covering many languages that previously lacked a dedicated deep transfer learning model of this type. We demonstrated the value of these models compared to the multilingual BERT model through evaluation on the Universal Dependencies multilingual dependency parsing data, showing that a WikiBERT model will provide better performance than multilingual BERT on average, and in multiple cases providing a more than 10\  relative decrease in LAS error compared to the multilingual model.  The availability of the WikiBERT collection of models opens up a broad range of potential avenues for research into the strengths, weaknesses and challenges in both mono- and multilingual language modeling. Due to scheduling constraints, this initial manuscript must necessarily leave most such questions for future work.    We gratefully acknowledge the support of the Academy of Finland, and CSC --- the Finnish IT Center for Science for providing computational resources for this effort.    include your own bib file like this:    
"," Deep neural language models such as BERT have enabled substantial recent advances in many natural language processing tasks. Due to the effort and computational cost involved in their pre-training, language-specific models are typically introduced only for a small number of high-resource languages such as English. While multilingual models covering large numbers of languages are available, recent work suggests monolingual training can produce better models, and our understanding of the tradeoffs between mono- and multilingual training is incomplete. In this paper, we introduce a simple, fully automated pipeline for creating language-specific BERT models from Wikipedia data and introduce 42 new such models, most for languages up to now lacking dedicated deep neural language models. We assess the merits of these models using the state-of-the-art UDify parser on Universal Dependencies data, contrasting performance with results using the multilingual BERT model. We find that UDify using WikiBERT models outperforms the parser using mBERT on average, with the language-specific models showing substantially improved performance for some languages, yet limited improvement or a decrease in performance for others. We also present preliminary results as first steps toward an understanding of the conditions under which language-specific models are most beneficial. All of the methods and models introduced in this work are available under open licenses from \url{https://github.com/turkunlp/wikibert}.",87
"  \PARstart{T}{he} global web with electronic information, including most notably the WWW, provides a huge resource of unbounded information to understand the world. Most text data from social media and the Internet is unstructured in the machine view and merely composed by some human-readable natural languages. But to fully exploit the information requires the ability to extract unstructured content automatically. To get interesting, representative and human-interpretable information from those text data, many data mining techniques have been created, and various kinds of algorithms implemented to develop the capability to extract structured information from data sources. Among these techniques, one of the most promising way to obtain hidden knowledge is Event Extraction, which was defined in Automatic Content Extraction  evaluation. An event is a specific occurrence involving participants, it is something that happens, it can frequently be described as a change of state.  [th] 	 	   Event extraction becomes a useful technique obtained more and more attention in the research community and industry applications for information retrieval and information extraction systems. It has been studied for a long time, both from the perspective of philosophy and the perspective of machine learning, but still remains a great challenging task. The purpose of event extraction is to determine the type of events and extract arguments with different roles from unstructured text automatically by machine. With the development of knowledge graph, good event type and argument roles extraction are expected to significantly help information retrieval and information extraction systems.  Considering that, natural language exhibits syntactic properties that would naturally combine words to phrases. Order-insensitive models are insufficient to fully capture the semantics of natural language due to their inability to account for differences in meaning as a result of differences in word order or syntactic structure. We, therefore, apply contextual sensitive sequential models to extract event triggers and arguments. In particular, bidirectional transformer models are a linguistically attractive option due to their relation to syntactic interpretations of sentence structure, which is promising in event extraction.  Due to the complexity of the event extraction task, researchers usually build complex neural networks to train the classification model end-to-end by force. The performances of these neural network models are good, but the numbers of their parameters are large. This results in prohibitively computational consumption.  In this paper, to improve the accuracy of assigning argument roles, we propose an elegant framework which has fewer parameters to model relationships among context in an order sensitive setting, as well as using the pre-trained representations with local features . Specifically, our work makes the following contributions:       In this paper, we propose a novel pipeline event extraction model based on a  dilate gated convolutional neural network, which utilizing word embedding generated by the pre-trained model, as well as elaborately constructed token features. We designed an event extraction model based on the bidirectional transformer for event argument roles assignment at the second stage. Numerical experiments conducted on real-world dataset demonstrate the efficacy of the proposed model EE-DGCNN, and further analysis of the relevant factors related to extraction performance is conducted.  Based on the comparative analysis results, the proposed model is enhanced with elaborately selected strategies, which enjoy the performance beyond all the state of the art baselines with a significant improvement in F score.                                             
"," Event Extraction plays an important role in information-extraction to understand the world. Event extraction could be split into two subtasks: one is event trigger extraction, the other is event arguments extraction. However, the F-Score of event arguments extraction is much lower than that of event trigger extraction, i.e. in the most recent work, event trigger extraction achieves 80.7\%, while event arguments extraction achieves only 58\%. In pipelined structures, the difficulty of event arguments extraction lies in its lack of classification feature, and the much higher computation consumption. In this work, we proposed a novel Event Extraction approach based on multi-layer Dilate Gated Convolutional Neural Network  which has fewer parameters. In addition, enhanced local information is incorporated into word features, to assign event arguments roles for triggers predicted by the first subtask. The numerical experiments demonstrated significant performance improvement beyond state-of-art event extraction approaches on real-world datasets. Further analysis of extraction procedure is presented, as well as experiments are conducted to analyze impact factors related to the performance improvement.",88
"  \renewcommand{\thefootnote}{}  A large body of evidence suggests that humans are expectation-based language processors, insofar as real-time language comprehension involves making predictions about upcoming material . One strong piece of evidence supporting this view comes from the domain of computational modeling, where next-word log probabilities from statistical language models  turn out to correlate well with online processing measures---that is, to have good ---including gaze duration in eye-tracking studies and self-paced reading times , and the N400 measure in EEG studies . Crucially, as statistical LMs improve on the broad-coverage objective function of perplexity , so too do they improve at predicting real-time processing data .  Many of the previous studies linking information-theoretic measures and human psychometric data  were conducted using -gram models, which track local word co-occurrences and are blind to information outside of the -gram window. Recently, however, neural network models such as Long Short-Term Memory Recurrent Neural Networks  and Transformers  have set new standards in natural language processing, achieving state-of-the-art perplexity results. We present a broad evaluation of these modern neural network models as predictors of human reading behavior, testing the influence of both model inductive bias and the scale of training data provided to the model. % This related work is in the wrong place. % How well these contemporary architectures fare as models of human sentence processing remains an open question:  find that Transformers outperform RNNs at predicting some processing data and  find no difference between various types of RNNs. But it remains an open question how these models compare to -gram models, how their performance is affected by training data size, and whether the linear relationship between perplexity and predictive power plateaus after models achieve a certain level of quality. % ?? we don't answer the last one either.  One important unanswered question involves the role of syntactic knowledge in the link between statistical models and real-time processing. Experimental evidence, such as studies of garden-path effects, demonstrates that humans deploy hierarchically structured representations to drive predictions about upcoming material . This suggests that language models with similar syntactic capacity --- represented implicitly or explicitly --- may be the best candidates for predicting human processing data. %enhanced with explicit hierarchical representations may perform better than sequentially structured models at predicting human processing data. However, results from computational modeling paint a complicated story: while  found that models without explicit hierarchical structure are best at predicting human reading times of naturalistic text, a follow-up study conducted by  argued that perplexity, not inductive bias or syntactic capacity, was the primary factor in determining a the ability of NLP models of that time to predict human reading times.  The more recent work of , , and  confirm the general finding that perplexity is the primary determinant of model fit to human comprehension measures, but also find differences among model architectures once perplexity is controlled for. %In recent work, targeted syntactic evaluations have shown modern neural network models to make more human-like syntactic generalizations than -gram models , with explicit hierarchical structure leading to better generalizations in some cases . This raises the question of how perplexity and inductive bias affect the ability of neural network models to predict human processing data.  Here we contribute to this emerging picture through a scaled-up and carefully controlled assessment of language models' ability to predict measures of human reading behavior. Following , we train a fleet of neural-network language models varying both in inductive bias  and in the amount of data provided to them at training time. % We train our language models on four datasets of varying sizes with four architectural variants: one -gram, a vanilla LSTM-RNN, a Recurrent Neural Network Grammar  and one Transformer. The Recurrent Neural Network Grammar , is given explicit syntactic supervision during training. We evaluate models' \predictivePower for human reading times on three online processing datasets: the Dundee eye-tracking corpus , selections from the Brown corpus and the Natural Stories self-paced reading time corpus . Across model architectures and training datasets, our results broadly confirm the strong linear relationship between surprisal  and reading time originally documented by  and confirmed by .  Like previous studies, we also find a generally positive relationship between a model's next-word prediction accuracy and its ability to predict human reading times, supporting the findings of  on a broad set of neural network models. Beyond the role of perplexity, we find that deep Transformer models demonstrate the best \predictivePower, and -gram models achieve greater \predictivePower than would be expected based on their perplexity.  We next address the issue of syntactic knowledge. Rather than positing a binary distinction between ``hierarchical'' and ``non-hierarchical'' models, we draw on recent work in language model evaluation to quantify models' syntactic knowledge at a finer grain . We compare each models' \predictivePower against this measure of syntactic knowledge. After controlling for a model's next-word prediction accuracy, we find that syntactic knowledge does not explain significant variance in a model's \predictivePower. %Addressing the issue of syntactic structure, we compare each models' predictive power against a syntactic generalization score, derived using the methodology and test suites presented in . After controlling for model perplexity, we find that syntactic generalization ability does not explain differences between models' predictive power.  %Taken together, these results indicate that, in addition to excelling at many natural language processing tasks, neural language models can serve as a powerful basis for human cognitive modeling. However, the disassociation between \predictivePower and syntactic generalization ability indicates that different modeling approaches may be needed to capture different aspects of human language use. \joncomment{TODO: rewrite paragraph for clarity.}      This paper tested the relationship between language model surprisal estimates and human reading behavior across a broad class of state-of-the-art language models, trained on varying amounts of language data. We confirmed the generally linear relationship between word-level surprisal and human reading time in each of our replications, and discovered that within model architecture, the relationship between a language model's next-word prediction performance and its \predictivePower is mostly monotonic. However, the influence of language model architecture was substantial. Furthermore, the influence of model architecture on \predictivePower is not the same as the influence of model architecture on performance on controlled grammatical tests: we found no clear relationship between the two types of evaluation metrics, once perplexity is controlled .    In this paper, we have extended previous research linking the outputs of predictive models to human reading time data, and demonstrated how a variety of model evaluation metrics, including test perplexity and syntax generalization score, can be used to gain insight about real-time language processing. We trained a fleet of 29 language models on corpora of varying sizes and tested their predictive power on three reading time datasets. By doing so, we were able to extend the results of  into the neural domain, demonstrating that decrease in perplexity corresponds to an increase in model-predictive power for state of-the-art models in the sub-100 perplexity range. We find that transformer models tend to perform best especially when enhanced with byte pair encoding. Surprisingly, although -gram models have much higher test perplexity than neural models, they achieve comparable  for two of the test corpora. This is in line with the findings present in , who find their LSTM model to underperform relative to their -gram models.\footnote{The LSTM model in Figure 1 is the only model that falls outside the regression's 95\  confidence interval.}    We test the relationship between a model's ability to learn human-like syntactic generalizations and its predictive power. Despite the variety in model architecture, training data size and even training corpus pre-processing, we find no significant relationship between a model's ability to perform well on targeted syntactic evaluation and its predictive power.  Our results complement and add to those of  and , who use similar methodology to assess the \predictivePower of Transformers and gated vs.\ simple RNNs. The relatively strong performance of our -gram model accords with 's  finding that simple RNNs, which are more sensitive to local relationships, perform as well as LSTMs and other gated models. Together these results demand a more thorough investigation into the relationship between locality and predictive power. One point of contrast is that  find no advantage for Transformer models at predicting human reading times in eye-tracking data, although they do find an advantage for self-paced reading. The difference may be due to the assessment metric, testing dataset size, byte-pair encoding or model size . Further investigation is required.  Interpreting our results in light of the findings presented in , who assess the relationship between perplexity and syntactic generalization abilities, our findings suggest a dissociation between two aspects of cognitive modeling using language models. On one hand, syntactic generalization abilities are largely determined by model architecture, with structurally supervised models and deep Transformers outperforming recurrent neural networks and -gram models. On the other hand, model ability to predict human reading times is determined more by model ability to accurately predict the next word across a range of contexts, not just in specialized syntactic testing.  For these tasks, model architecture seems to play less of an absolute role, although GPT-2 models trained on larger datasets and enhanced with BPE achieve the highest scores on all three testing corpora.   The findings presented in this paper suggest that different language comprehension contexts---isolated-sentence reading with controlled materials targeting specific grammatical contrasts, versus reading of more naturalistic materials---bring to the fore different types of human linguistic expectations that are in many cases best captured by different contemporary NLP models.  As new model architectures and training procedures continue to emerge, continued examination of the relationship with psychometric data can help guide the way towards increasingly human-like high-performance computational models of language.    \todo[inline]{What is RT--surprisal predictive power measuring?}   \todo[inline]{any conflicts between these results and Goodkind \& Bicknell?}    The authors would like to thank the anonymous reviewers for their feedback. J.G.~is supported by an Open Philanthropy AI Fellowship. J.H.~is supported by the NIH under award number T32NS105587 and an NSF Graduate Research Fellowship. R.P.L.~gratefully acknowledges support from the MIT-IBM Watson AI Lab, a Google Faculty Research Award, and a Newton Brain Science Award.    \setlength{ \setlength{     
"," % Another option: Humans read language by integrating novel words and structures with their expectations of what is likely to come next. SOMETHING SOMETHING ``expectation-based processing.''  %\joncomment{Human reading behavior is tuned to the statistics of natural language. It is well known, for example, that the time human subjects take to read a word can be predicted from estimates of the word's probability in context. % %that the probability of a word in context is predictive of the time human subjects take to read it. %\jenncomment{However, these studies have largely focused on a class of models  that only track local word co-occurrences.} %We test this relationship across a broad class of language models with different architectures and training data, linking their word-level probability distributions to multiple available measures of human reading behavior. Consistent with previous work, we find a logarithmic relationship across multiple orders of magnitude between the probability assigned to a word and the amount of time humans take to read it. %We next evaluate features of these models which make them better or worse at predicting human reading behavior. We find that the goodness of fit of this relationship  generally increases as language models improve on other measures of word-prediction accuracy. However, we find nontrivial differences in predictive power depending on model architecture. Across all testing corpora, deep Transformer models and $n$-gram models exhibit greater predictive power than would be expected based on their word-prediction accuracy. %Finally, we ask whether a model's predictive power is influenced by its syntactic knowledge, as measured by a battery of syntactic generalization tests. Surprisingly, we find no significant relationship between syntactic knowledge and predictive power. %These results indicate that measures of human reading behavior may conflate TODO %TODO concluding sentence}  Human reading behavior is tuned to the statistics of natural language: the time it takes human subjects to read a word can be predicted from estimates of the word's probability in context.  %However, studies that link real-time processing behavior to information theoretic measures have largely focused on a class of models  that track only local word co-occurrences.  However, it remains an open question what computational architecture best characterizes the expectations deployed in real time by humans that determine the behavioral signatures of reading.  Here we test over two dozen models, independently manipulating computational architecture and training dataset size, on how well their next-word expectations predict human reading time behavior on naturalistic text corpora.  Consistent with previous work, we find that across model architectures and training dataset sizes the relationship between word log-probability and reading time is linear.  We next evaluate how features of these models determine their psychometric predictive power, or ability to predict human reading behavior. In general, the better a model's next-word expectations , the better its psychometric predictive power. However, we find nontrivial differences in psychometric predictive power across model architectures.  For any given perplexity, deep Transformer models and $n$-gram models generally show superior psychometric predictive power over LSTM or structurally supervised neural models, especially for eye movement data.  Finally, we compare models' psychometric predictive power to the depth of their syntactic knowledge, as measured by a battery of syntactic generalization tests developed using methods from controlled psycholinguistic experiments. Once perplexity is controlled for, we find no significant relationship between syntactic knowledge and predictive power. These results suggest that, at least for the present state of natural language technology, different approaches may be required to best model human real-time language comprehension behavior in naturalistic reading versus behavior for controlled linguistic materials designed for targeted probing of syntactic knowledge.  Keywords:  Language modeling, real-time language comprehension, deep learning, eye-tracking, self-paced reading",89
" The past several years have witnessed the rapid development of neural machine translation  based on an encoder--decoder framework to translate natural languages . Since NMT benefits from a massive amount of training data and works in a cross-lingual setting, it becomes much hungrier for training time than other natural language processing  tasks.  Based on self-attention networks , Transformer  has become the most widely used architecture for NMT. Recent studies on improving Transformer, e.g. deep models equipped with up to 30-layer encoders, and scaling NMTs which use a huge batch size to train with 128 GPUs,  face a challenge to the efficiency of their training. Curriculum learning , which aims to train machine learning models  and ~, is gaining an intuitive appeal to both academic and industrial NMT systems.  The basic idea of CL is to train a model using examples ranging from ``easy'' to ``difficult'' in different learning stages, and thus the criterion of difficulty is vital to the selection of examples.  summarize two kinds of difficulty criteria in CL for NMT: 1) linguistically motivated sentence difficulty, e.g. sentence length, word frequency, and the number of coordinating conjunctions, which is easier to obtain~; 2) model-based sentence difficulty, e.g. sentence uncertainties derived from independent language models or the models trained in previous time steps or epochs, which tends to be intuitively effective but costly~.  In this paper, we propose a novel norm-based criterion for the difficulty of a sentence, which takes advantage of both model-based and linguistically motivated difficulty features. We observe that the norms of the word vectors trained on simple neural networks are expressive enough to model the two features, which are easy to obtain while possessing learning-dependent features.  For example, most of the frequent words and context-insensitive rare words will have vectors with small norms.  [t]  {|c|}{Vanilla} \\ {*}{}  & 16 & In catalogues, magazines  \\  & 27 & Nevertheless, it is an   \\ {*}{}  & 38 & The company ROBERT   \\   & 37 & Ottmar Hitzfeld played   \\ {|c|}{The Proposed Method} \\ {*}{}  & 3 & Second Part. \\    & 4 & It was not. \\ {*}{}   & 5 & Thank you very much. \\    & 4 & We know that. \\ \hline      Unlike existing CL methods for NMT, relying on a hand-crafted curriculum arrangement or a task-dependent hyperparameter, the proposed norm-based model competence enables the model to arrange the curriculum itself according to its ability, which is beneficial to practical NMT systems. We also introduce a novel paradigm to assign levels of difficulty to sentences, as sentence weights, into the objective function for better arrangements of the curricula, enhancing both existing CL systems and the proposed method.   Empirical results for the two widely-used benchmarks show that the proposed method provides a significant performance boost over strong baselines, while also significantly speeding up the training.  The proposed method requires slightly changing the data sampling pipeline and the objective function without modifying the overall architecture of NMT, thus no extra parameters are employed.    We have proposed a novel norm-based curriculum learning method for NMT by: 1) a novel sentence difficulty criterion, consisting of linguistically motivated features and learning-dependent features; 2) a novel model competence criterion enabling a fully automatic learning framework without the need for a task-dependent setting of a feature; and 3) a novel sentence weight, alleviating any bias in the objective function and further improving the representation learning. Empirical results on the medium- and large-scale benchmarks confirm the generalizability and usability of the proposed method, which provides a significant performance boost and training speedup for NMT.  
"," A neural machine translation  system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm  of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT.  Experimental results for the WMT'14 English--German and WMT'17 Chinese--English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score  and training speedup .",90
"  Spoken dialogue systems and voice assistants have been developed to facilitate natural conversation between machines and humans. They provide services through devices such as Amazon Echo Show and smartphones to help the user do tasks  and, more recently, for open domain chitchat , all through voice. %  Recent advances have been facilitated by the huge amounts of data collected through such devices and have resulted in the recent success of deep machine methods, providing significant improvements in performance. However, not all languages are able to benefit from these advances, particularly those that are under-resourced. These include sign languages and it means that those who sign are not able to leverage such interactive systems nor the benefits that automatic transcription and translating of signing would afford.   Here, we advance the state of the art with respect to transcribing British Sign Language . Our aim is for automated transcription of the BSL into English leveraging video recognition technologies. BSL enables communication of meaning through parameters such as hand shape, position, hand orientation, motion, and non-manual signals . BSL has no standard notation for writing the signs, as with letters and words in English. %, which been mapped to speech through automatic speech recognition.   Analogous to the International Phonetic Alphabet , highly detailed mapping of visual indicators to written form are available, such as HamNoSys . Despite the expressiveness of the HamNoSys writing system, its practical uses are limited and only a handful of experts know how to use it. Recent methods for automatic speech recognition  use deep neural models to  bypass the need for phoneme dictionaries , which are then combined with language models. %Previous work , have shown that we can use visual features to automatically predict individual signs.    %. Therefore, all the glosses are ID glosses  .  %We will use the term gloss throughout this paper to refer to the ID-glosses.    %, therefore, the BSL is usually translated into written English for writing purposes, which is called glossing .      %BSL, as with most of other sign languages, has the same parameters, some of which were identified by Stokoe~ and are hand shape, position, hand orientation, motion, and non-manual signals.  %This means that all of these parameters contribute to the semantics during signing. The      %Language modelling is a crucial part of any spoken dialogue system as it helps the models to cope with uncertainties in perception. We aim to have a signer dialogue system that will be capable of transforming continuous signing into text, which in turn may be used by the service providers.  This paper focuses on language modelling, a common technique in the field of ASR and Natural Language Processing to model the likelihood of certain words following each other in a sequence. We improve modelling of the BSL glosses by proposing to use transfer learning approaches, such as fine-tuning and layer substitution. % that may be used for training dialogue systems, designed for the deaf people.  The use of transfer learning technique can overcome the data sparsity issue in statistical modelling for scarce resource languages by using similar resources that can be found in large quantities and then further training the models on a specific low resource data.  We show that a model, pre-trained on the  Penn Treebank  dataset\footnote{https://catalog.ldc.upenn.edu/ldc99t42} and fine-tuned on the BSL monolingual corpus\footnote{http://www.bslcorpusproject.org/} can yield better results. This is in contrast to the same architecture that is trained directly on the BSL dataset without pre-training. %Our contribution is that we show that a stacked LSTM model, trained on the preprocessed Penn Treebank  dataset\footnote{https://catalog.ldc.upenn.edu/ldc99t42} that contains English language monolingual data and then fine-tuned on the pre-processed BSL monolingual corpus\footnote{http://www.bslcorpusproject.org/}, can achieve superior model quality than if training the same architecture directly on the BSL dataset.  This is a somewhat surprising result as there are marked differences between the two languages, particularly with the respect to the syntax . %Further investigations are necessary to model more complete BSL with all its grammatical constructs.  The paper begins with presenting methods for modelling languages and how they can be utilised in the BSL modelling. Section gives an overview of how transfer learning can be achieved as well as the use of transfer learning in sign languages. Section gives an overview of the datasets that are used in this paper, their statistics, and pre-processing steps to create two monolingual corpora for statistical model training. Section describes in detail the setup for the experiments in this paper. Section presents the results of the models employed for this research and discusses these results and the limitations of the approach taken in terms of the data used in Section. The paper is then concluded and future work is proposed.%\\ %our contribution        This paper shows how transfer learning techniques can be used to improve language modelling for the BSL language at the gloss level. Statistical modelling techniques are used to generate language models and to evaluate them using a perplexity measure.  The choice of the transfer learning technique is guided by the scarcity of available resources of the BSL language and the availability of the English language dataset that shares similar language modelling vocabulary with the annotated BSL. Feed-forward and recurrent neural models have been used to evaluate and compare generated language models. The results show that transfer learning can achieve superior quality of the generated language models. However, our pre-processed BSL corpus lacks constructs that are essential for a sign language, such as classifier signs and others. Nevertheless, transfer learning for modelling the BSL shows promising results and should be investigated further.    Although this paper discusses the use of a model initially trained on English and presents promising preliminary results, the annotation of the BSL, used in this paper, is limited as this paper serves as a proof of concept. In particular, the annotation used is missing some of the grammatical aspects of the BSL, such as classifier signs and others. Inclusion of these into the BSL language modelling would increase the OOV count as the English language does not have equivalent language constructs. This raises a question whether a sign language can be modelled using other languages that may have these constructs. More generally, is it possible to model a language with transfer learning using other less-related languages? Similar questions have been partly answered for the written languages in the field of machine translation  by bringing words of different languages close to each other in the latent space. However, nothing similar has been done for the sign languages.  Despite promising preliminary results on the use of the transfer learning for the BSL, future work should incorporate the complete set of the BSL constructs.  From the methodological side of the modelling, additional advanced state of the art techniques should be experimented with to achieve greater quality of the generated models, such as attention mechanism for the recurrent neural networks. Finally, this paper focuses on key techniques for sign processing, which could be part of a larger conversational system whereby signers could interact with computers and home devices through their natural communication medium of sign.  Research in such end-to-end systems would include vision processing, segmentation, classification, and language modelling as well as language understanding and dialogue modelling, all tuned to sign language.  , such as sign language machine interpretation.           The following footnote without marker is needed for the camera-ready   version of the paper.   Comment out the instructions  and uncomment the 8 lines   under ""final paper"" for your variant of English.         space normally used by the marker      Place licence statement here for the camera-ready version. See      Section of the instructions for preparing a      manuscript.               final paper: en-uk version                  space normally used by the marker       This work is licenced under a Creative Commons        Attribution 4.0 International Licence.       Licence details:                       final paper: en-us version                  space normally used by the marker       This work is licensed under a Creative Commons        Attribution 4.0 International License.       License details:         }    
","  %Spoken dialogue systems reside between the user and a machine to facilitate natural interaction for the user be it for acheiving tasks or chit chat.   Automatic speech recognition and spoken dialogue systems have made great advances through the use of deep machine learning methods.  This is partly due to greater computing power but also through the large amount of data available in common languages, such as English. Conversely, research in minority languages, including sign languages, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a model developed for one language is reused as the starting point for a model on a second language, which is less resourced. In this paper, we examine two transfer learning techniques of fine-tuning and layer substitution for language modelling of British Sign Language. Our results show improvement in perplexity when using transfer learning with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus.  %that leverage similarities to another language source in order to bootstrap learning.    %Despite the fact that the spoken dialogue systems work quite well with the spoken or written languages, these systems still lack reliable visual perception. Furthermore, deaf users have to rely only on this communication medium. In this paper we focus on sign language modelling as a part of this overall improved version of the spoken dialogue system. In particular, we propose to use transfer learning techniques, such as weight finetunning and layer substitution for transferring knowledge from the Penn Treebank dataset, which is predominantly in English to the British Sign Language corpus modelling, using glosses as a proxy for the visual sign language. Our results show double improvement in perplexity when using the transfer learning with standard stacked LSTM models.",91
"  Task-oriented dialogue systems aim to assist users to efficiently accomplish daily tasks such as booking a hotel or reserving dinner at a restaurant. Complex systems like Alexa and Siri often contain thousands of task domains. However, a successful model on one task often requires hundreds or thousands of carefully labelled domain-specific dialogue data, which consumes a large amount of human effort. Therefore, how to agilely adapt an existing dialogue system to new domains with a scant number of training samples is an essential task in task-oriented dialogues.  In this paper, we investigate dialogue policy, or dialogue management, which lies in the center of a task-oriented dialogue system. Dialogue policy determines the next-step action of the agent given dialogue states and the user's goals. As a dialogue is composed of multiple turns, the feedback to a dialogue policy's decision is often delayed until the end of the conversation. Therefore, Reinforcement Learning  is usually leveraged to improve the efficiency and success rate in dialogue policy learning .  There have been a number of methods applying dialogue policy in multi-domain settings .  These models usually employ an all-in-one multi-hot representation for dialogue states. The state embedding vector is a concatenation of multiple segments, each as a multi-hot vector for the states in one domain. However, when there are unseen domains at inference time,  % the corresponding slot embeddings are not optimized or do not exist in the first place.  the corresponding parameters of its dialogue acts and slots are not optimized.  This significantly limits the adaptation performance of policy models.  To alleviate this problem, we note that there is often shareable low-level information between different domains. For instance, suppose the source domain is taxi-booking and the target domain is hotel-booking. Although the two domains have different ontologies, both domains share certain dialogue slots  and dialogue acts . These shared concepts bear a lot of similarities both in textual representation and corresponding agent policies. Thus, it is feasible to transfer domain knowledge via these commonalities in ontologies.  To this end, we propose a Deep Transferable Q-Network , based on Deep Q-Network   in reinforcement learning, which learns to predict accurate Q-function values given dialogue states and system actions. In DTQN, we factorize the dialogue state space into a set of lower-level feature spaces. Specifically, we hierarchically model cross-domain relations at domain-level, act-level and slot-level. State representations are then composed of several shareable sub-embeddings. For instance, slots like start time in different domains will now share the same slot-level embedding. Furthermore, instead of treating actions as independent regression classes as in DQN, we decompose the dialogue action space and our model learns to represent actions based on common knowledge between domains.  To adapt DTQN to few-shot learning scenarios, we leverage the meta-learning framework. Meta-learning aims to guide the model to rapidly learn knowledge from new environments with only a few labelled samples .  %In multi-domain dialogues, it refers to the scenario where the model is trained on several source domains  yet tested in unseen target domains , and .  % Previously, meta-learning has been employed in state tracking and language generation modules in dialogues .  Previously, meta-learning has been successfully employed in the Natural Language Generation  module in dialogues .  % However, these modules are supervised learning by nature.  However, NLG is supervised learning by nature.  Comparatively, there has been little work on applying meta-learning to the dialogue policy, as it is known that applying RL under meta-learning, a.k.a. meta-RL, is a much harder problem than meta supervised learning . %The reason is that the reinforcement learning model needs to interact with the environment and obtain reward signals, either in the form of a human teacher or a user simulator. With only a few interactions with the novel test environment, it is difficult for the RL model to receive adequate feedbacks to adapt its policy.  To verify this fact, we train the DTQN model under the Model-Agnostic Meta-Learning  framework . However, we find through experiments that the canonical MAML fails to let the policy model converge because the task training phase leverages off-policy learning while the task evaluation and meta-adaptation phase employ an on-policy strategy.  % Under novel domains during adaptation,  Thus, the model initially receives very sparse reward signals, especially on complex composite-domain tasks. As a result, the dialogue agent is prone to overfit the on-policy data and to get stuck at the local minimum in the policy space.  Therefore, we further propose Meta-DTQN with a dual-replay mechanism. To support effective off-policy learning in meta dialogue policy optimization, we construct a task evaluation memory to cache dialogue trajectories and prefill it with rule-based experiences in task evaluation. % throughout meta-training and meta-adaptation.  This dual-replay strategy ensures the consistency of off-policy strategy in both meta-training and meta-adaptation, and provides richer dialogue trajectory records to enhance the quality of the learned policy model. Empirical results show that the dual-replay mechanism can effectively increase the success rate of DTQN while reducing the dialogue length, and Meta-DTQN with dual replay outperforms strong baselines on the multi-domain task-oriented dialogue dataset MultiWOZ 2.0 .%  % ......, outperforms DQN, DTQN Results show that ...   %       Dialogue policy is the central controller of a dialogue system and is usually optimized via reinforcement learning. However, it often suffers from insufficient training data, especially in multi-domain scenarios. In this paper, we propose the Deep Transferable Q-Network  to share multi-level information between domains such as slots and acts. We also modify the meta-learning framework MAML and introduce a dual-replay mechanism. Empirical results show that our method outperforms traditional deep reinforcement learning models without domain knowledge sharing, in terms of both success rate and length of dialogue. As future work, we plan to generalize our method to more meta-RL applications in multi-domain and few-shot learning scenarios.  
"," Dialog policy determines the next-step actions for agents and hence is central to a dialogue system. However, when migrated to novel domains with little data, a policy model can fail to adapt due to insufficient interactions with the new environment. We propose Deep Transferable Q-Network  to utilize shareable low-level signals between domains, such as dialogue acts and slots. We decompose the state and action representation space into feature subspaces corresponding to these low-level components to facilitate cross-domain knowledge transfer. Furthermore, we embed DTQN in a meta-learning framework and introduce Meta-DTQN with a dual-replay mechanism to enable effective off-policy training and adaptation. In experiments, our model outperforms baseline models in terms of both success rate and dialogue efficiency on the multi-domain dialogue dataset MultiWOZ 2.0. % by better addressing reward sparsity. % solve the inconsistency between off-policy training and on-policy adaptation.",92
" The neural machine translation   is currently the simplest and yet the state-of-the-art approach for training improved translation systems . They out-perform other statistical machine translation approaches if there exists a large amount of parallel data between the languages . Given the 閳ユ笧ight閳 amount of qualitative parallel data only, the models can learn the probability of mapping sentences in the source language to their equivalents in another language -- the target language . This 閳ユ笧ight閳 amount of qualitative parallel data is usually very large and, therefore, expensive to compile because it requires manual translation. The absence of large amounts of high-quality parallel data in many languages has led to various proposals for leveraging the abundant monolingual data that exists in either or both of the languages. These approaches include the self-training , forward translation , back-translation , dual learning  and transfer learning .  The back-translation has been used in current state-of-the-art neural machine translation systems , outperforming other approaches in high resource languages and improving performance in low resource conditions . The approach involves training a target-to-source  model on the available parallel data and using that model to generate synthetic translations of a large number of monolingual sentences in the target language. The available authentic parallel data is then mixed with the generated synthetic parallel data without differentiating between the two  to train a final source-to-target  model. The quality of the forward translation model depends on the NMT architecture used in building the models , the quality of the backward model , the suitability of the synthetic data generation method used  and the ratio of the authentic data to the synthetic data . In low resource NMT, the authentic parallel data available is not sufficient to train a backward model that will generate qualitative synthetic data. Thus, various methods have been proposed to improve the quality of the backward model despite the lack of sufficient parallel data.  Hoang et al.  and Zhang et al.  used an iterative approach to enable the forward model to generate synthetic data that will be used to improve the backward model. Imamura et al.  suggest generating multiple synthetic sources through sampling given a target sentence. Niu et al  trained a bilingual model for both the backward and forward translations and they reported improvement in low resource translations. Graca et al.  proposed that selecting the most suitable synthetic data generation method will help reduce the inadequacies of the backward model. Dabre et al.  and Kocmi and Bojar  proposed the use of a high-resource parent language pair through transfer learning to improve the backward model.  This work proposes the use of self-training -- also referred within the document as self-learning and forward translation --  approach to improve the backward model. The output of the backward model -- which is ideally used with the authentic data to train the forward model in back-translation -- is used to improve the backward model itself. The self-training approach used is similar to that in  where a synthetic target-side data is used to improve the performance of the translation model instead of the synthetic source-side data in back-translation. But instead of using the approach to enhance the final model, we aim to enhance the backward model which then generates improved synthetic data for enhancing the final model. We also simplify the approach by removing the need for synthetic data quality estimation  or freezing of training parameters .  The work is similar to the iterative back-translation of Hoang et al.  and Zhang et al. . The iterative back-translation requires the use of the monolingual source and target data to improve the backward and forward models respectively. The backward model generates synthetic sources to improve the forward model while the forward does the same for the backward model. This process is repeated iteratively until the required quality of translations are obtained. Instead, this work relies only on the monolingual target data to improve both models. Whereas the approaches above perform iterative back-translation to improve both models, our work uses forward translation  to improve the backward model and back-translation to improve the forward model.  It was shown by Specia and Shah  and Zhang and Zong  that using the monolingual source -- or the synthetic target -- data will potentially reduce the performance of the decoder. To mitigate this, Ueffing  and Specia and Shah  used quality estimation  to determine the best-translated sentences to be used to retraining, while Zhang and Zong  proposed freezing the parameters of the decoder when training the model on the synthetic data. In this work, we showed that the self-learning approach is capable of improving a translation model even without synthetic data cleaning or freezing any learned parameters. We hypothesize that the amount of parallel data used in retraining the model is sufficient to improve the quality of the model if the model can differentiate between and learn effectively from the synthetic and natural data.   We make the following contributions in this paper:       Neural machine translation systems suffer when trained on scanty data - low resource languages. Back-translation is an approach that was introduced to improve the performance of these and other category of languages. But various studies have shown that in low resource set-ups, the performance require other special approaches to reach an acceptable standard for translation quality. This work, therefore, proposes a new method of using the target-side monolingual data more effectively to improve the performance of the back-translation approach. Whereas the back-translation was used to specifically improve the forward model, we used the self-training approach through forward translation to improve, also, the performance of the backward model. The method performed very well on low resource English-German and English-Vietnamese languages and can be applied to any other low resource neural machine translation. The method can be investigated also in high resource languages.  We investigated various approaches such as the forward translation, tagged forward translation and various pre-training and fine-tuning strategies with the later two implemented to enable the model differentiate between synthetic and authentic parallel data during training. We observed that the proposed method out-performed the backward model in standard back-translation. It was claimed in  and  that the model's performance may be affected when using self-training because of the noise in the synthetic data. Instead, we found that providing a means for the model to differentiate between synthetic and authentic parallel data is just sufficient for the self-training method to perform as desired. Even though the self-training is by itself successful at improving the model, using tags or pre-training and fine-tuning have shown to improve the model's performance.     The work was evaluated on the low resource IWSLT 14 English-German translation. We also used the IWSLT 15 English-Vietnamese parallel data to confirm the positive results obtained using the approach. In Table 5, we showed a sample translation from English to German. Our improved model was able to produce exact translation to most  of the referenced translation: ''... wir 3 milliarden stunden pro woche mit online-spielen'' and the other part where the translation generated was different, the meaning was the same: ''derzeit verbringen'' 'vs' ''im moment geben''. The self-trained model was also able to generate exact translation to most of the referenced text but the model could only specify the adverb ''now'' instead of the referenced ''right now''. The improved model  generated ''at the moment'' which was a better equivalent to the next best translation system. Though the rest of the models could not perform better than the two discussed, the quality superiority of our approach can be seen on the models trained. For the forward model, the effects of the improved models were observed in their performances. In Table 6, we also translated a given German source text to English. The performances of the last two models , and especially the last model, were superior than the rest of the other models.  The pre-training and fine-tuning approach has shown to be the better approach when applying the method we proposed in this work. Unlike in , we investigated different approaches that will suit better for our approach. We found that pre-training first on the synthetic data and thereafter fine-tuning the model on the authentic data is the best strategy. Fine-tuning on the synthetic data was found to hurt the model. This can be attributed to the lack of quality in the synthetic data used for fine-tuning compared to the authentic data used during pre-training, supporting the same claim in the work of  that fine-tuning on the synthetic data does not improve performance, it only hurts it.    To the best of our knowledge, this is the first work that investigated an all-round utilization of the synthetic data to improve neural machine translation especially on low resource languages. These category of languages lag their high resource counterparts even if the same methods for improving their performance are applied. The back-translation has been shown to improve translation performance across board but in low resource languages, the performance is still less than desirable. We applied joint backward and forward translation to utilize the target-side monolingual data in improving the performance of neural machine translation systems in low resource languages. Experimental results obtained on English-German and English-Vietnamese have shown that the approach is superior to that of the widely successful back-translation approach. The approach is straightforward and can be applied on any low resource language translation to achieve a better and more acceptable translation performance. It could also be applied on high resource languages to improve the performance.  We showed that the approach is capable of improving the performance of the model even without using specialized data cleaning methods such as quality estimation. We also showed that the quality of the backward model is improved when the model can differentiate between the two data. This is also true for all models trained on synthetic and authentic data as shown in the training of the forward models. The work can be extended by comparing the performance of the proposed method with the other implementations of the self-learning approach when improving the backward model. Repeated retraining of the backward model -- iterative self-training -- can be explored in future works to determine the extent to which the backward model閳ユ獨 output can be used to improve itself. We also intend to investigate the efficacy of the approach on high resource languages.      --- Bibliography ---     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.            
"," Improving neural machine translation  models using the back-translations of the monolingual target data  is currently the state-of-the-art approach for training improved translation systems. The quality of the backward system -- which is trained on the available parallel data and used for the back-translation -- has been shown in many studies to affect the performance of the final NMT model. In low resource conditions, the available parallel data is usually not enough to train a backward model that can produce the qualitative synthetic data needed to train a standard translation model. This work proposes a self-training strategy where the output of the backward model is used to improve the model itself through the forward translation technique. The technique was shown to improve baseline low resource IWSLT闁14 English-German and IWSLT闁15 English-Vietnamese backward translation models by 11.06 and 1.5 BLEUs respectively. The synthetic data generated by the improved English-German backward model was used to train a forward model which out-performed another forward model trained using standard back-translation by 2.7 BLEU.",93
" During the COVID-19 pandemic, society was brought to a standstill, affecting many aspects of our daily lives. With increased travel due to globalization, it is intuitive that countries have followed earlier affected regions in outbreaks and measures to contain to them .   A unique form of information that can be used for modeling disease propagation comes from social media. This can provide researchers with access to unfiltered data with clues as to how the pandemic evolves. Current research on the COVID-19 outbreak concerning social media includes word frequency and sentiment analysis of tweets and studies on the spread of misinformation. Social media has also been utilized for other disease predictions. Several papers propose models to identify tweets in which the author or nearby person has the attributed disease .  and  utilize word frequencies to align tweets to disease rates. A shortcoming of the above models is they do not consider how one region's outbreak may relate to another. Many of the proposed models also rely on lengthy keyword lists or syntactic features that may not generalize across languages. Text embeddings from models such as multilingual BERT  and LASER  %\ww{LASER is more like multilingual word embeddings? I haven't checked the details, but is it LM?}  can allow us to combine features and make connections across languages for semantic alignment.   We present an analysis of Twitter usage for cross-lingual COVID-19 outbreak alignment. We study the ability to correlate social media tweets across languages and countries in a pandemic scenario. Based on this demonstration, researchers can study various cross-cultural reactions to the pandemic on social media. We aim to analyze how one country's tweets align with its own outbreak and if those same tweets can be used to predict the state of another country. This can allow us to determine how actions taken to contain the outbreak can transfer across countries with similar measures. We show that we can achieve strong results with cross-lingual transfer learning.    [t]      Our contributions include:            In this paper, we performed an analysis of cross-lingual transfer learning with Twitter data for COVID-19 outbreak alignment using cross-lingual sentence embeddings and keyword frequencies. We showed that even with our limited sample sizes, we can utilize knowledge of countries with earlier outbreaks to correlate with cases in other countries. With larger sample sizes and when training on a variety of points during the outbreak, we can obtain stronger correlations to other countries. We hope our analysis can lead to future integration of social media in epidemiological prediction across countries, enhancing outbreak detection systems.  
","  The spread of COVID-19 has become a significant and troubling aspect of society in 2020. With millions of cases reported across countries, new outbreaks have occurred and followed patterns of previously affected areas. Many disease detection models do not incorporate the wealth of social media data that can be utilized for modeling and predicting its spread. It is useful to ask, can we utilize this knowledge in one country to model the outbreak in another? To answer this, we propose the task of cross-lingual transfer learning for epidemiological alignment. Utilizing both macro and micro text features, we train on Italy's early COVID-19 outbreak through Twitter and transfer to several other countries. Our experiments show strong results with up to 0.85 Spearman correlation in cross-country predictions.",94
"    How to model language acquisition is among the central questions in linguistics and cognitive science in general. Acoustic speech signal is the main input for hearing infants acquiring language. By the time acquisition is complete, humans are able to decode and encode information from or to a continuous speech stream and construct a grammar that enables them to do so . In addition to syntactic, morphological, and semantic representations, the learner needs to learn phonetic representations and phonological grammar: to analyze and in turn produce speech as a continuous acoustic stream represented by mental units called .  Phonological grammar manipulates these discrete units and derives surface forms from stored lexical representations. The goal of linguistics and more specifically, phonology, is to explain how language-acquiring children construct a phonological grammar, how the grammar derives surface outputs from inputs, and what aspects of the grammar are language-specific in order to tease them apart from those aspects that can be explained by general cognitive processes or historical developments .   Computational models have been invoked for the purpose of modeling language acquisition and phonological grammar ever since the rise of computational methods and computationally informed linguistics . One of the major shortcomings of the majority of the existing proposals is that learning is modeled with an already assumed level of abstraction . In other words, most of the proposals model phonological learning as symbol manipulation of discrete units that already operates on the abstract, discrete phonological level. The models thus require strong assumptions that phonetic learning has already taken place, and that phonemes as discrete units have already been inferred from continuous speech data .     This paper proposes that language acquisition can be modeled with Generative Adversarial Networks  . More specifically, phonetic and phonological computation is modeled as the mapping from random space to generated data of a Generative Adversarial Network  trained on raw unannotated acoustic speech data in an unsupervised manner . To the author's knowledge, language acquisition has not been modeled within the GAN framework despite several advantages of this architecture. The characteristic feature of the GAN architecture is an interaction between the Generator network that outputs raw data and the Discriminator that distinguishes real data from Generator's outputs .  A major advantage of the GAN architecture is that learning is completely unsupervised, the networks include no langauge-specific elements, and that, as is argued in Section  below,  phonetic learning is modeled simultaneously  with phonological learning.   The discussion on the relationship between phonetics and phonology is highly complex . Several opposing proposals, however, argue that the two interact at various different stages and are not dissociated from each other . A network that models learning of phonetics from raw data and shows signs of phonological learning is likely one step closer to reality than models that operate with symbolic computation and assume phonetic learning has already taken place independently of phonology .   We argue that the latent variables in the input of the Generator network  can be modeled as approximates to phonetic or potentially phonological representations that the Generator learns to output into a speech signal by attempting to maximize the error rate of a Discriminator network that distinguishes between real data and generated outputs. The Discriminator network thus has a parallel in human speech:  the imitation principle .  The Discriminator's function is to enforce that the Generator's outputs resemble  the inputs as closely as possible. The GAN network thus incorporates both the pre-articulatory production elements  as well as the imitation principle  in speech acquisition. While other neural network architectures might be appropriate for modeling phonetic and phonological learning as well, the GAN architecture is unique in that it combines a network that produces innovative data  with a network that forces imitation in the Generator. Unlike, for example, autoencoder networks, the Generative Adversarial network lacks a direct connection between the input and output data and generates innovative data rather than data that resembles the input as closely as possible.    We train a Generative Adversarial Network architecture implemented for audio files in    on  raw speech data that contains information for an allophonic distribution: word-initial pre-vocalic aspiration of voiceless stops .  The data is curated in order to control for non-desired effects, which is why only sequences of the shape \#TV and \#sTV\footnote{T represents voiceless stops /p, t, k/, V represents vowels , and \# represents a word boundary.} are fed to the model. This allophonic distribution is  appropriate for testing learnability in a GAN architecture, because the dependency between the presence of [s] and duration of VOT is not strictly local. To be sure, the dependency is local in phonological terms, as [s] and T are two segments and immediate neighbors, but in phonetic terms, a period of closure intervenes between the aspiration and the period  of frication noise of [s]. It is not immediately clear whether a GAN model is capable of learning such non-local dependencies. To our knowledge, this is the first proposal that tests whether neural networks are able to learn an allophonic distribution based on raw acoustic data.      The hypothesis of the computational experiment presented in Section  is the following: if  VOT duration is conditioned on the presence of [s] in output data generated from noise by the Generator network, it means that the Generator network has successfully learned a phonetically non-local allophonic distribution.  Because the allophonic distribution is not strictly local and has to be learned and actively controlled by speakers , evidence for this type of learning is considered phonological learning in the broadest sense.  Conditioning the presence of a phonetic feature based on the presence or absence of a phoneme that is not automatic is, in most models, considered part of phonology and is derived with phonological computation. That the tested distribution is non-automatic and has to be actively controlled by the speakers is evident from L1 acquisition: failure to learn the distribution results in longer VOT durations in the sT condition documented in L1 acquisition .     The results suggest that phonetic and phonological learning can be modeled simultaneously, without supervision, directly from what language-acquiring infants are exposed to: raw acoustic data. A GAN model trained on an allophonic distribution is successful in learning to generate acoustic outputs that contain this allophonic distribution  . Additionally, the model  outputs innovative data for which no evidence was available in the training data, allowing a direct comparison between human speech data and the GAN's generated output. As argued in Section , some outputs are consistent with human linguistic behavior and suggest that the model recombines individual sounds, resembling phonemic representations and productivity in human language acquisition .    This paper also proposes a technique for establishing the Generator's internal representations.  The inability to uncover networks' representations  has been used as an argument against neural network approaches to linguistic data . We argue that the internal representation of a network can be, at least partially, uncovered. By regressing annotated dependencies between the Generator's latent space and output data, we identify values in the latent space that correspond to linguistically meaningful features in generated outputs. This paper demonstrates that manipulating the chosen values in the latent space has  phonetic effects in the generated outputs, such as the presence of [s] and the amplitude of its frication. In other words, the GAN learns to use random noise as an approximation of phonetic  representations.  This paper proposes that dependencies, learned during training in a latent space that is limited by some interval, extend beyond this interval. This crucial step allows for the discovery of several phonetic properties that the model learns.   By modeling phonetic and phonological learning with neural networks without any language-specific assumptions, the paper also addresses a broader question of how many language-specific elements are needed in models of grammar and language acquisition. Most of the existing models require at least some language-specific devices, such as rules in rule-based approaches or pre-determined constraints with features and feature matrices in connectionist approaches. The model proposed here lacks language-specific assumptions . Comparing the performance of substance-free models with competing proposals and human behavior should result in a better understanding of what aspects of phonological grammar and acquisition are domain-specific .  In the following, we first survey existing theories of phonological grammar and literature on computational approaches to  phonology .  In Section , we present the model in  based on  and provide acoustic and statistical analysis of  the training data. The network's outputs are first acoustically analyzed and described in Sections  and . In Section , we present a technique for establishing the network's internal representations and test it with two generative tests. In Sections  and , we analyze phonetic properties of the network's internal representations. Section  compares the outputs of the model with L1 acquisition, speech impairments, and speech errors.             The results of this paper suggest that we can model phonology not only with rules , exemplars , finite-state automata , input-output optimization , or with neural network architecture that already assumes some level of abstraction , but as a mapping between random latent variables and output data in deep neural networks that are trained in an unsupervised manner from raw acoustic data. To the author's knowledge, this is the first paper testing learning of allophonic distributions in an unsupervised manner from raw acoustic data using neural networks and the first proposal to use GANs for modeling language acquisition. The Generative Adversarial model of phonology  derives outputs that resemble speech from latent variables.  The results of the computational experiment suggest that the network learns the conditional allophonic distribution of VOT duration. We propose a technique that identifies variables in the latent space that correspond to phonetic and phonological properties in the output, such as the presence of [s], and show that by manipulating these values, we can generate data with or without [s] in the output as well as control its  intensity and spectral properties of its frication noise.  While at least seven latent variables control the presence of [s], each of them likely has a phonetic function that controls spectral properties of the frication noise. The proposed technique thus suggests that the Generator network learns to encode phonetic and phonological information in its latent space. Finally, the model generates innovative outputs, suggesting its productive nature. The behavior of the model is compared against speech acquisition, speech errors, and speech impairment; several parallels are identified.  The current proposal models one allophonic distribution in English.  Training GAN networks on further processes and on languages other than English as well as probing the networks at different training steps should yield more information about learning representations of different features, phonetic and phonological processes, and about computational models of the cognitive aspects of human speech production and perception in general. This paper outlines a methodology for establishing internal representations and testing predictions against generated data, but represents  just a first step in  a broader task of modeling phonetic and phonological learning  in a Generative Adversarial framework.  The proposed model also has implications beyond modeling the cognitive basis of human speech. The results of establishing internal representations of the Generator network have implications for more applicable tasks in natural language processing. Identifying latent variables that correspond to output sounds allows for a model that generates desired input strings with different output properties. Discussing the details of such models is beyond the scope of this paper.     
","  %%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details.  Training deep neural networks on well-understood dependencies in speech data can provide new insights into how they learn internal representations. This paper argues that acquisition of speech can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes a methodology to uncover the network's internal representations that correspond to phonetic and phonological properties.  The Generative Adversarial architecture is uniquely appropriate for modeling phonetic and phonological learning because the network is trained on unannotated raw acoustic data and learning is unsupervised without any language-specific assumptions or pre-assumed levels of abstraction. A Generative Adversarial Network was trained on an allophonic distribution in English, in which voiceless stops surface as aspirated word-initially before stressed vowels, except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network's generated speech signal contains the conditional distribution of aspiration duration. The paper proposes a technique for establishing the network's internal representations that identifies latent variables that  correspond to, for example, presence of [s] and its spectral properties. By manipulating these variables, we actively control the presence of [s] and its frication amplitude in the generated outputs. This suggests that the network learns to use latent variables as an approximation of phonetic and phonological representations. Crucially, we observe that the dependencies learned in training extend beyond the training interval, which allows for additional exploration of learning representations. The paper also discusses how the network's architecture and innovative outputs resemble and differ from linguistic behavior in language acquisition, speech disorders, and speech errors,  and how well-understood dependencies in speech data can help us interpret how neural networks learn their representations.   \tiny   %All article types: you may provide up to 8 keywords; at least 5 are mandatory.",95
" Internet users use social media to voice their views and opinions. Medical social media is a part of social media in which the focus is limited to health and related issues . User generated texts in medical social media include tweets, blog posts, reviews on drugs, health related question and answers in discussion forums.  This rich source of data can be utilized in many health related applications to enhance the quality of services provided . However, analysis of user generated texts is challenging due to noisy nature. User generated texts are written using informal language in descriptive words with irregular abbreviations, slang words and emoticons.   Medical concept normalization aims at discovering medical concepts in free-form text. In this task, health related mentions are mapped to standard concepts in a clinical knowledge base. Common public express health related conditions in an informal way using layman terms while clinical knowledge base contain concepts expressed in scientific language. This variation  in the languages of common public and knowledge bases makes concept normalization an essential step in understanding user generated texts. This task is much beyond simple string matching as the same concept can be expressed in a descriptive way using colloquial words or in  multiple ways using aliases, acronyms, partial names and morphological variants. Further, noisy nature of user generated texts and the short length of health related mentions makes the task of concept normalization more challenging.  Research in medical concept normalization started with string matching techniques  followed by machine learning techniques . The inability of these methods to consider semantics into account shifted research towards deep learning methods with embeddings as input . For example, Lee et al. and Tutubalina et al.  experimented with RNN on the top of domain specific embeddings. Further, lack of large labeled datasets and necessity to train deep learning models like CNN or RNN from scratch  shifted research towards using pretrained language models like BERT  and RoBERTa . Miftahutdinov and Tutubalina  experimented with BERT based fine-tuned models while Kalyan and Sangeetha  provided a comprehensive evaluation of BERT based general and domain specific models. Recently, Pattisapu et al.  work is based on RoBERTa  and SNOMED-CT graph embeddings. In this work, concept normalization is viewed as text matching problem and appropriate standard concept is chosen based on cosine similarity between RoBERTa encoded concept mention and target concept embeddings.   The main drawback in existing         Our model normalizes input concept mention by jointly learning the representations of input concept mention and target concepts. By learning the representations of target concepts along with input concept mention, our model a) exploits target concepts information unlike existing text classification approaches  and b) eliminate the need to separately generate target concept embeddings unlike existing text matching approach  which is time and resource consuming. Our model achieves the best results across three standard data sets surpassing all existing methods with an accuracy improvement of up to 2.31\%.        In this work, we deal with medical concept normalization in user generated texts. Our model overcomes the drawbacks in existing text classification and text matching approaches by jointly learning the representations of input concept mention and target concepts. By learning target concept representations along with input concept mention representations, our approach a) exploits valuable target concept information unlike existing text classification approaches and b) eliminates the need to separately generate target concept embeddings unlike existing text matching approach. Our model surpass all the existing methods across three standard datasets by improving accuracy up to 2.31\ . In future, we would like to explore other possible options to include target concept information which may further improve the results.    
"," Medical concept normalization helps in discovering standard concepts in free-form text i.e., maps health-related mentions to standard concepts in a vocabulary.  It is much beyond simple string matching and requires a deep semantic understanding of concept mentions. Recent research approach concept normalization as either text classification or text matching. The main drawback in existing a) text classification approaches is ignoring valuable target concepts information in learning input concept mention representation b) text matching approach is the need to separately generate target concept embeddings which is time and resource consuming. Our proposed model overcomes these drawbacks by jointly learning the representations of input concept mention and target concepts. First, it learns input concept mention representation using RoBERTa. Second, it finds cosine similarity between embeddings of input concept mention and all the target concepts. Here, embeddings of target concepts are randomly initialized and then updated during training. Finally, the target concept with maximum cosine similarity is assigned to the input concept mention. Our model surpass all the existing methods across three standard datasets by improving accuracy up to 2.31\%.",96
"   Machine Translation  systems are usually trained to output a single translation. However, many possible translations of a given input text can be acceptable.  This situation is common in online language learning applications such as Duolingo, Babbel, and Busuu.  In applications of this type, learning happens via translation-based activities while evaluation is performed by comparing learners' responses to a large set of human acceptable translations. Figure shows an example of a typical situation extracted from the Duolingo application.   %-----------  %--------- The main set up of the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education  is such that one starts with a set of English sentences  and then generates high-coverage sets of plausible translations in the five target languages: Portuguese, Hungarian, Japanese, Korean,  and Vietnamese. For instance, if we want to translate the English  sentence ``is my explanation clear?"" to Portuguese , all the translated Portuguese sentences illustrated in Table would be acceptable.\footnote{Examples taken from shared task description at: \url{https://sharedtask.duolingo.com/}.}    One challenge for training a sufficiently effective model we faced is the limited size of the source training data released by organizers . We circumvent this limitation by training a model on a large dataset acquired from the OPUS corpus , which gives us a powerful MT system that we build on . We then exploit the STAPLE-provided training data in multiple ways  to extend this primary model as a way to nuance the model to the shared task domain.    In essence, the shared task is a mixture of MT and paraphrase. This poses a second challenge: there is no paraphrase dataset to train the system on. For this reason, we resort to using outputs from the MT system in place of paraphrases. This required generating multiple sentences for each source sentence. To meet this need, we generate multiple translation hypotheses  using a wide beam search , perform `round-trip' translations exploiting these multiple outputs , and employ ensembles of checkpoints .  %This allows us to acquire several potentially useful Portuguese sentences from each English sentence .  %-------- %Third, we duplicate each source English sentence with each of the parallel target sentences to create a reasonably sizeable training dataset that we fine-tune the OPUS-trained model on.    A third challenge is that the target Portuguese sentences provided for training by organizers are produced by learners of English at various levels of fluency. This makes some of these Portuguese translations inarticulate . MT systems are not usually trained to produce inarticulate translations , and hence we needed to offer a solution that matches the different levels of language learners who produced the translations. Intuitively, we view MT systems trained at various stages  as learners with various levels of fluency. As such, we employ an ensemble of checkpoints to generate translations matching the different levels of learner fluency . Ultimately, our contributions lie in alleviating the 3 challenges listed above.  The remainder of the paper is organized as follows: Section is a brief overview of related work. In Section, we describe the data we use for both training and fine-tuning our models. Section presents the proposed MT system. Section describes our different methods. We discuss our results in Section, and conclude in Section.  \\     [h]               &  is my explanation clear? \\    & - minha explica鑾借尗o 鑼 clara?  \\           %   Translations     & a minha explica鑾借尗o est璋 clara? \\                     &  - a  minha explica鑾借尗o 鑼 clara?  \\                 &  - est璋 clara minha explica鑾借尗o?  \\               & - minha explana鑾借尗o est璋 clara? \\             & - 鑼 clara minha explica鑾借尗o? \\               &  you look so pretty! \\    & - voc閿 est璋 t鑼玱 bonita! \\           %   Translations     & a minha explica鑾借尗o est璋 clara? \\                     &  - voc閿 est璋 muito linda! \\                 &  - voc閿 est璋 muito bonita!  \\               & - voc閿 parece t鑼玱 linda!  \\             & -  voc閿 parece t鑼玱 bonita!\\              \hline                           In this work, we described our contribution to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education . Our system targeted the English-Portuguese sub-task. Our models effectively make use of an approach based on -Best prediction and multi-checkpoint translation. Our use of the OPUS dataset for training proved quite successful. In addition, based on our results, our intuitive deployment of a multi-checkpoint ensemble coupled with n-Best decoded translations seem to mirror leaner proficiency. As future work, we plan to explore other methods on new language pairs.\\  
"," We describe our submission to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education ~. We view MT models at various training stages  as human learners at different levels. Hence, we employ an ensemble of multi-checkpoints from the same model to generate translation sequences with various levels of fluency. From each checkpoint, for our best model, we sample n-Best sequences  with a beam width $=100$. We achieve $37.57~macro\ F_1$ with a $6$ checkpoint model ensemble on the official English to Portuguese  shared task test data, outperforming a baseline Amazon translation system of $21.30~ macro\ F_1$ and ultimately demonstrating the utility of our intuitive method.    %We focus on evaluating a range of models and parameters for the English-Portuguese sub-task including the   $n$-Best prediction, multi-model for the translation, paraphrasing and model fine-tuning.",97
" Over the past decade, numerous online community forums such as Stack Overflow and Quora have emerged, enabling users to ask domain-specific questions and obtain appropriate answers. Since many users share the same interests and often ask the same questions, these online forums face the challenge of identifying semantically similar or equivalent questions in order to avoid redundancies and improve their service quality and user experience . Successfully measuring the semantic similarity between pairs of questions is, however, a very difficult hurdle to overcome, since natural languages are ambiguous at all major levels of linguistics. Two questions could vary in length, punctuation, sentence structure, and choice of words and still have the same intention. For example, consider the following two questions:  How old are you?  What is your age? Both questions have the same intention and can be answered with the same answer, but they both have a different vocabulary and sentence structure. These aspects, although seemingly trivial, are not always easy to detect. Recent years have seen great progress in detecting semantically similar questions with deep learning. In particular, Recurrent Neural Networks  , which are inherently ideal for sequential data such as text, as well as attention mechanisms  have achieved state-of-the-art results.   In this work, we tackle the task of identifying semantically similar questions from the officially released Quora dataset\footnote{Quora Dataset: https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs}. We adopt the question similarity definition suggested by Bogdonova et al. :  ""Two questions are considered to be semantically equivalent if they can be answered by the same answer.""  Unlike more popular approaches, we propose a simple but effective method based solely on word embeddings and a Convolutional Neural Network  architecture introduced by Yoon Kim . Our experimental results are comparable to more complex state-of-the-art models and also shed light on the strengths of word embeddings as well as the importance of cleaning noisy text data.  The outline of the paper is as follows. Section 2 gives a brief overview of related work on the detection of semantically equivalent questions. All details of our proposed model as well as the necessary data pre-processing steps are presented in sections 3 and 4. Our model is evaluated in Section 5 and compared with other approaches. Furthermore, the limitations of our model and future research directions are discussed. Lastly, concluding remarks are made in section 6.    We presented a simple, effective CNN approach built on top of word embeddings to detect duplicated questions from the Quora dataset. Correcting spelling errors in the data set with the pre-trained Word2Vec model improved our model performance by 0.2\  on average. We employed four different embedding approaches, which all led to similar results. Using global max-pooling, min-pooling and max-pooling on chunks to extract important features from each convolutional layer proved to be very effective. We also experimented with the average-pooling operation, but it performed poorly for our task. Overall, our competitive results show the potential of CNNs for the detection of semantically equivalent questions, a task that is typically solved by using RNNs and / or attention mechanisms.  
"," Detecting semantic similarities between sentences is still a challenge today due to the ambiguity of natural languages. In this work, we propose a simple approach to identifying semantically similar questions by combining the strengths of word embeddings and Convolutional Neural Networks . In addition, we demonstrate how the cosine similarity metric can be used to effectively compare feature vectors. Our network is trained on the Quora dataset, which contains over 400k question pairs. We experiment with different embedding approaches such as Word2Vec, Fasttext, and Doc2Vec and investigate the effects these approaches have on model performance. Our model achieves competitive results on the Quora dataset and complements the well-established evidence that CNNs can be utilized for paraphrase detection tasks.",98
" Neural machine translation  has made great progress in the past decade.  In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining high-quality document translation. To get better contextual information, researchers have proposed many methods  for document-level translation . Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization  and sentiment classification .  However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT.  To address the above problem, we propose to improve document-level NMT with the aid of discourse structure information. First, we represent each input document with a Rhetorical Structure Theory-based discourse tree .  Then, we use a Transformer-based path encoder to embed the discourse structure path of each word and combine it with the corresponding word embedding before feeding it into the sentence encoder. In this way, discourse structure information can be fully exploited to enrich word representations and guide the context encoder to capture the relevant context of the current sentence. Finally, we adopt HAN  as our context encoder to model context information in a hierarchical manner.  Our contributions are as follows:   We propose a novel and efficient approach to explicitly exploit discourse structure information for document-level NMT.  Particularly, our approach is applicable for any other context encoder of document-level NMT;  We carry out experiments on English-to-German translation task and experimental results show that our model outperforms competitive baselines.         This paper has presented a novel discourse structure-based encoder for document-level NMT.  The main idea of our encoder lies in enriching the input word embeddings with their path embeddings based on discourse structure. Experimental results on English-to-German translation verify the effectiveness of our proposed encoder.  In the future, we plan to extend our encoder to other NLP tasks, such as simultaneous translation. Simultaneous translation, as well as document-level NMT, has difficulty in modeling the long context so that it may be effective to improve the re-translation with the structure information modeled by our encoder. Finally, we will focus on refining document-level NMT with variational neural networks, which has shown effecitive in previous studies of sentence-level NMT .  
"," Recently, document-level neural machine translation  has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network  . Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the word embedding before it is fed into the encoder.  Experimental results on the English-to-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN.",99
"   Neural Machine Translation   is a significant recent development in large scale translation . The traditional translation model introduced by Koehn et al. 2003  was trained on a single large neural model with layers that are trained separately requiring many resources and effort. Today, most industry players have adopted a neural network based machine translation system derived from the Recurrent Neural Network  encoder-decoder model introduced by Cho et al. 2014 . For machine translation, the encoder is used with the source language to encode the sentence input into a vector representation for the decoder. The decoder uses the encoded sequence to begin predicting the target sequence. There were several advancements to this model by the introduction of different types of RNNs such as LSTM  , GRU  , and Bi-RNN   which was introduced to address the vanishing gradient problem  that was encountered during the training of the simple recurrent neural network.    Gated recurrent networks failed to fully resolve the problem of the encoder-decoder network  which is the ability to learn and maintain information of the encoder for longer sentences. This is where the attention mechanism was introduced by Graves et al. 2014 that is based on the cosine similarity of the sentences , Bahdanau et al. 2014 which concatenates the encoder and decoder information , and Loung et al. 2015 that uses the dot product of the the encoder and decoder information to score the attention on the target sequence . The introduction of attention mechanisms increased the scalability of machine translation at the cost of performance during training.  The latest development in the machine translation space is the introduction of the Transformer model by Vaswani et al. 2017 . The Transformer model focuses more on self-attention and fully leverages recurrent networks. It promotes self-attention in both the encoder and the decoder where the encoding of the source sequences are done in parallel. This reduces the training time significantly. The decoder prediction is auto-regressive which means it predicts each word at a time in a regressive state. Vaswani claims that the results of the transformer model has a significant improvement in prediction accuracy when compared to other recent models in the NMT space with the use of a German translation task .  The transformer model is still in the incubation and adoption stage in current industry practice. This is due to its restricted context length during translation . Furthermore, at present all RNN encoder-decoder based machine translation models still use a single model architecture for a translation job. For example, if a task requires translation from Spanish to English, one model will be trained. Another model would be trained to translate from English to Spanish. One model corresponds to one translation task, hence separate models are required. In this research, we seek to build a singular model to translate multiple languages. For the purpose of this research we have considered English-Spanish and Spanish-English translation using the same model.  All machine translation mechanisms to date use language specific encoders for each source language . This paper will detail a novel method of hosting multiple neural machine translation tasks within the same model as follows. Section will cover related works on the fundamental concepts of the sequence to sequence Recurrent Neural Network based Encoder-Decoder model, the additive attention model by Bahdanau, and wrap-up with the Dual Learning method introduced by Microsoft. Section outlines the architecture for the universal vector model and discusses each layer. Section discusses the training method for the universal model, while Section explains the dataset and how it is used for training. Section is an overview of the BLEU score. The translation results of the Universal Vector is explained in Section then Section presents the analysis of the BLEU score, loss results, and attention model performance. Section goes over limitations and potential steps to take in the future with Section discussing previously considered experiments. Finally, the paper is concluded with some parting thoughts on the development of this novel model in Section.      In this paper, the idea of a ""Universal Vector"" is proposed as a new facet of NMT that can be used to translate between multiple languages in the same vector space. Models are usually built to translate in one direction. There exists some work that has been done in using both directions between a source and target language for reinforcement learning of training sets. However, the ""Universal Vector"" model is a singular model that can be trained in both directions  for more than one pair of languages.   The ""Universal Vector"" model detailed in this paper was built to test the proposition by modifying an RNN based Encoder-Decoder model. Existing attention mechanisms were also modified and used to create context vectors that increased performance in predicting the next translated text for overall target phrase translation. Multiple fully connected layers are added, one for each target language, to facilitate translations into multiple target languages.   The model is trained with parallel English and Spanish datasets. Phrases from both languages are trained from English to Spanish and Spanish to English within a recurrent network using Dual Training based methods. It was tested with many examples of both Spanish and English phrases. The attention mechanism was evaluated by viewing heat maps of where the model selectively focused on input text for its corresponding translated text.  While the results are promising, with more time and resources the experiment would provide better results. With more computing power the model can be trained using more words with more languages in a reasonable amount of time. In the future, better accepted benchmarks in translation such as those provided by the annual Workshop on Machine Translation can be used. While limited in scope, these results point to potential for greater accuracy on using a singular model for translating between multiple languages.   
","     Neural Machine Translation  leverages one or more trained neural networks for the translation of phrases. Sutskever introduced a sequence to sequence based encoder-decoder model which became the standard for NMT based systems. Attention mechanisms were later introduced to address the issues with the translation of long sentences and improving overall accuracy. In this paper, we propose a singular model for Neural Machine Translation based on encoder-decoder models. Most translation models are trained as one model for one translation. We introduce a neutral/universal model representation that can be used to predict more than one language depending on the source and a provided target. Secondly, we introduce an attention model by adding an overall learning vector to the multiplicative model. With these two changes, by using the novel universal model the number of models needed for multiple language translation applications are reduced.  % Keywords may be used, but they are not required. %",100
" Hausa is a language spoken in the western part of Africa. It belongs to the Afro--Asiatic phylum and it is the second most spoken native language on the continent, after Swahili. The language is spoken by more than 40 million people as a first language and about 15 million people use it as a second and third language. Most of the speakers are concentrated in Nigeria, Niger and Chad -- all resulting to both anglophone and francophone influences . Our work on curating datasets and creating evaluation benchmark for English--Hausa Neural Machine Translation  is inspired by the socio--linguistic facts of the Hausa language. Hausa has been referred to as the largest internal political unit in Africa. There has been extensive linguistic academic research on Hausa and the language benefits from the existence of trans--border communication in the West African Sahel belt and the availability of international radio stations like the BBC Hausa and Voice of America Hausa .  The exponential growth of social media platforms have eased communication among users. However, the advances in technological adoption have also informed the need to translate human languages. In low--resource countries, the language inequality can be ameliorated by using machine translation to bridge gaps in technological, political and socio--economic advancements. The recent successes in NMT over Phrased--Based Statistical Machine Translation  for high--resource data conditions can be leveraged to explore best practices, data curation and evaluation benchmark for low--resource NMT tasks. Using the JW300, Tanzil, Tatoeba and Wikimedia public datasets, we trained and evaluated baseline NMT models for Hausa language.       Evaluating the model on the test set, we observed that the word--level tokenization outperform the BPE by a BLEU score factor of \textasciitilde1.27--1.42 times . The qualities of the English to Hausa translations using both word--level and BPE subword tokenizations were rated positively by first language speakers. Table  shows some of the translations example.   \\  & Why is that of vital importance? \\  Reference: & Me ya sa hakan yake da muhimmanci? \\  Hypothesis: & Me ya sa wannan yake da muhimmanci? \\    
"," Neural Machine Translation  for low-resource languages suffers from low performance because of the lack of large amounts of parallel data and language diversity. To contribute to ameliorating this problem, we built a baseline model for English--Hausa machine translation, which is considered a task for low--resource language. The Hausa language is the second largest Afro--Asiatic language in the world after Arabic and it is the third largest language for trading across a larger swath of West Africa countries, after English and French. In this paper, we curated different datasets containing Hausa--English parallel corpus for our translation. We trained baseline models and evaluated the performance of our models using the Recurrent and Transformer encoder--decoder architecture with two tokenization approaches: standard word--level tokenization and Byte Pair Encoding  subword tokenization.",101
"  Although neural machine translation  has achieved state-of-the-art performance in recent years~, most NMT models still suffer from the slow decoding speed problem due to their autoregressive property: the generation of a target token depends on all the previously generated target tokens, making the decoding process intrinsically nonparallelizable.  Recently, non-autoregressive neural machine translation  models~ have been investigated to mitigate the slow decoding speed problem by generating all target tokens independently in parallel, speeding up the decoding process significantly. Unfortunately, these models suffer from the multi-modality problem~, resulting in inferior translation quality compared with autoregressive NMT. To be specific, a source sentence may have multiple feasible translations, and each target token may be generated with respect to different feasible translations since NAT models discard the dependency among target tokens. This generally manifests as repetitive or missing tokens in the translations. Table shows an example. The German phrase ``{}'' or ``\rl{{}'' are translated w.r.t. ``'' are translated w.r.t. ``\rl{{'' are generated. Similarly, ``{[!t]   }     \toprule     { mit diesem Ansatz\\     \midrule     { doing this today\\     { doing this today\\     %\midrule \midrule     { \rl{of farmers} doing this today\\     %\midrule     {  doing this today\\     }                  '' in the first segment is predicted based on all the tokens colored green). Repetitive segments  To improve decoding speed, we assume that a translation can be divided into several segments which can be generated simultaneously.  To better capture target-side dependency, the tokens inside a segment is autoregressively generated conditioned not only on the previously generated tokens in this segment but also on those in other segments. On one hand, we observe that repetitive tokens are more likely to occur within a short context. Therefore, autoregressively generating a segment is beneficial for reducing repetitive tokens. On the other hand, by conditioning on previously generated tokens in other segments, the model is capable of guessing what feasible translation candidates have been chosen by each segment and adapts accordingly, e.g., recovering from missing token errors. As a result, our model captures more target-side dependency such that the multi-modality problem can be alleviated naturally.  To make the model capable of recovering from repetitive token errors, we introduce a segment deletion mechanism into our model. Informally speaking, our model will mark a segment to be deleted once it finds the content has been translated in other segments.  We conduct experiments on three benchmark datasets for machine translation to evaluate the proposed method. The experimental results show that \SemiNAT is able to decode over 4 faster than the autoregressive counterpart while maintaining comparable performance. The source code of this work is released on \url{https://github.com/ranqiu92/\MODELNAME}.    In this work, we propose a novel semi-autoregressive model \SemiNAT to alleviate the multi-modality problem, which performs translation by generating segments non-autoregressively and predicts the tokens in a segment autoregressively. By determining segment length dynamically, \SemiNAT is capable of recovering from missing token errors and reducing repetitive token errors. By explicitly detecting and deleting repetitive segments, \SemiNAT is able to recover from repetitive token errors. Experiments on three widely-used benchmark datasets show that our \SemiNAT model maintains comparable performance with more than  decoding speedup compared with the AT model.  
"," Non-autoregressive neural machine translation  predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model \SemiNAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, \SemiNAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4$\times$ speedup while maintaining comparable performance compared with the corresponding autoregressive model.",102
"   %what is medical NER? what are the new trends?  Named entity recognition  in medical texts involves the automated recognition and classification of relevant medical/clinical entities, and has numerous applications including information extraction from clinical narratives , identifying potential drug interactions and adverse affects , and de-identification of personal health data .  In recent years, medical NER systems have improved over previous baseline performance by incorporating developments such as deep learning models , contextual word embeddings , and domain-specific word embeddings . Typically, research groups report their results using common evaluation metrics  on standardized data sets. While this facilitates exact comparison, it is difficult to know whether modest gains in F-score are associated with significant qualitative differences in the system performance, and how the benefits and drawbacks of different embedding types are reflected in the output of the NER system.       %Medical NER systems are becoming really powerful and with common evaluation metrics, sometimes it is very hard to measure the improvements. For example, for i2b2 dataset when non-contextual embeddings are replaced with contextual ones we see a significant improvement but when using domain specific contextual embeddings, making modifications in the structure of the model does not seem to improve the results. We believe as the systems get better and better they become more and more similar and they only differ in tiny details. So, better metrics are needed to measure the slight differences to make sure that the changes in models are properly reflected in the evaluation metrics.    This work aims to investigate the types of errors and their proportion in the output of modern deep learning models for medical NER. We suggest that an evaluation metric should be a close reflection of what users experience when using the model. We investigate different types of errors that are penalized by exact F-score and identify a specific error type where there is high degrees of disagreement between the human user experience and what exact F-score measures: namely, errors where the extracted entity is correctly labeled, but the span only overlaps with the annotated entity rather than matching perfectly. We obtain expert human judgement for 5296 such errors, ranking the severity of the error in terms of end user experience. We then compare the commonly used F-score metrics with human perception, and investigate if there is a way to automatically analyze such errors as part of the system evaluation. The code that calculates the number of different types of errors given the predictions of an NER model and the corresponding annotations is available upon request and will be released at \url{https://github.com/nrc-cnrc/NRC-MedNER-Eval} after publication. We will also release the collected expert judgements so that other researchers can use it as a benchmark for further investigation about this type of errors.      Medical NER systems that are based on most recent deep learning structures generate a high amount of outputs that match with the hand-labelled entities in terms of tag but only overlap in the span. While the exact f-score penalizes all of these predictions and relaxed f-score credits all of them, a human user accepts a significant proportion of them as valid entities and rejects the rest. \\ A reformatted version of the NER training dataset can be used to train an entity classifier for evaluation of extracted entities with right label and overlapping span. We showed that there is a high degree of agreement between human expert and this entity classifier in accepting or rejecting span mismatches. This classifier is used to calculate a learning-based evaluation metric that outperforms relaxed F-score in approximating the experience of a forgiving user.     Limitations    future work:             
"," When comparing entities extracted by a medical entity recognition system with gold standard annotations over a test set, two types of mismatches might occur, label mismatch or span mismatch. Here we focus on span mismatch and show that its severity can vary from a serious error to a fully acceptable entity extraction due to the subjectivity of span annotations. For a domain-specific BERT-based NER system, we showed that 25\% of the errors have the same labels and overlapping span with gold standard entities. We collected expert judgement which shows more than 90\% of these mismatches are accepted or partially accepted by the user. Using the training set of the NER system, we built a fast and lightweight entity classifier to approximate the user experience of such mismatches through accepting or rejecting them. The decisions made by this classifier are used to calculate a learning-based F-score which is shown to be a better approximation of a forgiving user's experience than the relaxed F-score. We demonstrated the results of applying the proposed evaluation metric for a variety of deep learning medical entity recognition models trained with two datasets.",103
" Slot tagging ,  a key module in the task-oriented  dialogue system ,  is usually formulated as a sequence  labeling problem .  Slot tagging faces the rapid changing  of domains,  and the labeled data is usually scarce for new domains with only a few samples.  Few-shot learning technique  is appealing in this scenario  since it learns the model  that borrows the prior experience  from old domains and adapts to new domains quickly with only very few examples . 	   Our contributions are summarized as follows:  We propose a few-shot CRF framework for slot tagging that computes emission score as word-label similarity and estimate transition score by transferring previously learned label dependencies.  % integrates word similarities and label dependencies for with a CRF-based framework.   We introduce the collapsed dependency transfer mechanism to transfer label dependencies across domains with different label sets.   We propose the L-TapNet to leverage semantics of label names to enhance label representations, which help to model the word-label similarity.      In this paper,  we propose a few-shot CRF model for slot tagging of task-oriented dialogue.    that integrates the prior knowledge of word-label similarities and label dependencies.    Our framework is able to conduct sequence labeling with only a few labeled samples.  To compute transition score under few-shot setting,  we propose the collapsed dependency transfer mechanism,  which transfers the prior knowledge of the label dependencies across domains with different label sets. And we propose L-TapNet to calculate emission score, which improves label representation with label name semantics.    during computing word-label similarity.  Experiment results validate that both the collapsed dependency transfer and L-TapNet can improve the tagging accuracy.    Analysis results prove the superiority of collapsed dependency transfer over transition rules.     ====================== AAAI version =====================   In this paper,    we propose a CRF-based framework for few-shot linguistic sequence labeling,     that integrates the prior knowledge of word similarities and label dependencies.      Our framework is able to conduct sequence labeling with only a few labeled samples.    Within the framework, we propose the collapsed dependency transfer mechanism,    which transfers the prior knowledge of the label dependencies across domains with different label set.   And we introduce the pair-wise embedding to few-shot sequence labeling for better word representation and more effective similarities modeling.    Experiments on slot tagging and named entity recognition tasks validate that both the collapsed dependency transfer mechanism and pair-wise embedding improve the labeling accuracy.    Analysis results prove the superiority of collapsed dependency transfer over transition rules.   
"," In this paper, we explore the slot tagging with only a few labeled support sentences . Few-shot slot tagging faces a  unique challenge compared to the other few-shot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain,  due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field  to transfer abstract label dependency patterns as transition scores. In the few-shot setting,  the emission score of CRF can be calculated as a word's similarity to the representation of each label.  To calculate such similarity,  we propose a Label-enhanced Task-Adaptive Projection Network  based on  the state-of-the-art few-shot classification model -- TapNet,  by leveraging label name semantics in representing labels.  Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by  14.64 F1 scores in the one-shot setting.\footnote{Code is available at: \url{https://github.com/AtmaHou/FewShotTagging}}",104
"  Machine reading comprehension has been a popular topic in the past years, and a variety of models have been proposed to address this problem, such as BiDAF, Reinforced mnemonic reader, and ReasoNet. However, most existing works focus on finding evidence and answer in a single document.  In fact, in reality, many questions can only be answered after reasoning across multiple documents. Table shows a multi-choice style reading comprehension example, which is from WikiHop dataset. In the example, we can only answer the question `what is the place of death of alexander john ellis?' after extracting and integrating the facts `Alexander John Ellis is buried in Kensal Green Cemetery' and `Kensal Green Cemetery is located in Kensington' from multiple documents, which is a more challenging task. [htb] 	} 		\toprule 		% after \\: : place of death, \textcolor{magenta}{alexander john ellis}, ? \\ 		\midrule 		Support doc1: \textcolor{magenta}{Alexander John Ellis}, was an English mathematician ... is buried in \textcolor{orange}{Kensal Green Cemetery}. \\ 		Support doc2: The areas of College Park and \textcolor{orange}{Kensal Green Cemetery} are located in the London boroughs of Hammersmith \& Fulham and \textcolor{violet}{Kensington} \& Chelsea, respectively. \\ 		......\\ 		\midrule 		Candidates: college park, france, \textcolor{violet}{Kensington}, London \\ 		\midrule 		Answer: \textcolor{violet}{Kensington} \\ 		 	   The main challenge is that the evidence is distributed in different documents and there is a lot of noise in the documents. We need to extract this evidence from multiple documents, but it is difficult to capture their dependencies for reasoning. Many works used graph convolution networks to deal with this problem, such as Entity-GCN, BAG and HDE. They transform documents into an entity graph, and then import the entity graph into graph convolution networks to simulate the process of multi-hop reasoning.  However, these GCN-based approaches have some disadvantages. Firstly, they generated the entities only from the question and candidate answers, lacking much key information for multi-hop reasoning. For example, as the example in Table, the entity `Kensal Green Cemetery' is an important clue to answer the question, but the above approaches ignored this information. Secondly, the traditional GCNs only update the central node based on the aggregated information of adjacent nodes and use this to simulate the process of reasoning. But the question information is not fully utilized and there is a lot of irrelevant information during information propagating across documents in the multi-hop reasoning.  In this paper, we propose a novel approach to solve the above problem. We introduce a path-based reasoning graph for multiple documents. Compared to traditional graphs, the path-based reasoning graph contains multiple reasoning paths from questions to candidate answers, combining both the idea of the GCN-based and path-based approaches. Thus, we construct a path-based reasoning graph by extracting reasoning paths from supporting documents and then adding reasoning nodes  in these paths to the entity graph. And then, we apply a Gated-RGCN to learn the representation of nodes. Compared to GCNs, Gated-RGCN utilizes attention and question-aware gating mechanism to regulate the usefulness of information propagating across documents and add question information during reasoning, which is closer to human reasoning processes.  Our contributions can be summarized as follows:       dataset, and our approach achieves new state-of-the-art accuracy. Especially, our ensemble model surpasses the human performance by .      In this paper, we propose a novel approach for multi-hop reading comprehension across documents. Our approach extends the entity graph by introducing reasoning entities, which can form the reasoning path from question to candidates. In addition, our approach incorporates the question in the multi-hop reasoning through a new gate mechanism to regulate how much useful information propagating from neighbors to the node. Experiments show that our approach achieves state-of-the-art accuracy both for single and ensemble models.  Our future work would focus on the interpretability of the multi-hop reading comprehension across documents. In addition, we would like to build the entity graph with reasoning entities dynamically during the reasoning, and apply our model on other datasets.  
","    Multi-hop reading comprehension across multiple documents attracts much attention recently. In this paper, we propose a novel approach to tackle this multi-hop reading comprehension problem. Inspired by human reasoning processing, we construct a path-based reasoning graph from supporting documents. This graph can combine both the idea of the graph-based and path-based approaches, so it is better for multi-hop reasoning. Meanwhile, we propose Gated-RGCN to accumulate evidence on the path-based reasoning graph, which contains a new question-aware gating mechanism to regulate the usefulness of information propagating across documents and add question information during reasoning. We evaluate our approach on WikiHop dataset, and our approach achieves state-of-the-art accuracy against previously published approaches. Especially, our ensemble model surpasses human performance by 4.2\%.",105
"  The field of machine translation  has been revolutionised in the past few years by the emergence of a new approach: neural MT . %. NMT is a dynamic research area and we have witnessed two mainstream architectures already, the first of which is based on recurrent neural networks  with attention  while the second, referred to as Transformer, makes use of the self-attention mechanism in non-recurrent networks .  Several studies have analysed in depth, using both automatic and human evaluation methods, the resulting translations of NMT systems under the recurrent architecture and compared them to the translations of the previous mainstream approach to MT: statistical MT , e.g. . %van2018fine However, while the Transformer architecture has brought, at least when trained with sufficient data, considerable gains over the recurrent architecture , the research conducted to date that analyses the resulting translations of these two neural approaches is, to the best of our knowledge, limited to automatic approaches. %there is only limited research to date that analyses the resulting translations of these two neural approaches, and such studies to date are limited, to the best of our knowledge, to automatic approaches.  %TODO be careful: there are some papers that compare recurrent vs Transformer, e.g. % %https://www.aclweb.org/anthology/C18-1054.pdf %TODO check whether there are other papers  % %TODO they do an error analysis of ZH->EN, but the taxonomy is not a standard one, etc.  In this paper we conduct a detailed human analysis of the outputs produced by state-of-the-art recurrent and Transformer NMT systems. Namely, we manually annotate the errors found according to a detailed error taxonomy which is compliant with the hierarchical listing of issue types defined as part of the Multidimensional Quality Metrics  framework. We carry out this analysis for the news domain in the English-to-Chinese translation direction. %First,  To this end, we define an error taxonomy that is relevant to the problematic linguistic phenomena of this translation direction. %Subsequently, we This taxonomy is then used to annotate errors produced by NMT systems that fall under the recurrent and Transformer architectures. %Finally, we analyse the annotations.  The main contributions of this paper can then be summarised as follows:      } .}     %TODO anything else?   The rest of the paper %will be  is arranged in the following way. Section  presents a brief review of related work. %research on comparative error analysis of MT systems. Next, Section  outlines the recurrent- and Transformer-based NMT systems and the dataset used in our experiments. %with a contrast between Transformer and RNN approaches, as well as the dataset.  Subsequently, Section  %gives details of  presents the methodology for error annotation and the definition of the error taxonomy, followed by results and statistical analysis of the annotation. %\red{Section  describes an additional annotation on the Transformer-based NMT system with the best human evaluation result in WMT2019 and comparison with the one used in our experiment.} Finally, Section gives a conclusion and suggestions for future work.  %TODO not sure whether we'd need that section. In https://ufal.mff.cuni.cz/pbml/108/art-klubicka-toral-sanchez-cartagena.pdf we had the related work in the intro. It depends on how much related work we have.      This paper presented a fine-grained manual evaluation for EnglishChinese on  the two mainstream architectures of NMT: RNN and Transformer. The evaluation was approached in the form of a human error annotation based on a customised MQM error taxonomy.  The error taxonomy was developed from the MQM core taxonomy for MT evaluation. Chinese linguistic features and issues emerged in the calibration set were taken into account by including customised error types, such as ,  and . The error type  underpins investigating westernised Chinese phenomena of extraneous function words by specifying it into three word classes: Preposition, Adverb and Particle.  \todo{I don't understand this last sentence, does it refer to the 3 error types ? So the issue of Westernised Chinese is linked to those 3?}  From our analysis, it is clear that Transformer-based systems generate significantly more accurate, fluent and comprehensible translation with less westernised Chinese expressions. However, Transformer systems do not handle typography as well as RNN. We also note that none of the MT systems did produce any errors related to unpaired-marks and only one system produced errors related to classifiers, which were very unfrequent . We can conclude that Transformer systems produce an overall better translation compared to RNN when translating from English to Chinese, which corroborates findings of prior studies on other language pairs.  A limitation worth mentioning is that our annotation was conducted by only two annotators on a limited amount of data.  sample of English-to-Chinese news translation.   Our taxonomy could be of use for further error analysis on Chinese MT quality.  Future research could include a larger annotation sample to investigate if punctuation is a a common issue in NMT systems based on Transformer and to verify that NMT is able to produce correct classifiers. Also, as Transformer still shows a major problem in mistranslation, the error taxonomy can be extended with more specific categories to explore this issue in more detail.  The annotations for the three MT systems and the code used for the analysis thereof are publicly available.\footnote{}  We could only find limited information on the Transformer NMT system used in our research, which suggests that a more well-documented NMT with Transformer should be evaluated for a more convincing representation.\todo{having now KSAI I think this sentence can be removed}  
","  This research presents a fine-grained human evaluation to compare the Transformer and recurrent approaches to neural machine translation , on the translation direction English-to-Chinese. To this end, we develop an error taxonomy compliant with the Multidimensional Quality Metrics  framework that is customised to the relevant phenomena of this translation direction. We then conduct an error annotation using this customised error taxonomy on the output of state-of-the-art recurrent- and Transformer-based MT systems on a subset of WMT2019's news test set. The resulting annotation shows that, compared to the best recurrent system, the best Transformer system results in a 31\% reduction of the total number of errors and it produced significantly less errors in 10 out of 22 error categories.  We also note that two of the systems evaluated do not produce any error for a category that was relevant for this translation direction prior to the advent of NMT systems: Chinese classifiers.  %while it underperforms the recurrent-based system on one . %We develop a Chinese error taxonomy compliant with the Multidimensional Quality Metrics  framework. The tagset is customised to reflect Chinese linguistic phenomena and NMT features. Two annotators are engaged to do the manual annotation task. We utilise the method of  to statistically evaluate the differences in overall performance and for each issue type between the two NMT systems. The analysis showed that the NMT with Transformer reduces the amount of error by 32\% in total, in comparison to the NMT with RNN. Also, Transformer produced significantly less errors in 10 out of 22 categories.",106
"   Natural Language Processing  algorithms are constantly improving, gradually approaching human level performance . However, those algorithms often depend on the availability of large amounts of manually annotated data from the domain where the task is performed. Unfortunately, collecting such annotated data is often costly and laborious, which substantially limits the applicability of NLP technology.  %If NLP technology is to be widely applied, this challenge should be overcome.   Domain Adaptation , training an algorithm on annotated data from a source domain so that it can be effectively applied to other target domains, is one of the ways to solve the above bottleneck. Indeed, over the years substantial efforts have been devoted to the DA challenge . Our focus in this paper is on unsupervised DA, the setup we consider most realistic. In this setup labeled data is available only from the source domain while unlabeled data is available from both the source and the target domains.   While various approaches for DA have been proposed , with the prominence of deep neural network  modeling, attention has been recently focused on representation learning approaches. Within representation learning for unsupervised DA, two approaches have been shown particularly useful. In one line of work, DNN-based methods which employ compress-based noise reduction to learn cross-domain features have been developed . In another line of work, methods based on the distinction between pivot and non-pivot features  learn a joint feature representation for the source and the target domains. Later on, , and  married the two approaches and achieved substantial improvements on a variety of DA setups.  Despite their success, pivot-based DNN models still only utilize labeled data from the source domain and unlabeled data from both the source and the target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. With the recent game-changing success of contextualized word embedding models trained on such massive corpora , it is natural to ask whether information from such corpora can enhance these DA methods, particularly that background knowledge from non-contextualized embeddings has shown useful for DA . %\textcolor{red}{Eyal: Can we write here . Our model, named PERL: Pivot-based Encoder Representation of Language, builds on massively pre-trained contextualized word embedding models such as BERT . To adjust the representations learned by these models so that they close the gap between the source and target domains, we fine-tune their parameters using a pivot-based variant of the Masked Language Modeling  objective, optimized on unlabeled data from both the source and the target domains. We further present R-PERL  which facilitates parameter sharing for pivots with similar meaning. %utilizes word embedding information learned in the BERT pre-training stage. %We call our method PERL: Pivot-based Encoder Representation of Language.  %Recently,  and  proposed a DA approach where they fine-tune a massively pre-trained BERT model by optimizing its standard MLM objective using target-domain unlabeled data. However, not only is our pivot-based fine-tuning approach more effective , but it is also more efficient. As discussed in \S, the number of classification parameters PERL has to fine-tune is 300 times lower than the above pivot-ignorant approaches, resulting in a substantial run-time reduction. We perform extensive experimentation in various unsupervised DA setups of the task of binary sentiment classification . First, for compatibility with previous work, we experiment with the legacy product review domains of  . We then experiment with more challenging setups, adapting between the above domains and the airline review domain  used in  , as well as the IMDB movie review domain  . We compare PERL to the best performing pivot-based methods  and to DA approaches that fine-tune a massively pre-trained BERT model by optimizing its standard MLM objective using target-domain unlabeled data . PERL and R-PERL substantially outperform these baselines, emphasizing the additive effect of massive pre-training and pivot-based fine-tuning.  % %We further demonstrate that a variant of PERL, designed for in-domain fine-tuning, substantially improves the in-domain performance of a BERT-based sentiment classifier. This is demonstrated for the IMDB dataset, a benchmark for binary sentiment classification, %trained on increasing amounts of labeled examples .  As an additional contribution, we show that pivot-based learning is effective beyond improving domain adaptation accuracy. Particularly, we show that an in-domain variant of PERL substantially improves the in-domain performance of a BERT-based sentiment classifier, for varying training set sizes . We also show that PERL facilitates the generation of effective reduced-size DA models. Finally, we perform an extensive ablation study  that uncovers PERL's crucial design choices and demonstrates the stability of PERL to hyper-parameter selection compared to other DA methods.   %Finally, we show that PERL facilitates the generation of effective reduced-size models.      We presented PERL, a domain-adaptation model which fine-tunes a massively pre-trained deep contextualized embedding encoder  with a pivot-based MLM objective. PERL outperforms strong baselines across 22 sentiment classification DA setups, improves in-domain model performance, increases its cross-configuration stability and yields effective reduced-size models.  Our focus in this paper is on binary sentiment classification, as was done in a large body of previous DA work. In future work we would like to extend PERL's reach to structured  and generation  NLP tasks.  
","     Pivot-based neural representation models have lead to significant progress in domain adaptation for NLP. However, previous works that follow this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. To alleviate this, we propose PERL: A representation learning model that extends contextualized word embedding  models such as BERT  with pivot-based fine-tuning. PERL  outperforms strong baselines across 22 sentiment classification domain adaptation setups, improves  in-domain model performance, yields effective reduced-size models and increases model stability.\footnote{Our code is at \url{https://github.com/eyalbd2/PERL}.}     \footnote{This paper was accepted to TACL in June 2020}",107
"  % commonsense  % Commonsense Validation and Explanation % in this paper, we describe our model, which was proposed for the Semeval Task4. The competition .. . More formally,.. % We propose ensemble approach composed of two models:...  if the ensemble model get more improvement, we can write our strategy to inject it.   % 濮濄倕顦╅張澶婃禈figure illustrates the main components of our mode. % the evaluation on the competition's test set resulted in a ... scores... The result ranked ..th in the final leaderboard.   %Motivation   %Natural Language Understanding  has received increasing research attention in recent years. The key factor is to understand the meaning of words and sentences, and further to judge whether the sentence has a sense or not.   How to integrate common sense into natural language models is attracting more and more attention. Common sense, as ordinarily conceived, present themselves as the aspect of the grammar of expressions and sentences on which their semantic properties and relations depend. And a critical difference of text understanding between humans and machines lies in the fact that humans can access commonsense knowledge while processing text, which helps them draw inferences about effects that are not mentioned in a paragraph. Therefore, it's a fundamental question on how to validate whether a system has a commonsense capability, and more importantly, let the system explain how it reasons using hidden facts. Existing benchmarks measure commonsense knowledge indirectly and without explanation. Also, existing datasets test common sense indirectly through tasks that require extra knowledge, such as co-reference resolution, or reading comprehension. They verify whether a system is equipped with common sense by testing whether it can give a correct answer to make the complete sentence reasonable. However, there are some restrictions on such benchmarks. First, they do not provide a straight quantitatively standard to measure sense masking capability. Second, they do not explicitly identify the key factors required in a sense-making process. And also, they do not need the model to explain why it makes that prediction.      %Instead, the model should use 閳ユ竷ommon  sense閳 or world knowledge to make inferences and then better understand the meaning of sentences.  Common sense reasoning require the agent or the model to utilize a world knowledge to take inferences or deep semantic understanding, not only the pattern recognition.   Some empirical analysis has been done previously for common sense reasoning, mainly focus on the form of question answering  .  But question-answering is hard to directly evaluate the commonsense in contextualized representations. And there has been few work investigating commonsense in pre-trained language models, such as ELMo and  BERT. Introduced by , sense-making is a task to  tests whether a model can differentiate sense-making and non-sensemaking statements. Specifically, the statements typically differ only in one keyword which covers nouns, verbs, adjectives, and adverbs. There are two existing approaches that can address this problem, one simple way is to use  more commonsense knowledge can be learned from larger training sets. On the other hands, some works  focus on effectively utilizing external, structured commonsense knowledge graphs, such as  ConceptNet and COMET . Insipred by previous works, more researchers are trying to fuse commonsense knowledge and language model, and apply them to downstream tasks. Recently, a new hybrid approach has been proposed for common sense reasoning. The core idea behind it is multi-task learning, which has been widely applied in natural language tasks.   %Though it has been recognized that commonsense reasoning is one of the central  %challenges in the field.   But existing work in this area has been frustratingly slow, and much of the work is completely theoretical. The field might well benefit if commonsense argumentation were systematically described and evaluated. To tackle it, this system focuses on a benchmark to directly test whether a  system can differentiate sentences that make sense from those that do not make sense. Our results indicate that pre-trained models are not able to demonstrate well on the benchmark, and  some remaining  cases demonstrating that human level is not achieved yet. Thus, we design a new procedure to handle the commonsense challenge inspired by human cognition. It firstly explain its understanding of the given sentences by a language model, and induce the hidden common sense fact. And then, the  explanation is used as a supplementary input to the prediction module. Still, we believe that our approach also can be applied to more challenging data sets.  The organization of this paper is as follows: in Section 2, we introduce the basic information about pre-trained language model and task definition. We then describe the framework of our model in Section 3. Empirical results are given and discussed in Section 4. And then we provide more exhaustive analysis for some bad cases that appeared at our experiment in Section 5. Finally, we conclude this survey and in Section 6.                         In this paper we present our model on the task of Commonsense Validation and Explanation  in SemEval-2020. We explore multi-task learning to jointly learn how to inference the hidden common-sense fact and do common-sense reasoning with the RoBERTa model and achieved competitive results.  Our result analysis indicates that our 閳ユ欢xplain, Reason and Predict閳 approach helps improve the performance of RoBERTa and have a strong reasoning capability. The biggest regret in this competition is that we did not incorporated with world knowledge by introducing some knowledge graphs. Due to implicit common sense knowledge and different explanation perspectives, we still need more efforts on the model architecture and find more elegant ways to inject knowledge. Our positive results point to future work in extending the ERP approach to a variety of other types of common sense reasoning tasks.   conceptNet is a future direction.  
","  This paper describes our system submitted to task 4 of SemEval  2020: Commonsense Validation and Explanation  which consists of three   sub-tasks. The challenge is to directly   validate whether the system can recognize natural language statements that make sense from those that do not, and also require to generate reasonable explanation. Based on BERT architecture with multi-task setting, we propose an  effective and   interpretable ``Explain, Reason and Predict""  system to solve the   three sub-tasks about commonsense:  Validation, and    Explanation,  Reasoning, following the order of the competition. Inspired by cognitive studies of common sense, our system first   generate a reason or understanding of the sentences and then choose which one   statement makes sense, which is achieved by multi-task learning. The rational experiment validates our assumption and boost the performance. During the   post-evaluation, our system has reached 92.9\% accuracy in subtask A ,  89.7\% accuracy in subtask B , and BLEU score of 12.9 in subtask C \footnote{All results before 29 April, 2020}.",108
"  This short example shows a contrived example on how to format the authors' information for IJCAI--PRICAI--20 Proceedings.    We proposed a multi-layered meta-graph based graph neural network model  for Open KB canonicalization. MGNN integrates semantic information  and structural information  through canonical embedding aggregation. It adapted a meta-graph based neighbor acquisition and learned node canonical embedding with meta-graph based hybrid loss. Our model outperforms baselines on a general Open KB dataset.      
", This short example shows a contrived example on how to format the authors' information for {.,109
"  Aspect extraction is an important task in sentiment analysis of, e.g., user reviews. The goal of aspect extraction is twofold:  to extract words/tokens describing features of the item the author shares their opinion about,  to attribute each extracted word/token to a group/cluster related to a certain feature. For example, given a sentence ``The stew was delicious'' from a restaurant review, one might extract  ``stew'' as an aspect word representing the ``food'' aspect. The words ``steak'', ``borscht'', ``fish'' etc. could also be attributed to the aspect ``food''.  Recent advances in neural attention-based architectures have made it into a method of choice for modern natural language processing. It is currently well established  that neural models are able to identify latent topical aspects in user-generated texts in an unsupervised way. The purpose of topic modeling is to cluster words  of the input text into coherent topics, or ; e.g., the words criminal and federal are part of the topic justice for the domain of law journals, while oxidation and reaction are part of the topic chemistry for the domain of research papers. In probabilistic topic models, the topics/aspects are usually defined as distributions over words/tokens; the topic distribution can then serve as a compressed description of the document for other models.  Unsupervised methods for aspect extraction and topic modeling are an active field of research, especially since they can be applicable to texts in any domain. In particular, one model that has recently proved to be successful is the unsupervised neural attention-based aspect extraction model . One of the most prominent advantages of attention-based models over traditional topic modes such as latent Dirichlet allocation  is that the former encode word-occurrence statistics into word embeddings and apply an attention mechanism to remove irrelevant words, learning a set of aspect embeddings.  While recent studies on a set of user reviews have demonstrated that neural attention models can provide aspects of significantly higher quality than the classical LDA model or its modifications developed over the last decade , we have found in  recent research and  practical experience  that these models have significant limitations on long texts such as, e.g.,  newsgroup posts as compared to user reviews on  or similar.  One possible explanation for this effect lies in the style differences between the two domains. Review writers expressing opinions and describing items of value  usually stay focused on the topic and do not venture into general exposition. This means that the implicit assumption of a sentence-based model such as ABAE that every sentence relates to a single aspect usually makes sense.  On the contrary, newsgroup texts or longer reviews are ``too general'' compared to -like reviews, i.e., too many sentences are ``general''  and do not contain aspect words that ABAE or similar models are implicitly trained to encode and recover.    The attention mechanism imposes restrictions on the model: ABAE learns a poor representation of texts at the broad, general level rather than in terms of latent topics discovered from the collection. For a prolonged example, Table shows sample topical aspect words extracted for the  newsgroup from the  dataset. Each row in Table contains eight most probable words for the corresponding aspect extracted by the ABAE model. In the left column, Table shows examples of poor topical aspects learned by directly applying ABAE on all sentences of newsgroup documents and topic words that are much more coherent and readily interpretable.   [t]  \toprule ABAE trained on all sentences in a post  & ABAE trained on selected sentences  \\ \midrule  \textless num\textgreater~\textless pad\textgreater~ raffle anyone copy  & wiring green cable box gfci grounded case \\ time frequency chip source take much& voltage input supply output signal power circuit \\ \textless num\textgreater greggo \textless unk\textgreater  mc68882rc33 ~\textless pad\textgreater~ raffle &  edu university uk mail fax email internet\\  \textless unk\textgreater raffle \textless pad\textgreater greggo mc68882rc33 ~\textless num\textgreater~ ca input & dc digital wave drive per data decimal state  \\ dtmedin b30 catbyte ingr uunet com uucp look& com dtmedin b30 catbyte uunet ingr uucp al everywhere \\ mail edu university writes com email uk & radar detector number someone radio law shack \\ copy anyone know could would help get & ca mb bison baden inqmind de sys6626 mind bb bari\\ edu university uk henry toronto mail & best year around machine least seems band \\ input pin output data latch voltage & phone neoucom departmentedu oh usa computer uhura  \\ input output data voltage pin high & pin input latch output data voltage supply\\ ca mb bison baden inqmind de sys6626 bb & ground wire neutral conductor box outlet grounding  \\ connected outlet hot wire grounding neutral & uk mail university email com edu fax internet  \\ ground wire conductor neutral outlet connected & would anyone know copy get could want\\ mc68882rc33 \textless pad\textgreater greggo input raffle voltage & pin input neutral voltage connected wire current\\ phone neoucom edu department oh usa computer service & copy anyone know could would help  \\     How can we extract better aspects in longer and more general texts, e.g., in newsgroup posts, with standard ABAE? In this work we propose an intuitive solution to this problem based on sentence filtering.   The idea is to train a simple binary probabilistic text classifier able to separate the texts of a particular  domain of interest from texts on other topics. For example, all news' sentences about sport are labeled as in-domain texts for the target domain `sport', while texts about politics, electronics or weather are considered as out-of-domain examples. This binary classifier allows to estimate the probability of each sentence to be an ``in-domain'' sentence in the target dataset. Sentences with scores lower than a certain threshold can then be treated as ``out-of-domain''  ones and dropped from the training set for aspect extractors. Note that sentence classification here is not a goal in itself but serves as preprocessing for subsequent aspect extraction.  In this work we show that this simple technique allows to achieve better interpretability of the resulting aspects. For a clear example, see the right column of Table that contains top words for aspects also extracted by ABAE but after the proposed preprocessing. Note that token sets in the two columns intersect often, but aspects in the left column are ``noisier'', less coherent, and harder to interpret. The aspects become better as the model is not trying to encode sentences that could be attributed to any other domain and is free to concentrate on ``relevant'' sentences.  The paper is structured as follows. Section  briefly surveys related work. In Section, we begin with the model description, describing attention mechanisms and the existing ABAE model. In Section, we present an approach to sentence filtering using out-of-domain classification. The experimental setup and results on several datasets are presented in Section. We conclude with a summary of our results and possible future research directions in Section.   %     In this work, we have presented a simple yet effective method of filtering out-of-domain sentences in order to improve the quality of ABAE-based models in newsgroup posts in terms of topic coherence. The presented results on the  dataset demonstrate that the proposed filtering method indeed improves the overall topic quality: ABAE trained on in-domain sentences discovers better topics than both   We see several potential directions for future work. First, there are more sophisticated topic models than the basic LDA, which could be even more sensitive to in- and out-of-domain data. We posit that the proposed technique can help some of them even more.   Second, another potential research direction could be to use more complex techniques for this in/out-of-domain classification, e.g., the method described in. In general, we feel that the proposed filtering approach is a universal technique that can bring improvements across different topic models and neural architectures.  Finally, although we consider our claim fully supported by the evidence provided in this work, to make the proposed technique practical one also has to devise a reliable method of choosing the threshold. The threshold clearly depends on both the dataset and out-of-domain classification models. As the models are yet to be compared , the technique for choosing the threshold is left for further study as well.  
"," Deep learning architectures based on self-attention have recently achieved and surpassed state of the art results in the task of unsupervised aspect extraction and topic modeling. While models such as neural attention-based aspect extraction  have been successfully applied to user-generated texts, they are less coherent when applied to traditional data sources such as news articles and newsgroup documents. In this work, we introduce a simple approach based on sentence filtering in order to improve topical aspects learned from newsgroups-based content without modifying the basic mechanism of ABAE. We train a probabilistic classifier to distinguish between out-of-domain texts  and in-domain texts . Then, during data preparation we filter out sentences that have a low probability of being in-domain and train the neural model on the remaining sentences. The positive effect of sentence filtering on topic coherence is demonstrated in comparison to aspect extraction models trained on unfiltered texts.",110
" Word embedding is a technique used in Natural Language Processing  to map a word to a numeric vector, in a way that semantic similarity between two words is reflected in geometric proximity in the embedding space. This allows NLP algorithms to keep in consideration some aspects of meaning, when processing words. Typically word embeddings are inferred by algorithms from large corpora based on statistical information. These are unsupervised algorithms, in the sense no explicit information about the meaning of words is given to the algorithm. Word embeddings are used as input to multiple downstream systems such as text classifiers  or machine translations .  An important problem in designing word embeddings is that of evaluating their quality, since a measure of quality can be used to compare the merits of different algorithms, different training sets, and different parameter settings. Importantly, it can also be used as an objective function to design new and more effective procedures to learn embeddings from data. Currently, most word embedding methods are trained based on statistical co-occurrence information and are then assessed based on criteria that are different than the training ones.  Cosine similarity and euclidean distances have shown the ability to represent semantic relationships between words such as in GloVe where the vector representations for the words man, woman, king and queen are such that :   Schnabel et al.  identifies two families of criteria: intrinsic and extrinsic, the first family assessing properties that a good embedding should have , the second assessing their contribution as part of a software pipeline .   % We propose a new criterion to measure the quality of word embeddings that can lead to an objective measure of their quality. Our method can be considered both intrinsic and extrinsic based on the definition of . We use word embeddings as input features for a downstream task and measure changes in the performance of that task, a quality of extrinsic testing. We also test for syntactic or semantic relationships between words similar to intrinsic testing, however our test isn't a direct evaluation of the embedding.  We propose a criterion of quality for word embeddings, and then we present a statistical methodology to compare different embeddings. The criterion would fall under the intrinsic class of methods in the classification of Schnabel et. al. , and has similarities with both their coherence criterion and with their categorization and relatedness criteria. However it makes use of the notion of ``concept learnability"" based on statistical learning ideas. We make use of extensional definitions of concepts, as they have been defined by .  Intuitively, a concept is a subset of the universe, and it is learnable if it is possible for an algorithm to recognise further members after learning a random subset of its members.   The key part in this study is that of a ``concept"". If the set of all words in a corpus is called a vocabulary , we define any subset of the vocabulary as a concept. We call a concept learnable if it is possible for a learning algorithm to be trained on a random subset of its words, and then recognise the remaining words. We argue that concept learnability captures the essence of semantic structure, and if the list of words has been carefully selected, vetted and validated by rigorous studies, it can provide an objective way to measure the quality of the embedding.  In the first experiment we will measure the learnability of Linguistic Inquiry and Word Count  lists. We compare LIWC lists to randomly generated word lists for popular pretrained word embeddings of three different algorithms . We show that LIWC concepts are represented in all embeddings through statistical testing.  In our second experiment we compare the learnability of different types of embedding algorithms and settings, using a linear classifier. We compare three of these embedding methods  to each other. We use the same method as previous, however for this experiment we train with the same hyper parameters and corpus across all three word embeddings . We show that from this experiment fastText performs the best, performing significantly better than both word2vec and GloVe.  This study is a statistical analysis of how a given word embedding affects the learnability of a set of concepts, and therefore how well it captures their meaning. We report on the statistical significance of how learnable various concepts are under different types of embedding, demonstrating a protocol for the comparison of different settings, data sets, algorithms. At the the same time this also provides a method to measure the semantic consistency of a given set of words, such as those routinely used in Social Psychology, eg. in the LIWC technique.    In this paper, we have shown that word embeddings are able to capture the meaning of human defined word lists. We have shown the ability of embedding algorithms in learning concepts from word lists. In particular we have shown this quality in word2vec, GloVe and fastText. We have shown that learning embeddings from real data can represent real world concepts defined extensionally, utilising word lists provided by LIWC.  We have also shown the relative performance of GloVe, fastText, and word2vec when using LIWC word lists to form concepts using similar corpora that derive most of their corpora from Wikipedia. fastText performs better in the majority of situations for all word lists we have tested from LIWC, while GloVe outperforms word2vec generally. However as all algorithms use slightly different corpora, this result may change depending on the corpora used.  This measure of performance of word embeddings can be used in the future as a measure of ``quality"" of word embeddings. While there are other methods that look at the performance of word embeddings by evaluating their performance in a specific task , our method differs in that it looks at an embeddings general ability to understand human defined concepts. There has also been criticism of evaluating word embeddings using only word similarity tasks . This method can also be used in another way as a measure of the quality of word lists and their ability to accurately describe a concept, providing an assumption or proof that an embedding is performing suitably to the users needs.  Future work with this method would involve extensive testing of the method using with varying differing hyper parameters to see the optimal performance of these embedding algorithms. An example of this is the impact of embedding dimension on performance. Another experiment could be looking at the performance of this test on deep contextualized embeddings such as ELMo  and BERT . These embeddings have been shown to have better performance on many tasks that employ word embeddings. While these embeddings are optimized for their specific end tasks, they train embeddings before that tuning process takes place. There is potential to compare these embeddings by testing the extracted embedding with a linear classifier, or fine tuning their full model to our task. However a key benefit for sentence embeddings is the context of words around them, which our task will not benefit from.  Further work could be focused on the performance of different word lists and concepts within word embeddings. The benefit of this could be to validate word lists that are not as carefully curated as LIWC word lists. These word lists may come from different fields, as LIWC is focused on clinical psychology other word lists may perform differently. Different source corpora may also change the performance of these word lists due to the meaning of some words changing from domain to domain.  
"," Word Embeddings are used widely in multiple Natural Language Processing  applications. They are coordinates associated with each word in a dictionary, inferred from statistical properties of these words in a large corpus. In this paper we introduce the notion of ``concept'' as a list of words that have shared semantic content. We use this notion to analyse the learnability of certain concepts, defined as the capability of a classifier to recognise unseen members of a concept after training on a random subset of it. We first use this method to measure the learnability of concepts on pretrained word embeddings. We then develop a statistical analysis of concept learnability, based on hypothesis testing and ROC curves, in order to compare the relative merits of various embedding algorithms using a fixed corpora and hyper parameters. We find that all embedding methods capture the semantic content of those word lists, but fastText performs better than the others. \\ \newline \Keywords{Word Embedding, Linear Classifier, Concepts",111
"  Event Detection  is an important task in Information Extraction  in Natural Language Processing . Event Detection is the task to detect event triggers from a given text  and classify it into one of the event types of interest. The following sentence is an example of ED:  In 1997, the company hired John D. Idol to take over as chief executive.  In this example, an ideal event detection system should detect the word hired as an event, and classify it to class of Personnel:Start-Position, assuming that Personnel:Start-Position is in the set of interested classes.  % Event detection has been applied in a variety of downstream applications .    The current works in ED typically employ traditional supervised learning based on feature engineering  and neural networks . The main problem with supervised learning models is that they can not perform well on unseen classes . As a result, supervised learning ED can not extend to unseen event types. A trivial solution is to annotate more data for unseen event types, then retraining the model with newly annotated data. However, this method is usually impractical because of the extremely high cost of annotation .   A human can learn about a new concept with limited supervision e.g. one can detect and classify events with 3-5 examples . This motivates the setting we aim for event detection: few-shot learning . In FSL, a trained model rapidly learns a new concept from a few examples while keeping great generalization from observed examples . Hence, if we need to extend event detection into a new domain, a few examples are needed to activate the system in the new domain without retraining the model. By formulating ED as FSL, we can significantly reduce the annotation cost and training cost while maintaining highly accurate results.  In a few shot learning iteration, the model is given a support set and a query instance. The support set consists of examples from a small set of classes. A model needs to predict the label of the query instance in accordance with the set of classes appeared in the support set. Typical methods employ a neural network to embed the samples into a low-dimension vector space , then, classification is done by matching those vectors based on vector distances . One potential problem of prior FSL methods is that the model relies solely on training signals between query instance and the support set . Thus, the matching information between samples in the support set has not been exploited yet. We believe that this is not an efficient use of training data because dataset in ED is very small . Therefore, in this study, we propose to train an ED model using matching information  between query instance and the support set and  between the samples in the support themselves. This is implemented by adding two auxiliary factors into the loss function to constrain the learning process.  We apply the proposed training signals to different FSL models on the benchmark event detection dataset . The experiments show that the training signal can improve the performance of the examined FSL models. To summarize, our contributions to this work include:            In this paper, we address the problem of extending event detection to unseen event types through few-shot learning. We investigate four metric-based few-shot learning models with different encoder types . Moreover, we propose two novel loss functions to provide more training signals to the model exploiting domain-matching information in the support set. Our extensive experiments show that our method increases the efficiency of using training data, resulting in better classification performance. Our ablation study shows that both intra-cluster matching and inter-cluster matching contributes to the improvement.  
","  Current event detection models under supervised learning settings fail to transfer to new event types. Few-shot learning has not been explored in event detection even though it allows a model to perform well with high generalization on new event types. In this work, we formulate event detection as a few-shot learning problem to enable to extend event detection to new event types. We propose two novel loss factors that matching examples in the support set to provide more training signals to the model. Moreover, these training signals can be applied in many metric-based few-shot learning models. Our extensive experiments on the ACE-2005 dataset  show that the proposed method can improve the performance of few-shot learning.",112
" Fast, accurate machine translation is a fundamental goal with a wide range of applications both in research and production. State-of-the-art neural machine translation systems generate translations autoregressively where words are predicted one-by-one conditioned on all previous words . This sequential property limits parallelization since multiple tokens in each sentence cannot be generated in parallel. A flurry of recent work developed ways to  parallelize the decoder with non-autoregressive machine translation  input from users. This is where current \nar models shine---we can make full use of parallelism across decoding positions in a GPU. % For this reason, much prior work in \nar only measures speed using this metric . The second scenario aims at a situation where we want to translate a large amount of text as quickly as possible. In this case, we see that \ar models run faster than \nar models by a large margin. Computation at each time step is large enough to exploit parallelism in a GPU, which cancels out the benefit from parallel \nar decoding. Further, \ar models can speed up by caching all previous hidden states  and computing each step in linear time complexity with respect to the sequence length. % % In contrast, \nar models necessitate a fresh run of quadratic self and cross attention in every decoding iteration.  % Interestingly, using a deep encoder and a shallow decoder in \nar models fails to retain the original translation accuracy by using 6 layers each . This suggests that departure from \ar decoding necessitates more computational capacity in the decoder side, and the strategy is effective specifically for \ar models.  In particular, our analysis demonstrates that an \nar decoder requires more layers % to learn target word ordering . % % % % % In summary, our contributions are the following:        configuration.       %  % % % % % % % % % %  % % % % % % %  % % % % % % % % % % % % % % % % % % % % % % % % % % % % %  % % % % %   % % % % % % % % % % %   We presented theoretical and empirical studies to demonstrate that autoregressive neural machine translation can be dramatically sped up by a simple layer allocation strategy: deep encoder, shallow decoder. Compared to strong non-autoregressive models, deep-shallow autoregressive models achieve substantial improvement in translation quality with comparable inference speed.   Our results suggest that layer allocation, knowledge distillation, and speed measurement are important aspects that future work on non-autoregressive machine translation should take into consideration.   More generally, a model with a deep encoder and a shallow decoder can be used for any sequence-to-sequence task, including large-scale pretraining .    
"," % % % During the recent years, much effort has been invested in non-autoregressive neural machine translation, which appears to be an efficient alternative to state-of-the-art autoregressive machine translation on modern GPUs.  In contrast to the latter where generation is sequential, the former allows generation to be parallelized across target token positions. Non-autoregressive machine translation provides a tradeoff between translation quality and inference speed, but some of the latest models have achieved impressive tradeoffs compared to autoregressive baselines. In this work, we reexamine this tradeoff and argue that autoregressive baselines can be substantially sped up without loss in accuracy. Specifically, we study autoregressive models with encoders and decoders of varied depths. Our extensive experiments show that given a sufficiently deep encoder, a  autoregressive decoder can substantially outperform strong non-autoregressive models with comparable inference speed. We show that the speed disadvantage for autoregressive baselines compared to non-autoregressive methods has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. Our results establish a new protocol for future research toward fast, accurate machine translation. Our code is available at \url{https://github.com/jungokasai/deep-shallow}. %",113
"  	 	%Description about Neural Topic Models:  	Topic models  such as %latent dirichlet allocation  	LDA ~ 	%and its variants~  	facilitate document-level semantic knowledge in the form of topics, explaining the thematic structures hidden in a document collection. In doing so, they learn document-topic associations in a generative fashion by counting word-occurrences across documents. Essentially, the generative framework assumes that each document is a mixture of latent topics, i.e., topic-proportions and each latent topic is a unique distribution over words in vocabulary.    	Beyond a document representation, topic models also offer interpretability via topics .  Recently, neural topic models  have been shown to outperform LDA-based models. Thus, we consider neural network based topic models in this work.  	 	%Description about Neural Language Models:  	Language models   have recently gained success in natural language understanding by predicting the next  word in a sequence given its preceding and/or following context, accounting for linguistic structures such as word ordering. However, LM are often contextualized by an n-gram window or a sentence, ignoring  global semantics in context beyond the sentence boundary especially in modeling documents. To capture long-term semantic dependencies, recent works  have attempted to introduce document-level semantics in LMs at sentence-level by marrying topic and language models, e.g., augmenting LSTM-based LMs with a latent document-topic proportion  obtained from a topic model for the document in which the sentence appears.    	 	[t] 		%\vskip 0.2in 		 			% 			% 			 			_d}_d with complementary explainable topics  provide an abstract  and a fine granularity  outlook, respectively.  To our knowledge, the scheme of augmenting LMs with both the latent document-topic proportion and explanatory topics remains unexplored.  	%a descriptive outlook by  with broader contexts.  	 	Contribution 1: Complementing the latent document-topic proportion, we also leverage explanatory topics in augmenting LMs with topical semantics in a neural composite language modeling  framework, consisting of a neural topic model  and a neural language model .   	 	%Our proposed model and its Effects/Results:  	Motivation 2: A sentence in a document may have a different topical discourse than its neighboring sentences or the document itself.  Illustrated in Figure  , an NTM generates two different document-topic proportions  for input document  and sent\#2+sent\#3 while modeling sent\#1 in the NLM. Observe that the sent\#1 expects a topic proportion dominated by topic T3 .  	 	Contribution 3: We evaluate the proposed NCLM framework over range of tasks such as language modeling, word sense disambiguation, document classification and information retrieval. Experimental results suggest that both the explanatory topics and sentence-topic associations help in improving natural language understanding.     	%Through extensive experiments, we assert NCLM's enhanced performance over other RNN-based language models that leverage document-level semantics in multiple sentence-level NLP tasks such as language modeling, and word sense disambiguation. 	Implementation of NCLM is available at: \url{https://github.com/YatinChaudhary/NCLM}. 	 	%table of notations  	[t] 		\vskip -0.1in 		  		%} 		 		\vskip -0.1in 		 			 				% 				\renewcommand*{\arraystretch}{1.2} 				%\resizebox{.995\textwidth}{!}{ 				\resizebox{\textwidth}{!}{ 					% 					{c|l||c|l} 						%{c|}{{c||}{{c|}{{c}{ ^{|Z|}dd\text{-}ss\text{-}ydssy\mathbf{W}, \mathbf{U} ^{H \times Z}]\mathcal{N} ^{K}\mathcal{N}\mathbf{E} ^{D_E \times Z}[] ^{K}\mathcal{N}\mathbf{r}_m\mathbf{o}_m ^{H}[\mathbf{h}_{d\text{-}s}\mathbf{z}_{d\text{-}s}^{att}] ^{|Z|}d\text{-}s\mathbf{o}_{d}^{LTA}\mathbf{o}_{d}^{ETA}\mathbf{o}_{d}^{LETA}^{H}d\mathbf{o}_m\mathbf{t} ^{K \times topN}\mathbf{o}_{d,s}^{LTA}\mathbf{o}_{d,s}^{ETA}\mathbf{o}_{d,s}^{LETA}^{H}s\mathbf{o}_m$    \\ 						} 				% 			 		 		\vskip -0.2in 	 	%  	We have presented a neural composite language modeling framework that leverages both the latent and explainable topic representations  	when a topic and an LSTM-based language model is composed. Moreover, we have introduced sentence-topic associations along with document-topic proportion   	to retain sentence-level topical discourse. Experimental results on several language understanding tasks have supported our multi-fold contributions.  	 	
"," 		Marrying topic models and language models exposes language understanding to a broader source of document-level context beyond sentences via topics.   		While introducing topical semantics in language models, existing approaches incorporate latent document topic proportions and ignore topical discourse in sentences of the document.  		This work extends the line of research by additionally introducing an explainable topic representation in language understanding, obtained from a set of key terms correspondingly for each latent topic of the proportion.    		Moreover, we retain sentence-topic associations along with document-topic association by modeling topical discourse for every sentence in the document.   		We present a novel neural composite language model that exploits both the latent and explainable topics along with topical discourse at sentence-level in a joint learning framework of topic and language models.  		Experiments over a range of tasks such as language modeling, word sense disambiguation, document classification, retrieval and text generation demonstrate ability of the proposed model in improving language understanding.",114
" % about unsupervised topic modeling and transfer learning in topic modeling  Unsupervised topic models, such as LDA , RSM , DocNADE , NVDM , etc.  have been popularly used to discover topics from large document collections.   However in sparse data settings, the application of topic modeling is challenging due to limited context in a small document collection or short documents   and the topic models produce incoherent topics.  To deal with this problem, there have been several attempts   that introduce prior knowledge such as pre-trained word embeddings  to guide meaningful learning.      We have presented a novel lifelong neural topic modeling framework that models a stream of document collections and exploits prior knowledge from several domains over lifetime in form of pre-trained topics, word embeddings and generative homologies in historical collections. Experimental results show that our proposed approaches of joint topic regularization, selective-data augmented learning and word-embedding guided topic learning within the lifelong framework help modeling three sparse datasets, quantified by information retrieval, topic coherence and generalization.       
"," Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of  documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly:  sharing generative homologies  over lifetime to transfer prior knowledge, and  minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling  framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task. Code: \url{https://github.com/pgcool/Lifelong-Neural-Topic-Modeling}",115
"  Neural networks benefit from large quantities of labeled training data.  However, in many settings labeled data is much harder to come by than unlabeled data:  current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide~.  Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them - a process that requires learning good representations of speech.  In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to fine-tune the model on labeled data.  This has been particularly successful for natural language processing~ and is an active research area for computer vision~.  In this paper, we present a framework for self-supervised learning of representations from raw audio data.  Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations~, similar to masked language modeling~. The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors~ ~.  As part of training, we learn discrete speech units~ via a gumbel softmax~ to represent the latent representations in the contrastive task  which we find to be more effective than non-quantized targets. After pre-training on unlabeled speech, the model is fine-tuned on labeled data with a Connectionist Temporal Classification  loss~ to be used for downstream speech recognition tasks     Previous work learned a quantization of the data followed by a contextualized representations with a self-attention model~, whereas our approach solves both problems end-to-end. Masking parts of the input with Transformer networks for speech has been explored ~, but prior work relies either on a two-step pipeline or their model is trained by reconstructing the filter bank input features. Other related work includes learning representations from auto-encoding the input data~ or directly predicting future timesteps~.   Our results show that jointly learning discrete speech units with contextualized representations achieves substantially better results than fixed units learned in a prior step. We also demonstrate the feasibility of ultra-low resource speech recognition:  when using only 10 minutes of labeled data, our approach achieves word error rate  4.8/8.2 on the clean/other test sets of \libri{}. We set a new state of the art on TIMIT phoneme recognition as well as the 100 hour clean subset of \libri{}.  Moreover, when we lower the amount of labeled data to just one hour, we still outperform the previous state of the art self-training method of~ while using 100 times less labeled data and the same amount of unlabeled data. When we use all 960 hours of labeled data from \libri{}, then our model achieves 1.8/3.3 WER .      We presented \wvpp{}, a framework for self-supervised learning of speech representations which masks latent representations of the raw waveform and solves a contrastive task over quantized speech representations. Our experiments show the large potential of pre-training on unlabeled data for speech processing:  when using only 10 minutes of labeled training data, or 48 recordings of 12.5 seconds on average, we achieve a WER of 4.8/8.2 on test-clean/other of \libri{}.   Our model achieves results which achieve a new state of the art on the full Librispeech benchmark for noisy speech. On the clean 100 hour \libri{} setup, wav2vec 2.0 outperforms the previous best result while using 100 times less labeled data. The approach is also effective when large amounts of labeled data are available.  We expect performance gains by switching to a seq2seq architecture and a word piece vocabulary.    
","   We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.   wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned.   Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets.   When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data.   Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER.   This demonstrates the feasibility of speech recognition with limited amounts of labeled data.\footnote{Code and models are available at \url{https://github.com/pytorch/fairseq}}",116
"  The work aims at resolving the long-standing plight of unfamiliarity with command line interface in UNIX based systems. This will not only improve the efficiency of the user but also improve the learning curve for the beginners. The concerned research work treats the problem of UNIX Command Line Prediction as a sequence prediction problem instead of the traditionally adapted provision of recommendation systems. RNN  is able to 閳ユ涪heoretically閳 use information from the past in predicting the future. However, plain RNNs suffer from vanishing and exploding gradients problem making them hard to use practically. For this problem, we used LSTM which uses gates to flow gradients back in time solving the vanishing and exploding gradient problem. Thus, with the advent of a command line prediction system involving accurate prediction, a GUI prototype of the UNIX shell can be brought in place subsequently realising the serious necessity of a user-friendly environment for amateur end users. Our model delves into the user閳ユ獨 bash history and learns from it while providing a thorough path to him/her from the past usage patterns of the professionals and scientists. We have also been able to establish a novel method and outperform the 50\% threshold of accuracy set by previous works. The former maximum accuracy was accomplished by  in which  was extended to employ consideration of error output with the history of commands and dynamic file name replacement, attaining 47.9\% accuracy.    	This work has given a novel insight for sequence prediction. A good recommender system of Unix/Linux command line system can provide a better user and learning interface. In general, where the words greatly lack semantics but can be aided by inclusion of background knowledge in the form of KB. We introduce a reasonable approach to prepare an exhaustive KB by extracting knowledge from the data obtained from the source. It is undoubtedly possible to attain better results of the proposed system by including a richer semantic lexicon . Moreover, there is a great need to explore such an algorithm that takes into consideration both the probabilistic, and the domain knowledge and cares for the efficiency in marking the evolution for such a system. Thus, this evolution needs a great deal of experimentation as such a change may typically degrade the performance of the model instead of improving it if the domain knowledge is not adequately considered and if proper weights are not considered. To achieve further improvement that should be deemed necessary nevertheless the sophistication involved  we decreased the number of parameters and omitted weight consideration to generate multiple iterations of the KB and performed a series of experiments to obtain a general algorithm to produce a self-sufficient KB. The semantic lexicon developed  is costly to produce. The process becomes more elaborated when UNIX commands are involved owing to absence of semantics between them. Therefore, better performances can be achieved in future by developing a more thorough semantic lexicon that can represent semantic intelligence better. One of the inevitable reason the other works might not prefer using a neural net for this task despite its accuracy and efficiency might be attributed to the fact that good quality and a higher quantity of data is required which is indeed quite necessary to reach greater accuracy. In this work, we proposed and experimented a unique strategy involving the use of both KB and Seq2seq prediction while achieving a commendable accuracy yet maintaining a general workflow for the complete work without any specific modifications to the KB in consideration. Future direction includes the identification of a more suitable deep learning algorithm for such kind of problem statement. 	           
"," 		Despite being an open-source operating system pioneered in the early 闁90s, UNIX based platforms have not been able to garner an overwhelming reception from amateur end users. One of the rationales for under popularity of UNIX based systems is the steep learning curve corresponding to them due to extensive use of command line interface instead of usual interactive graphical user interface. In past years, the majority of insights used to explore the concern are eminently centered around the notion of utilizing chronic log history of the user to make the prediction of successive command. The approaches directed at anatomization of this notion are predominantly in accordance with Probabilistic inference models. The techniques employed in past, however, have not been competent enough to address the predicament as legitimately as anticipated. Instead of deploying usual mechanism of recommendation systems, we have employed a simple yet novel approach of Seq2seq model by leveraging continuous representations of self-curated exhaustive Knowledge Base  to enhance the embedding employed in the model. This work describes an assistive, adaptive and dynamic way of enhancing UNIX command line prediction systems. Experimental methods state that our model has achieved accuracy surpassing mixture of other techniques and adaptive command line interface mechanism as acclaimed in the past.",117
" Recent work in neural machine translation  has led to dramatic improvements in both research and commercial systems. However, a key weakness of contemporary systems is that performance can drop dramatically when they are exposed to input perturbations , even when these perturbations are not strong enough to alter the meaning of the input sentence.  Consider a Chinese sentence, %``{UTF8}{gkai}{鏉╂瑦鐏︽鐐存簚濞屸剝婀侀幘鐐扮瑐娴ｅ繐顔峔textcolor{red}{閹存澊閸栧娅岄敍灞界杽閸︺劍妲告總鍥姉閵嗗'' ``zhejia feiji meiyou zhuangshang zhujia huo yiyuan, shizai shi qiji''. If we change the word ``huo~ continuous noise which is modeled as a real-valued vector applied to word embeddings, and  discrete noise which adds, deletes, and/or replaces characters or words in the observed sentences. In both cases, the challenge is to ensure that the noisy examples are still semantically valid translation pairs. In the case of continuous noise, it only ensures that the noise vector lies within an -norm ball but does not guarantee to maintain semantics. %an  constraint on the noise vector does not guarantee that the semantics are preserved. %\jacob{can we say any more about why continuous noise doesn't work well?} While constructing semantics-preserving continuous noise in a high-dimensional space proves to be non-trivial, state-of-the-art NMT models are currently based on adversarial examples of discrete noise. For instance,  generate adversarial sentences using discrete word replacements in both the source and target, guided by the NMT loss. This approach achieves significant improvements over the Transformer on several standard NMT benchmarks. Despite this promising result, we find that the generated adversarial sentences are unnatural, and, as we will show, suboptimal for learning robust NMT models.  %But despite the empirical success of this approach, the generated adversarial sentences are unnatural \jacob{disfluent?}, and, as we will show, suboptimal for learning robust NMT models.  In this paper, we propose { adversarial sentences from the vicinity distribution according to their interpolated embeddings. Our intuition is that the introduced vicinity distribution may increase the sample diversity for adversarial sentences. Our idea is partially inspired by \mixup, a technique for data augmentation in computer vision, and we also use a similar vicinity distribution as in \mixup~ to augment the authentic training data. Our { %\jacob{to Lu: yes I'm fine with that, as long as it's clear what the new distribution is. The understanding that I tried to convey in this paragraph -- which may be wrong! -- is that the Cheng et al 2019 paper introduced the method of building adversarial examples from discrete noise, and that the contribution of the present submission is the combination of   and  .} %In both cases, the central difficulty is how to specify a ``vicinity'' distribution around individual examples that adds diversity while preserving semantics.  %In this paper we consider a new approach, which we call ancyname, based on interpolating between  of training examples.  %This basic idea is applied in two ways.  %First, given a pair of authentic training instances, we construct novel instances by interpolating between their embeddings, similar to the \mixup approach in computer vision, but with the critical extension to sequence-to-sequence learning. Second, we apply this same interpolation approach to pairs of adversarial examples, which are generated from discrete noise \jacob{I see that this helps, but why?}.  %This provides a novel characterization of the vicinity distribution around the training set, increasing diversity while preserving semantics.  %\jacob{i'm struggling with this paragraph and have tried a fresh start above} %% J: Summarizing this paragraph to make sure I understand it %% 1. we introduce a vicinity distribution around each training example  %% 2. Unlike Cheng et al 2019, we generate adversarial examples in the discrete space and then virtual adversarial examples in the continuous space. %% 3. We think this increases diversity %% 4. Mixup: another approach for sampling in the continuous space, using pairs of authentic examples %% 5. We sample from both vicinity distributions  %In this paper, we propose { adversarial sentences from the vicinity distribution according to their interpolated embeddings. %Our intuition is that the introduced vicinity distribution increases the sample diversity for adversarial sentences. Our approach is partially inspired by mixup, a technique for data augmentation in computer vision; a contribution of this paper is to extend this work to the sequence-to-sequence task. % Our ancyname approach finally trains on the embeddings sampled from both vicinity distributions. As a result, we augment the training using virtual sentences in the feature space as opposed to the data space. The novelty of our paper is the new vicinity distribution for adversarial examples and the augmentation algorithm for sequence-to-sequence learning.  Extensive experimental results on three translation benchmarks  show that our approach achieves significant improvements of up to  BLEU points over the Transformer, outperforming the former state-of-the-art in adversarial learning  by up to  BLEU points. When compared with widely-used data augmentation methods, we find that our approach yields better performance even without using extra corpora. We conduct ablation studies to gain further insights into which parts of our approach matter most. In summary, our contributions are as follows:         %------------------------------------------------------------------------------------------------------------   We have presented an approach to augment the training data of NMT models by introducing a new vicinity distribution defined over the interpolated embeddings of adversarial examples. To further improve the translation quality, we also incorporate an existing vicinity distribution, similar to \mixup for observed examples in the training set. We design an augmentation algorithm over the virtual sentences sampled from both of the vicinity distributions in sequence-to-sequence NMT model training. Experimental results on Chinese-English, English-French and English-German translation tasks demonstrate the capability of our approach to improving both translation performance and robustness.  
"," In this paper, we propose a new adversarial augmentation method for Neural Machine Translation . The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, of which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, { achieves significant improvements over the Transformer , and substantially outperforms other data augmentation techniques  without using extra corpora.",118
" Text generation, as a basic natural language processing task, has many applications, such as dialogue robots , machine translation , paraphrasing  and so on. With the rise of deep learning, different neural networks are introduced to generate text.  For example, researchers use the recurrent neural network   to train the language model because of its capability to process sequential data. However, the RNN suffers from the gradient vanishing problem  when the sequence becomes longer. To address this problem, Long Short-Term Memory  is further adopted as a sequential neural network model to generate sentences.  Lately, the Generative Adversarial Networks  framework  has been introduced into the NLP community. GAN has two different models for completing the data-generating task. One of them is Generator G, which is responsible for generating data, and another one is discriminator D, which determines whether the input data is the real data or not. The generator G continuously optimizes generated data based on the judgment of discriminator D. After several epochs, the generated data will become more realistic.  However, GAN was originally designed to process continuous data, and using discrete data as input would make it impossible to update the gradients of the GAN framework. To process discrete data, several variants of the GAN model for generating text have been proposed. These GAN variants could achieve good performances in text generation task, such as MaskGAN , RankGAN , and TextGAN .   In order to make these models fit the distribution of real text data better, the number of parameters of text generation models based on neural network are increased, which means that training these neural network models often takes a lot of time even using GPU. Conventionally, topic-related text generation models incorporate an arbitrary topic as an input by adopting mechanisms like attention . Therefore, each time when the user wants to generate new sentences with another topic or sentimental tendency, the text generation models have to be retrained with all parameters to satisfy new requirements. In some scenarios, e.g., news generation, spending lots of time retraining model is not practical and the user wants new responding quickly.   To tackle this problem, a novel text generation model based on GAN is proposed, which is called User-Defined Generative Adversarial Networks . The key idea is to separate the sentence syntax model as the basic model and the topic-related model as a higher-level model, and these two could be trained independently from each other. So, when the topic or other user-defined information is modified, e.g., sentimental tendency, only one of both models needs to be retrained. In this way, once the basic syntax model is established, the following training will become much faster, since only the higher-level model has to be retrained.  In our proposed method, the discriminator is constructed based on this idea. One of the discriminators called discriminator-general, which learns to determine the proper context information and whether the input sentence is a valid syntactic structure. Another discriminator is called the discriminator-special, which ensures the output is user-defined. Inspired by SeqGAN , we use the evaluation results of the generated text from discriminators as a reward to guide the generator to select future actions, which is to generate an updated word.   For training the discriminator-special, it will take feature vectors as input, instead of sentences. The feature vector is defined based on the sentiment detection and topic relevance of generated sentence. The cosine similarity based on TF-IDF and length penalty are jointly adopted to represent topic relevance.   Note that the UD-GAN is designed to be more practical to generate short paragraphs, which means sentences generated by it should be context-aware and behave like a paragraph together with surrounding sentences. To achieve this idea, discriminator-general is designed with hierarchical multiple LSTM layers. The LSTM at the top of the network processes paragraph-level information while the bottom LSTMs process sentence-level information.  The organization of the paper is as follows: First, we discussed the related works of our method in the section 2. The proposed method is described in the Section 3, including the feature extraction and model definition and training. In the Section 4, the experiment settings and evaluation results of the comparing methods are depicted. Finally, the concluding remarks and future works are described in the Section 5.  [ht]        [1] \STATE {Initialize ,  and  with random weights,  and }  \STATE {Pre-train  using MLE on real text data set} \STATE {Generate negative samples using  to train  and } \STATE {Generate synthetic positive samples to train } \STATE {Minimizing the cross entropy to pre-train } \STATE {Minimizing the cross entropy to pre-train} \FOR{ to } \FOR{ to } \STATE {Generate a sequence G\thetak\leftarrow 1PG\thetaD\phil\leftarrow 1TG\thetaD\gammaG\thetaD\gamma\thetaD\gammaD\phi \STATE {Do 8G\thetaD\gamma$ with negative and synthetic feature vectors via Eq.} \ENDFOR \ENDFOR      In this paper, we propose a UD-GAN method to re-train text generation model more efficiently to generate sentences that are consistent with the new user-defined topic and sentimental tendency. We compared the accuracy and fluency of sentences generated by UD-GAN with other GAN-based text generation models. The experimental results showed that sentences generated by UD-GAN are competent. Meanwhile, UD-GAN takes much less time in the re-train stage than other models. According to experimental results, UD-GAN can also successfully generate sentences related to the user-defined topic and sentimental tendency, while baselines does not have this capability. Besides, UD-GAN can also generate paragraph-level text.   However, the sentences generated by UD-GAN are still inferior to the state-of-the-art method, i.e., LeakGAN, in terms of fluency. And the current paragraph-level information used here does not include complex linguistic information, such as the order of sentences. In future work, we will try to maintain the existing advantages of UD-GAN while improving the readability of generated text.   
","   This study focused on efficient text generation using generative adversarial networks . Assuming that the goal is to generate a paragraph of a user-defined topic and sentimental tendency, conventionally the whole network has to be re-trained to obtain new results each time when a user changes the topic. This would be time-consuming and impractical. Therefore, we propose a User-Defined GAN  with two-level discriminators to solve this problem. The first discriminator aims to guide the generator to learn paragraph-level information and sentence syntactic structure, which is constructed by multiple-LSTMs. The second one copes with higher level information, such as the user-defined sentiment and topic for text generation. The cosine similarity based on TF-IDF and length penalty are adopted to determine the relevance of the topic. Then, the second discriminator is re-trained with generator if the topic or sentiment for text generation is modified. The system evaluations are conducted to compare the performance of the proposed method with other GAN-based ones. The objective results showed that the proposed method is capable of generating texts with less time than others and the generated text are related to the user-defined topic and sentiment. We will further investigate the possibility of incorporating more detailed paragraph information such as semantics into text generation to enhance the result.",119
"  Text prediction is a challenging problem in machine learning and natural language processing, while at the same time there is a growing need for novel techniques for efficient and accurate text prediction in several application domains, such as in dictation and typing systems for people with disabilities or clinical text prediction for healthcare practitioners . More concretely, with text prediction we refer to the task of predicting the next block of text in an online fashion, where block can refer to different text granularity levels, e.g., sentences, words, syllables, or characters  .  The main focus of this paper is medical text with the concrete task of predicting the next word given an incomplete text. We also refer to this problem as  for medical text. When applied in the clinical setting , physicians can vastly benefit from a fast and accurate predictive keyboard system, since it can assist them with  a speedy compilation of the intended text,  means for prevention of potential text errors due to work overload,  means for speedier patient discharge.  Initial efforts towards solving the predictive keyboard problem for radiology reports are described by Eng and Eisner 2004 , where a 3-Gram language model achieves an average keystroke reduction of a factor of 3.3. Following this line of research, we employed N-Gram-based statistical language modeling, which refer to as N-GLM, to predict the next word of a clinical text. We vary N from 1 to 10 and show that 4-Gram models achieve 38\% accuracy when predicting the next word in a clinical text, outperforming other N-GLMs. Observe that accuracy in this case measures the fraction of times when the next word was predicted correctly, hence inducing an equivalent typing speedup at the word level. We additionally investigated two neural language models that employ  a Recurrent Neural Network  language model based on Long-Short Term Memory, which we refer to as LSTMLM  and  a Gated Recurrent Unit  based language model, which we refer to as GRULM. This model achieves higher levels of accuracy compared to 4-GLM, since our experimental evaluation demonstrates that accuracy can reach up to 51.3\% . An example of the output of this task is depicted in Table , where we can observe the next word predictions made by LSTMLM and 4-GLM, with the correctly predicted words indicated in '[]'.  Next, we outline the related work in the area of clinical text prediction, followed by a summary of our contributions.  [!t]     |}\hline          LSTMLM & ""the lungs are clear without [evidence] [of] focal infiltrate [or] [effusion] [there] [is] [no] [pneumothorax] [the] [visualized] [bony] [structures] [reveal] [no] [acute] [abnormalities]""   \\\hline         4-GLM  & ""the lungs are [clear] without evidence [of] [focal] infiltrate or effusion [there] [is] [no] pneumothorax [the] visualized bony [structures] reveal [no] [acute] [abnormalities]""\\\hline               The study of the benefits of computer-assisted text generation dates back to more than two decades ago . When applied to clinical notes, such as radiology reports, a statistical 3-Gram language model  achieved substantial keystroke reductions . Recently, an even simpler 3-Gram language model  outperformed the earlier  while also decreasing the typing time for the clinician by one third . These results demonstrate that N-Gram models can provide promising solutions to our problem, and hence in this paper we provide a more extensive evaluation of these models on medical text. Besides computer-assisted typing, language models have also been used for spelling correction in clinical notes . This work does not focus on spelling correction, but what these works verify is that the words suggested by the language model during typing, are also checked for their correctness , hence the generated text will be of equal or even higher quality.   With the recent advance of deep learning, deep neural networks, such as Long-short Term Memory   models, have improved the performance in natural language processing  tasks of the biomedical field, such as Name Entity Recognition  ; medical codes prediction ; relation classification ; predicting hospital readmission . And language modeling is also part of this advent, since it is often employed as a pre-training step . For the task of next word prediction in a medical setting, however, neural language modeling is heavily under explored. To our knowledge, the only application of a neural language model was that of a baseline LSTM network , which was improved when structured information from electronic health data  was integrated . The authors reported 8\% Accuracy  for the baseline LSTM, which ranks it much lower than competing statistical language models . However, neural networks have been reported to outperform statistical language modeling in non-medical domains .  In this work, we compare statistical and neural language modeling, a comparison which has not been studied before, and we show that the neural approach outperforms the statistical approach in next word prediction by a large margin.    The main contributions of this paper can be summarized as follows:  We highlight the importance of the problem of keyword prediction for clinical text, and demonstrate how language models can be employed for providing scalable solutions to this problem;  We provide an extensive benchmark on clinical text obtained from two real-world medical datasets by comparing the performance of the N-GLM model for different values of N in terms of accuracy and keystroke reduction;  We additionally compare an RNN language model based on LSTM and GRU on the same datasets and demonstrate their superiority against N-GLMs as they can achieve an accuracy of up to 51.3\%, indicating a speedup  of the same degree, and a keyword reduction of up to 41.12\%, indicating a speedup  of the same degree.    We highlighted the importance of predictive keyboard for medical text and demonstrated the benefits for the physicians in terms of speedups in completing their clinical text reports. Our experimental evaluation on radiology reports from two real-world medical datasets showed that neural language models can achieve an accuracy of up to 51.3\ , which implies that the obtained speedups correspond to a similar factor at the word level. Directions for future work include the investigation of alternative statistical and deep learning models, the consideration of additional medical datasets , and measuring the speedups in a real-world application with models deployed in healthcare systems.  
"," A language model can be used to predict the next word during authoring, to correct spelling or to accelerate writing . Language models, however, have only been applied in a very small scale to assist physicians during authoring . But along with the assistance to the physician, computer-based systems which expedite the patient's exit also assist in decreasing the hospital infections. We employed statistical and neural language modeling to predict the next word of a clinical text and assess all the models in terms of accuracy and keystroke discount in two datasets with radiology reports. We show that a neural language model can achieve as high as 51.3\% accuracy in radiology reports . We also show that even when the models are employed only for frequent words, the physician can save valuable time.",120
"  Despite suggetsions that the stock market is not predictable , many investors and researchers seek methods that can provide market fluctuation predictions to aid investment strategy. Advances in Machine Learning  and Natural Language Processing  have led to a shift in focus from technical to fundamental analysis. This new approach uses data such as news articles and historical stock prices, and is based upon the Efficient Market Hypothesis which states that an asset price reflects all available information . Advances in predictive models have also led to more complex trading strategies. Most research regarding trading strategies and ML is focused on technical analysis , however, some works consider news and other fundamental data as part of their strategy . The development of trading strategies based solely on fundamental data are rare throughout the relevant literature.  Early research shows no relation between headlines and stock volatility , however the development of more advanced predictive models and availability of larger datasets has led to more accurate market trend predictions based on headlines. Although complete news articles  or social media content  are used in some works, the use of headlines has become most common in this area of research due to the belief that they contain less noise than other sources of textual data . Headlines are commonly sourced from major financial news outlets such as the Wall Street Journal . A wide range of prediction targets are considered throughout the relevant literature, including major indices such as the S\&P 500  and collections of individual companies . The time-span of market fluctuations analysed is also varied. For example Mittermayer  focuses on intra-day predictions whereas long-term trends are briefly considered in the work of Ding . Methods such as Support Vector Machines  and complex Decision Trees  remain popular for predictive tasks of this nature. These commonly use a Bag of Words  feature representation approach, where words are represented independently without consideration of word-order or context. Variations of this method include -gram BoW, where phrases of length  are extracted as features as opposed to single words, and Term Frequncy-Inverse Document Frequency , which introduces consideration of a word's frequency within a sentence and across the entire collection. However these representations typically lead to sparsity issues when applied to a large corpus . Probabalistic approaches such as the Naive Bayes method can also be applied to tasks of this nature .   The development of Artificial Neural Networks  has provided new classification and feature representation methods for text-based tasks. An ANN is a collection of nodes known as neurons that are interconnected in layers. Originally proposed by Rosenblatt , the architecture is based on the transmission of signals and firing of biological neurons in a nervous system. Variations on the basic ANN architecture have been made to produce types of neural network with additional mathematical features suited to different tasks. Convolutional Neural Networks  have gained popularity in text-based tasks. Commonly used for image recognition, CNNs  utilise a convolutional layer to detect patterns in input data that can be used for accurate classification or prediction. For example, in image detection these patterns may represent edges and shapes of a specific object depicted by its pixel values. CNNs have demonstrated state-of-the-art performance in multiple NLP tasks, including sentence classification  and sentence modelling . Some applications of CNNs to market prediction exist in the literature, both for major indices  and discrete price prediction .  This work presents a CNN for predicting next-day stock price fluctuations of three major technology companies using headlines relating to each company. Next-day returns are used due to the inability to access the large amounts of historical intra-day stock price data required for intra-day fluctuation prediction. However the effect of news headlines has been found to resonate during the next-day period . Experiments are conducted to identify an optimal model configuration for trend classification in terms of the word-embedding  and convolutional layer states. Using class predictions from these experiments, trading simulations are presented based on day-averaged predictions for each asset. Finally, modifications to both the baseline trading strategy and labelling of headlines are made with the intention of reducing risk present in simulated trading.     In this work, a Convolutional Neural Network is implemented to predict next-day stock fluctuations for three technology-based assets. Word-embeddings in three states; self-learnt, static and non-static, are considered as well as single-width and multi-width convolutional layers. Experiments seeking to identify an optimal configuration in terms of accuracy and F1-score showed the presence of a filter width  or  as optimal. This result arises as key phrases depicting the headline's overall sentiment can be evaluated in their entirety. Word-embeddings in the non-static state were found to be able to adapt to the specific context of the task whilst retaining relationships based on general context and hence provide the best classification performance. A multi-width non-static implementation was found to be the optimal configuration of the CNN architecture, leading to a testing accuracy of 61.7\ . However the benefit of using multiple filter widths compared to a single width was not compelling in the conducted experiments.  A simple trading strategy using the mean sigmoid prediction for headlines relating to each asset on each testing day was implemented. The optimal model configuration for classification was found to produce the best simulated trading results in terms of returns and two common trading performance metrics; PP and ATP. This configuration was found to more than triple an initial investment over the 838 testing day period.  Two methods to reduce the perceived risk in investments made across the testing set were developed. The implementation of a moderately strict buy threshold led to some reduction in risk, however further increase in this threshold resulted in increased risk compared to the baseline strategy. Alteration of the task to multi-class showed no reduction in risk on its own, but the combination of this with a strict buy threshold yielded an ATP more than double that achieved using the baseline strategy. Both of the discussed methods revealed downfalls of basing market predictions solely on company headlines. Therefore, combining methods presented in this work with predictions based on the technical analysis of stock trends should be considered in further research. Furthermore, headlines describing the general state of the economy could be considered in parallel to company-specific headlines.   The training of the discussed model on separate collections of headlines grouped by business sector  should be undertaken in further work to form collections of embeddings and weights tuned to each sector. Subsequent testing headlines can then be evaluated using the set of weights and embeddings corresponding to the sector of the asset in question. This method has the ability to detect phrases based on the specific context of each sector as has been demonstrated for the technology sector in this work. However further work is needed to validate if this is beneficial compared to training a single collection of embeddings and weights for all sectors.   Conflict of Interest: The authors declare that they have no conflict of interest.   {} 
"," This work presents a Convolutional Neural Network  for the prediction of next-day stock fluctuations using company-specific news headlines. Experiments to evaluate model performance using various configurations of word-embeddings and convolutional filter widths are reported. The total number of convolutional filters used is far fewer than is common, reducing the dimensionality of the task without loss of accuracy. Furthermore, multiple hidden layers with decreasing dimensionality are employed.  A classification accuracy of 61.7\% is achieved using pre-learned embeddings, that are fine-tuned during training to represent the specific context of this task. Multiple filter widths are also implemented to detect different length phrases that are key for classification. Trading simulations are conducted using the presented classification results. Initial investments are more than tripled over a 838 day testing period using the optimal classification configuration and a simple trading strategy. Two novel methods are presented to reduce the risk of the trading simulations. Adjustment of the sigmoid class threshold and re-labelling headlines using multiple classes form the basis of these methods. A combination of these approaches is found to more than double the Average Trade Profit  achieved during baseline simulations.",121
"    The fields of Programming Languages  and Natural Language Processing  have long relied on separate communities, approaches and techniques. Researchers in the Software Engineering community have proposed the Software Naturalness  hypothesis which argues that programming languages can be understood and manipulated with the same approaches as natural languages.   The idea of transferring representations, models and techniques from natural languages to programming languages has inspired interesting research. However, it raises  the question of whether language model approaches based purely on source code can compensate for the lack of structure and semantics available to graph-based approaches which incorporate compiler-produced features. The application of language models to represent the source code has shown convincing results on code completion and bug detection.  The use of structured information, such as the Abstract Syntax Tree , the Control Flow Graph  and the Data Flow Graph , has proven to be beneficial for vulnerability identification in source code. However, the extraction of structured information can be very costly, and requires the complete project. In the case of the C and C++ languages, this can only be done through complete pre-processing and compilation that includes all the libraries and source files. Because of these requirements, approaches based on structural information are not only computationally expensive but also inapplicable to incomplete code, for instance a pull request.  This work explores the software naturalness hypothesis, employing the pre-training/fine-tuning paradigm widely used with transformer-based language models   to address tasks involving the analysis of both syntax and semantics of the source code in the C language. To investigate syntax, we first introduce a novel sequence labeling task that directly probes the language model's understanding of AST, as produced by Clang .  Furthermore, we investigate the capabilities of LMs in handling complex semantics in the source code through the task of vulnerability identification . All of our experiments involved LMs pre-trained from scratch on the C language source code of 100 open source repositories. The use of language models with source code rather than natural language leads to multiple challenges.  Data sparsity is a major problem when building language models, leading to out-of-vocabulary  terms and poor representations for rare words.  These issues are particularly severe for PL because variable and function names can be of almost arbitrary length and complexity.  There is a tradeoff between the granularity of tokenization and availability of long-range context for the LM.  Fine-grained tokenizations break identifiers into many small common tokens, alleviating issues with rare tokens, but at the risk of spreading important context across too many tokens. We address the OOV and rare words problems by investigating three choices for tokenization, which span the context/vocabulary size tradeoff  from the extreme of character-based tokenization to subword tokenization styles familiar to the NLP community. We indicate that the choice of the pre-training objective is closely connected to the choice of the vocabulary. In particular, with character based tokenization the pre-training task seems too easy. We introduce a more difficult, whole word masking  pre-training objective.  In our experiments, we show that our language model is able to effectively learn how to extract AST features from source code. Moreover, we obtain compelling results compared to graph-based methods in the vulnerability identification task. While current approaches to code analysis depend heavily on features derived from the AST, our approach works using raw source code without leveraging any kind of external features. This a major advantage, since it avoids a full compilation phase to avoid the extraction of structural information. Indeed, our model can identify vulnerabilities during the software development stage or in artifacts with incomplete code, which is a valuable feature to increase productivity. We show the merits of simple approaches to tokenization, obtaining the best results using the character based tokenization with WWM pre-training.  The contributions of this work are summarized as follows:     1) we investigate the application of transformer-based language models for complex source code analysis tasks;     2) we demonstrate that character-based tokenization and pre-training with WWM eliminate the OOV and rare words;      3) this work is the first to investigate whether such language models can discover AST features automatically;     4) our language model outperforms graph-based methods that use the compiler to generate features. \documentclass{article}  % if you need to pass options to natbib, use, e.g.: %     \PassOptionsToPackage{numbers, compress}{natbib} % before loading neurips_2020  % ready for submission % \usepackage{neurips_2020}  % to compile a preprint version, e.g., for submission to arXiv, add add the % [preprint] option: %     \usepackage[preprint]{neurips_2020}  % to compile a camera-readyi version, add the [final] option, e.g.: %     \usepackage[final]{neurips_2020}  % to avoid loading the natbib package, add option nonatbib: \usepackage[preprint, nonatbib]{neurips_2020} %\usepackage[nonatbib]{neurips_2020}  \usepackage[utf8]{inputenc} % allow utf-8 input \usepackage[T1]{fontenc}    % use 8-bit T1 fonts \usepackage{hyperref}       % hyperlinks \usepackage{url}            % simple URL typesetting \usepackage{booktabs}       % professional-quality tables \usepackage{amsfonts}       % blackboard math symbols \usepackage{nicefrac}       % compact symbols for 1/2, etc. \usepackage{microtype}      % microtypography \usepackage[colorinlistoftodos]{todonotes}  \usepackage{adjustbox} \usepackage{graphicx} \usepackage{tabularx} \usepackage{xspace}  \def\tokenkind{{{FFmpeg\xspace} {QEMU\xspace} {LM\xspace} {VI\xspace}  \makeatletter [1]{%   \textsuperscript{\@fnsymbol{#1}}% } \makeatother    \title{Exploring Software Naturalness through\\Neural Language Models}  \author{Luca Buratti\thanks{Equal Contribution. In no particular order.} \\ IBM Research \\  \And Saurabh Pujar\printfnsymbol{1} \\ IBM Research \\  \And Mihaela Bornea\printfnsymbol{1} \\ IBM Research \\  \And Scott McCarley\printfnsymbol{1} \\ IBM Research \\  \And Yunhui Zheng \\ IBM Research \\  \And Gaetano Rossiello \\ IBM Research \\  \And Alessandro Morari \\ IBM Research \\  \And Jim Laredo \\ IBM Research \\  \And Veronika Thost \\ IBM Research \\  \And Yufan Zhuang \\ IBM Research \\  \And Giacomo Domeniconi \\ IBM Research \\  \\ }     The Software Naturalness hypothesis argues that programming languages can be understood through the same techniques used in natural language processing. We explore this hypothesis through the use of a pre-trained transformer-based language model to perform code analysis tasks. Present approaches to code analysis depend heavily on features derived from the Abstract Syntax Tree  while our transformer-based language models work on raw source code. This work is the first to investigate whether such language models can discover AST features automatically. To achieve this, we introduce a sequence labeling task that directly probes the language model's understanding of AST. Our results show that transformer based language models achieve high accuracy in the AST tagging task. Furthermore, we evaluate our model on a software vulnerability identification task. Importantly, we show that our  approach obtains vulnerability identification results comparable to graph based approaches that rely heavily on compilers for feature extraction.                       Our model architecture is a multi-layer bidirectional transformer  based on BERT , which we refer to as  = [x_{1}=CLS, x_2,\ldots,x_{T-1},x_{T}=SEP]\mathbf{H} = [h_{1}=h_{CLS},h_2,  \ldots,h_{T}=h_{SEP}] ^{T\times 768}h_t ^{768}\mathbf{W_{LM}} ^{768\times |V|}V\mathbf{W_A} ^{768\times |V_{AST}|}V_{AST}h_{CLS}\mathbf{w} ^{768}$. We fine-tune using the cross entropy between the true labels and   evaluated across all context windows.    This work explores the  software naturalness hypothesis by using language models on multiple source code tasks. Unlike graph based approaches that use structural features like the AST, CFG and DFG, our model is built on raw source code. Furthermore, we show that the AST \tokenkind and \cursorkind can be learned with the LM. LMs can work even better than graph based approaches for VI because they avoid the computational cost and requirements of full compilation. This makes our approach suitable for a wider range of scenarios, such as pull requests. We propose two character based tokenization approaches that solve the OOV problem while having very small vocabularies. We suggest ways to improve them with aggregation and WWM. These approaches work just as well and sometimes even better than a subword tokenizer like sentencepiece that has been previously explored for source code.  In future work we propose joint learning of the AST and VI tasks on top of LM to further improve code analysis, without using the compiler to extract structured information.
"," The Software Naturalness hypothesis argues that programming languages can be understood through the same techniques used in natural language processing. We explore this hypothesis through the use of a pre-trained transformer-based language model to perform code analysis tasks. Present approaches to code analysis depend heavily on features derived from the Abstract Syntax Tree  while our transformer-based language models work on raw source code. This work is the first to investigate whether such language models can discover AST features automatically. To achieve this, we introduce a sequence labeling task that directly probes the language model's understanding of AST. Our results show that transformer based language models achieve high accuracy in the AST tagging task. Furthermore, we evaluate our model on a software vulnerability identification task. Importantly, we show that our  approach obtains vulnerability identification results comparable to graph based approaches that rely heavily on compilers for feature extraction.",122
" % Authors wishing to code their contribution with \LaTeX{}, as well as those who have already coded with \LaTeX{}, will be provided with a document class that will give the text the desired layout. Authors are requested to adhere strictly to these instructions; {, then the LLNCS class should not give you any major difficulties. It will change the layout to the required LLNCS style (it will for instance define the layout of \verb|  Bidirectional neural LMs are a powerful instrument for different tasks including substitutes generation for WSI, but some tricks should be used to apply them to this task properly. We have proposed and compared several methods of combining forward and backward LMs for better substitutes generation. Also we have proposed a technique for selecting individual number of clusters per word and this improved results even further. We improved previous best results on two datasets from RUSSE 2018 WSI shared task for the Russian language by a large margin. Finally, we have compared several Russian LMs regarding their performance for WSI.  
", The abstract should summarize the contents of the paper using at least 70 and at most 150 words. It will be set in 9-point font size and be inset 1.0 cm from the right and left margins. There will be two blank lines before and after the Abstract. \dots,123
"   Cross-lingual learning aims to build models which leverage data from other languages to improve performance. This has been a long standing interest in the speech community~ which includes systems able to transcribe multiple languages~.  However, the vast majority of work in speech processing has focused on supervised cross-lingual training which requires labeled data in multiple languages. Transcribed speech is often much scarcer than unlabeled speech and requires non-trivial human annotation.  % Supervised pretraining for seq2seq~   Unsupervised representation learning, or pretraining, does not require labeled data and has received a lot of recent attention in computer vision~ after much success in natural language processing~.  For the latter, cross-lingual pretraining has been shown to be very effective, particularly, for low resource languages~. In speech processing, most work in this area has focused on monolingual unsupervised representation learning~.   In this paper, we focus on the cross-lingual setting by learning representations on unlabeled data that generalize across languages. We build on the pretraining approach of~ which jointly learns contextualized speech representations as well as a discrete vocabulary of latent speech representations. The latter serves to effectively train the model with a contrastive loss~ and the discrete speech representations are shared across languages~. Different to recent work on unsupervised cross-lingual pretraining, we fine-tune the Transformer part of the model instead of freezing all pretrained representations~ or feeding them to a separate downstream model~. We extend the work of~ by pretraining on multiple languages instead of just English and we experiment on top of a stronger baseline.   We evaluate \xlsr{} on 14 languages of the BABEL benchmark~ which is conversational telephone data and ten languages of CommonVoice~, a corpus of read speech . Multilingual pretraining outperforms monolingual pretraining in most cases, except for resource rich languages and we show that increased model capacity significantly closes the gap. % Moreover, pretraining on multiple languages still outperforms pretraining on just English when controlling for the amount of data. We also demonstrate that \xlsr{} representations can be fine-tuned simultaneously on multiple languages to obtain a multilingual speech recognition system whose performance is competitive to fine-tuning a separate model on each language~\autoref{sec:results}). % On CommonVoice, we report a relative phoneme error rate  reduction of 72\% compared to the previous best known results~.  % On BABEL we outperform prior supervised multilingual work on a comparable data setup by a relative 38\% in terms of character error rate~, and by a relative 16\% when measuring word error rate~;~\autoref{sec:results}).      In this work, we investigated unsupervised cross-lingual speech representations learned from the raw waveform. We show that pretraining on data in multiple languages improves both over monolingual pretraining as well as prior work, with the largest improvements on low-resource languages.  Fine-tuning the model on multiple languages at once enables a single multilingual speech recognition model competitive to individually fine-tuned models. Analysis of the discrete latent speech representations reveals that the model shares capacity across languages and particularly so with related languages.    
"," This paper presents \xlsr{} which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72\% compared to the best known results. On BABEL, our approach improves word error rate by 16\% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.\footnote{}",124
" In 2019, Americans spent \2781nn89.783$\%.   One of the key criticisms of deep learning  methods is  that the decision process of the DL model is intractable thereby making it difficult to understand the  reasoning behind its decisions. For applications such as AD detection, it is imperative that some form of reasoning  be provided, because of the human angle involved.  Hence in this work, .  The main contributions of our work are:   The rest of the paper is organized as follows. Section, describes related work. Section, discusses the proposed explainable deep learning models. Section, details the datasets, the experimental set up, data bias compensation mechanism, example explanations and discussions of the results. Section discuses the conclusions we draw from this work.     Modified Addded icnn We proposed three explainable architectures using CNN  and attention to detect Alzheimer's disease using two kinds of features: part-of-speech and language embeddings. One architecture uses only the PoS feature, one uses only the universal sentence embedding and the third is a unified architecture that uses both of these features. We propose the use of attention layers and 1-D CNN layer to capture explanations at 2 levels: one each at the intra-feature level  and inter-feature-class level. The  intra-feature level attention weights and 1-D CNN filters capture the relative importance the  model places on the individual features in the category, whereas the  inter-feature level attention weights gives us an idea of the relative  importance that the model placed between the two classes of features.  Extensive testing on the popular DementiaBank datasets and comparisons with several recently published models as well as minor modifications of our own models show that the C-Attention-FT architecture performs best in terms of accuracy and F1 scores, and the C-attention-FT+Embedding performs best in terms of precision and AUC while at the same time being able to generate explanations of the action of the AI.  We also show by examples how to generate explanations for the actions of the models.  Our results agree with some of the previous work that shows that AD patient's tend to use more  pronouns instead of nouns.  Our work thus is an inexpensive, non-invasive, explainable AI model that can detect AD at good performance metric. Since it is based on only the spoken language, it can be potentially easily implemented in an app setting there by giving the option of taking it at home. This in turn can have a positive impact on patient compliance and  therefore early detection of AD. 
"," \label{sec:abs} % In this work we propose three explainable deep learning architectures to automatically detect patients with Alzheimer闁炽儲鐛 disease based on their language abilities. The architectures use:  only the part-of-speech features;  only language embedding features and  both of these feature classes via a unified architecture.  We use self-attention mechanisms and interpretable 1-dimensional Convolutional Neural Network  to generate two types of explanations of the model's action: intra-class explanation and inter-class explanation. The inter-class explanation captures the relative importance of each of the different features in that class, while the inter-class explanation captures the relative importance between the classes. Note that although we have considered two classes of features in this paper, the architecture is easily expandable to more classes because of its modularity. Extensive experimentations and comparison with several recent models show that our method outperforms these methods with an accuracy of $92.2$\% and F1 score of $0.952$ on the DementiaBank dataset while being able to generate explanations. We show by examples, how to generate these explanations using attention values.",125
"  Alexa's ASR platform relies on a large hand-curated lexicon which is comprised of word-pronunciation pairs. This lexicon can never provide complete coverage over the vocabulary as it is often not worth the time or cost required to create these phonetic sequence mappings compared to how infrequently certain words occur during any given Alexa interaction. % too long, can also mention pretty high G2P accuracy  Grapheme to Phoneme systems, on the other hand, can learn these mappings automatically with high accuracy and are responsible for transcribing any out-of-vocabulary  tokens into phonemic representations.  These phonemic representations are an important component that lies between the language model and acoustic model of an ASR system. These OOV tokens often include rare and foreign words, the amount of which may vary depending on the language.  The challenge in designing the G2P system is to create a many-to-many mapping system that will learn not only the mapping between one grapheme and one phoneme, but also where one phoneme is represented by multiple graphemes . And for certain languages like English, these mappings can be inconsistent and ambiguous, especially in the case of names and foreign words.  Sequence to sequence  neural network models are one such way of learning the mappings between graphemes and phonemes where the input and output sequence can vary in length. Originally designed for machine translation, they have been applied on wide variety of problems, such as generative language models.  More recent Seq2Seq models heavily incorporate attention mechanism  and residual learning, while other models use encoder-decoder architectures that are recurrent  or self-attention  based.   Recurrent seq2seq models pose a distinct advantage due to their ability to take in the input history when determining the output state, often outperforming n-gram models on classification tasks due to an n-gram閳ユ獨 heavy dependence on the previous n graphemes . This means that Recurrent Neural Networks  are better suited for sequence problems where longer term context and 閳ユ笩oft閳 input embeddings are important. Long-Short-Term Memory  networks  are in turn better at handling longer sequences and can have more layer depth as they are less prone to diminishing and exploding gradients. Bi-directional LSTMs that consider both past and future contexts have become increasingly popular over RNNs or uni-directional LSTMs that only consider past contexts. % remove LSTM part   Seq2Seq models can be trained on multiple languages at once and used in multi-task and multi-modal learning scenarios. In context of G2P conversion, they allow joint learning of the alignment and translation of graphemes and phonemes in an end-to-end fashion. Therefore, they are a natural fit for our multilingual G2P task, especially since we can train it on large pronunciation lexicons with relatively short sequences lengths.  Seq2Seq models have been found to perform better on these short sequences than compared to very long sequences  . However, until now neural G2P models have not shown superior results on their own , compared to traditional joint-sequence based n-gram models for G2P.  In this paper, we examine whether Seq2Seq LSTM models perform better compared to traditional joint-sequence based n-gram models, given the latest advancements in Seq2Seq modeling for single language pairs. In addition, we investigate whether we can build a single multilingual G2P model which may outperform individual models trained on single language lexicons by utilizing transfer learning thereby improving performance on under-resourced languages and foreign words from different locales. The goal of such a system is to have a single multilingual model that matches or improves over the results of monolingual models without the degradation in accuracy introduced by using multilingual dataset. In particular, the model must be able to distinguish between languages where the same grapheme is paired to different phones, such that the model does not learn a single pairing and apply this pairing to all instances of the grapheme, regardless of input language.  Thus, we wish to avoid situations where a larger lexicon overwhelms a smaller lexicon and erroneously labels a particular grapheme. By using Seq2Seq LSTM models we achieve better PER and WER than low-resource monolingual models, while reducing the potential influence larger-resource lexicons might have in a multilingual model. %In addition, we note that the model which accepts the language label  as input performs better than models which also take as input a language distribution.      In this work, we propose a novel approach to G2P that allows us to exploit large amounts of multilingual data to enhance prediction accuracy compared to monolingual models, especially for low resource languages. Single multilingual G2P model is much more flexible and easier to maintain in production environment, compared to monolingual n-gram based models - it's easier to fine-tune it with the latest training data updates or adapt to a new language.  We are also experimenting with several general techniques, such as model ensembling, self-training, selecting n-best hypothesis based on the score of separately trained confidence model, optimizing on sequence level metric  and using Transformers  as the core architecture. Each of these is bringing good accuracy improvements to the G2P model in general, although they are not directly related to the multilingual and low resource aspects we explored in this paper.     To start a new column  and help balance the last-page   column length use \vfill\pagebreak.   -------------------------------------------------------------------------  \vfill  \pagebreak    
","  Grapheme-to-phoneme  models are a key component in Automatic Speech Recognition  systems, such as the ASR system in Alexa, as they are used to generate pronunciations for out-of-vocabulary words that do not exist in the pronunciation lexicons .   Most G2P systems are monolingual and based on traditional joint-sequence based n-gram models . As an alternative, we present a single end-to-end trained neural G2P model that shares same encoder and decoder across multiple languages. This allows the model to utilize a combination of universal symbol inventories of Latin-like alphabets and cross-linguistically shared feature representations. Such model is especially useful in the scenarios of low resource languages and code switching/foreign words, where the pronunciations in one language need to be adapted to other locales or accents. We further experiment with word language distribution vector as an additional training target in order to improve system performance by helping the model decouple pronunciations across a variety of languages in the parameter space. We show 7.2\% average improvement in phoneme error rate over low resource languages and no degradation over high resource ones compared to monolingual baselines.",126
" ASK is an increasingly important part of Alexa user experience. In ASK work flow, the skill developer provides a set of  , and a list of , which can be mapped to actions, and a set of example phrases defining the grammar of an intent. .\\    From these examples NLU and language modeling  models are built for the skill. Note that it is up to the developer to anticipate all ways their users will interact with the skill. Interactions not covered by the provided examples often have much lower ASR recognition and NLU classification accuracy. Coming up with an exhaustive list of examples can be a hard task for the developer and incomplete coverage can be a frustrating experience to the user. In this work, we propose to use paraphrasing to expand the coverage of developer-provided examples, and thus reduce burden on skill developers and make skill interactions more natural to Alexa customers. Instead of relying on the developer to come up with an exhaustive list of examples for a given intent, in the proposed work flow, we will only require a few examples and then use paraphrasing model to generate other ways a customer might phrase the same command, and then use that data to build better NLU and LM models. Figure  gives an example of the desired paraphrases for a customer utterance.    Paraphrasing is used in various Natural Language Processing applications, such as natural language generation, summarization, information extraction, sentence compression and question answering. Traditional paraphrase generation methods exploit hand-crafted rules  or automatically learned complex paraphrase patterns , use thesaurus-based  or semantic analysis driven natural language generation approaches , or leverage statistical machine translation ; .  In this paper, we propose to use neural machine translation  as a simple and flexible approach to MT to address the paraphrase generation problem. We observe that in translation, there is not a single correct translation target, but rather several variants of the sentence, carrying the same meaning, or paraphrases. From this perspective, translation can be seen as paraphrasing the source sentence in a different language. Therefore, NMT is quite natural approach to paraphrasing. It has been shown to have comparable performance to the phrase-based translation systems , and it is very flexible and modular, allowing to reuse pre-trained components, such as word embeddings or other networks trained on different datasets.  The remainder of the paper is organized as follows:  Section  presents a brief overview of the sequence to sequence models and techniques used in this work, Section  describes the available data, Section  explains the experimental setup, Section  presents the evaluation results, Section  analyzes the results and discusses future work, Section  is conclusion. %, followed by .       In this work, we propose a novel approach to paraphrasing, and a training protocol that allows us to exploit large amount of out-of-domain data. We also propose a method for paraphrasing sentences with slots. We demonstrate the effectiveness of our model NER and IC tasks, showing substantial improvement over the baseline. This allows us to reduce the amount of needed manual annotations, and make it easier for developers to create high-quality skill grammars.    \medskip             
"," AAAI creates proceedings, working notes, and technical reports directly from electronic source furnished by the authors. To ensure that all papers in the publication have a uniform appearance, authors must adhere to the following instructions.",127
"  Due to emergence as global language, English as a second language has been established as primary medium of instruction in higher education in several developing countries including India. Reading in a second language  differs from reading in a first language  in distinct ways. L1 learners have well-developed oral proficiency, vocabulary knowledge, and tacit grammar knowledge at the time they start learning to read, which leads to fluent processing of text information. L2 learners have limited oral proficiency and learners, and underdeveloped grammar knowledge. Therefore, compared to L1 learners, L2 learners are invariably slower and less accurate in processing text. One highly recommended procedure for improving L2 learners' oral fluency is learners readings .   Researchers of different domains such as prosody, acoustics, lexicon, syntactics proposed several measures to evaluate L2 learners' oral proficiency and narrative production. These measures can be applied for evaluating the impact of learners' repeated readings on their proficiency improvement. Also, availability of computational tools belonging to above mentioned domains make possible to develop a CALL system for L2 learners. A Computer Assisted Language Learning  system enables convenient and low-cost language learning, which focuses on developing the speaking, listening, and writing skills, and some of them are put to practice .   RR requires learners to reread a passage several times to achieve a pre-established level of fluency. The goals for RR are to increase learners' reading speed, transfer learning to new passages, and improve comprehension. There are two forms of repeated reading: unassisted and assisted. With unassisted repeated reading, learners are given reading passages that contain recognizable words at their independent reading levels. Each learner silently or orally reads his/her passage several times until he/she reaches the predetermined level of fluency. Assisted repeated reading, on the other hand, involves repeated reading whilst or after listening to either a teacher reading the same text or a recorded version . RR has been found to increase fluency and comprehension for first-language  \& second-language  learners not only with treatment texts but with new, unpracticed learners . Thus, RR leads to improvements in speech prosody, a component of reading fluency indicative of learners' comprehension of texts .   Prosody describes variation in intonation, duration, rhythm, and intensity, is a critical component of perceived fluency in spoken language, as prosodic variation signals not only syntactic and semantic structure of sentences but also emotion. For example, Kuhn et al.  stated `in addition to the role of rate and accuracy, prosodic fluency requires appropriate expression or intonation coupled with phrasing that allows for the maintenance of meaning' .   Several researchers have assessed the relationship between prosody and acoustics, about how prosodically fluent learners cue syntactic structure and semantic structure. For example, speakers often cue syntactic phrase boundaries through the employment of intonational phrase boundaries, the presence of silence between words and a pitch excursion, which can be rising in interrogatives sentences or falling in declaratives. Imoto et al.  addressed sentence-level stress detection of English for Computer-Assisted Language Learning by Japanese learners .  %Researchers deal with this complexity by relying on trained human annotators' perception of prosodic features, as in the Tones and Break Indices  systems .  Trofimovich et al.  used acoustic features  to determine how accurately five prosody features  were produced by L2 English speakers .    Horst et al.  found that repeated readings of L2 text help learners to identify the meaning and form of words without access to a dictionary or other learning support . With phonological support, it provides a supportive environment for both incidental and intentional novel vocabulary acquisition. Many L2 development studies have reported that a variety of lexical richness measures, along with measures of accuracy, fluency, and grammatical complexity, can be used as reliable and valid indices of the learner's developmental level or overall proficiency in an L2 .    Ortega  stated `Syntactic complexity  refers to the range of forms that surface in language production and the degree of sophistication of such forms' . The measures used to examine syntactic complexity in L2 English writing development include length of production unit , amount of embedding, subordination and coordination, range of structural types, and structural sophistication. Several studies have examined relations between the syntactic complexity of speech and the speakers' holistic speaking proficiency levels. Iwashita's  study on Japanese L2 speakers found that length-based complexity features  are good predictors for oral proficiency . To realize a voice-interactive CALL system, Anzai et al. proposed n-gram model based methods for improving recognition accuracy of speech with grammatical mistakes .    In this research, we constructed the dataset and analysed the effect of repeated readings on learners' performance based on four criteria: oral fluency, lexical richness, syntactic maturity and overall score. To measure oral fluency, we extracted acoustic-prosodic features by applying various feature-sets. Lexicon and syntactic features were extracted from transcripts. We proposed methods to classify the learners' performance into three catogories: basic, average, and advance. We analysed the methods in three variations : days, articles, and both. We found that the proposed methods can track the improvements in the learners' oral fluency and narrative production. The methods also provide the facilities to compare individual's learning rate with others.  The accuracies of five classifiers demonstrate the feasibility of developing an automatic system to evaluate learners' performance.  In future work, we plan to expand the dataset by increasing the number of the participants as well as adding diverse reading exercises on various text. The analysis of disfluencies in speech considered to be studied further in order to improve performance of the proposed system.  
"," Repeated reading  helps learners, who have little to no experience with reading fluently to gain confidence, speed and process words automatically. The benefits of repeated readings include helping all learners with fact recall, aiding identification of learners' main ideas and vocabulary, increasing comprehension, leading to faster reading as well as increasing word recognition accuracy, and assisting struggling learners as they transition from word-by-word reading to more meaningful phrasing. Thus, RR ultimately helps in improvements of learners' oral fluency and narrative production. However, there is no open audio datasets available on oral responses of learners based on their RR practices. Therefore, in this paper, we present our dataset, discuss its properties, and propose a method to assess oral fluency and narrative production for learners of English using acoustic, prosodic, lexical and syntactical characteristics. The results show that a CALL system can be developed for assessing the improvements in learners' oral fluency and narrative production.",128
"  NMT is a new approach to machine translation that has achieved great success in the last a few years . Compared to plain SMT , a neural language model decoder  is better at long-distance re-ordering, and attention mechanisms  have been proven effective in modeling long-distance dependencies, while these two issues were both challenging for SMT.  The Transformer , which has outperformed previous RNN/CNN based translation models , is based on multi-layer multi-head attention networks and can be trained in parallel very efficiently. Though attentional networks can connect distant words via shorter network paths than RNNs, empirical results show that its ability in capturing long-range dependencies does not significantly outperform RNNs, and it is still a problem for the Transformer to fully model long-distance dependencies .  Using phrases instead of words enables conventional SMT to condition on a wider range of context, and results in better performance in re-ordering and modeling long-distance dependencies. It is intuitive to let the NMT model additionally condition on phrase level representations to capture long-distance dependencies better, but there are two main issues which prevent NMT from directly using phrases:    Instead of using phrases directly in NMT, in this work, we address the issues above with the following contributions:     Our approach empirically brings about significant and consistent improvements over the strong Transformer model . We conducted experiments on the WMT 14 English-German and English-French news translation task, and obtained  and  BLEU improvements respectively on top of the strong Transformer Base baseline, which demonstrates the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. It also shows effectiveness with the Transformer Big setting. We also conducted length analysis with our approach, and the results show how our approach improves long-distance dependency capturing, which supports our conjecture that phrase representation sequences can help the model capture long-distance relations better.     Considering that the strong Transformer translation model still has difficulty in fully capturing long-distance dependencies , and that using a shorter phrase sequence  is an intuitive approach to help the model capture long-distance features, in this paper, we first propose an attention mechanism to generate phrase representations by merging corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to help it capture long-distance relationships. We obtained statistically significant improvements on the WMT 14 English-German and English-French tasks over the strong Transformer baseline, which demonstrates the effectiveness of our approach. Our further analysis shows that the Transformer with phrase representation empirically improves its performance especially in long-distance dependency learning.  
"," The Transformer translation model  based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation . Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies . Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation  approach through the use of larger translation blocks  and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.",129
" Dialog systems research has primarily been focused around two main types of applications -- task-oriented dialog systems that learn to use clarification to aid in understanding a user's goal~, and open-ended dialog systems that are expected to carry out unconstrained ``chit chat'' conversations~.  Much of this research, assumes access to training dialogs of the type the system is expected to perform, and aims to build a dialog system that can then engage in the same type of interactions.   This is also the case with most machine learning research, which is focused on a learning problem in the context of a fixed domain and task, that do not change between training and test time. However, when these systems are used in real-world scenarios, the domain is often wider than that of the original training set, and the requirements of the task may change over time.  Lifelong learning research aims to develop machine learning systems that can be robust to this kind of change, making use of knowledge from previous tasks to improve the performance and sample efficiency of future tasks. Lifelong learning can reduce the dependence of learned systems on narrow well-defined tasks and large annotated datasets. %Lifelong learning was first introduced in the context of control problems in robotics by  as a method to generalize the knowledge from previously learned control tasks to new control tasks over time.  %Another early work is by  where an agent must build a theory of a domain and and choose which of multiple learnable tasks to learn next.  In this position paper, we present the problem of designing dialog systems that enable lifelong learning as an important challenge problem, in particular for applications involving physically situated robots.    Lifelong learning is particularly relevant for robotics applications that involve an agent physically interacting with its environment because it is difficult and expensive to obtain labelled data during training that adequately covers all scenarios that the agent is likely to encounter during operation.  Recent work has tried to address this issue using simulation techniques to increase the robustness of differences in the task and domain from training and operation time~. However, a complementary direction would be to leverage interactions with humans that such a system is likely to have during operation to obtain additional labelled data to adapt to changes in domains or tasks that occur during operation.   Dialog systems by their very nature place the system in a position where it is interacting with a human user. Thus the system is in a position where it can query the user for additional information which may be useful for future interactions. With dialog systems, as with most machine learning systems, it is reasonable to assume that if the trained dialog system interacts with users in the future, those dialogs can be used to further improve the system. In particular, since open-ended dialog systems do not in principle make assumptions about the domain of discussion or the types of dialog acts that the user and system engage in, they can be considered to be performing some form of lifelong learning.  However, there are many types of information that dialog systems can explicitly query users for during operation. Some examples include entries to be added to a knowledge base , new words that refer to concepts for which the system has a learned model , or labels that can be used to train supervised models .   We propose a new focus area for dialog systems research that includes identifying such information-gathering dialog acts that are relevant to different types of dialog systems, learning dialog systems that make use of such dialog acts, and user studies and other supportive research necessary for making such systems more usable in real-world scenarios. We call this area ``dialog for supporting lifelong learning,'' and present it as an interesting challenge problem for dialog researchers, and review some initial directions on work in this area.  We believe this is especially relevant for dialog systems on embodied robots, as these systems face more difficulties due to the shortage of available training data, and can hence benefit more by using learning techniques that are better designed to adapt to novel test data. We believe this area presents a number of interesting challenges regarding dataset and task design, speech processing, sample efficiency, and dialog user analyses.  The rest of this position paper discusses existing work in the area and challenges for the future.    Dialog systems have the potential to be important for lifelong learning, as a mechanism for collecting additional useful labelled data during operation.  This is particularly relevant in the context of physically situated dialog systems such as robotic ones as data collection for these applications is more expensive and time-consuming.  The topic of adapting dialog systems for lifelong learning presents interesting challenges such as developing new types of dialog acts and simulators, addressing challenges around potential user frustration, non-cooperation or misuse, and designing methods to demonstrate this learning to users to encourage further cooperation.   The topic of adapting dialog systems for lifelong learning presents a number of interesting challenge problems for the dialog community including      
"," Dialog systems research has primarily been focused around two main types of applications -- task-oriented dialog systems that learn to use clarification to aid in understanding a goal, and open-ended dialog systems that are expected to carry out unconstrained ``chit chat'' conversations.  However, dialog interactions can also be used to obtain various types of knowledge that can be used to improve an underlying language understanding system, or other machine learning systems that the dialog acts over.  In this position paper, we present the problem of designing dialog systems that enable lifelong learning as an important challenge problem, in particular for applications involving physically situated robots.   We include examples of prior work in this direction, and discuss challenges that remain to be addressed.",130
" Cross-lingual text summarization  is the task of compressing a long article in one language into a summary in a different language.  Due to the dearth of training corpora, standard sequence-to-sequence approaches to summarization cannot be applied to this task.  Traditional approaches to XLS thus follow a pipeline, for example, summarizing the article in the source language followed by translating the summary into the target language or vice-versa. Both of these approaches require separately trained summarization and translation models, and suffer from error propagation.        Prior studies have attempted to train XLS models in an end-to-end fashion, through knowledge distillation from pre-trained machine translation  or monolingual summarization models~, but these approaches have been only shown to work for short outputs.  Alternatively,  proposed to automatically translate source-language summaries in the training set thereby generating pseudo-reference summaries in the target language. With this parallel dataset of source documents and target summaries , an end-to-end model is trained to simultaneously summarize and translate using a multi-task objective.  %They consider this dataset as the parallel data, and train models to simultaneously summarize and translate using a multi-task objective.  %  construct a large-scale cross-lingual summarization dataset  using a machine translation  model. They train a model to do end-to-end CLS with a multi-task learning objective of being able to do MS, MT and CLS simultaneously.  Although the XLS model is trained end-to-end, it is trained on MT-generated reference translations and is still prone to compounding of translation and summarization errors.   In this work, we propose to train an end-to-end XLS model to directly generate target language summaries given the source articles by matching the semantics of the predictions  with the semantics of the source language summaries. To achieve this, we use reinforcement learning  with a bilingual semantic similarity metric as a reward. This metric is computed between the machine-generated summary in the target language and the gold summary in the source language.  Additionally, to better initialize our XLS model for RL, we propose a new multi-task pre-training objective based on machine translation and monolingual summarization to encode common information available from the two tasks. To enable the  model to still differentiate between the two tasks, we add task specific tags to the input.  We evaluate our proposed method on English--Chinese and English--German XLS test sets. These test corpora are constructed by first using an MT-system to translate source summaries to the target language, and then being post-edited by human annotators.  Experimental results %  demonstrate that just using our proposed pre-training method without fine-tuning with RL improves the best-performing baseline by up to 0.8 ROUGE-L points. Applying reinforcement learning yields further improvements in performance by up to 0.5 ROUGE-L points. Through extensive analyses and human evaluation, %,  we show that when the bilingual semantic similarity reward is used, our model generates summaries that are more accurate, longer, more fluent, and more relevant than summaries generated by baselines.     In this work, we propose to use reinforcement learning with a bilingual semantic similarity metric as rewards for cross-lingual document summarization. We demonstrate the effectiveness of the proposed approach in a resource-deficient setting, where target language gold summaries are not available. We also propose simple strategies to better initialize the model towards reinforcement learning by leveraging machine translation and monolingual summarization.    Besides, we propose a strategy to leverage the auiliary objectives of monolingual text summarization and machine translation.  In future work, we plan to explore methods for stabilizing reinforcement learning as well to extend our methods to other datasets and tasks, such as using the bilingual similarity metric as a reward to improve the quality of machine translation.  
"," Cross-lingual text summarization aims at generating a document summary in one language given input in another language. It is a practically important but under-explored task, primarily due to the dearth of available data. Existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. In this work, we propose an end-to-end cross-lingual text summarization model. The model uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language. We also introduce techniques to pre-train the model leveraging monolingual summarization and machine translation objectives. Experimental results in both English--Chinese and English--German cross-lingual summarization settings demonstrate the effectiveness of our methods. In addition, we find that reinforcement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines.\footnote{\url{https://github.com/zdou0830/crosslingual_summarization_semantic}.}",131
"  %\let\thefootnote\relax\footnotetext{* indicates the corresponding author.}   Text classification is one of the fundamental tasks in Natural Language Processing . The goal is to classify texts according to certain criteria. It has many practical applications such as sentiment analysis  and topic categorization .  %Traditional approaches, on the basis of the text representation, extract features for a general classifier. %For instance, in the model of bag-of-words, statistics on unigrams, bigrams, and -grams are used as features . %These traditional methods ether totally ignore the word order or constrain their focus on  small word tuples, %resulting in inevitable information loss. Moreover, they suffer from the problems of data sparsity and high dimensionality.  Traditional approaches, for example, bag-of-words, extract features such as statistics on unigrams and bigrams for a general classifier. In recent years, the development of pre-trained word embeddings and deep neural networks has brought new inspiration to various NLP tasks. Word embeddings are used to map words to an implicit space where semantic relationships are preserved in their reciprocal closeness commonly measured by the Euclidean norm. This type of representation can alleviate the data sparsity problem . Moreover, researchers demonstrate that pre-trained word embeddings are able to capture meaningful syntactic and semantic regularities . With the help of word embeddings, deep learning models such as convolutional neural network   and recurrent neural network   are proposed to further process the semantic representation within texts. This methodology has made impressive progress in text classification .   CNN has been proven to be a powerful semantic composition model for modeling texts . CNN treats texts as a 2D matrix by concatenating embedding of words together. It utilizes a 1D convolution operator to perform the feature mapping, and then conducts a 1D pooling operation over the time domain for obtaining a fixed-length output feature vector. Based on the convolution operation, it is able to capture both local and position-invariant features in texts.  Alternative popular neural network model, RNN , treats texts as sequential data and analyzes texts word by word.  Fixed-length hidden units in RNN stores the contextual information up to the present word. Long Short-term Memory  units  and gated recurrent units   are two popular prototypes that aim to solve gradient vanishing and gradient explosion problems.   Some approaches attempt to combine CNN and RNN to incorporate the advantages of both models . However, most of them integrate CNN and RNN only by streamlining the two networks , which might decrease the performance of them.   %In this paper, we propose a novel method to combine these two models. The intuition underlying our model is that %different words contribute differently to the meaning of the whole text and the key parts can be well extracted by CNN. % As an example in Table  , both sentences are labeled as Science and Technology. % Obviously, one can verify that the words in bold are the most informative. This type of information, due to its locality, % as mentioned above, can be captured by a CNN due to its capability of extracting local and position-invariant features.  In addition, prior works neglect the fusion of contextual information when learning the word's contextual representation.  %Another point that is neglected by prior work is on the fuse of bidirectional word representations.  Most methods incorporating bi-directional RNN to model each word's contextual representation usually choose concatenating the forward and backward hidden states at each time step .  As a result, the resulting vector does not have interaction information of the forward and backward hidden states.  %However, hidden state in one direction becomes ``myopic'', and may be against to the meaning collected by another hidden state.  Meanwhile, the hidden state in one direction may be ``myopic'' and against the meaning collected by another hidden state.  Intuitively, a word閳ユ獨 contextual representation is more accurate when holistic semantics are  collected and fused from two directions.  Failure in doing so may lose true meaning for the focus word,  especially for polysemic words whose meanings are context-sensitive.  %We believe hidden state in one direction becomes ``myopic'', and may be against to the meaning collected by another hidden state.   %It is a good choice to use a representation that fuse hidden states of forward and backward RNN more directly.  %This problem can be solved by improving the interaction between forward and backward hidden states. %In other words, it cannot capture the compositional meaning of context. %It is a good choice to use a representation that fuse hidden states of forward and backward RNN more directly. %   %Different words contribute differently to the representation of the text meaning. Take XXX as example, both sentence are labeled as Science and Technology. The words in bold are most informative of each sentence.  %%閺鐟板煂鏉╂瑩鍣锋禍%  %Different words contribute differently to the representation of the text meaning. %We hypothesize importance of each word in text can be inferred from local context. %% For example, it can give practitioners hints on %%  how semantically conflict words jointly determining the polarity in sentimental %%  analysis. After revisiting this problem, our paper we assume that some key %%words in text may be enough in determining the semantic meaning and polarity. %Take  as an example, both sentences should be classified as . The key words which are important to determine the category is %in bold. % % %%For example, in Table . The key words that determine the category %%can be well extracted just based on local context. %% %%we assume that some of words have stronger correlations to the %%meaning of text than others. %% %%However, none of them could provides insight into which words in text contribute to the classification decision which can be of value in applications and analysis . %%However, none of these methods could capture different aspects of text and gives for classification and give importance of each words in text. % % % %    %\fbox{ %{0.8\linewidth} %    %        % %} % %   In this paper, we propose a neural network model that incorporates CNN and RNN in a novel way. The intuition underlying our model is that different words contribute differently to the meaning of the whole text and the key parts can be well extracted by CNN. As an example in Figure , the words in bold are the most informative for the sentences labeled as Science and Technology. Due to the capacity of capturing local and position-invariant features, CNN is utilized to extract local features and learn a 2D matrix that shows the importance of each word from different aspects. Meanwhile, the bi-directional RNN is applied to learn  word contextual representations. A neural tensor layer is introduced on the top of the bi-directional RNN to obtain the fusion of the bi-directional contextual information surrounding the focus word. We call this novel neural network as convolutional recurrent neural network  and apply it to the task of text classification. By combining the convolution and recurrent structure, our method preserves the advantage of both CNN and RNN.  Our contributions in this paper are listed as follows:  %- We propose a self-attention mechanism which based on CNN to extract different aspect of text.        %In this paper, we assume that some of words have stronger correlations to the %meaning of text than others and these correlations could be partly captured %based on the local information in text. Therefore, in order to achieve %the relative importance among words. we propose a CNN-based self-attention %neural network for text classification. The relative importance %in the context of neural network, helps a model to dedicate the capacity to %those having strong correlations with desired output. our method use CNN on word sequence %to get attention information, requiring no extra inputs.  \iffalse   In this paper, we propose convolutional recurrent neural networks for text classification. In our model, we use a convolutional neural network to compute the weight matrix which shows the relative importance of each word from different aspects. Meanwhile, we use the bi-directional RNN to process text and introduce neural a tensor layer to fuse forward and backward context information to obtain the word's contextual representation. Finally, the weight matrix and the word's contextual representation are combined to get the text representation. The experimental results indicate that the proposed method not only achieves better performance compared with previous methods, but also provides insight into which words in the text contribute to the classification decision which can be of value in applications and analysis. In the future, we want to explore  features  extracted from the method to learning   methods, such as model space .    
","  Convolutional neural network  and recurrent neural network  are two popular architectures used in text classification. Traditional methods to combine the strengths of the two networks rely on streamlining them or concatenating features extracted from them. In this paper, we propose a novel method to keep the strengths of the two networks to a great extent. In the proposed model, a convolutional neural network is applied to learn a 2D weight matrix where each row reflects the importance of each word from different aspects. Meanwhile, we use a bi-directional RNN to process each word and employ a neural tensor layer that fuses forward and backward hidden states to get word representations. In the end, the weight matrix and word representations are combined to obtain the representation in a 2D matrix form for the text. We carry out experiments on a number of datasets for text classification. The experimental results confirm the effectiveness of the proposed method.",132
"  Human learners can acquire any of the world's languages from finite data.  The acquisition of a particular language involves two factors:  data from that language,  and the learner's inductive biases, which are the factors that determine how the learner will generalize beyond the particular utterances in the data . Many inductive biases are shared by all humans ( often refers to cognitive biases, we use it to encompass all pressures that shape the language that a learner learns; see Figure and the Background section.}    , a technique in which a learner is exposed to a variety of tasks, each of which comes with a limited amount of data. This process instills in the learner a set of inductive biases which allow it to learn tasks similar to those it has seen before from limited data. In our setting, each ``task"" is a different language, and the inductive biases that result from meta-learning are encoded in a neural network's initial state. This initial state is found in a data-driven manner; by controlling the data, we can influence which inductive biases will be encoded in the initial state, and the initial state can then be analyzed to verify that it encodes the universal inductive biases that it is intended to encode.    As a first case study, we show the effectiveness of this approach on the acquisition of a language's syllable structure, a paradigmatic example of universal linguistic inductive biases. We define a set of inductive biases relating to syllable structure that we intend to give our model, and we then translate this set of inductive biases into a space of possible languages from which we have a model meta-learn. Through analysis of the meta-learned initial state, we verify that meta-learning has successfully imparted the inductive biases that it was intended to impart; for example, the model has meta-learned that the presence of certain input-output mappings in a language implies the presence of other input-output mappings.\footnote{Our code is at \url{https://github.com/tommccoy1/meta-learning-linguistic-biases}; there is also a demo at \url{http://rtmccoy.com/meta-learning-linguistic-biases.html}.}         We have demonstrated how meta-learning can impart universal inductive biases specified by the modeler. This example-based approach to imparting inductive biases does not require an explicit theory of the biases in question; rather, imparting the biases only requires these biases to be translated into a distribution of possible languages. While the meta-learned biases are not as transparent as those encoded in probabilistic symbolic models, analysis of the model's learning behavior can be used to evaluate whether  meta-learning has produced the desired biases, as we have shown.  In our case study, we found evidence that meta-learning had successfully imparted all of our target inductive biases , including both some abstract biases  and some more concrete biases . These results show that linguistic inductive biases that have previously been framed in symbolic terms can be reformulated in the context of neural networks, facilitating cognitive modeling that combines the power of neural networks with the controlled inductive biases of symbolic approaches.      One important feature of the proposed approach is that it imparts soft biases rather than hard constraints. For example, after meta-learning, the model could learn attested language types more readily than unattested types---but it still could learn the unattested ones. This capability is at odds with some theories that predict that unattested language types should be unlearnable, but there are reasons to believe that the consistent patterns seen in language typology and language acquisition may be best viewed as biases rather than constraints: almost all linguistic universals have exceptions~; for example, the Arrernte language has been argued to be an exception to the syllable structure typology we have adopted. Further, humans in artificial language learning experiments are capable of learning ``unnatural"" languages .       Several other works have discussed meta-learning from a cognitive perspective , and in applied settings meta-learning has been  applied to language to create technology in low-resource languages. Our novel contribution is the use of meta-learning to analyze the interplay between data and linguistic inductive biases.    Our approach can be used to test the behavioral effects of a particular inductive bias .  Finally, this framework is general enough that it can be straightforwardly applied to cognitive domains other than language .        
"," How do learners acquire languages from the limited data available to them?  This process must involve some inductive biases---factors that affect how a learner generalizes---but it is unclear which inductive biases can explain observed patterns in language acquisition. To facilitate computational modeling aimed at addressing this question, we introduce a framework for giving particular linguistic inductive biases to a neural network model; such a model can then be used to empirically explore the effects of those inductive biases. This framework disentangles universal inductive biases, which are encoded in the initial values of a neural network's parameters, from non-universal factors, which the neural network must learn from data in a given language. The initial state that encodes the inductive biases is found with meta-learning, a technique through which a model discovers how to acquire new languages more easily via exposure to many possible languages. By controlling the properties of the languages that are used during meta-learning, we can control the inductive biases that meta-learning imparts. We demonstrate this framework with a case study based on syllable structure. First, we  specify the inductive biases that we intend to give our model, and then we translate those inductive biases into a space of languages from which a model can meta-learn. Finally, using existing analysis techniques, we verify that our approach has imparted the linguistic inductive biases that it was intended to impart.   Keywords:  meta-learning, inductive bias, language universals,  syllable structure typology, neural networks",133
" Language models  predict a probability distribution over text, and are a fundamental technology widely studied in the natural language processing  community . Modern LMs are almost exclusively based on  recurrent  or self-attentional  neural networks. These models are of interest scientifically as one of the purest tests of our ability to capture the intricacies of human language mathematically . They also have broad downstream applications in generating text in systems such as machine translation , summarization , or dialog generation , as well as in the unsupervised representation learners that now power many applications in NLP . % Text generation is a key element of many NLP tasks, such as machine translation, summarization, and dialogue generation. % These systems often adopt an encoder-decoder architecture where the decoder is based on neural language models \gn{Maybe cite a few, just for completeness?}. % In unsupervised setting, such neural language models usually generate sentences word-by-word from scratch.  % which may lack the inductive bias to faithfully represent the full diversity of complex utterances and does not expose an interpretable way of the generation process.  However, there has been a recent move towards  neural LMs  that generate sentences by first selecting examples from an external datastore. % For instance,  model the token-level probability at test time by interpolating the language model with a kNN distribution from the nearest context-token pairs in the datastore, while~ store external memories on sentence level and feature a prototype-then-edit process of  selecting a  sentence from a the prototype datastore, and  editing this prototype to the final desired output. % In this paper, we focus on the prototype-then-edit model family which is a lot lighter relatively in terms of memory and time cost at test time. % though being still absolutely very expensive that we will discuss later.  Intuitively, these non-parametric LMs are attractive because they help remove some of the pressure on the parametric model to memorize the entirety of the language it must model. % These intuitive advantages are also borne out in superior performance on language modeling tasks , as well as down-stream applications such as dialogue response generation~, machine translation~, and code generation~. % In addition, the prototypes and continuous representations of the edits in prototype-based models lend an element of interpretability to the modeling process. % On the down side, however, previous prototype-driven generation methods usually need to store and index a large prototype candidate pool , leading to significant issues with memory and speed efficiency at test time.   In this paper, we hypothesize that, in fact, a  set of prototypes is sufficient to achieve the great majority of the gains afforded by such non-parametric models. Intuitively, in a large corpus many sentences look very similar and may be represented by minor transformations of a single prototype sentence. For example, the sentence ``I ordered a burger with fries'' can serve as the prototype for data samples with the form ``I ordered [NOUN PHRASE] with [NOUN PHRASE]''. This is evidenced by~'s observation that 70 of the test set in the Yelp restaurant review corpus~ is within word-token Jaccard distance 0.5 of one training sentence.   To take advantage of this intuition, we propose a novel generative model that samples prototypes from a  prototype distribution, which itself is sampled from a symmetric Dirichlet prior, as shown in Figure . The Dirichlet prior with appropriate hyperparameters is able to encourage a  prototype selection distribution, allowing us to reduce the prototype support set at test time to greatly improve efficiency. Moreover, we utilize amortized variational inference~ to train our model, which introduces a learnable prototype retriever to identify prototypes useful for generating each sentence . This is different from~ where prototypes for each sentence are fixed before training through edit distance heuristics.  % \gn{This is a slight bit repetitive with the abstract. Not sure if it's bad enough to modify.} We evaluate our approach on the MSCOCO~ and Yelp restaurant review~ corpora. Our method is able to improve perplexity over the neural language model baseline by up to 14 points and previous neural editor model by 6 points while achieving over 1000x memory savings and a 1000x speedup at test time . Interestingly, we find that the learned prototypes are able to represent different features when varying sparsity levels -- a strong sparsity prior forces the model to share prototypes and the induced prototypes turn out to represent more generic features . On the text generation side, our model is able to generate sentences that resemble the given prototype while allowing for smooth interpolation on the edit space as well .     % mention soft prototype work % wang2019neural     In this work, we propose a novel generative model that discovers a sparse prototype set automatically by optimizing a variational lower bound of the log marginal data likelihood. We demonstrate its effectiveness on language modeling and its efficiency advantages over previous prototype-driven generative models. The framework proposed here might be generalized to automatically discover salient prototypes from a large corpus. New kinds of prototype structure in text might be discovered through either injecting different biases into the model , or incorporating prior knowledge into the prototype library before training.   -- the prototype library may not even be training samples, for example, it can be a pool of symbolic forms like POS sequences, syntactic trees, or others.  Finally, the approach might be easily extended to conditional generation , and we envision that inducing a sparse prototype set in this case may potentially facilitate controlling text generation through prototypes.    For example, the sparse prototypes that mainly capture syntactic information in the paper may be used to control the syntactic features  of text for data-to-text tasks like news article generation.  We leave exploration  in this direction as our future work.  
"," % We propose a novel generative model that learns to identify a sparse prototype support set to generate sentences. We assume that a small set of sentences serve as prototypes  to be edited to generate a large number of diverse examples. Different from previous prototype-driven models which usually require to store and index a large retrieval database, our model is trained to identify only a small number of sentences as prototypes automatically. This is achieved by imposing a Dirichlet prior on the prototype selection distribution to encourage sparsity. In experiments, our model outperforms previous neural edit models on language modeling while bringing 100x memory saving and 50x speed-up at test time. More interestingly, we show that the learned prototypes are able to capture semantics or syntax at different granularity as we vary the sparsity of prototype selections, and certain sentence attributes can be controlled by specifying the prototype for generation.\footnote{Code will be released after the review period.}  Prototype-driven text generation uses non-parametric models that first choose from a library of sentence ``prototypes'' and then modify the prototype to generate the output text. While effective, these methods are inefficient at test time as a result of needing to store and index the entire training corpus. Further, existing methods often require heuristics to identify which prototypes to reference at training time. In this paper, we propose a novel generative model that automatically learns a  prototype support set that, nonetheless, achieves strong language modeling performance. This is achieved by  imposing a sparsity-inducing prior on the prototype selection distribution, and  utilizing amortized variational inference to learn a prototype retrieval function. In experiments, our model outperforms previous prototype-driven language models while achieving up to a 1000x memory reduction, as well as a 1000x speed-up at test time. More interestingly, we show that the learned prototypes are able to capture semantics and syntax at different granularity as we vary the sparsity of prototype selection, and that certain sentence attributes can be controlled by specifying the prototype for generation.\footnote{Code is available at \url{https://github.com/jxhe/sparse-text-prototype}.}",134
" %AMINE: Common sense or commonsense??  %Common sense reasoning is one of the long standing challenging problems - previous work on capturing common sense knowledge of a model deals with indirect measure. Humans are good at reasoning based on the knowledge they possess, but for machine learning models,  it has always been strenuous. Even the simple tasks like reference resolution for example : The trophy would not fit in the brown suitcase because it was too i. What was too big?, for humans, it's implicit that it's the trophy that's too big to fit in a suitcase. Still, for Natural Language Understanding  models, it's challenging to understand what it refers too. The earlier work by Devlin~ and Rahman~ has shown a impressive results on reference resolution task. Common sense reasoning is one of the long-standing problems in natural language understanding.  Previous work on modeling common sense knowledge deals mainly with indirect  tasks %measures such as co-reference resolution , % ,  or selecting the plausible situation based on the given subject or scenario . %. %%% Comment by CH % Humans are good at reasoning based on the knowledge they have, but for machine learning models, this has always been challenging, even for simple tasks like co-reference resolution. For instance, consider this reference resolution example: ``The trophy would not fit in the brown suitcase because 	extbf{it was too big閳ユ絹. The question is ``What was too big?閳ユ絹. For humans, it is implicit that it is the trophy that is too big to fit in a suitcase. Still, for Natural Language Understanding  models, it is challenging to understand what 閳ユ泛textit{it閳 refers too. The earlier works by Devlin et al. ~ and Rahman and Ng  have shown impressive results on the co-reference resolution task. % %Although, those systems does a great job in reference resolution it does not mean the system process common sense knowledge. If we give a false statement  to the system  for example I bought an elephant and kept it in the fridge. What it refers too? the system is capable of classifying it refers to elephant but the statement itself does not make sense and system cannot classify or generate an explanation to it. %%% Comment by CH % While those systems do a great job in reference resolution, they do not process common sense knowledge. For example, if  the system is fed with a false statement  like ``I bought an elephant and kept it in the fridge閳ユ絹, and is  asked 	extit{``What does ``it閳 refer to?閳ユ絹,  it is able to classify 	extit{``it閳ユ絹 as referring to 	extit{``elephant閳ユ絹. % but it is not capable of classifying the statement as non-sensical or of generating a coherent explanation on why it does not make sense. %Nevertheless, the system is not capable of distinguishing the statement between sensical and non-sensical. %%% Comment by CH % Nevertheless, it is not capable of distinguishing a sensical statement from a non-sensical one.  % %Even the existing the language model by X,Y are capable of  generating response based on the passage. But all these systems are incapable for generating response as a standalone system to generate response or explanations when a false statement is the input. %In order for a NLU system to start being more lifelike, it needs to have common sense. As common sense vary from person to person to person depending upon various factors. We define 	extbf{common sense for a NLU system as a basic ability for the system to make a practical judgement based on knowledge and experience, which have already been learned through other sources. %%% Comment by CH % Existing language models by , ,   are also capable of generating a response based on a textual passage, but when a false statement is introduced as input, all these models, as standalone systems, fail to generate a response that includes explanations. A more lifelike NLU system needs to encompass a common sense component. For an NLU system, we define common sense    % In this paper we tackle the task of generating    In this paper, we present our system that we devised to tackle Task C, Explanation , of the SemEval 2020 Task 4 - Commonsense Validation and Explanation . %the task 4 of common sense reasoning.  %For a  Given a false or non-sensical statement, the task consists of generating the reason why the given statement does not make sense. We propose a mUlti-task learNIng for cOmmonsense reasoNing . %  language model. %, which is a . %The UNION is a multi-head transformer architecture, we train the language model using ComVE , Open-book , Common sense Explanation   and Open Mind Common Sense   dataset. We train all the dataset parallel in a multi-task learning technique using average weight over the loss function for all the dataset. % The backbone of UNION is a multi-head transformer architecture.  It combines datasets including ComVE , OpenBook , Common sense Explanation   and Open Mind Common Sense   in a multi-task framework.  % parallel training is performed, using average weight over the loss function for all the datasets. % The backbone of UNION is a large pre-trained language model -- GPT2. % Rather than training the language model from scratch, we leverage a pre-trained language model as it saves a significant amount of time, computation power and data to train.    % In contrast, we propose a mUlti-task learNIng for cOmmonsense reasoNing %  language model. Rather, than training the language model from the scratch, we leverage the pretrained language model as it save a significant amount of time, computation power and data to train. The UNION is a multi-head transformer architecture, we train the language model using ComVE , Open-book , Common sense Explanation   and Open Mind Common Sense   dataset. We train all the dataset parallel in a multi-task learning technique using average weight over the loss function for all the dataset.  We compare the proposed system to different baselines % and degenerate versions of it,  and report a significant improvement in BLEU score and other evaluation metrics.  % %Our initial submission was on training using ComVE, CoS-E, Open-Book data and we achieved a BLEU score of 15.7 while pretraining the model with OMCS dataset further improved the BLEU score by 0.7.  %Our proposed model also achieves a human evaluation score of 2.10, and it is the highest human evaluation score obtained in the ComVE dataset in the Sem Eval task. Our proposed model achieves a human evaluation score of 2.10, which ranked first on the final leader board for Task C of SemEval 2020 Task 4. In our initial submission, we used ComVE, CoS-E, and OpenBook datasets for training. In that case, a BLEU score of 15.7 is achieved. Pretraining the model with OMCS dataset further improved the BLEU score by 0.7. In addition, we show some of our generations in the appendix.    In this research, we propose a UNION model for multi-tasking on a couple of commonsense related datasets, which helped our UNION model to achieve state-of-the-art in the ComVE by achieving the highest human evaluation score.  In addition, we propose several auxiliary metrics to better evaluate different models for commonsense response generation tasks without human evaluations.  In the future, we would like to explore other possible areas of commonsense reasoning like quantitative reasoning, logic puzzles, and visual common sense reasoning.   
"," In this paper, we describe our mUlti-task learNIng for cOmmonsense reasoNing  system submitted for Task C of the SemEval2020 Task 4, which  % - Commonsense Validation and Explanation , which  %to solve the problem of  % The task is to generate a reason explaining why a given false statement  %does not make sense. is non-sensical.  %The human evaluation for the generated explanation by our model is carried out by the Sem-Eval organizers.  % The SemEval organizers carried out a human evaluation for the generated explanations. % by our system.  %UNION.  However, we found in the early experiments that simple adaptations such as fine-tuning GPT2 often yield dull and non-informative generations .  In order to generate more meaningful explanations, we propose UNION, a unified end-to-end framework, to utilize several existing commonsense datasets so that it allows a model to learn more dynamics under the scope of commonsense reasoning.  In order to perform model selection efficiently, accurately and promptly,  we also propose a couple of auxiliary automatic evaluation metrics  %to evaluate  so that we can extensively compare the models from different perspectives. Our submitted system not only results in a good performance in the proposed metrics but also outperforms its competitors with the highest achieved score of 2.10 for human evaluation while remaining a BLEU score of 15.7.  Our code is made publicly available at GitHub \footnote{\url{https://github.com/anandhperumal/ANA-at-SemEval-2020-Task-4-UNION}}. %which also  % Results show  %consolidate the % good performances achieved by our system. %in evaluation.",135
"   % %%  Neural networks are providing ground-breaking results in many complex tasks such as object detection, speech recognition or sentiment analysis. This success is usually attributed to the ability of deep neural networks to generalize well when trained with high quantities of data.  % %%  Among the various neural network types, Convolution Neural Networks  have become particularly popular because of their ability to mimic the functionality of the human brain visual cortex. As a results, CNNs are applied in image-related tasks such as object detection, fingerprint recognition, computer vision etc. The basic structure of a CNN was first applied by LeCun  in  for recognizing images of handwritten digits. A decade of hibernation  passed and they showed back in the late 2000s rebranded as . At this time they also became essential part of various proposed architectures such as  in ,  in  and more.   %  %%  Many natural language processing researchers explored use of CNNs or Recurrent Neural Networks  for text mining tasks such as sentiment analysis, reporting excellent results with little computation load. However, neural network models are usually data hungry and require bigger datasets of training samples. The other problem is the difficulty in finding the optimal hyperparameter setup or design choices when using various types of networks. Optimal network configuration depends on characteristics of available data which should be taken into account.  % \par  %%  We present in this paper the work we conducted for constructing two relatively big datasets of emotionally labeled songs and the results of many experiments with text datasets of different size and document lengths for simplifying neural network construction. For emotional labeling of songs, we utilized social tags crawled from  music portal. We also adapted a model of music emotions that is highly compatible with the popular model of Russell, together with an annotation scheme based on emotion tags each song has received. Furthermore, the works in  and  are extended both quantitatively and qualitatively. The first introduces three variants of a neural network architecture that uses convolution and max-pooling layers for text feature extraction and selection as well as a regularized feed-forward layer for classification. In the second paper, various relations between data properties and neural network parameters with respect to optimal performance are explored. In this work we report accuracy scores of a higher number of experiments with more datasets . Our results can help researchers to simplify hyper-parameter optimization of neural networks that are used for sentiment analysis experiments.  % \par  %% summary of results  % A fact that we noticed is that bigger datasets are better interpreted by repeating several stacks of parallel convolutions followed by max-pooling layers. An interesting regularity is the one that relates length of documents with pooling region size. The later is the parameter that dictates the size of produced feature maps. According to our results, top scores are achieved when pooling region size is set to produce feature maps that are 6 to 18 units long. Also, convolutions with filter lengths one, two and three are usually enough. Utilizing convolutions of longer filters did not improve results. Regarding the three neural network design we proposed, the basic version with max-pooling layers directly following each convolution layer resulted the best one. The flexibility it offers and its low training time make it a good option as a prototyping basis for practitioners.  % \par  %%  The rest of the paper is structured as follows: Section  presents an overview of various neural network models recently used in text mining tasks. Section describes the steps that were followed for the construction of the two music emotion datasets. Section presentes preprocessing steps, utilized datasets, and obtained network parameter optimization results. In Section, we describe the three network architectures we propose. Section  presents the high-level architectural parameters and decisions, together with the literature baselines we compare against. Section discusses obtained results and finally, Secton concludes.  %         In this paper, we first described the steps we followed for the creation of two datasets of song emotions that were publicly released for music emotion recognition community. We also experimented with different neural networks of convolution and max-pooling layers for analyzing sentiment polarity of different types of texts. Pretrained word embeddings of GoogleNews were utilized as generic features for a compact representation of text documents. A primary goal of our experiments was the observation of certain patterns that relate data properties to network parameters with respect to optimal classification results. The results reveal a regularity between length of documents and size of feature maps. Optimal classification scores are achieved when pooling region size is adjusted to produce feature maps that are 6 to 18 units long. Regarding datasets size, top results are achived when deeper networks are applied on the bigger datasets. Based on this result, we further examined three architecture designs we propose, comparing them with relevant baseline models. The three of them follow the design paradigm that has been highly successful in image recognition: complex feature extraction and selection combined with a simple classifier.       Our basic version with regional max-pooling following each convolution stack resulted the most successful architecture. It is fast and flexible, easily adapting to various kinds of text data. It may thus serve as a good starting point for practitioners. A future improvement of the neural network design we proposed here, could be combining sentiment predictions of text phrases using aggregation schemes like Dempster-Shafer Inference or Abductive Reasoning . That could be especially fruitful when analyzing sentiment polarity of long documents.            If you have bibdatabase file and want bibtex to generate the    bibitems, please use     
"," % The fabulous results of convolution neural networks in image-related tasks, attracted attention of text mining, sentiment analysis and other text analysis researchers. It is however difficult to find enough data for feeding such networks, optimize their parameters, and make the right design choices when constructing network architectures. In this paper we present the creation steps of two big datasets of song emotions. We also explore usage of convolution and max-pooling neural layers on song lyrics, product and movie review text datasets. Three variants of a simple and flexible neural network architecture are also compared. Our intention was to spot any important patterns that can serve as guidelines for parameter optimization of similar models. We also wanted to identify architecture design choices which lead to high performing sentiment analysis models. To this end, we conducted a series of experiments with neural architectures of various configurations. Our results indicate that parallel convolutions of filter lengths up to three are usually enough for capturing relevant text features. Also, max-pooling region size should be adapted to the length of text documents for producing the best feature maps. Top results we got are obtained with feature maps of lengths 6 to 18. An improvement on future neural network models for sentiment analysis, could be generating sentiment polarity prediction of documents using aggregation of predictions on smaller excerpt of the entire text.  %",136
" 	Recently, task agnostic pre-training with large-scale transformer models  and general text corpora has achieved great success in natural language understanding  as well as natural language generation, especially open-domain dialogue generation. For instance, based on the general language model GPT-2 , DialoGPT  is further trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena  scales up the network parameters to 2.6B and employs more social media conversations in the training process, leading to significant improvement on response quality. To mitigate undesirable toxic or bias traits of large corpora, Blender  further fine-tunes the pre-trained model with human annotated datasets and emphasizes desirable conversational skills of engagingness, knowledge, empathy and personality.  	 	Besides the above attempts from model scale and data selection, PLATO  aims to tackle the inherent one-to-many mapping problem to improve response quality. The one-to-many mapping refers to that one dialogue context might correspond to multiple appropriate responses. It is widely recognized that the capability of modeling one-to-many relationship is crucial for response generation . PLATO explicitly models this one-to-many relationship via discrete latent variables, aiming to boost the quality of dialogue generation. 	 	In this work, we will try to scale up PLATO to PLATO-2 and discuss its effective training schema via curriculum learning . There are two stages involved in the whole learning process, as sketched in Figure . In the first stage, under the simplified one-to-one mapping modeling, a coarse-grained generation model is trained for appropriate response generation  	under different conversation contexts. The second stage continues to refine the generation with a fine-grained generation model and an evaluation model. The fine-grained generation model explicitly models the one-to-many mapping relationship for diverse response generation. To select the most appropriate responses generated by the fine-grained generation model, the evaluation model is trained to estimate the coherence of the responses. 	  	 	As for response selection, previous studies have employed variant scoring functions, including forward response generation probability , backward context recover probability  and bi-directional coherence probability . However, the forward score favors safe and generic responses due to the property of maximum likelihood, while the backward score tends to select the response with a high overlap with the context, resulting in repetitive conversations. In order to ameliorate the above problems, we adopt the bi-directional coherence estimation in the evaluation model of PLATO-2, whose effectiveness is also verified in the experiments. 	 	We trained PLATO-2 models with different sizes: 1.6 Billion parameters and 310 Million parameters. In addition to the English models, we also trained Chinese models with massive social media conversations. Comprehensive experiments on both English and Chinese datasets demonstrate that PLATO-2 outperforms the state-of-the-art models. We have released our English models and source codes at GitHub, hoping to facilitate the research in open-domain dialogue generation. 	 	  	In this work, we discuss the effective training of open-domain chatbot PLATO-2 via curriculum learning, where two stages are involved. In the first stage, one coarse-grained model is trained for general response generation. In the second stage, two models of fine-grained generation and evaluation are trained for diverse response generation and response coherence estimation. Experimental results demonstrate that PLATO-2 achieves substantial improvements over the state-of-the-art methods in both Chinese and English evaluations.   	 	
"," 		To build a high-quality open-domain chatbot, we introduce the effective training process of PLATO-2 via curriculum learning. There are two stages involved in the learning process. In the first stage, a coarse-grained generation model is trained to learn response generation under the simplified framework of one-to-one mapping. In the second stage, a fine-grained generation model and an evaluation model are further trained to learn diverse response generation and response coherence estimation, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-of-the-art results.",137
" %  .  While deep reinforcement learning  have emerged as a promising solution for complex and high-dimensional decision-making problems, the determination of an effective reward function remains a challenge, especially in multi-domain task-oriented dialog systems. Many recent works have struggled on sparse-reward environments and employed a handcrafted reward function as a breakthrough . However, such approaches are often unable to guide the dialog policy through user goals. For instance, as illustrated in Figure , the user can't reach the goal because the system  that exploits the handcrafted rewards completes the dialog session too early. Moreover, the user goal usually varies as the dialog proceeds.   %   addresses the issue of resolving the ambiguities in selecting a distribution that does not exhibit additional preferences for any decisions.  Inverse Reinforcement Learning   and MaxEnt-IRL  tackles the problem of recovering reward function and using this reward function to generate optimal behavior. Although Generative adversarial imitation learning  , which exploits the GANs framework , has proven that the discriminator   % However, these IRL methods are extremely demanding and have difficulty producing optimal policy when environmental dynamics vary. [16]{r}{0.36\textwidth} {     % [14]{r}{0.38\textwidth} % { % } Do 	extcolor{purple{the block}} in front of 	extcolor{yellow{the tiny yellow cylinder}} and 	extcolor{red{the tiny thing}} that is to the right of 	extcolor{green{the large green shiny object}} have the same color? {} No} % } %  %   %  %  % As standards of living rise and the world閳ユ獨 population grows, the demands for freshwater have been increasing. % Dialog policy       % we propose VRB which uses variational information bottleneck to constrain uninformative gradients in reward estimator and thereby optimizing dialog policy generator. % discerning reward engineering defining. has proven successful on remarkable As handling elaborate sophisticate goals across multi-domain, reward sparsity,  % This is a significant barrier for multi-domain task-oriented dialog system. can be defined as a reward function, GAIL fails to generalize and recover the reward function. Adversarial inverse reinforcement learning   enables GAIL to take advantage of disentangled rewards. Guided dialog policy learning   uses AIRL framework to construct the reward estimator for multi-domain task-oriented dialogs. However, these methods often encounter difficulties in balancing the performance of the policy generator and reward estimator, and produce excessively uninformative gradients.     %  The VRB focuses on capturing discriminative features, by exploiting in-formation bottleneck on mutual information. % The VRB exploits information bottleneck on mutual information between encoded dialog state-dialog action pairs and human dialogs, thereby ensuring a highly informative representation. % enforces upper bound on %    to  VRB exploits a stochastic encoder and enforces an upper bound on mutual information between the encoding and inputs.  In this paper, we propose the Variational Reward Estimator Bottleneck , an effective regularization algorithm. The VRB uses information bottleneck  to constrain unproductive information flows between dialog state-action pairs and internal representations of the reward estimator, thereby ensuring highly informative gradients and robustness. The experiments demonstrate that the VRB achieves the state-of-the-art performances on a multi-domain task-oriented dataset. %  encourages the reward estimator to learn approximation between distributions of human dialog sessions and the policy generator which mimics human behaviors.  %  Note that VRB can infuse meaningful reward information into the policy generator and thereby can guide dialog policy through user goal more successfully. %  where a generator and discriminator are learned adversarially and simultaneously, ameliorating each other. Though GAIL  \clearpage   In this paper, we develop a novel and effective regularization method known as the Variational reward estimator bottleneck  for multi-domain task-oriented dialog systems. VRB contains a stochastic encoder which enables the reward estimator to be maximally informative, as well as provides information bottleneck regularization, which constrains unproductive information flows between the inputs and reward estimator. The empirical results demonstrate that VRB achieves a new state-of-the-art performances on two different user simulators and a multi-turn and multi-domain task-oriented dialog dataset.   \iffalse   For papers accepted to the main conference, we will invite authors to provide a translation    of the title and abstract and a 1-2 page synopsis of the paper in a second    language of the authors' choice. Appropriate languages include but are not    limited to authors' native languages, languages spoken in the authors' place    of affiliation, and languages that are the focus of the research presented.   \fi   
"," % This is a significant barrier for approximation between the distributions of human dialogs and policy generator which imitates human behaviors.    Despite its notable success in adversarial learning approaches to multi-domain task-oriented dialog system, training the dialog policy via adversarial inverse reinforcement learning often fails to balance the performance of the policy generator and reward estimator. During optimization, the reward estimator often overwhelms the policy generator and produces excessively uninformative gradients. We proposes the Variational Reward estimator Bottleneck , which is an effective regularization method that aims to constrain unproductive information flows between inputs and the reward estimator. The VRB focuses on capturing discriminative features, by exploiting information bottleneck on mutual information. Empirical results on a multi-domain task-oriented dialog dataset demonstrate that the VRB significantly outperforms previous methods. %     approximation between distributions of human dialog sessions and the policy generator which imitates human behaviors. % VRB uses constraint on mutual information to capture discriminative features. % which make this task more complicated.",138
"    Robots must execute commands that are extended in time while being responsive to changes in their environments. % A popular representation for such commands is linear temporal logic, LTL. % Commands expressed in LTL encode both spatial and temporal constraints that should be true while executing the command. % Executing such commands is particularly difficult in robotics because integration is required between the complex symbolic reasoning that finds satisfying sequences of moves for an LTL command and data-driven perceptual capabilities required to sense the environment. % While individual formulas can be learned by deep networks with extensive experience, we demonstrate how to compose together tasks and skills to learn a general principle of how to encode all LTL formulas and follow them without per-formula experience. % We demonstrate how to integrate the learning abilities of neural networks with the symbolic structure of LTL commands to achieve a new capability: learning to perform end-to-end zero-shot execution of LTL commands.  Given a command represented as an LTL formula, our approach turns that formula into a specific recurrent deep network which encodes the meaning of that command; see~. % The resulting network takes as input the current map state, extracts the environment around the robot, processes it with a co-trained feature extraction network, and predicts which actions will satisfy the formula. % This compositional approach ties together neural networks and symbolic reasoning allowing any LTL formula to be encoded and followed, even if it has never been seen before at training time.  In our experiments, we generate random LTL formulas and train an RL agent to follow those formulas. % We develop a mechanism for generating hard and diverse LTL formulas, as random instances tend to be homogeneous and trivially solved. % This is generally useful for other large-scale experiments on following commands that can be encoded as LTL formulas. % In two different domains,  and , we show that this approach can learn to execute never-before-seen formulas. % The  domain is more akin to boolean satisfiability, where an accepting string must be generated for an LTL formula. % The  domain is a simplified Minecraft introduced by Andreas \etal 2017  to test the integration with robotics; see  for an example of the network in  executing a command in the  domain. % In all cases, we compare against baselines to demonstrate that each part of our model plays a key role in encoding temporal structures. % All components of our networks are learned end-to-end, in a process that automatically isolates the meaning of each sub-network allowing us to compose sub-networks together in novel ways.    This work makes four contributions: % [1.]  % This work can be seen as a novel approach to composing the policies of multi-task reinforcement learning agents in a principled manner according to a particular logic. % While we only discuss LTL here, this approach suggests how other logics might similarly be encoded to create new powerful zero-shot deep approaches to reinforcement learning. %  The best aspects of symbolic reasoning in robotics are compatible with deep networks when both are correctly formulated. % Perhaps in the future, such approaches could be used for model checking with LTL formulas.  % This general approach has the potential to significantly increase the space of % commands that are executable by trained robotic agents. % % % It also  % % Taking as input an LTL formula and finding satisfying assignments for model % checking and robotics is well-explored. %  % LTL on finite trace,   % benchmarking LTL satisfiability checking   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      We created a principled network that is able to encode the dynamics required to solve LTL formulas in a compositional manner.   This represents a new and powerful type of multi-task learning, where learning occurs on one set of tasks and generalizes to all others.   Coupled with a semantic parser, this model could execute linguistic commands that refer to temporal relations; we intend to pursue this in the future.   While we believe that such network architectures are useful for other logics, such as first-order logic, how precisely they should be extended to even more exotic modal logics or second-order logics is unclear.   We would additionally like to encompass more powerful logics like CTL.   Theoretically characterizing the structures required in neural networks to generalize out of domain to new problems generated from particular logics is a new and interesting problem that may impact our understanding of other types of generalization.  Perfect, 100\  generalization, performance on both Symbol and Craft appears to be within reach, although computationally this remains rather expensive.   Around 10 to 100 times more computationally intensive than the results reported here.   We intend to investigate what additional priors, curricula, or training algorithms can speed up learning to saturate performance in a more effective time frame.   Overall, depending on the experiment and number of formulas, the results here took between 13 hours to 3 days to generate with each run executing on an Nvidia Titan X.   Note that while relatively computationally intensive, such networks need only be trained once for any domain; due to their ability to zero-shot generalize to new formulas.  As it stands, the reward function designed here to supervise the RL agent is powerful but lacks some critical feedback.   For example, the agents do not know what went wrong.   Some feedback about the constraint that was violated could localize errors within particular sub-networks.   This would be akin to telling someone that they had picked up gold instead of silver; clearly very useful information.   Providing such targeted feedback to a compositional network seems possible in theory, one would need to more carefully determine which sub-networks could be at fault and emphasize updates to those sub-networks for a failed trial.   Turning this intuition into a practical learning mechanism remains an open problem.  In the future, we intend to combine this work with that presented by Kuo \etal 2020  and create a single agent capable of following complex linguistic commands in continuous environments.  
","   We demonstrate a reinforcement learning agent which uses a compositional   recurrent neural network that takes as input an LTL formula and determines   satisfying actions. The input LTL formulas have never been seen before, yet   the network performs zero-shot generalization to satisfy them. This is a novel   form of multi-task learning for RL agents where agents learn from one diverse   set of tasks and generalize to a new set of diverse tasks. The formulation of   the network enables this capacity to generalize. We demonstrate this ability   in two domains. In a symbolic domain, the agent finds a sequence of letters   that is accepted. In a Minecraft-like environment, the agent finds a sequence   of actions that conform to the formula. While prior work could learn to   execute one formula reliably given examples of that formula, we demonstrate   how to encode all formulas reliably. This could form the basis of new   multi-task agents that discover sub-tasks and execute them without any   additional training, as well as the agents which follow more complex   linguistic commands. The structures required for this generalization are   specific to LTL formulas, which opens up an interesting theoretical question:   what structures are required in neural networks for zero-shot generalization   to different logics?",139
"    The ability to understand and communicate in natural language can improve the accessibility of systems such as robots, home devices and computers to non-expert users.  %Such a system minimally requires a dialog agent that can understand high level natural language commands, and detect and indicate when it has failed to understand what a user requires. %Voice assistant applications that understand high level instructions in natural language are increasingly becoming a part of a variety of devices. Since language is often be ambiguous, it is desirable for such systems to engage in a dialog with the user to clarify their intentions and obtain missing information.  We use clarification to refer to any dialog act that enables the system to better understand an ongoing user request. Common clarification questions obtain or clarify the value of a slot or argument that is part of a goal the user is trying to communicate. %Sometimes, information might be missing in the original command, requiring the system to get more information to fully identify the action desired by the user.  A particular application may also contain domain-specific vocabulary or concepts that were not encountered during training. For example, a system in a shopping domain may need to be updated with the introduction of new clothing styles. Hence, it is desirable for a system to adapt to the operating environment using information from user interactions. We use the term active learning to refer to dialog acts used to obtain such knowledge with the primary purpose of improving the underlying language understanding model and thereby improving performance on future interactions.  Prior work on dialog and user interaction typically focuses either exclusively on clarification, or active learning.  The primary contributions of this work are introducing a dialog task that combines both clarification and active learning, and learning a corresponding dialog policy for this setting that outperforms a static baseline policy. Specifically, we train a hierarchical dialog policy to jointly learn to choose clarification and active learning queries in interactive image retrieval for a fashion domain.    A sample interaction is shown in Figure . We consider an application where a dialog system is combined with a retrieval system to help a customer find an article of clothing. Instead of just showing a large number of retrieved results, the dialog system attempts to use clarifications to refine the search query, and active learning questions to obtain labelled examples for novel concepts unseen during training.  %Clarification questions have been used in dialog systems for robotics applications but these typically use very constrained questions, such as selecting an item from a list .  Task-oriented dialog often requires the system to identify one or more user goals using a slot-filling model.  These systems learn to choose between a set of clarification questions that confirm or acquire the value of various slots.  However, for tasks such as natural language image retrieval, it is difficult to extend the slot-filling paradigm for clarification, as there is no standard set of slots into which descriptions of images can be divided. Also, learned models are needed to identify aspects such as objects or attributes, which are difficult to pre-enumerate. %Note that in both these cases, the response itself need not be a word from a list, but it is assumed that a language understanding component can map the response to a single slot value.  %Such a description is in fact likely to correspond to a single slot value in a larger task - say a task where a robot can store, fetch and manipulate objects.   Some tasks such as GuessWhat?! or discriminative question generation allow the system to ask unconstrained natural language clarification questions.  However they require specially designed models to ensure that learned questions actually decrease the search space.  Such open ended questions are also difficult to answer in simulation, which is often necessary for learning good dialog policies. Hence, in these tasks, the system often learns to ask ``easy'' questions that can be reliably answered by a learned answering module.  In this work, we explore a middle-ground approach with a form of attribute-based clarification.  We use the term ``attribute'' to refer to a mix of concepts including categories such as ``shirt'' or ``dress'', more conventional attributes such as colors, and domain specific attributes such as ``sleeveless'' and ``V-neck''. Although we work with a dataset that contains a fixed set of attributes annotated for each image, we simulate the setting where novel visual attributes are encountered at test time.   Dialog interaction can also be used to improve an underlying model using Opportunistic Active Learning   .  Active learning allows a system to identify unlabeled examples which, if labeled, are most likely to improve the underlying model.  OAL  incorporates such queries into an interactive task in which an agent may ask users questions that are irrelevant to the current dialog interaction to improve performance in future dialog interactions.  Opportunistic queries are more expensive than traditional active learning queries as they may distract from the task at hand, but they can allow the system to perform more effective lifelong learning.  Such queries have been shown to improve performance in interactive object retrieval.  However, this, and other works in reinforcement learning  of policies for active learning do not account for the presence of other interactive actions such as clarification.  We present a dialog task that combines natural language image retrieval with { attribute-based clarification. We then learn a hierarchical dialog policy that jointly learns to choose both appropriate clarification and active learning questions in a setting containing both uncertain visual classifiers and novel concepts not seen during training.  We observe that in our challenging setup, it is necessary to jointly learn dialog policies for choosing clarification and active learning questions to improve performance over employing one-shot retrieval with no interaction.    We demonstrate how a combination of RL learned policies for choosing attribute-based clarification and active learning queries can be used to improve an interactive system that needs to retrieve images based on a natural language description, while encountering novel attributes at test time not seen during training. Our experiments show that in challenging datasets where it is difficult to obtain an accurate attribute classifier, learned policies for choosing clarification and active learning queries outperform strong static baselines. We further show that in this challenging setup, a combination of learned clarification and active learning policies is necessary to obtain improvement over directly performing retrieval without interaction.   
"," Intelligent systems need to be able to recover from mistakes, resolve uncertainty, and adapt to novel concepts not seen during training.  Dialog interaction can enable this by the use of clarifications for correction and resolving uncertainty, and active learning queries to learn new concepts encountered during operation. Prior work on dialog systems has either focused on exclusively learning how to perform clarification/ information seeking, or to perform active learning.  In this work, we train a hierarchical dialog policy to jointly perform both clarification and active learning in the context of an interactive language-based image retrieval task motivated by an online shopping application, and demonstrate that jointly learning dialog policies for clarification and active learning is more effective than the use of static dialog policies for one or both of these functions.",140
"  Deep learning methods have achieved state-of-the-art performance in many applications when learning to solve a single problem. In this domain, current efforts are partly focused on engineering new neural architectures or exploring novel methods to improve the accuracy of these single-task models. There is another strain of research that explores learning models that share a certain degree of knowledge among different related tasks. A common approach is to use multitask learning .  In this setting, two or more related tasks are jointly trained such that it is expected that the model achieves better generalization for all the tasks. The main assumption is that related tasks might contain complementary information, which should act as an inductive bias to guide the model towards a better optimum as compared to training one model for each task in isolation .  Multitask learning has been applied to many problems over time, particularly in NLP.  proposed a unified architecture to mutually learn different sequence labeling tasks, a language modeling task and a semantically related words problem.  used an MTL-based model to solve 10 different tasks in natural language  by casting them as question answering.  proposed an MTL-based Named Entity Recognition  in biomedical text mining. In Biomedicine, there are a variety of named entities datasets for the different sub-domains . Some of these datasets are very small, which harms the performance of a model trained to classify named entities. However, obtaining labeled datasets is usually done manually by experts and are expensive to develop. Therefore,  cast domain-specific NER datasets as the different tasks and proposed a model to mutually learn them to improve generalization. Their results disclose yet another benefit of MTL models, by learning multiple tasks the model capitalizes from more data, such that tasks with small training sets can generalize better.  Even though great average improvements were achieved, the results are often mixed when evaluating each task in isolation and comparing them with the state-of-the-art performance of single-task models . The challenge of selecting a set of tasks, which guarantees better generalization for all tasks in multitask settings, remains partially unsolved. This is, to a certain extent, due to the few theoretical investigations that clearly expose the conditions under which multitask learning leads to better performance.  The few works that do try to theoretically understand generalization in MTL, however, rely mostly on strong assumptions about how tasks are related or are only applied to simple scenarios, such as settings where all tasks contribute with datasets of the same size.  proposed generalization bounds for MTL using VC-dimension and covering numbers, however, he assumes that task relatedness is given a priori.  used the Rademacher complexity to analyze linear multitask methods. In a later work,  have expanded their evaluation to nonlinear methods and assumed that similar tasks shared a common feature representation. They proposed a bound based on the Gaussian average while evaluating their theoretical results for noiseless binary classification tasks.  The mixed results obtained in MTL motivate to study models that are more theoretically grounded while avoiding being trapped into prior assumptions and constrained problems. In this work, we skip strictly deriving bounds for a well-defined domain and rather follow the scientific methodology that inspired Johannes Kepler to be the first to correctly explain the elliptical orbit of planets amongst other riddles involving planetary motion. In his work, Kepler applied the scientific methodology of using observational data to discover functions relating variables of interest. The laws empirically obtained by Kepler were later mathematically proven by the theoretical works of Isaac Newton.  Following an approach similar to Kepler's, we will try to learn formulas from data by using a method called Symbolic Regression. The empirically obtained expressions are able to explain the generalization performance of multitask models with respect to given parameter . With that, we also expect to find a more elementary way to group tasks in multitask models and elucidate the benefits of MTL.         In this work, we developed a simulated environment of sequence labeling tasks to explore the complexities of learning multiple tasks jointly. Using the data generated from a task simulator, we were able to investigate the generalization patterns of MTL models. Further, we resorted to a learning algorithm to obtain symbolic expressions that can assist practitioners to understand the factors which contribute to the generalization performance of MTL models in the sequence labeling domain. Our findings were in accordance with previous theoretical works, yet we were able to add additional insights to the unexplored setting of unbalanced datasets. In addition to that, we explicitly quantified task relatedness rather than making any assumption about it or trying to learn it implicitly as a parameter of the model.   
"," We study and quantify the generalization patterns of multitask learning  models for sequence labeling tasks. MTL models are trained to optimize a set of related tasks jointly. Although multitask learning has achieved improved performance in some problems, there are also tasks that lose performance when trained together. These mixed results motivate us to study the factors that impact the performance of MTL models. We note that theoretical bounds and convergence rates for MTL models exist, but they rely on strong assumptions such as task relatedness and the use of balanced datasets. To remedy these limitations, we propose the creation of a task simulator and the use of Symbolic Regression to learn expressions relating model performance to possible factors of influence. For MTL, we study the model performance against the number of tasks , the number of samples per task  and the task relatedness measured by the adjusted mutual information . In our experiments, we could empirically find formulas relating model performance with factors of $$, $$,  which are equivalent to sound mathematical proofs in , and we went beyond by discovering that performance relates to a factor of $$.",141
" %1. background Inspired by the success of BERT on natural language understanding, there has been a surging research interest in developing multimodal pre-training methods for vision-and-language representation learning . When finetuned on downstream tasks, these pre-trained models have achieved state-of-the-art performance across diverse V+L tasks, such as Visual Question Answering , Visual Commonsense Reasoning , and Referring Expression Comprehension.  %2. motivation However, due to the immense capacity of large-scale pre-trained models yet limited amount of labeled data in downstream tasks, aggressive finetuning often falls into the overfitting trap. , a method to combat adversarial attacks in order to create robust neural networks, has recently shown great potential in improving the generalization ability of pre-trained language models and image classifiers. A natural question that came to our mind: can we apply similar adversarial training techniques to V+L problems to improve model performance?  % 3. our proposed method We propose Villa %, %\jj{How about Avalan? The modeling part seems a bit stretchy.}  , which advocates the use of adversarial training for V+L representation learning. As illustrated in Figure, Villa consists of two training stages:   adversarial pre-training ; followed by   adversarial fine-tuning . Intuitively, if well-designed, multimodal pre-training tasks such as image-conditioned masked language modeling and image-text matching can resonate well with many downstream tasks that require visual grounding and reasoning abilities. This leads to our hypothesis that the improved generalization ability of pre-trained models learned during APT stage can be readily transferred to the AFT stage for diverse tasks. In other words, APT is able to uniformly lift model performance for all downstream tasks in a task-agnostic way, while AFT can further enhance the finetuned models by leveraging task-specific supervision signals.  To bring in more flexibility in generating adversarial examples for robust training, we propose to perform adversarial training on the embedding level for multi-modalities, instead of operating on image pixel and sub-word token level in conventional practice. For text modality, we add adversarial perturbations to word embeddings. For image modality, most previous work observes that robustness is at odds with generalization, , trained models are able to resist adversarial attacks on clean images at the expense of performance. Distinctive from these studies, we directly add adversarial perturbations to extracted image-region features, as our end goal is the final V+L model performance rather than crafting adversarial image examples. Experiments show that this strategy leads to large performance gain on clean inputs.   Adversarial training procedure is time-consuming and computationally expensive. To power efficient large-scale training, we adopt the recently proposed ``free'' adversarial training strategy, which obtains the gradients of parameters with almost no extra cost when computing the gradients of inputs. In addition to requiring adversarial perturbations to be label-preserving, we also introduce KL-divergence-based regularization to enforce the confidence level of the prediction to be close, characterized by the ``dark'' knowledge hidden in the probability vectors. This promotes higher smoothness of the training objective and has empirically proven as important regularization effective for further performance boost.   For evaluation, we mostly focus on UNITER, the current best-performing V+L model with state-of-the-art performance across many popular V+L benchmarks, and enhance UNITER with Villa through comprehensive experiments on six V+L tasks: VQA, VCR, NLVR, Visual Entailment, Referring Expression Comprehension, and Image-Text Retrieval. Villa is a generic framework that can be applied to any multimodal pre-training method. To demonstrate its versatility, we further apply it to LXMERT on VQA, GQA, and NLVR tasks for generalizability test.  [t!]       framework for vision-and-language representation learning.}               The main contributions are summarized as follows.  We present Villa, the first known effort on adversarial pre-training and adversarial finetuning for V+L representation learning.  Instead of operating on pixel and word token level, we propose to add adversarial perturbations in the embedding space of multi-modalities, and introduce a smoothness-inducing adversarial regularization term on top of the ``free'' adversarial training strategy.  Villa achieves new state of the art across six popular V+L tasks. In particular, by relying on standard bottom-up image features only, Villa improves the single-model performance of UNITER-large from 74.02 to 74.87 on VQA, and from 62.8 to 65.7 on VCR. With ensemble, VQA performance is further boosted to 75.85.  %To the authors' best knowledge, this is the first work that studies adversarial training for V+L models.         In this paper, we present , an advanced adversarial training  framework for better vision-and-language representation learning. By performing AT in both pre-training and finetuning stages, and by adding adversarial perturbations to the embedding space,   achieves consistent performance boost on all the benchmarks evaluated.   Even after adopting the ``free'' training strategy,  As AT is time-consuming, for future work, we plan to study how to accelerate AT so that it can be more feasible for large-scale pre-training in practice.   Further, how to incorporate AT with V+L model compression is another interesting research direction.  \jj{Do we want to reveal that our next project is Adv V+L compression? :)}   \clearpage  
","   We present Villa, the first known effort on large-scale adversarial training for vision-and-language  representation learning. Villa consists of two training stages:  task-agnostic adversarial pre-training; followed by  task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the ``free'' adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply Villa to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR$^2$.\footnote{Code is available at \url{https://github.com/zhegan27/VILLA}.}",142
"  Mathematicians solve problems by relying on rules, correct derivations and proven methods of computation that are guaranteed to lead to a correct solution. Over time, they have developed a rich set of computational techniques that can be applied to many problems, and were said to be  ``unreasonably effective'' . Most of those computational techniques are not intuitive, they have to be derived from theory and applied by trained scientists or built into software libraries. They are the building blocks of every advanced computation.   Many recent studies showed that deep learning models can learn complex rules from large datasets, only from examples. In natural language processing, models learn to output grammatically correct sentences without prior knowledge of grammar and syntax , or to automatically map one language into another . In mathematics, deep learning models have been trained to perform logical inference , SAT solving , basic arithmetic  and symbolic integration .  In this paper, we investigate whether deep learning models can be trained to perform complex computations and to deduce the qualitative behavior of mathematical objects, without built-in mathematical knowledge. We consider three questions of higher mathematics: the local stability and controllability of differential systems, and the existence and behavior at infinity of solutions of partial differential equations. All three problems have been widely researched and have many applications outside of pure mathematics. They have known solutions that rely on advanced symbolic and computational techniques, from formal differentiation, Fourier transform, geometrical full-rank conditions, to function evaluation, matrix inversion, and computation of complex eigenvalues. Surprisingly, we find that neural networks can solve these problems with a very high accuracy, by simply looking at instances of problems and their solutions, while being totally unaware of the underlying theory. These results are unintuitive given the advanced numerical techniques required by the theory and the difficulty of neural networks to perform simple arithmetic tasks, suggesting that the model might be using a different approach than the known theory to correctly predict the output. %   After reviewing prior applications of deep learning to differential equations and symbolic computation, we introduce the three problems we consider, describe how we generate datasets, and detail how we train our models. Finally, we present our experiments and discuss their results.     In this paper, we show that by training transformers over generated datasets of mathematical problems, advanced and complex computations can be learned, and qualitative and numerical tasks performed with high accuracy. Our models have no built-in mathematical knowledge, and learn from examples only. It seems that our models have learned to solve these problems, but this does not mean they learned the techniques we use to compute their solutions. Problems such as non-autonomous control involve long and complex chains of computations. Yet, even very small models  achieve high accuracy.  Most probably, our models learn shortcuts that allow them to solve specific problems, without having to learn or understand their theoretical background.  Such a situation is common in everyday life. Most of us learn and use language without understanding its rules. On many practical subjects, we have tacit knowledge and know more than we can tell . This may be the way neural networks learn advanced mathematics. Understanding what these shortcuts are and how neural networks discover them is a subject for future research.    
"," Can advanced mathematical computations be learned from examples? Using transformers over large generated datasets, we train models to learn properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect estimates of qualitative characteristics of the systems, and good approximations of numerical quantities, demonstrating that neural networks can learn advanced theorems and complex computations without built-in mathematical knowledge.",143
"  The non-local block, which models long-range dependency between pixels, has been widely used for numerous visual recognition tasks, such as object detection, semantic segmentation, and video action recognition. Towards better understanding the non-local block's efficacy, we observe that it can be viewed as a self-attention mechanism for pixel-to-pixel modeling. This self-attention is modeled as the dot-product between the features of two pixels in the embedding space. At first glance, this dot-product formulation represents  relationships. After further consideration, we find that it may encode  information as well, in the sense that a pixel may have its own independent impact on all other pixels. Based on this perspective, we split the dot-product based attention into two terms: a whitened pairwise term that accounts for the impact of one pixel { over all the pixels.  [t]             We investigate the visual properties of each term without interference from the other. Specifically, we train two individual networks, with either the whitened pairwise term or the unary term removed in the standard attention formula of the non-local block. It is found that the non-local variant using the whitened pairwise term alone generally learns within-region relationships , while the variant using the unary term alone tends to model salient boundaries . However, the two terms do not learn such clear visual clues when they are both present within a non-local block, as illustrated in the top row of Fig.. This observation is verified via statistical analysis on the whole validation set. Also, the standard non-local block combining both terms performs even worse than the variant that includes only the unary term . This indicates that coupling the two terms together may be detrimental to the learning of these visual clues, and consequently affects the learning of discriminative features.  To address this problem, we present the disentangled non-local  block, where the whitened pairwise and unary terms are cleanly decoupled by using independent Softmax functions and embedding matrices. With this disentangled design, the difficulty in joint learning of the whitened pairwise and unary terms is greatly diminished. As shown in second row of Fig., the whitened pairwise term learns clear within-region clues while the unary term learns salient boundaries, even more clearly than what is learned when each term is trained alone.  The disentangled non-local block is validated through various vision tasks. On semantic segmentation benchmarks, by replacing the standard non-local block with the proposed DNL block with all other settings unchanged, significantly greater accuracy is achieved, with a 2.0\% mIoU gain on the Cityscapes validation set, 1.3\% mIoU gain on ADE20k, and 3.4\% on PASCAL-Context using a ResNet-101 backbone. With few bells and whistles, our DNL obtains state-of-the-art performance on the challenging ADE20K dataset. Also, with a task-specific DNL block, noticeable accuracy improvements are observed on both COCO object detection and Kinetics action recognition.    In this paper, we first study the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term and a unary term. Via both intuitive and statistical analysis, we find that the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design for learning visual clues on various vision tasks, such as semantic segmentation, object detection and action recognition.     \clearpage   } \normalsize    
","  The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics. Code is available at \\\url{https://github.com/yinmh17/DNL-Semantic-Segmentation}, \\ \url{https://github.com/Howal/DNL-Object-Detection}.",144
" Online recruitment platforms, e.g., LinkedIn, %%ww: add Ping'an system here make it easy for companies to post jobs and for job seekers to submit resumes. In recent years, the number of both job posts and resumes submitted to online recruitment platforms is growing rapidly. For example, in U.S, there are over 3 million jobs posted on LinkedIn in every month. %% https://technode.com/2019/07/10/online-job-site-boss-zhipin-is-profitable-and-ready-for-ipo-ceo-says/ % Fig. shows the monthly job posts and resumes on our company's internal recruitment platform.  Traditionally, resumes submitted for each job are reviewed manually by the recruiter to decide whether to offer the candidates the job interview. However, manual reviewing is slow and expensive to handle the overwhelming new job posts and resumes on online platforms. It is essential to design effective algorithms to do job-resume matching automatically. This problem is called person-job fit.   Multiple approaches have been proposed for person-job fit. Earlier solutions consider person-job fit as a recommendation problem and apply collaborative filtering  algorithms. However, CF algorithms ignore the content of the job post and the resume, e.g., the working experience of the candidate and the job requirement. In contrast, when we do manual reviewing, we read the resume to understand the candidate ; we read the job post to understand the requirements; then we make a decision, i.e., whether the candidate should be offered an interview. We can see that the content of the resume and job post plays a key role in person-job fit. It is thus vital to extract effective representation of the content.   Recently, deep learning models have largely improved the performance of natural language processing tasks, including semantic matching and question answering. Deep-learning-based methods are consequently introduced for person-job fit, focusing on learning effective representations of the free text of the job post and resume. The learned representations are then compared to generate a matching score. However, they only process the text paragraphs including the working experience and job requirements, and fail to comprehend other  structured fields like the education, skills, etc. This is partly because  %these fields are short in terms of number of words, whereas  deep learning models are typically applied for natural language sentences instead of  structured fields. As a result, valuable information from these fields are left unexploited.        Moreover, most of existing deep-learning-based solutions ignore the historical applications of the candidate and the job post. It is common for a candidate to apply multiple jobs and for a job to receive multiple resumes, as shown in Figure. The numbers are derived from our experiment dataset. In specific, about 36\% have applied more than one jobs and about 88\% jobs have received more than one resumes.  The history data could provide extra information of the candidate and job. Specifically, sometimes the job description is not written carefully or comprehensively, e.g., missing some requirements or unclear preference between two skills ; sometimes the recruiter's requirement or expectation may change dynamically, e.g., increasing if the received resumes so far are of very high quality. For such cases, the history data including accepted resumes and rejected resumes of a job could help to infer the recruiter's implicit intentions not elaborated in the job description.     In addition, deep learning models are typically difficult to interpret due to complex internal transformations, although a lot of attention has been paid to this issue. As a result, deep-learning-based person-job fit solutions face the interpretation problem. In real deployment, yet, it is necessary to explain why a candidate is accepted or rejected for a given job post.   Towards these issues, in this paper, we propose a feature fusion solution. First, we propose a semantic entity extraction step to extract semantic entities, including the university and working years, from the whole resume and job post. All extracted entities are then concatenated into a vector, which captures the high-level semantics of the content and is easy to interpret. The semantic vector is further transformed through an adapted DeepFM model to learn the correlations among the entities. We also apply a convolutional neural network  over the text fields in the resume  following existing works. The outputs from DeepFM and CNN are fused via concatenation to produce a feature vector representing the explicit intention of a resume .  Second, to exploit the history information, we propose a new encoding scheme for the job-resume pair from an application. All the historical applications, including both the accepted and rejected cases, of a candidate  are processed by a LSTM model to learn the implicit intention.  Last, we do a late fusion of the representations for the explicit and implicit intentions to represent the job and candidate comprehensively.   Extensive experimental evaluations over real data show that our solution outperforms existing methods. We also conduct ablation studies to verify the effectiveness of each component of our solution. In addition, case studies are presented to demonstrate the contribution of semantic entities to model interpretation. Our solution has been deployed partially for one year. Some experience on improving the efficiency and reducing the cost will be shared at the end of this paper.  Our contributions include            In this paper, we have introduced a feature fusion based method, called PJFFF, for the person-job fit problem. Our method differs to existing solutions in two aspects. First, instead of purely relying on deep learning models to process the free text fields in a resume and job post, PJFFF extract semantic entities from the whole resume and job post in order to learn comprehensive explicit features. Second, we propose a new scheme to encode the historical application data of each candidate and each job post. They are fed into LSTM models to learn implicit intentions of the candidate and recruiters respectively. Finally, the two sets of features are fused together via concatenation to produce an effective representation. Experimental study confirms the superiority of our method over existing methods and verifies the contributions from each component of PJFFF.        
"," Person-job fit is to match candidates and job posts on online recruitment platforms using machine learning algorithms.  The effectiveness of matching algorithms heavily depends on the learned representations for the candidates and job posts.  In this paper, we propose to learn comprehensive and effective representations of the candidates and job posts via feature fusion. First, in addition to applying deep learning models for processing the free text in resumes and job posts, which is adopted by existing methods, we extract semantic entities from the whole resume  and then learn features for them. By fusing the features from the free text and the entities, we get a comprehensive representation for the information explicitly stated in the resume and job post. Second, however, some information of a candidate or a job may not be explicitly captured in the resume or job post. Nonetheless, the historical applications including accepted and rejected cases can reveal some implicit intentions of the candidates or recruiters. Therefore, we propose to learn the representations of implicit intentions by processing the historical applications using LSTM. Last, by fusing the representations for the explicit and implicit intentions, we get a more comprehensive and effective representation for person-job fit. Experiments over 10 months real data show that our solution outperforms existing methods with a large margin. Ablation studies confirm the contribution of each component of the fused representation. The extracted semantic entities help interpret the matching results during the case study.",145
"   There is a growing line of work  demonstrating that neural networks can leak information about the underlying training data in unexpected ways. In particular, many of these works show that generative sequence models, which include commonly-used language models, are prone to  rarely-occurring phrases in the data. Large-scale learning often involves training over sensitive data, and such memorization can result in blatant leaks of privacy . Thus, for any novel learning framework of interest, it is crucial to test the resilience of models trained in the framework against such memorization. Techniques to mitigate memorization must be identified to ensure the privacy of training data.  The framework of Federated Learning  has emerged as a popular approach for training neural networks on a large corpus of decentralized on-device data . FL operates in an iterative fashion: in each round, sampled client devices receive the current global model from a central server to compute an update on their locally-stored data, and the server aggregates these updates using the Federated Averaging algorithm to build a new global model. A hallmark of FL is that each participating device only sends model weights to the central server; raw data never leaves the device, remaining locally-cached. Although this, by itself, is not sufficient to provide formal privacy guarantees  for the training data, it is important to note that the canonical setting of FL does differ in many aspects from the well-studied  setting where all the data is stored at a central server. In this work, we initiate a formal study to understand the effect of the different components of FL, compared to the central learning setting, on unintended memorization in trained models.  We also investigate the effect of using a training procedure, with a formalized privacy guarantee, on such memorization. To this end, we use Differential Privacy  , which has become the standard for performing learning tasks over sensitive data. DP has been adopted by companies like Google, Apple, Microsoft, and LinkedIn, as well as the US Census Bureau. Intuitively, DP prevents an adversary from confidently making any conclusions about whether any particular user's data was to train a model, even while having access to the model and arbitrary external side information.      We build on the ``secret sharer"" framework from  that was designed to measure the unintended memorization in generative models.  At a high-level,  examples  are inserted into a training corpus, and a model trained on this corpus is then evaluated using various techniques to measure the extent to which the model has  the canaries. Since datasets in FL are inherently partitioned according to users, we adapt this framework to the FL regime by  introducing two parameters to control the presence of a  canary in such settings.  An illustration of our federated secret sharer framework is shown in Figure.  Given a canary with parameters  and , we let  be the  probability with which each user in a dataset is selected to be a secret sharer of the canary , whereas  denotes the probability with which each example in such a secret sharer's data is replaced by the canary .    Our empirical evaluations  for this paper demonstrate the following key contributions. First, we show clustering the training data according to users,  has a significant effect in reducing unintended memorization. Note that such a clustering of the data happens by design in FL settings. Next, given data clustered according to users, we show that replacing the learning optimizer from SGD to Federated Averaging provides a further reduction in such memorization. Lastly, we demonstrate that training in FL with a strong user-level DP guarantee results in models that exhibit the least amount of unintended memorization.  \mypar{Organization of the paper} We provide a formal definition of differential privacy in Section. In Section, we identify the main components separating central learning from the canonical setting of FL. Section contains the results of our empirical evaluation. We state the conclusions of this work in Section.      There is a wide variety of work demonstrating unexpeted information leakage from datasets in unexpected ways:  design a general  whereas there are other works  that design . Apart from  , other works have also studied memorization in generative text models.  The FL paradigm, which is a major focus of this work, has been used to train multiple production scale models. We refer the reader to  which provides an excellent overview of the state-of-the-art in the field, along with a suite of interesting open problems.  This work also studies the effectiveness of a user-level DP guarantee in reducing unintended memorization.  While many works on DP focus on  DP guarantees , recent works  have designed techniques tailored to user-level DP guarantees.       \usepackage{amsthm,amsmath} \usepackage{bbm} \usepackage{times} \usepackage{array,float} \usepackage{url} \usepackage{amstext,amssymb} \usepackage{hyphenat} \usepackage{verbatim} \usepackage{dsfont} \usepackage{bm} \usepackage{wrapfig} \usepackage[noend]{algorithmic} \usepackage{algorithm} \usepackage{graphicx,color} \usepackage{xparse,etoolbox} \usepackage{threeparttable} \usepackage{dsfont} \usepackage{xspace} \usepackage{subcaption} \usepackage[utf8]{inputenc} \usepackage{outlines} \usepackage{comment} \usepackage{footnote} \usepackage{multirow} \usepackage{ulem} \usepackage{cleveref} \usepackage{authblk}  {}} {}} {Theorem} {Lemma}[section] [theorem]{Remark} [1]{\|#1\|_[lemma]{Informal Theorem} [lemma]{Corollary} [lemma]{Problem} [lemma]{Definition} [lemma]{Fact} [lemma]{Assumption} [lemma]{Claim} {Input:} \renewcommand{\algorithmicensure}{Output:} [theorem]{Definition} {Corollary}[section] [1]{ #1} {} {\mathcal{M}}  {\Delta} {\mathcal{N}} {}   {} {[2]{{#2}}  [1]{\left\|#1\right\|_{{} } [1]{\left\|#1\right\|_{F}} [1]{\|#1\|_{\C}} [1]{\|#1\|_{\Q}} [1]{\|#1\|_{\Q^*}} [1]{\|#1\|_{\Q,q}} [1]{\|#1\|_{}} [1]{\|#1\|_{\C^*}} {\theta} {\text{polylog}\,} {[1]{\left\|#1\right\|_F} {{{\theta^{priv}}  \renewcommand{\paragraph}[1]{}  } } }} }  [1]{{#1}\,} {Informal Theorem}[section]    {\tilde{h}}  {\tilde{x}} {\tilde{y}} {\tilde{z}}   {\mathbb{I}}   {\mathsf{True}} {\mathsf{False}} {\mathsf{Flag}}  {\mathsf{PrvLearn}} }  }}  {\mathsf{Priv}}   {\mathsf{AUX}}     }  }  {\mathsf{left}} {\mathsf{right}}  {\gamma_{\mathsf{f}}}     \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}   {{[2]{\underset{#1}{\mathbb{P}}\left[ #2 \right]} }\left[ #2 \right]} [2]{\underset{#1}{\mathbf{Var}}\left[ #2 \right]}  {\operatorname{\rm Bin}} {\operatorname{\rm Pois}} {} {} \def\polylog{\operatorname{polylog}} {} } } } } {}}  [1]{\|#1\|} [1]{\|#1\|_2} [1]{\|#1\|_1} [1]{\|#1\|_{{{\tilde{{\mathcal{A}}  {\tilde{\mathcal{I}}} }} }} }}    } {a_{     }       {\tilde{{\tilde{j}}  }      }  } {\mathcal{Z}_{+}} {\mathcal{Z}_{-}} {},~\frac{1}{}\}^m}   {{{{} {\mathbb{E}}    }      {\theta^\perp} {w^\dagger} {\tilde{w}} {\tilde{F}} {\tilde{f}} {\mathbb{R}} {\mathcal{R}} {\mathbb{B}}  {\operatorname{\rm Enc}} {\operatorname{\rm Dec}}  \renewcommand{\S}{\mathbb{S}} {\tilde{  {\tilde{\mathcal{U}}} {\tilde{D}} {[1]{:}}} }  {LDP} {{{\tilde J}^{\text{priv}}}} {{{J}^\#}}  \renewcommand{ [1]{{TODO: #1}}}    {\mathcal{A}}     }   {\mathsf{poly}}  {\mathsf{Median}}  {\mathsf{GenProj}}  {\mathsf{GenHist}}  {\mathsf{RndGen}}  {\mathsf{Error}}   {\mbox{on-average}}  {\mbox{FO}} {{{{{{{{   {\mathsf{Prefixes}} {\mathsf{NewPrefixes}} {\mathsf{SuccHist}} {\mathsf{FreqList}} {\mathsf{Final}} {\mathsf{FreqEst}} {{{{{\Gamma} {{{\widehat{\ds}} {{} \def\pfwe{\A_{} {{{{[2]{{{#1}^{}}} \def}  {{\widehat v}} {{\widehat u}} {joint differential privacy\,} {Joint Differential Privacy\,} {Joint differential privacy\,} {{{Y}  {{\widehat{Y}}} {\Theta^{[1]{\left\|#1\right\|_{{{\widehat{\mat{Y}}}} {1}\tag{\theequation}} {{\widehat{V}}} {{\widehat{\Pi}}} {{\Pi^{ \renewcommand\Authands{\qquad}   \makeatletter } \makeatother   \renewcommand{\paragraph}[1]{\medskip }  {List} {\mathsf{P_{error}}} {\mathsf{P_{min-error}}}  [1]{{\left}}   \title{Understanding Unintended Memorization \\ in Federated Learning} \author{Om Thakkar} \author{Swaroop Ramaswamy} \author{Rajiv Mathews} \author{Fran鑾給ise Beaufays} \affil{Google LLC,\\ Mountain View, CA, U.S.A. \\    @google.com}}                 \newpage \appendix       In this work, we conduct a formal study to understand the effect of the different components of Federated Learning, on the unintended memorization in trained models, as compared to the well-studied central learning. From our results, we observe that the components of FL exhibit a synergy in reducing such memorization. In particular, user-based heterogeneity of data  has a significant effect in the reduction, and training using Federated Averaging reduces it further. Moreover, we observe the least amount of memorization in the models where we train in FL with strong user-level differential privacy guarantees.  Recent work has shown that, in general, such heterogeneity in the training data can result in a slower and unstable convergence due to factors such as ``client-drift"". For all of the experiments with non-IID data, we observe that the utility of the trained models is comparable to those trained on IID data, and we leave further exploration into why client-drift may not play a significant role in our experiments for future work. Lastly, the secret-sharer line of methods for measuring unintended memorization operate at the granularity of a record. For future work, it will be interesting to design stronger attacks targeting data at the granularity of a user, and measure the resilience of models trained via FL, against such memorization.   
"," Recent works have shown that generative sequence models  have a tendency to memorize rare or unique sequences in the training data. Since useful models are often trained on sensitive data, to ensure the privacy of the training data it is critical to identify and mitigate such  memorization. Federated Learning  has emerged as a novel framework for large-scale distributed learning tasks. However, it differs in many aspects from the well-studied  setting where all the data is stored at the central server. In this paper, we initiate a formal study to understand the effect of different components of canonical FL on unintended memorization in trained models, comparing with the central learning setting. Our results show that several differing components of FL play an important role in reducing unintended memorization. Specifically, we observe that the clustering of data according to users---which happens by design in FL---has a significant effect in reducing such memorization, and using the method of Federated Averaging for training causes a further reduction. We also show that training with a strong user-level differential privacy guarantee results in models that exhibit the least amount of unintended memorization.",146
"  Conversational search has recently attracted much attention as an emerging information retrieval  field. The ultimate goal of conversational search systems is to address user information needs through multi-turn natural language conversations. This goal is partially addressed in previous work with several simplifying assumptions. For example, the TREC Conversational Assistance Track  in 2019 has focused on multi-turn conversational search, in which users submit multiple related search queries. Similarly, conversational question answering based on a set of related questions about a given passage has been explored in the natural language processing  literature. However, the existing settings are still far from the ideal  scenario, in which both user and system can take any permitted action at any time to perform a natural conversation. In other words, most existing work in conversational search assumes that users always ask a query and the system only responds with an answer or a ranked list of documents.   Recent conversational information seeking platforms, such as Macaw, provide support for multi-turn, multi-modal, and mixed-initiative interactions. There have been recent efforts to go beyond the ``user asks, system responds'' paradigm by asking clarifying questions from the users, including offline evaluation of search clarification, clarifying question generation for open-domain search queries, and preference elicitation in conversational recommender systems. Past research in the area of search clarification has shown significant promise in asking clarifying questions. However, utilizing user responses to clarifying questions to improve the search performance has been relatively unstudied. In this paper, we propose a model that learns an accurate representation for a given user-system conversation. We focus on the conversations in which the user submits a query, and due to uncertainty about the query intent or the search quality, the system asks one or more clarifying questions to reveal the actual information need of the user. This is one of the many necessary steps that should be taken to achieve an ideal mixed-initiative conversational search system.  Motivated by previous research on improving query representation by employing other information sources, such as the top retrieved documents in pseudo-relevance feedback, we propose a neural network architecture that uses multiple information sources for learning accurate representations of user-system conversations. We extend the Transformer architecture by proposing a novel attention mechanism. In fact, the sequence transformation in Transformer networks are guided by multiple external information sources in order to learn more accurate representations. Therefore, we call our network architecture  or \model. %It learns both self-attention and cross-attention from multiple external information sources.  We train an end to end network based on the proposed architecture for two downstream target tasks: document retrieval and next clarifying question selection. In the first target task, the model takes a user-system conversation and scores documents based on their relevance to the user information need. On the other hand, the second task focuses on selecting the next clarifying question that would lead to higher search quality. For each target task, we also introduce an auxiliary task and train the model using a multi-task loss function. The auxiliary task is identifying the actual query intent description for a given user-system conversation. For text representation, our model takes advantage of BERT, a state-of-the-art text representation model based on the Transformer architecture, modified by adding a ``task embedding'' vector to the BERT input to adjust the model for the multi-task setting.   In our experiments, we use two sets of information sources, the top retrieval documents  and the pool of different clarifying questions for the submitted search query. The rational is that these sources may contain some information that helps the system better represent the user information needs. We evaluate our models using the public Qulac dataset and follow the offline evaluation methodology recently proposed by . Our experiments demonstrate that the proposed model achieves over  relative improvement in terms of MRR compared to competitive baselines, including state-of-the-art pseudo-relevance feedback models and BERT, for the document retrieval task. We similarly observe statistically significant improvements in the next clarifying question selection task compared to strong baselines, including learning to rank models that incorporate both hand-crafted and neural features, including BERT scores.   In summary, the major contributions of this work include: [leftmargin=*]       % However, the existing settings are still far away from the ideal mixed initiative scenario, where each entity of system can take any permitted action at any time.  % Information seeking conversational search is an emerging field in information retrieval. The definition of the task in the general sense is acquiring users' information needs from a search engine through a natural form of a conversation. This goal is partially addressed in the previous works with some simplification assumption. However, the existing settings are still far away from the ideal mixed initiative scenario, where each entity of system can take any permitted action at any time.  % Significant number of existing works on conversational search are in ``user asks, system responds'' setting, that is, users issue one or series of queries, and system responds the user's information need with a result list. In contrast, here we try to get one step closer to conversational search in a mixed initiative scenario, where user and system communicate in a more interactive way. We study conversational search systems in the setting where in cases that the initial query is ambiguous or faceted, the system can get back to users with a clarifying or follow up questions, rather than a merely result list, to get better understanding of what user intend is, and serves the ultimate goal, which is addressing user information need, in a more satisfying way.     % In this paper we introduce \model, which is an extension to the transformer architecture with the ability of utilizing external information. We applied our model for task of representation learning in conversation search, where the clarifying questions form the system side is also allowed , and the system is responsible to retrieve a list of passage or a document level answers from a corpus . To address this problem, we hypothesis that the conversation context itself is not enough for a sufficient history representation learning. Accordingly, we suggest, to understand a more universal representation of the conversation context, the model, also, utilize an external source information into the learning process. Generally speaking, the external source could be any source which contains related information to the initial query. In this paper, we conducted our experiments on two different external source: 1- Pool of the clarifying questions for each query, which enable the system to select the most appropriate clarifying question among the pool, from the user when it is required.  % Our model integrated the external information into the learning process with enjoying the novel \model attention mechanism. For the contextualized representation of the text, we use the state of the art model for language modeling, Bert , while we modify each transformer layer of the Bert, and embed a \model layer in it. We also add ``task embedding'' to original Bert architecture which, helps us to reuse one model for multiple task, and use the recourse in a more efficient way.   % We evaluated our model in two down-stream task: 1- re-ranking 2- Selecting the next clarifying question for the conversation. The idea is if a representation is good, it should improves the end goals. In re-ranking task, we show how a more comprehensive representation for the conversation context leads to a better result list. In the second task, selecting the next clarifying question, we show how the learned representation helps the system to take wiser choices in each turn of the conversation, and ask better clarifying questions from the user.      % All in all, our contribution in this paper can be summarized as:    % The following part of this paper is as follows: section  is about the importance of the task, and existing shortcomings, section  is about the previous works related to this task, section  describe our proposed model, section  elaborate on experimental setup, and section  conclude all of our findings.         %   In this paper, we introduced Guided Transformer  by extending the Transformer architecture. \model can utilize external information sources for learning more accurate representations of the input sequence. We implemented \model for conversational search tasks with clarifying questions. We introduced an end to end model that uses a modified BERT representations as input to \model and optimizes a multi-task objective, in which the first task is the target downstream task and the second one is an auxiliary task of discriminating the query intent description from other query intents for the input user-system conversation. We evaluated the proposed model using the recently proposed Qulac dataset for two downstream tasks in conversational search with clarification:  document retrieval and  next clarifying question selection. The experimental results suggested that the models implemented using \model substantially outperform state-of-the-art baselines in both tasks.   In the future, we intend to extend \model to a broad range of text understanding tasks using external sources. In the realm of conversational search, future work can focus on user past history as an external source for personalized conversational search. There are multiple different external sources that can use \model as an underlying framework for accurate representation learning for conversational search, such as query logs, clickthrough data, and knowledge bases. Most importantly, we are interested in extending this study to a fully mixed-initiative conversational search system, in which users can ask follow-up questions and at the same time the system can take different actions.     } This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-1715095. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.   
"," Asking clarifying questions in response to ambiguous or faceted queries has been recognized as a useful technique for various information retrieval systems, especially conversational search systems with limited bandwidth interfaces. Analyzing and generating clarifying questions have been studied recently but the accurate utilization of user responses to clarifying questions has been relatively less explored. In this paper, we enrich the representations learned by Transformer networks using a novel attention mechanism from external information sources that weights each term in the conversation. We evaluate this Guided Transformer model in a conversational search scenario that includes clarifying questions. In our experiments, we use two separate external sources, including the top retrieved documents and a set of different possible clarifying questions for the query. We implement the proposed representation learning model for two downstream tasks in conversational search; document retrieval and next clarifying question selection. Our experiments use a public dataset for search clarification and demonstrate significant improvements compared to competitive baselines.  % Hamed: I just edited the abstract. It's not perfect and doesn't contain some information about multi-tasking, etc. But it should be sufficient for abstract submission.  % Conversational search in information retrieval is an emerging task which recently attracted much attention. Here, we want to focus on conversational search systems in a setting that system can get back to user with a clarifying question, for faceted or ambiguous queries. The task that we want to study is learning a neural representation for an interactive conversation history between a user and a system. We hypothesis that, the conversation context, itself is not enough for learning a good history representation. Accordingly, we introduce two external resources for boosting the representation learning. To be able to utilize information of the external resource into modeling, we extended the architecture of transformer, such that, in addition to self-attention mechanism, it could apply some external attention to the input as well. We train our model in multi-task learning regime, and evaluate on two down-stream task: 1-re-ranking, and 2- selecting next clarifying question. Experiments on both tasks show significant improvement over both supervised, and non-supervised baselines.",147
"  Humans can learn from captioned images because of their ability to associate words to image regions. For instance, humans perform such word-region associations while acquiring facts from news photos, making a diagnosis from MRI scans and radiologist reports, or enjoying a movie with subtitles. %Likewise, computers could better perform diagnosis, retrieval, and other tasks if they could associate words with images in medical reports, new photos, and other multimedia documents. %Humans often acquire new visual information by associating novel textual information with parts of an image. We perform such an association between text and image regions while understanding images in medical reports, news photos, movies with subtitles etc.  This word-region association problem is called word or phrase grounding and is a crucial capability needed for downstream applications like visual question answering, image captioning, and text-image retrieval.%, and is typically approached as a problem of explicit assignment or alignment between individual words and regions.  %% Captioned images, found for example in medical reports, news photos, and safety reports, can be used to learn diagnosis, retrieval, and other tasks if the words can be associated with specific image regions.  This word-region association problem is called {   %Phrase grounding\textemdash the problem of mapping noun phrases in a caption to the corresponding image regions\textemdash has vast applications in visual question answering, image captioning, image retrieval, and referential expression generation. This challenging problem not only requires a categorical understanding of objects and their attributes in the scene, but also the ability to relate phrases to detected object regions in the image.     Existing object detectors can detect and represent object regions in an image, and language models can provide contextualized representations for noun phrases in the caption. However, learning a mapping between these continuous, independently trained visual and textual representations is challenging in the absence of explicit region-word annotations. We focus on learning this mapping from weak supervision in the form of paired image-caption data without requiring laborious grounding annotations.%The current state-of-the-art approaches either require manual region-phrase correspondence in training or rely on weakly supervised discovery of such correspondence. While the former is costly and time consuming, the latter often requires heuristics and inductive bias to distinguish the corresponding image region for a caption among many other correlated candidate regions.  %without direct grounding supervision %that gracefully handle synonyms , hypernyms , polysemes , attributes , and interactions . However, learning a mapping between these continuous, independently trained visual and textual representations without direct grounding supervision continues to be a challenging open problem.   Current state-of-the-art approaches formulate weakly supervised phrase grounding as a multiple instance learning  problem. The image can be viewed as a bag of regions. For a given phrase, all images with captions containing the phrase are treated as positive bags while remaining images are treated as negatives. Models aggregate per region features or phrase scores to construct image-level predictions that can be supervised with image-level labels in the form of phrases or captions. Common aggregation approaches include max or mean pooling, noisy-OR, and attention. Popular training objectives include binary classification loss  or caption reconstruction loss  or ranking objectives .   Fig. provides an overview of our proposed contrastive training. We propose a novel formulation of the weakly supervised phrase grounding problem as that of maximizing a lower bound on mutual information between set of region features extracted from an image and contextualized %\JK{After reading Sec 2, I'm confused about ""contextualized"". See comment in Sec 2.5.}  word representations. We use pretrained region and word representations from an object detector and a language model and perform optimization over parameters of word-region attention instead of optimizing the region and word representations themselves. Intuitively, to compute mutual information with a word's representation, attention must discard nuisance regions in the word-conditional attended visual representation, thereby selecting regions that match the word. For any given word, the learned attention thus functions as a soft selection or grounding mechanism over regions.   %In this work, we propose a principled weakly supervised framework that discovers phrase grounding by maximizing the mutual information  between image and caption pairs. As computing MI is intractable, we formulate the problem as maximizing the InfoNCE lower bound on the mutual information which recently has shown promising results in representation learning for images, videos, image-and-text, and video-and-text .   Since computing MI is intractable, we maximize the recently introduced InfoNCE lower bound on mutual information. The InfoNCE bound requires a compatibility score between each caption word and the image to contrast positive image and caption word pairs with negative pairs in a minibatch. We use two objectives. % and . The first objective  contrasts a positive pair with negative pairs with the same caption word but different image regions. The second objective  contrasts a positive pair with negative pairs with the same image but different captions. We show empirically that sampling negative captions randomly from the training data to optimize  does not yield any gains over optimizing  only. Instead of random sampling, we propose to use a language model to construct context-preserving negative captions by substituting a single noun word in the caption.     We design the compatibility function using a  attention mechanism. The  and , computed from words and regions respectively, are used to compute a word-specific attention over each region which acts as a soft alignment or grounding  between words and regions. The compatibility score between regions and word is computed by comparing attended visual representation and the word representation.     %We design the compatibility function using a  attention mechanism.  vectors, generated for each word in the caption, are compared against , generated for each region, to get word-to-region attention scores. The attention scores are used as weights to linearly aggregate region  vectors. The compatibility score between the image and the word is obtained by comparing the aggregated region  vector to the word  vector.  %This imposes several challenges in our problem; Since we are interested in phrase grounding, an image is represented as a set of candidate regions generated by a detector. In this case, it is not trivial how to define a flexible and expressive compatibility function between a phrase and a set of regions. Moreover, in contrast to the previous work which uses mutual information for representation learning, we are interested in image region and phrase alignments. Thus, the compatibility function should be designed such that MI maximization naturally yields grounding. Finally, InfoNCE loss trains the compatibility function by contrasting positive pairs against negative instances. Hence, negative instance sampling plays a key role in the quality of training.  %Our model is shown in Fig.. We show that by formulating the compatibility function as an attention-based image-word matching model, we can learn to ground phrases using the InfoNCE loss. The compatibility scores between an image and a caption word are computed by comparing the contextualized word representation with a word-specific attended image representation. Intuitively, the bound on mutual information can thus be maximized when the attention model selects only the regions corresponding to the words, while discarding nuisance regions.   %To train the parameters of our attention-based model, we propose two objective functions, based on the InfoNCE loss. The first objective trains the model by contrasting positive region-phrase pairs against negative instances whose image regions are replaced . This term forces the model to discover the corresponding image region for each word. We complement this objective by defining a loss function that contrasts positive pairs against those with negative captions . However, for the latter loss function instead of blindly sampling uniformly from negative words, we propose a novel approach that extracts plausible negative words related to the original caption.  %For the second issue, we show that pre-trained language models are able to extract hard-negative words that are related to an image, but are less likely to be present in it. This technique enables us to consider more negative words in the InfoNCE loss without actually increasing the batch size blindly. For the third issue, we provide empirical evidence that maximizing the InfoNCE bound is positively correlated with the accuracy of phrase grounding, but higher InfoNCE bound does not necessarily translates to a better phrase grounding.   %region-word attention to maximize the InfoNCE lower bound on mutual information between image and caption pairs leads to phrase grounding. In our formulation, InfoNCE requires computing a compatibility score between each caption word and the image. A lower InfoNCE loss corresponds to higher scores for true image-caption pairs and lower scores for false pairs. The scores between an image and a caption word are computed by comparing the contextualized word representation with a word-specific attended image representation. The lower bound on mutual information can thus be maximized by using attention to select only the regions corresponding to the words while discarding nuisance regions.    Our key contributions are:  a novel MI based contrastive training framework for weakly supervised phrase grounding;  an InfoNCE compatibility function between a set of regions and a caption word designed for phrase grounding; %using a  attention mechanism and strong pretrained region and word representations  and  a procedure for constructing context-preserving negative captions that provides  absolute gain in grounding performance. % - A mutual information based pseudo-objective for weakly supervised phrase grounding   % - Novel captions   Our work is closely related to three active areas of research. We now provide an overview of prior arts in each. %We highlight the differences between our work and the related work in each area.  \myparagraph{Weakly Supervised Phrase Grounding.} Weakly supervised phrase localization is typically posed as a multiple instance learning  problem where each image is considered as a bag of region proposals. Images whose captions mention a word or a phrase are treated as positive bags while rest of the images are treated as negatives for that word or phrase. Features or scores for a phrase or the entire caption are aggregated across all regions to make a prediction for the image. Common methods of aggregation are max or average pooling, noisy-OR, or attention. With the ability to produce image-level scores for pairs of images and phrases or captions, the problem becomes an image-level fully-supervised phrase classification problem or an image-caption retrieval problem. An alternatives to the MIL formulations is the approach of Ye~ Recently MI-based approaches have shown promising results on a variety representation learning problems. Computing the MI between two representations is challenging as we often have access to samples but not the underlying joint distribution that generated the samples. Thus, recent efforts rely on variational estimation of MI. An overview of such estimators is discussed in  while the statistical limitations are reviewed in .  In practice, MI-based representation learning models are often trained by maximizing an estimation of MI across different transformations of data. For example, deep InfoMax maximizes MI between local and global representation using MINE. Contrastive predictive coding inspired by noise contrastive estimation assumes an order in the features extracted from an image and uses summary features to predict future features. Contrastive multiview coding maximizes MI between different color channels or data modalities while augmented multiscale Deep InfoMax and SimCLR extract views using different augmentations of data points. Since the infoNCE loss is limited by the batch size, several previous work rely on memory banks to increase the set of negative instances.  \myparagraph{Joint Image-Text Representation Learning.} With the advances in both visual analysis and natural language understanding, there has been a recent shift towards learning representation jointly from both visual and textual domains. ViLBERT and LXMERT learn representation from both modalities using two-stream transformers, applied to image and text independently. In contrast, UNITER, VisualBERT, Unicoder-VL, VL-BERT and B2T2 propose a unified single architecture that learns representation jointly from both domains. Our method is similar to the first group, but differs in its fundamental goal. Instead of focusing on learning a task-agnostic representation for a range of downstream tasks, we are interested in the quality of region-phrase grounding emerged by maximizing mutual information. Moreover, we rely on the language modality as a weak training signal for grounding, and we perform phrase-grounding without any further finetuning.    %Generic Image and text %Video and language. %VQA and captioning       In this work, we offer a novel perspective on weakly supervised phrase grounding from paired image-caption data which has traditionally been cast as a multiple instance learning problem. We formulate the problem as that of estimating mutual information between image regions and caption words. We demonstrate that maximizing a lower bound on mutual information with respect to parameters of a region-word attention mechanism results in learning to ground words in images. We also show that language models can be used to generate context-preserving negative captions which greatly improve learning in comparison to randomly sampling negatives from training data.      \clearpage   ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.   
"," Phrase grounding, the problem of associating image regions to caption words, is a crucial component of vision-language tasks. We show that phrase grounding can be learned by optimizing word-region attention to maximize a lower bound on mutual information between images and caption words. Given pairs of images and captions, we maximize compatibility of the attention-weighted regions and the words in the corresponding caption, compared to non-corresponding pairs of images and captions. A key idea is to construct effective negative captions for learning through language model guided word substitutions. Training with our negatives yields a $.   %A key idea is to construct effective word substitutions using a language model, leading to much more effective learning than randomly sampling negative captions. By training either on COCO-Captions or the much smaller Flickr30K Entities train set , we achieve state-of-the-art results for weakly supervised phrase grounding on Flickr30K Entities test set.",148
" % models are big and grow Large neural networks tend to perform better than smaller ones. This observation motivated researchers to train ever larger networks, resulting in models containing billions of parameters becoming commonplace ) only defines the layer operation and dimensions. The necessary weights are disentangled from specific layers and obtained through one or more  that can use parameters as they deem fit.  Thus, SSNs can implement  network with an arbitrary number of parameters.  There are three main questions to consider when implementing SSNs: 1) How can we effectively reuse parameters? 2) Where is it most constructive to reuse parameters? 3) What do we do when a layer asks for more parameters than is available in a parameter store?  The ``how"" of reusing parameters is the problem most addressed in prior work, most notably Savarese~\etal, and we generalize and expand upon their parameter sharing methodology.  However, to the best of our knowledge, we are the first to try to investigate methods for automatically determining ``where"" parameter sharing can be performed, even when layers are different sizes/types. Instead, prior work hand selected where sharing would occur.  Similarly, to the best of our knowledge, we are the first to investigate ``what"" can be done to get more weights to implement a layer without adding more parameters (.  Each Parameter Group has its own Parameter Allocator,    \setlength            We introduced Shapeshifter Networks that automatically learn parameter sharing strategies by decoupling weights from layers. They allow neural networks to get the benefits of parameter sharing---reduced memory requirements during training and inference, potentially improved accuracy---without any manual tuning.  We show SSNs can even be used to train large datasets like ImageNet, where SSNs report a 3\  improvement on Error@5 over a same sized network without parameter sharing. Surprisingly, we also find that parameters can be shared among very different layers.  Further, we demonstrate that SSNs can be combined with knowledge distillation and parameter pruning methods to achieve state-of-the-art results that also reduce FLOPs at test time. One could think of SSNs as spreading the same number of parameters across more layers, increasing effective depth, which benefits generalization, although this requires further exploration.   Future directions include more study of parameter grouping and combination methods, as well as studying the performance implications of these methods.  Given that applying SSNs is nearly transparent, we hope that parameter sharing can become a standard part of deep learning workflows.
"," Fitting a model into GPU memory during training is an increasing concern as models continue to grow.  To address this issue, we present Shapeshifter Networks , a flexible neural network framework that decouples layers from model weights, enabling us to implement any neural network with an arbitrary number of parameters.  In SSNs each layer obtains weights from a parameter store that decides where and how to allocate parameters to layers.  This can result in sharing parameters across layers even when they have different sizes or perform different operations. SSNs do not require any modifications to a model's loss function or architecture, making them easy to use.  Our approach can create parameter efficient networks by using a relatively small number of weights, or can improve a model's performance by adding additional model capacity during training without affecting the computational resources required at test time.  We evaluate SSNs using seven network architectures across diverse tasks that include image classification, bidirectional image-sentence retrieval, and phrase grounding, creating high performing models even when using as little as 1\% of the parameters.   %Our approach is based on the observation that many neural networks are severely overparameterized, resulting in significant waste in computational resources as well as being susceptible to overfitting. SSNs address this by learning where and how to share parameters between layers in a neural network while avoiding degenerate solutions that result in underfitting. Specifically, we automatically construct parameter groups that identify where parameter sharing is most beneficial.  Then, we map each group's weights to construct layers with learned combinations of candidates from a shared parameter pool.  SSNs can share parameters across layers even when they have different sizes, perform different operations, and/or operate on features from different modalities.  We evaluate our approach on a diverse set of tasks, including image classification, bidirectional image-sentence retrieval, and phrase grounding, creating high performing models even when using as little as 1\% of the parameters.  We also apply SSNs to knowledge distillation, where we obtain state-of-the-art results when combined with traditional distillation methods.  %We present Shapeshifter Networks, a flexible neural network framework that improves performance and reduces memory requirements on a diverse set of scenarios over standard neural networks.  Our approach is based on the observation that many neural networks are severely overparameterized, resulting in significant waste in computational resources as well as being susceptible to overfitting.  Our Shapeshifter Networks address this by learning where and how to share parameters between layers in a neural network while avoiding degenerate solutions that result in underfitting. Specifically, we automatically construct parameter groups that identify where parameter sharing is most beneficial.  Then, we map each group's weights to construct layers with learned combinations of candidates from a shared parameter pool.  Our Shapeshifter Networks can share parameters across layers even when they have different sizes, perform different operations, and/or operate on features from different modalities.  We show that our approach improves performance and efficiency on a diverse set of tasks, including image classification, bidirectional image-sentence retrieval, phrase grounding, and knowledge distillation, especially when few parameters are available.",149
"  When using language, humans have a remarkable ability to recombine known parts to understand novel sentences they have never encountered before . For example, once humans have learned the meanings of ``walk'', ``jump'' and ``walk twice'', it is effortless for them to understand the meaning of ``jump twice''. This kind of ability relies on the compositionality that characterizes languages. The principle of compositionality refers to the idea that the meaning of a complex expression  is determined by the meanings of its constituents  together with the way these constituents are combined  . Understanding language compositionality is a basic and essential capacity for human beings, which is argued to be one of the key skills towards human-like machine intelligence .  Recently,  made a step towards exploring and benchmarking compositional generalization of neural networks. They argued that leveraging compositional generalization was an essential ability for neural networks to understand out-of-domain sentences. The test suite, their proposed Simplified version of the CommAI Navigation  dataset, contains compositional navigation commands, such as ``walk twice'', and  corresponding action sequences, like . Such a task lies in the category of machine translation, and thus is expected to be well solved by current state-of-the-art translation models . However, experiments on SCAN demonstrated that modern translation models dramatically fail to obtain a satisfactory performance on compositional generalization. For example, although the meanings of ``walk'', ``walk twice'' and  ``jump'' have been seen, current models fail to generalize to understand ``jump twice''. Subsequent works verified that it was not an isolated case, since convolutional encoder-decoder model  and Transformer  met the same problem. There have been several attempts towards SCAN, but so far no neural based model can successfully solve all the compositional challenges on SCAN without extra resources .  In this paper, we propose a memory-augmented neural model to achieve compositional generalization by Learning Analytical Expressions . Motivated by work in cognition which argues compositionality can be captured by variable slots with symbolic functions , our memory-augmented architecture is devised to contain two cooperative neural modules accordingly: Composer and Solver. Composer aims to find structured analytical expressions from unstructured sentences, while Solver focuses on understanding these expressions with accessing Memory . These two modules are trained to learn analytical expressions together in an end-to-end manner via a hierarchical reinforcement learning algorithm . Experiments on a well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, reaching  accuracies in all tasks . As far as we know, our model is the first neural model to pass all compositional challenges addressed by previous works on SCAN without extra resources. We open-source our code at \url{https://github.com/microsoft/ContextualSP}.     In this paper, we propose to achieve compositional generalization by learning analytical expressions. Motivated by work in cognition, we present a memory-augmented neural model which contains two cooperative neural modules Composer and Solver. These two modules are trained in an end-to-end manner via a hierarchical reinforcement learning algorithm. Experiments on a well-known benchmark demonstrate that our model solves all challenges addressed by previous works with \
","  Compositional generalization is a basic and essential intellective capability of human beings, which allows us to recombine known parts readily. However, existing neural network based models have been proven to be extremely deficient in such a capability. Inspired by work in cognition which argues compositionality can be captured by variable slots with symbolic functions, we present a refreshing view that connects a memory-augmented neural model with analytical expressions, to achieve compositional generalization. Our model consists of two cooperative neural modules, Composer and Solver, fitting well with the cognitive argument while being able to be trained in an end-to-end manner via a hierarchical reinforcement learning algorithm. Experiments on the well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, solving all challenges addressed by previous works with $100$\% accuracies.",150
" Although infants are not supposed to acquire the symbolic representational system at the sensorimotor stage, based on Piaget's definition of infant development, the preparation of language development, such as a pre-symbolic representation for conceptualization, has been set at the time when the infant starts babbling . %in the form of conceptualizing objects and sensorimotor primitives .  Experiments have shown that infants have established the concept of animate and inanimate objects, even if they have not yet seen the objects before . Similar phenomena also include the conceptualization of object affordances such as the conceptualization of containment . This conceptualization mechanism is developed at the sensorimotor stage to represent sensorimotor primitives and other object-affordance related properties.   During an infants' development at the sensorimotor stage, one way to learn affordances is to interact with objects using tactile perception, observe the object from visual perception and thus learn the causality relation between the visual features, affordance and movements as well as to conceptualize them. This learning starts with the basic ability to move an arm towards the visual-fixated objects in new-born infants , continues through object-directed reaching at the age of 4 months , and can also be found during the object exploration of older infants . From these interactions leading to visual and tactile percepts, infants gain experience through the instantiated `bottom-up' knowledge about object affordances and sensorimotor primitives. Building on this, infants at the age of around 8-12 months gradually expand the concept of object features, affordances and the possible causal movements in the sensorimotor context . %In this period, infant gradually starts to notice the surrounding world and perceives objects' visual features . For instance, they realize that it is possible to pull a string that is tied to a toy car to fetch it instead of crawling towards it. {An associative rule has also been built that connects conceptualized visual feature inputs, object affordance and the corresponding frequent auditory inputs of words, across various contexts . At this stage, categories of object features are particularly learned in different contexts due to their affordance-invariance . }  Therefore the integrated learning process of the object's features, movements according to the affordances, and other knowledge is a globally conceptualized  process through visual and tactile perception.  This conceptualized learning is a precursor of a pre-symbolic representation of language development. {This learning is the process to form an abstract and simplified representation for information exchange and sharing\footnote{For comparison of conceptualization between engineering and language perspectives, see .}.  To conceptualize from visual perception, it usually includes a planning process: first the speaker receives and segments visual knowledge in the perceptual flow into a number of states on the basis of different criteria, then the speaker selects essential elements, such as the units to be verbalized, and last the speaker constructs certain temporal perspectives when the events have to be anchored and linked .  Assuming this planning process is distributed between ventral and dorsal streams, the conceptualization process should also emerge from the visual information that is perceived in each stream, } {associating the distributed information in both streams.  As a result, the candidate concepts of visual information are statistically associated with the input stimuli. For instance, they may represent a particular visual feature with a particular class of label  . Furthermore, the establishment of such links also strengthens the high-order associations that generate predictions and generalize to novel visual stimuli . Once the infants have learned a sufficient number of words, they begin to detect a particular conceptualized cue with a specific kind of wording. At this stage, infants begin to use their own conceptualized visual `database' of known words to identify a novel meaning class and possibly to extend their wording vocabulary .   Thus, this associative learning process enables the acquisition and the extension of the concepts of domain-specific information  with the visual stimuli. }  This conceptualization will further result in a pre-symbolic way for infants to communicate when they encounter a conceptualized object and intend to execute a correspondingly conceptualized well-practised sensorimotor action towards that object. For example, behavioral studies showed that when 8-to-11-month-old infants are unable to reach and pick up an empty cup, they may point it out to the parents and execute an arm movement intending to bring it to their lips. The conceptualized shape of a cup reminds infants of its affordance and thus they can communicate in a pre-symbolic way. Thus, the emergence from the conceptualized visual stimuli to the pre-symbolic communication also gives  further rise to the different periods of learning nouns and verbs in infancy development .  This evidence supports that the production of verbs and nouns are not correlated to the same modality in sensory perception: experiments performed by  suggest that nouns are more related to the movement orientation caused by the intrinsic properties of an object, while verbs are more related to the trajectories of an object. Thus we argue that such differences of acquisitions in lexical classes also relate to the conceptualized visual ventral and dorsal streams. The finding is consistent with's hypothesis that verb generation is modulated by the perception of conceptualization of movement and its spatio-temporal relationship.   For this reason, we propose that the conceptualized visual information, which is a prerequisite for the pre-symbolic communication, is also modulated by perception in two visual streams.  {Although there  have been studies of modeling the functional modularity in the development of ventral and dorsal streams , the bilinear models of visual routing , in which a set of control neurons dynamically modifies the weights of the `what' pathway on a short time scale, or transform-invariance models  by encouraging the neurons to fire invariantly while transformations are performed in their input stimuli. However, a model that explains the development of conceptualization from both streams and results in an explicit representation of conceptualization of both streams while the visual stimuli is presented is still missing in the literature. } This conceptualization should be able to encode the same category for information flows in both ventral and dorsal streams  like `object files' in the visual understanding  so that they could be discriminated in different contexts during language development.  On the other hand, this conceptualized representation that is distributed in two visual streams is also able to predict the tendency of appearance of an action-oriented object in the visual field, which causes some sensorimotor phenomena such as object permanence  showing the infants' attention usually is driven by the object's features and movements. For instance, when infants are observing the movement of the object, recording showed an increase of the looking times when the visual information after occlusion is violated in either surface features or location . Also the words and sounds play a top-down role in the early infants' visual attention . This could hint at the different development stages of the ventral and dorsal streams and their effect on the conceptualized prediction mechanism in the infant's consciousness.{ {Accordingly, the model we propose about the conceptualized visual information should also be able to explain the emergence of a predictive function in the sensorimotor system, e.g. the ventral stream attempts to track the object and the dorsal stream processes and predicts the object's spatial location, when the sensorimotor system is involved in an object interaction. } We have been aware of that this build-in predictive function in a forward sensorimotor system is essential: neuroimaging research has revealed the existence of internal forward models in the parietal lobe and the cerebellum that predict sensory consequences from efference copies of motor commands  and supports fast motor reactions .  Since the probable position and the movement pattern of the action should be predicted on a short time scale, sensory feedback produced by a forward model with negligible delay is necessary in this sensorimotor loop.  %Based on this finding, some model architectures have been proposed based on different combinations of forward models and other models .   %Among them, based on the theory of sensorimotor contingency  which assumes that a motor action of an agent is generated from the sensory awareness in the percept by a pre-learned mapping, some sensorimotor integration models simply aim at finding out how to emerge the pair-up of the sensorimotor integration. For instance, MLP   and SOM    are applicable methods to learn this relation. However, in terms of inclusion of forward models, all of these models assume a constant delay-time in the sensorimotor system. Thus the predictive mechanism , if necessary, is implicitly encoded into the weights of these SMC pair-up models.   %We also argue that, although the explicit representation of the sensory prediction may not increase the action performance or speed up the training, the inclusion of such  predictive information is akin to the biological separation of the forward model and the motor action model. An example is the song syllable learning which consists of an associational function with prediction of the feedback signal from audition. This forward model is learned and generated in the song system nucleus HVc . Similar predictive mechanisms also exist in the visual system to extrapolate from motor action control within the sensorimotor loop , which may also be supported by the V1 complex cells of the dorsal stream .    %Though the development sequence of two visual streams is still under debate, from the assertion of the joint development of two visual streams , the conceptualized encoding of the visual-for-action information in the dorsal stream and that of action-oriented-object information in the ventral stream may also assist in the integrated prediction consisting both visual object movement and features in the sensorimotor context. .  %This behaviour could account for the ability in this stage of sharing conceptualized experiences in the same category towards a specific property of the percept, such as colors, shapes, in a pre-symbolic way.   % The similar object-and-infant interaction also happens without object and its affordance, but only sensorimotor interaction with mother and infant  as well. To conclude these cases,  when the dynamic interactive process between object and the other person are being observed or executed, the emergence of the pre-symbolic representation happens simultaneously in the sensorimotor learning progress, a product of the conceptualization in linguistics.    %The concepturlization may also assists to develop new primitive knowledge with the initially sensed from percept or framed by trial-and-error actions  .   %The infants' conceptualization that results in the continuum of language development of pre-symbolic intentional communication  to the beginning of symbolic communication should be partly devoted by the infants' visual percept.   %One explanation of this anticipation is that it stem from conceptualization of different visual feature extraction, movement pattern extraction, etc .    %The fully development of both streams happens when the infant acquires motor ability in the sensorimotor stage. This is not a co-incident, but causal: the infant firstly obtain the ability to reach the object at the age of 5-month when the ventral stream has developed and she is able to recognize an object with its features. At the age of 6-8 month, more mature sensorimotor abilities, such as exploring the objects and adjusting the hand movement for grasping,  become possible, which is also accompanied with the incremental development of dorsal streams and its provision of vision-for-action function . Thus, these actions towards an object oriented task may indicate the mature development of dorsal stream and its improvement in the oriented actions of geometric properties of objects. %One may connect the conceptualization in the sensorimotor stage with the information of object identity and object movement in order to accomplish the infants' behavior such as object permanence. This pre-symbol representation of a certain object of interest further assists the infant interaction with sensorimotor behaviors.   %After this stage, the emergence of sensorimotor primitives and the infants can even anticipate the upcoming sensorimotor primitives with the emergence of dorsal stream in the later sensorimotor stage.    %The emergence of conceptualization on the perception of object affordance, the visual features and their possible movement in response to some motor actions  plays an essential cue for the object predictions in sensorimotor integration, such as object permanence.         % Regarding the predictive capabilities of the sensorimotor learning, a few computational models have been proposed.  %In terms of the internal models of sensorimotor learning, for instance, they attempt to figure out how the sensorimotor coupling or integration happens between motor commands and the set of sensory inputs.    % %However, disputation still exists in terms of which role of sensory  feedback plays in the sensorimotor integration.    % These loops rely on a forward model that integrates the sensory inflow and motor outflow to evaluate the consequence of the motor commands sent to a limb, such as the arm. In such a model, the probable position and velocity of an effector can be estimated with negligible delays and even predicted in advance, thus making feedback strategies possible for fast reaching movements. The parietal lobe and cerebellum appear to play a crucial role in this process. T  %Rather, reaching towards a target requires an integrative control scheme in which feedforward specification of the motor command, forward modeling of the dynamics of the arm and online updating of the initial pattern of muscle activation are synthesized in reliable feedback loops, which are thought to involve the cerebellum and PPC.   %Based on this concept of visual prediction, a predictive sensorimotor embodied architecture  was proposed. A RNN sensory prediction and reward-driven motor accomplish the tasks of visual cue prediction and a reward-driven motor action learning. However, since only one simple recurrent network is applied for visual extrapolation, a single set of spatio-temporal sequences in the sensory data space  can be learnt.  % Particularly, the predictive sensorimotor model we propose is  suitable to work as one of the building modules that takes into account the predictive object movement in a forward sensorimotor system to deal with object interaction from visual stimuli input as Fig. shows. This system is similar to 's sensorimotor integration, but it includes an additional sensory estimator  which takes into account the visual stimuli from the object so that it is able to predict the dynamics of both the end-effector  and the sensory input of the object. This object-predictive module is essential in a sensorimotor system to generate sensorimotor actions like tracking and avoiding when dealing with fast-moving objects, e.g. in ball sports. {We also assert that the additional inclusion of forward models in the visual perception of the objects can explain some predictive developmental sensorimotor phenomena, such as object permanence. % {  Therefore, the model we propose basically focuses on a predictive perception function based on contextual visual stimuli of the object by unfolding the higher-level abstract perception files, i.e. the PB units.  We assert that the additional inclusion of forward models in the visual perception for the visual objects can explain some predictive developmental sensorimotor phenomena, such as object permanence.}       %As shown in Fig., the desired position of the end-effector serves as reference sensory input. This is then applied as an input to the inverse model  to generate the necessary motor command. The efferent copy of this motor command is fed as the input to the end-effector forward model  which predicts the future position of the end-effector .  %At the same time, using the visual input, the object forward model  observes the object in the receptive field and predicts the possible movement from its innate model, which can be formulated as an HMM model. In this way, the comparison of the three values  results in the desired position of the end-effector for the next time-step.  In this paper, we will concentrate on the modeling for the object forward model , with emphasis on the built-in function of conceptualization of the visual ventral and dorsal streams.   In summary, we propose a model that establishes links between the development of ventral/dorsal visual streams and the emergence of the conceptualization in visual streams, which further leads to the predictive function of a sensorimotor system. To validate this proof-of-concept model, we also conducted experiments in a simplified robotics scenario. Two NAO robots were employed in the experiments: one of them was used as a `presenter' and moved its arm along pre-programmed trajectories as motion primitives. A ball was attached at the end of the arm so that another robot could obtain the movement by tracking the ball. Our neural network was trained and run on the other NAO, which was called the `observer'.  In this way, the observer robot perceived the object movement from its vision passively, so that its network took the object's visual features and the movements into account.  Though we could also use one robot and a human presenter to run the same tasks, we used two identical robots, due to the following reasons: 1. the object movement trajectories can be done by a pre-programmed machinery so that the types and parameters of it can be adjusted; 2. the use of two identical robots allows to interchange the roles of the presenter and observer in an easier manner.  {As other humanoid robots, a sensorimotor cycle that is composed of cameras and motors also exists in NAO robots.  Although its physical configurations and parameters of sensory and motor systems are different from those in human beings' or other biological systems,  our model only handles the pre-processed information extracted from visual stimuli. Therefore it is sufficient to serve as a neural model that is running in a robot CPU to explain the language development in the cortical areas. }       %   {In this paper a recurrent network architecture integrating the RNNPB model and the horizontal product model has been presented, which sheds light on the feasibility of linking the conceptualization of ventral/dorsal visual streams, the emergence of pre-symbol communication, and the predictive sensorimotor system.   Based on the horizontal product model, here the information in the dorsal and ventral streams is separately encoded in two network streams and the predictions of both streams are brought together via the horizontal product while the PB units act as a conceptualization of both streams. These PB units allow for storing multiple sensory sequences. After training, the network is able to recognize the pre-learned conceptualized information and to predict the up-coming visual perception. The network also shows robustness and generalization abilities.  Therefore, our approach offers preliminary concepts for a similar development of conceptualized language in pre-symbolic communication and further in infants' sensorimotor-stage learning. }   In terms of sensorimotor integration, the whole model is a forward model to predict the upcoming object appearance in the visual field, which should be essential to explain some of the predictive sensorimotor phenomena, such as object permanence.}   
"," The acquisition of symbolic and linguistic representations of sensorimotor behavior is a cognitive process performed by an agent when it is executing and/or observing own and others' actions. According to Piaget's theory of cognitive development, these representations develop during the sensorimotor stage and the pre-operational stage.  We propose a model that relates the conceptualization of the higher-level information from visual stimuli to the development of ventral/dorsal visual streams. This model employs neural network architecture incorporating a predictive sensory module based on an RNNPB  and a horizontal product model. We exemplify this model through a robot passively observing an object to learn its features and movements. During the learning process of observing sensorimotor primitives, i.e. observing a set of trajectories of arm movements  and its oriented object features, the pre-symbolic representation is self-organized in the parametric units.  These representational units act as bifurcation parameters, guiding the robot to recognize and predict various learned sensorimotor primitives. The pre-symbolic representation also accounts for the learning of sensorimotor primitives in a latent learning context. %[original submission version]The acquisition of symbolic and linguistic representation of sensorimotor behaviour is a cognitive process obtained by an agent when it is executing and/or observing own and others' actions. According to Piaget's theory of cognitive development, these representations develop during the sensorimotor stage and the pre-operational stage. We model this process through a robot action and gesture imitation learning experiment. This pre-symbolic language acquisition capability is based on a neural network architecture incorporating a predictive sensory module based on an RNNPB  and horizontal product model. During the imitation learning process of sensormotor primitives, i.e. a set of arm movement trajectories and features of their orientated object, the pre-symbolic representation is self-organized in the parametric units. These representational units act as bifurcation parameters, guiding the robot to recognize and follow various learnt sensorimotor primitives. The pre-symbolic representation also accounts for the learning of sensorimotor primitives in a latent learning context.   \tiny    Pre-symbolic Communication, Sensorimotor Integration, Recurrent Neural Networks, Parametric Biases, Horizontal Product   %All article types: you may provide up to 8 keywords; at least 5 are mandatory.",151
" LinkedIn serves as a job marketplace that matches millions of jobs to more than 675 million members. To create economic opportunity for every member of the global workforce, LinkedIn needs to understand the job marketplace precisely. However, understanding job postings is non-trivial due to its lengthy and noisy nature. Job postings usually cover a wide range of topics ranging from company description, job qualifications, benefits to disclaimers. It is challenging to model job postings directly in tasks such as job recommendation and applicant evaluation. To address this challenge, we develop job understanding models that take noisy job postings as input and output structured data for easy interpretation. To be specific,  that represent the characteristics of a job, for example, .  Standardizing job information has a tremendous impact on the LinkedIn ecosystem. Firstly, it helps recruiters to do better candidate targeting. Using the extracted key skill entities, recruiters can target the candidates that have the right skill set. Secondly, it helps members to find jobs easily. It is convenient for members to search jobs based on the professional entities standardized from the job postings such as occupation and requirements. Lastly, it improves the overall hire efficiency. By extracting assessment questions and key skill entities from jobs, LinkedIn can automatically evaluate job-applicant fit by comparing them with member-side entities.  However, developing a good job understanding model is a challenging task in many aspects.  First, we need to define an extensive professional entity taxonomy that covers a wide range of industries and occupations. Without a comprehensive taxonomy, it is hard to represent jobs using entities. Second, we need domain-specific natural language understanding models to understand jobs. Compared to ordinary articles, job posting text is often long, noisy, and has job-specific writing styles, general-purpose Natural Language Processing  models are less suitable for this task.  Third, job understanding models need to be market-aware. To be specific, it needs to go beyond simply identifying mentioned entities and understand market importance of entities via modeling each different job market and the hiring experts in the market.  Unfortunately, existing methods haven閳ユ獩 addressed all the above challenges. Models such as SPTM and TATF perform job-skill analysis on IT skills only. DuerQuiz focuses on job content only and ignores the market dynamics. Lastly, none of these models explicitly model market variance via establishing a feedback loop between models and hiring experts.  In this work, we present LinkedIn's deep job understanding and demonstrate LinkedIn's job posting flow powered by our work.  We combined machine learning techniques and linguist experts to curate the world's largest professional entity taxonomy.  To develop domain-specific content understanding model, we used deep transfer learning to adapt open-domain NLP models and professional entity embeddings trained on LinkedIn member profiles to our domain.  We engineered market-specific features and established a feedback loop with job posters. We showed the model outputs and allowed them to override the suggestions to collect the feedback. By developing the feedback loop, we were able to collect domain-specific and market-aware training data to continuously improve our model. Moreover, such a feedback loop also empowered the job posters while giving options to them to delegate decisions to AI models. .    At LinkedIn, we performed job standardization work over millions of jobs. We built the world閳ユ獨 largest and most complete professional entity taxonomy to categorize and standardize the professional entities in the job posting. We used deep transfer learning to develop title, skill, company, and assessment question entity taggers which standardize entities out of job postings. The models were tailored to be domain-specific and market-oriented, thereby keeping great sensitivity to different job markets. More importantly, we built and presented our customer feedback loop by tracking users閳 interactive behaviour on current models in LinkedIn閳ユ獨 job posting flow. Using the feedback loop, we were able to iteratively adjust our models to let them not only be more powerful but also stay more sensitive to the job market changes. Our online A/B test results showed that these user feedback loops improved job posters' satisfaction, increased the performance of our job understanding models, and ultimately led to better matches between jobs and our members.       The acknowledgments section is defined using the ""acks"" environment    . This ensures the proper    identification of the section in the article metadata, and the    consistent spelling of the heading.           The next two lines define the bibliography style to be used, and    the bibliography file. 
"," As the world闁炽儲鐛 largest professional network, LinkedIn wants to create economic opportunity for everyone in the global workforce. One of its most critical missions is matching jobs with processionals. Improving job targeting accuracy and hire efficiency align with LinkedIn's Member First Motto. To achieve those goals, we need to understand unstructured job postings with noisy information. We applied deep transfer learning to create domain-specific job understanding models. After this, , including titles, skills, companies, and assessment questions. To continuously improve LinkedIn闁炽儲鐛 job understanding ability, we designed an expert feedback loop where we integrated job understanding models into LinkedIn闁炽儲鐛 products to collect job posters闁 feedback. In this demonstration, we present LinkedIn闁炽儲鐛 job posting flow and demonstrate how the integrated deep job understanding work improves job posters闁 satisfaction and provides significant metric lifts in LinkedIn's job recommendation system.",152
" Recent advances in  Reinforcement Learning  have been driven by the development of novel simulation environments, such as the Arcade Learning Environment ~, StarCraft~, BabyAI~, Obstacle Tower~, Minecraft~, and Procgen Benchmark~.  These environments introduced new challenges for state-of-the-art methods and demonstrated failure modes of existing RL approaches. For example,  highlighted that methods performing well on other ALE tasks were not able to successfully learn in this sparse-reward environment. This sparked a long line of research on novel methods for exploration~ and learning from demonstrations~. However, this progress has limits: the current best approach on this environment, Go-Explore~, overfits to specific properties of ALE and Montezuma's Revenge. While Go-Explore is an impressive solution for Montezuma's Revenge, it exploits the determinism of environment transitions, allowing it to memorize sequences of actions that lead to previously visited states from which the agent can continue to explore.  We are interested in surpassing the limits of deterministic or repetitive settings and seek a simulation environment that is complex and modular enough to test various open research challenges such as exploration, planning, skill acquisition, memory, and transfer. However, since state-of-the-art RL approaches still require millions or even billions of samples, simulation environments need to be fast to allow RL agents to perform many interactions per second. Among attempts to surpass the limits of deterministic or repetitive settings,  are a promising path towards testing systematic generalization of RL methods~. Here, the game state is generated programmatically in every episode, making it extremely unlikely for an agent to visit the exact state more than once during its lifetime.  Existing procedurally generated RL environments are either costly to run~ or are, as we argue, of limited complexity~.  To address these issues, we present the  , a procedurally generated environment that strikes a balance between complexity and speed. It is a fully-featured  environment~ around the popular open-source terminal-based single-player turn-based ``dungeon-crawler'' game, ~.  Aside from procedurally generated content,  is an attractive research platform as it contains hundreds of enemy and object types, it has complex and stochastic environment dynamics, and there is a clearly defined goal .  Furthermore,  is difficult to master for human players, who often rely on external knowledge to learn about strategies and NetHack's complex dynamics and secrets.\footnote{``NetHack is largely based on discovering secrets   and tricks during gameplay. It can take years for one to become   well-versed in them, and even experienced players routinely discover   new ones.'' } Thus, in addition to a guide book released with  itself, many extensive community-created documents exist, outlining various strategies for the game~.  In summary, we make the following core contributions: [label=] , a fast but complex and feature-rich Gym   environment for RL research built around the popular terminal-based   game, , , and 's symbolic observation space by presenting in-depth qualitative analyses of   trained agents.      The  is a fast, complex, procedurally generated environment for advancing research in RL. We demonstrate that current state-of-the-art model-free RL serves as a sensible baseline, and we provide an in-depth analysis of learned agent behaviors.   provides interesting challenges for exploration methods given the extremely large number of possible states and wide variety of environment dynamics to discover. Previously proposed formulations of intrinsic motivation based on seeking novelty  or maximizing surprise  are likely insufficient to make progress on  given that an agent will constantly find itself in novel states or observe unexpected environment dynamics.  poses further challenges since, in order to win, an agent needs to acquire a wide range of skills such as collecting resources, fighting monsters, eating, manipulating objects, casting spells, or taking care of their pet, to name just a few. The multilevel dependencies present in  could inspire progress in hierarchical RL and long-term planning .  Transfer to unseen game characters, environment dynamics, or level layouts can be evaluated .  Furthermore, its richness and constant challenge make  an interesting benchmark for lifelong learning .  In addition, the extensive documentation about  can enable research on using prior  knowledge for learning, which could lead to improvements in generalization and sample efficiency .  Lastly,  can also drive research on learning from demonstrations  since a large collection of replay data is available. In sum, we argue that the  strikes an excellent balance between complexity and speed while encompassing a variety of challenges for the research community.  For future versions of the environment, we plan to support NetHack 3.7 once it is released, as it will further increase the variability of observations via . This version will also introduce scripting in the Lua language, which we will leverage to enable users to create their custom sandbox tasks, directly tapping into NetHack and its rich universe of entities and their complex interactions to define custom RL tasks.    \if 0 
"," Progress in Reinforcement Learning  algorithms goes hand-in-hand with the development of challenging environments that test the limits of current methods. While existing RL environments are either sufficiently complex or based on fast simulation, they are rarely both. Here, we present the~ , a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, . We argue that NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience. We compare \NLE{} and its task suite to existing alternatives, and discuss why it is an ideal medium for testing the robustness and systematic generalization of RL agents. We demonstrate empirical success for early stages of the game using a distributed Deep RL baseline and Random Network Distillation exploration, alongside qualitative analysis of various agents trained in the environment. \NLE{} is open source and available at \url{https://github.com/facebookresearch/nle}.",153
"      proposed a dense co-attention transformer model for attention using the regional features of . This work reproduced the co-attention with transformer for better effectiveness, leveraging the ability to select the features that are much more relevant.  Also, transformer model creates attention through combining all/any features, introducing lots of irrelevancy, inconsistency, and contamination.  The irrelevancy is extracted either from the influence of image features or the natural language. What our novel transformer model introduced an extra query based on the content of the features and tries to identify them. It helps in composing another set of features that can identify the activities and their importance in an image. If we consider the feature space to be , then it is important to generate attentions to extract different combinations  that can capture useful information from the attentions. However, models are biased to capture similar features  because of the trained weights. Present research considers encoding approaches  expecting that the important features will segregate. But it is a bad strategy  to encode through recurrent network function  as  where the features overlap to generate the final features instead of intelligent combination.  is the encoded attention at time . Encoding fails to scale and generalize well. Our approach creates an extra set of functionality that can help identify the attentions or features that are more helpful.           The fundamental problem, that we are trying to solve, is to counter the over-dependency of the models to combine multiple information. There are challenges that need to be solved to counter the lack of effectiveness when dealing with multiple frames/features. In VQA, there is the lack of identification of the feature that can be helpful. Furthermore, several layers of non-linear approximation reduces the visibility of many attributes in the feature space of the objects. This creates misjudgment and biases in the network and is difficult to remove. In this work, we have proposed a self-segregation strategy that can help in better effectiveness and can reduce the model contents to a great extent. There are two kinds of strategies to identify what is useful. One is self-understanding and another is coordinated-understanding of the feature space. In visual question answering, we can identify these useful features, when attended them with more sincerity. This sincerity comes with self-segregation and we have identified this problem as very intriguing. This could reduce the feature space before the generation of the representation. This reduction in feature space can also help in better performance and reduce the burden of the weights to capture different attributes and intricacies of the image. Figure  provide a comparative overview of different segregation techniques.  Since dual attention  has been very successful in many applications for enhancing the performance, we have used both segregation strategies in such architecture to show its influence on inference.  Our architectures  have successfully outperformed many of the previous works or as good as them.  Later, we combined the segregated attention to generate the relevant tensors for the applications.     The rest of the document is arranged with  %revisit of the literature in Section ,  architecture and details of implementation in Section ,  % in Section ,  analysis of the experiments in Section  and concluding remarks in Section .   The main contributions of this work are the followings. 1) strategies of segregation of the relevant information through self-segregation and coordinated-segregation in a deep dense co-attention Transformer network is introduced 2) solved the bottleneck of biases of multimodal interaction and any/all combinations of features selected for attention  3)  our segregation model helped in multi-modular co-attention model to perform better than many of the previous works 4) it is a simultaneous dense feature selector cum attention generation approach and has been introduced for the first time in the literature.   %   This work introduced a novel transformer architecture called Self-Segregation and Coordinated-Segregation Transformer for VQA application. It is capable of segregating the usable visual-reasoning information for attention.  It consists of layers of segregation transformers and fusion of segregated tensors for generation of visual-reasoning attentions. This filtered attention scheme, when estimated, contributes to enhanced performance and is one of the state-of-the-art architecture for VQA application with more control of the language-image intermediate feature sub-space. It is a strategy to identify the usefulness and importance than process any/all. Processing thousands of features are expensive and deep neural networks do not guarantee extraction of the best. Hence, segregation of the usable tensors is an important strategy for many applications. This strategy will help in reducing the feature space for better representation learning and less contamination from the unnecessary ones. Learning to segregate useful information assists in outstanding identification of the attributes, related visual structures and infer visual-reasoning space for better answering of visual questions.    The rest of the document is arranged with  the details and scope of the problem in Section ,  the description and statistics of the data in Section , Section , Section , Section , Section , Section , the intricacies of our methodology in Section ,  experiments, results and analysis in Section , revisit of the existing works in literature in Section , concluding remarks with future prospects in Section ,  
", Attention mechanism has gained huge popularity due to its effectiveness in achieving high accuracy in different domains. But attention is opportunistic and is not justified by the content or usability of the content. Transformer like structure creates all/any possible attention. We define segregating strategies that can prioritize the contents for the applications for enhancement of performance. We defined two strategies: Self-Segregating Transformer  and Coordinated-Segregating Transformer  and used it to solve visual question answering application.  Self-segregation strategy for attention contributes in better understanding and filtering the information that can be most helpful for answering the question and create diversity of visual-reasoning for attention. This work can easily be used in many other applications that involve repetition and multiple frames of features and would reduce the commonality of the attentions to a great extent. Visual Question Answering  requires understanding and coordination of both images and textual interpretations. Experiments demonstrate that segregation strategies for cascaded multi-head transformer attention outperforms many previous works and achieved considerable improvement for VQA-v2 dataset benchmark.,154
"  Code-switching is the use of more than one language in a single conversation or utterance and is prevalent in multilingual communities all over the world. Automatic Speech Recognition of code-switched speech is challenging due to the lack of transcribed code-switched speech data, a larger set of words to recognize, confusions between similar-sounding words in both languages and the lack of code-switched text data for language modeling, among other reasons. However, significant progress has been made in increasing the accuracy of ASR for code-switched languages with advances in acoustic modeling, language modeling, decoding strategies and data augmentation .  Code-switched speech co-occurs with monolingual speech, and it is imperative that ASR systems that perform well on code-switched speech also perform well on monolingual speech of one or both languages that are being mixed. Research on code-switched ASR and code-switched speech and language processing in general has focused on improving the accuracy of models on code-switched test sets. There has been very little focus so far on the effect of these techniques on monolingual speech recognition.  In this paper, we investigate the effect of standard techniques such as data pooling and fine-tuning on code-switched data on both code-switched and monolingual test sets and show that while these techniques show improvements on code-switched speech, there is a loss in performance on monolingual test sets. We hypothesize that this happens due to the phenomenon of ``catastrophic forgetting"", in which the model forgets the distribution of monolingual speech in favor of code-switched speech.   We experiment with three languages - Telugu, Tamil and Gujarati and their code-switched counterparts with English. We address two situations - the first being when we have access to a monolingual model, but not monolingual training data and want to adapt the model to recognize code-switched speech without sacrificing monolingual accuracy. For this scenario, we propose using the Learning Without Forgetting  technique  to ensure that the model does not forget the distribution of monolingual speech. The second scenario we address is when we have access to monolingual data in addition to the monolingual model. We propose various fine-tuning and regularization strategies that improve the performance on both code-switched and monolingual test sets. We suggest that future papers in code-switched speech and language processing should also report results on monolingual test sets to ensure that models can generalize across both code-switched and monolingual data.  The organization of this paper is as follows: Section 2 relates our work to prior work. Section 3 describes our training and test datasets and the experimental setup followed by a discussion of the results. Section 4 concludes. % What CS is, why it is challenging for ASR systems  % How current CS ASR approaches focus on higher accuracies on a CS test set - how real-world scenarios will always have monolingual speech in addition to CS - important to do well on both. Perform better on CS while not regressing  on monolingual.  % Contributions:  % Experiments to show that pooling/fine-tuning with CS does hurt mono performance - baseline experiment numbers on 3 CS datasets  % Approaches to fine-tuning without hurting mono performance      In this paper, we show that fine-tuning models for code-switching can lead to a drop in performance in monolingual models. Specifically, we show that a monolingual model that is fine-tuned on code-switched data improves on code-switched test sets, but degrades on monolingual test sets across ASR systems in three languages. A pooled model performs well on both monolingual and code-switched test sets, but fine-tuning this model with less code-switched data and regularization leads to best performance.   Building and Fine-tuning a pooled model relies on the availability of monolingual data that the original ASR was trained on. This may not always be feasible given that monolingual ASRs can be trained on thousands of hours of data. To address this issue, we propose using the Learning Without Forgetting  framework to build code-switched models, without sacrificing monolingual accuracy. Our experiments show that models built using the LWF framework outperform monolingual models fine-tuned with code-switched data. We also show that the loss in performance on monolingual datasets is mitigated by using the LWF technique. More generally, the LWF framework can be used to adapt speech recognition models to specific domains without forgetting the distribution of the original data they were trained on.  One limitation of our experiments is that the amount of monolingual and code-switched data used for training are of the same order of magnitude, while this is not usually the case in real-world systems due to the lack of available code-switched data. In future work, we plan to replicate these experiments on monolingual models trained with hundreds of hours of data and fine-tuned with much less code-switched data.  As future work, we plan to explore adding BLSTM layers as task specific layers. We plan to investigate adding adversarial training procedure to make the shared layer parameters to be agnostic to the specific task while encouraging the model to learn discriminative parameters at the task specific layers.   Code-switching is a special case of domain adaptation, because code-switched speech and text usually co-occur with monolingual speech and text. We suggest that future work in code-switched speech and text should also investigate the effects of models tuned for code-switching on monolingual test sets.  
","  Recently, there has been significant progress made in Automatic Speech Recognition  of code-switched speech, leading to gains in accuracy on code-switched datasets in many language pairs. Code-switched speech co-occurs with monolingual speech in one or both languages being mixed. In this work, we show that fine-tuning ASR models on code-switched speech harms performance on monolingual speech. We point out the need to optimize models for code-switching while also ensuring that monolingual performance is not sacrificed. Monolingual models may be trained on thousands of hours of speech which may not be available for re-training a new model. We propose using the Learning Without Forgetting  framework for code-switched ASR when we only have access to a monolingual model and do not have the data it was trained on. We show that it is possible to train models using this framework that perform well on both code-switched and monolingual test sets. In cases where we have access to monolingual training data as well, we propose regularization strategies for fine-tuning models for code-switching without sacrificing monolingual accuracy. We report improvements in Word Error Rate  in monolingual and code-switched test sets compared to baselines that use pooled data and simple fine-tuning.",155
"  Speaker diarization is the process of partitioning audio according to the speaker identity, which is an essential step for multi-speaker audio applications such as generating written minutes of meetings . Related techniques have been evaluated in telephone conversations , meetings , and various {.     We proposed a speaker-wise conditional inference method as an extension to the end-to-end neural diarization method. Experimental results showed that the proposed method outperformed the conventional EEND method in variable-speaker scenarios. When estimating a larger number of speakers, the proposed method showed its advantage more significantly. The proposed method achieved better speaker counting accuracy, but it was still hard to handle more than four speakers. We will explore such hard scenarios, including DIHARD challenges for our future work.           
","   Speaker diarization is an essential step for processing multi-speaker audio.   Although an end-to-end neural diarization  method achieved state-of-the-art performance, it is limited to a fixed number of speakers.   In this paper, we solve this fixed number of speaker issue by a novel speaker-wise conditional inference method based on the probabilistic chain rule. In the proposed method, each speaker's speech activity is regarded as a single random variable, and is estimated sequentially conditioned on previously estimated other speakers' speech activities.   Similar to other sequence-to-sequence models, the proposed method produces a variable number of speakers with a stop sequence condition.   We evaluated the proposed method on multi-speaker audio recordings of a variable number of speakers.   Experimental results show that the proposed method can correctly produce diarization results with a variable number of speakers and outperforms the state-of-the-art end-to-end speaker diarization methods in terms of diarization error rate.",156
"  %   %   One of the long-standing goals of speech and cognitive scientists is to develop a computational model of language acquisition . Early on in their lives, human infants learn to recognize phonemic contrasts, frequent words and other linguistic phenomena underlying the language . The computational modeling framework of generative models is well-suited for the problem of spoken language acquisition, as it relates to the classic analysis-by-synthesis theories of speech recognition . Although, generative models are theoretically elegant and informed by theories of cognition, most recent success in speech representation learning has come from self-supervised learning algorithms such as Wav2Vec , Problem Agnostic Speech Encoding  , Autoregressive Predictive Coding  , MockingJay   and Deep Audio Visual Embedding Network  . Generative models present many advantages with respect to their discriminative counterparts. They have been used for disentangled representation learning in speech . Due to the probabilistic nature of these models, they can be used for generating new data and hence, used for data augmentation  for Automatic Speech Recognition , and anomaly detection .  In this paper, we focus solely on designing a generative model for low-level linguistic representation learning from speech. We propose Convolutional Deep Markov Model , a Gaussian state-space model with non-linear emission and transition functions parametrized by deep neural networks and a Deep Convolutional inference network. The model is trained using amortized black box variational inference  . Our model is directly based on the Deep Markov Model proposed by Krishnan et. al , and draws from their general mathematical formulation for BBVI in non-linear Gaussian state-space models. When trained on a large speech dataset, ConvDMM produces features that outperform multiple self-supervised learning algorithms on downstream phone classification and recognition tasks, thus providing a viable latent variable model for extracting linguistic information from speech.  We make the following contributions: [1)]         In this work, we design the Convolutional Deep Markov Model , a Gaussian state-space model with non-linear emission and transition functions parametrized by deep neural networks. The main objective of this work is to demonstrate that generative models can reach the same, or even better, performance than self supervised models.  In order to do so, we compared the ability of our model to learn linearly separable representations, by evaluating each model in terms of PER and FER using a simple linear classifier. Results show that our generative model produces features that outperform multiple self-supervised learning methods on phone classification and recognition task on Wall Street Journal. We also find out that these features can achieve better performances than all other evaluated features when learning the phone recogniser with very few labelled training examples. Another interesting outcome of this work is that by using self-supervised extracted features as input of our generative model, we produce features that outperforms every other one in the phone recogniser task. Probably due to enforcing temporal structure in the latent space. Lastly, we argue that features learned using unsupervised methods are significantly worse than features learned by a fully supervised deep neural network acoustic model, setting the stage for future work.    
"," Probabilistic Latent Variable Models  provide an alternative to self-supervised learning approaches for linguistic representation learning from speech. LVMs admit an intuitive probabilistic interpretation where the latent structure shapes the information extracted from the signal. Even though LVMs have recently seen a renewed interest due to the introduction of Variational Autoencoders , their use for speech representation learning remains largely unexplored. In this work, we propose  Convolutional Deep Markov Model , a Gaussian state-space model with non-linear emission and transition functions modelled by deep neural networks. This unsupervised model is trained using black box variational inference. A deep convolutional neural network is used as an inference network for structured variational approximation. When trained on a large scale speech dataset , ConvDMM produces features that significantly outperform multiple self-supervised feature extracting methods on linear phone classification and recognition on the Wall Street Journal dataset. Furthermore, we found that ConvDMM complements self-supervised methods like Wav2Vec and PASE, improving on the results achieved with any of the methods alone. Lastly, we find that ConvDMM features enable learning better phone recognizers than any other features in an extreme low-resource regime with few labelled training examples.",157
"  Deep learning has penetrated machine learning in the past years, including speech technology and language modeling in particular. Despite the success of this architectural paradigm shift, application of Neural Network Language Models  in a single decoding pass is still challenging due to their structure and computational complexity. NNLMs can still be used in ASR, when passing to the 2-pass decoding scheme: in the first pass, a small footprint generic Language Model  is used, and the output of this step is a simplified recognition network with reduced search space. On this reduced lattice, a second decoding pass is applied with the NNLM for rescoring the hypotheses obtained in the first pass. Although by splitting the decoding into two parts we can leverage knowledge of the NNLMs and demonstrate significant Word Error Rate Reduction , it also introduces considerable processing delay.  Therefore, techniques exploiting the capabilities of NNLMs in a single-pass decoding approach have received particular attention recently. A possible technique is to augment the in-domain training data with a large text corpus generated by an NNLM. Of course, there is a compromise: the augmented model is no more suitable for capturing long contexts, and lose capability to support continuous space features. So far there has been no throughout evaluation of what NNLM capabilities can be transferred by neural text based data augmentation and how these compare to traditional Back-off N-gram Language Models , especially for the morphologically rich languages. The only exception is our earlier study for Hungarian showing that by combining subword lexical modeling with text based approximation of NNLM  we can greatly improve the performance of an online ASR system.  In this paper we significantly extend our previous work:  we quantify the amount of knowledge that can be transferred from the NNLM to single pass decoding with a BNLM augmented with data generated by the NNLM;  we show that the performance of offline decoding can also be significantly improved if we apply the augmented model in the first-pass for generating the lattice;  we evaluate the impact of training corpus size on the effectiveness of the data augmentation method. Rich morphology, per se, results in extremely large vocabularies, which constitutes a challenge for language modeling. Since data sparsity problems can be often handled by estimating language models on statically derived subword units , we will also evaluate morph-based models in our experiments.  In a related work, Suzuki et al. use a domain balanced mixture of the training corpora to train a shallow RNNLM for text generation and improve speech recognition results for Japanese, Korean and English tasks. For Korean subword-based language models are also utilized, but only for text generation, since in the language model of the ASR system subwords are merged. Using subword units for language models and ASR has been mostly considered for Finnish and Estonian, which are morphologically very rich languages. In, the authors managed to outperform word-based baseline model on Finnish and Estonian conversations by training subword RNNLMs and utilizing them in the second pass to rescore ASR lattices. N-gram based approximation of RNNLM was also investigated in a recent paper, where subword and character-based models were trained for Finnish and Arabic OOV keyword search tasks. Although the interpolation of approximated RNNLM and BNLM models improved OOV retrieval the proposed system was not evaluated on in-vocabulary tokens and no Word Error Rate  was presented either.       In this paper neural LMs were used to transfer their knowledge to traditional back-off LMs by generating samples for probability estimation. The morphological complexity of Hungarian was treated by using morph-based models evaluated on a call center ASR task. We found that by generating a text with 1 billion morphs, the WER can be reduced by 9\  relative while preserving real-time operation. The investigated neural text based data augmentation technique proved to be especially effective in under-resourced conditions provided that subword-based modeling is applied. With the augmented LMs we managed to transfer 45\  of WERR of the offline, 2-pass configuration to our online system. Finally, we also showed that augmented LMs can improve not only online but offline ASR results if they are used for generating the lattice for the 2nd decoding pass.   
","  Advanced neural network models have penetrated Automatic Speech Recognition  in recent years, however, in language modeling many systems still rely on traditional Back-off N-gram Language Models  partly or entirely. The reason for this are the high cost and complexity of training and using neural language models, mostly possible by adding a second decoding pass . In our recent work we have significantly improved the online performance of a conversational speech transcription system by transferring knowledge from a Recurrent Neural Network Language Model  to the single pass BNLM with text generation based data augmentation. In the present paper we analyze the amount of transferable knowledge and demonstrate that the neural augmented LM  can help to capture almost 50\% of the knowledge of the RNNLM yet by dropping the second decoding pass and making the system real-time capable. We also systematically compare word and subword LMs and show that subword-based neural text augmentation can be especially beneficial in under-resourced conditions. In addition, we show that using the RNN-BNLM in the first pass followed by a neural second pass, offline ASR results can be even significantly improved.",158
"  Recognizing code-switched speech is challenging for Automatic Speech Recognition  systems due to the lack of large amounts of labeled code-switched speech and text data for training Acoustic and Language Models. Recently, we showed that even if there is sufficient code-switched speech data to train models, there is a loss in performance on monolingual test sets when monolingual models are trained or fine-tuned with code-switched data . Since code-switched and monolingual speech co-occur, it is imperative that models perform well on code-switched speech while not deteriorating on monolingual speech.  With this goal in mind, in  we proposed strategies for learning how to recognize code-switched speech while not forgetting monolingual speech recognition in the following scenarios:\\ Case 1: If monolingual and code-switched data are both available and a model can be retrained from scratch, regularization strategies and fine-tuning a pooled model that uses all data leads to best results across data sets.\\ Case 2: If only a monolingual model is available and a new model cannot be retrained from scratch, the Learning Without Forgetting  framework can be used to improve performance on all test sets compared to a monolingual model fine-tuned on code-switched data.   In this work, we build upon our findings for Case 1, in which we have access to both monolingual and code-switched data and can retrain a model from scratch. When we train a joint model to learn both monolingual and code-switched speech recognition tasks with task specific and shared layer parameters, the model tends to drift towards one particular task. This drift is because shared layers try to learn task specific features which is not ideal for a joint model that needs to perform well on both tasks. Hence, we need to learn task invariant or agnostic shared layer parameters which lead to task agnostic features at shared layers and discriminant features at task specific layers.  In this work, we learn task agnostic shared layer parameters by adversarial discriminative learning. We show that it is possible to improve performance by using adversarial learning over our previously proposed techniques of fine-tuning and regularization on monolingual and code-switched test sets that span three language pairs - Tamil-English, Telugu-English and Gujarati-English. In this work, we assume that there exists a classifier that will classify code-switched and monolingual utterances prior to recognition by our model, however, our technique can also be used if this assumption does not hold.  The rest of the paper is organized as follows. Section 2 relates our work to prior work. Section 3 describes our experimental setup and results. Section 4 concludes.       In this paper, we show that fine-tuning models for code-switching can lead to a drop in performance in monolingual models. Specifically, we show that a monolingual model that is fine-tuned on code-switched data improves on code-switched test sets, but degrades on monolingual test sets across ASR systems in three languages. A pooled model performs well on both monolingual and code-switched test sets, but fine-tuning this model with less code-switched data and regularization leads to best performance.     Building and Fine-tuning a pooled model relies on the availability of monolingual data that the original ASR was trained on. This may not always be feasible given that monolingual ASRs can be trained on thousands of hours of data. To address this issue, we propose using the Learning Without Forgetting  framework to build code-switched models, without sacrificing monolingual accuracy. Our experiments show that models built using the LWF framework outperform monolingual models fine-tuned with code-switched data. We also show that the loss in performance on monolingual datasets is mitigated by using the LWF technique. More generally, the LWF framework can be used to adapt speech recognition models to specific domains without forgetting the distribution of the original data they were trained on.    One limitation of our experiments is that the amount of monolingual and code-switched data used for training are of the same order of magnitude, while this is not usually the case in real-world systems due to the lack of available code-switched data. In future work, we plan to replicate these experiments on monolingual models trained with hundreds of hours of data and fine-tuned with much less code-switched data.    As future work, we plan to explore adding BLSTM layers as task specific layers. We plan to investigate adding adversarial training procedure to make the shared layer parameters to be agnostic to the specific task while encouraging the model to learn discriminative parameters at the task specific layers.     Code-switching is a special case of domain adaptation, because code-switched speech and text usually co-occur with monolingual speech and text. We suggest that future work in code-switched speech and text should also investigate the effects of models tuned for code-switching on monolingual test sets.    This template can be found on the conference website. Templates are provided for Microsoft Word\textregistered, and \LaTeX. However, we highly recommend using \LaTeX when preparing your submission. Information for full paper submission is available on the conference website.    
","  Recognizing code-switched speech is challenging for Automatic Speech Recognition  for a variety of reasons, including the lack of code-switched training data. Recently, we showed that monolingual ASR systems fine-tuned on code-switched data deteriorate in performance on monolingual speech recognition, which is not desirable as ASR systems deployed in multilingual scenarios should recognize both monolingual and code-switched speech with high accuracy. Our experiments indicated that this loss in performance could be mitigated by using certain strategies for fine-tuning and regularization, leading to improvements in both monolingual and code-switched ASR. In this work, we present further improvements over our previous work by using domain adversarial learning to train task agnostic models. We evaluate the classification accuracy of an adversarial discriminator and show that it can learn shared layer parameters that are task agnostic. We train end-to-end ASR systems starting with a pooled model that uses monolingual and code-switched data along with the adversarial discriminator. Our proposed technique leads to reductions in Word Error Rates  in monolingual and code-switched test sets across three language pairs.",159
"  The attention-based encoder-decoder model paradigm has recently witnessed rapidly increased applications in end-to-end automatic speech recognition . It provides a generic framework for speech-to-text generation tasks, and achieves state-of-the-art performance on ASR as an alternative to CTC  models. The recent surge of end-to-end speech-to-text translation  studies is also due to the application of attention-based encoder-decoder models. And very recent works have demonstrated the possibility of combining the two related tasks, ASR and ST, under the same encoder-decoder architecture to achieve better performance. When targeting at ST only, transfer learning from ASR is helpful to warm-starting acoustic modeling  and enabling ST model training to focus more on learning language modeling and alignment .  In this paper, we study how to utilize ST to improve cross-lingual transfer learning for ASR. Transfer learning from high-resource languages is known to be an efficient way to improve end-to-end ASR for low-resource languages. Pre-trained or jointly trained encoder-decoder models, however, do not share the language modeling  for the same language, which is likely to be inefficient for distant target languages. We introduce ST as an auxiliary task to incorporate additional knowledge of the target language and enable transferring from that target language. Unlike previous ideas for leveraging translation data, our approach does not require any modification to the ASR model architecture. It leverages ST data instead of text-to-text translation data for ST training, which avoids speech-to-text modality adaption in the encoder. Moreover, we train ST with machine translation  pseudo-labels on high-resource ASR transcripts, which overcomes the shortage of real ST data and consistently brings gains to the transfer learning. MT pseudo-labeling also simplifies ST model training  and allows beam-searching diverse labels to alleviate overfitting. \documentclass[a4paper]{article}  \usepackage{INTERSPEECH2020} \usepackage{multirow} \usepackage{caption} \usepackage{subcaption} \usepackage{tikz} \usepackage{pgfplots} \usepgfplotslibrary{groupplots}  \title{Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation}  \address{Facebook AI, USA} @fb.com}     %           We show that cross-lingual  transfer learning for end-to-end ASR can be improved by adding ST as an intermediate step. It makes transfer learning smoother in the two-step process and incorporates additional knowledge of the target language to improve model performance. It leverages only MT pseudo-labels but no expensive human labels to train ST and does not require high-resource MT training data. Currently, our approach is based on attention-based encoder-decoder architecture. Our future work includes extending this transfer learning approach to other end-to-end architectures, such as CTC and RNN Transducer.  
"," Transfer learning from high-resource languages is known to be an efficient way to improve end-to-end automatic speech recognition  for low-resource languages. Pre-trained or jointly trained encoder-decoder models, however, do not share the language modeling  for the same language, which is likely to be inefficient for distant target languages. We introduce speech-to-text translation  as an auxiliary task to incorporate additional knowledge of the target language and enable transferring from that target language. Specifically, we first translate high-resource ASR transcripts into a target low-resource language, with which a ST model is trained. Both ST and target ASR share the same attention-based encoder-decoder architecture and vocabulary. The former task then provides a fully pre-trained model for the latter, bringing up to 24.6\% word error rate  reduction to the baseline . We show that training ST with human translations is not necessary. ST trained with machine translation  pseudo-labels brings consistent gains. It can even outperform those using human labels when transferred to target ASR by leveraging only 500K MT examples. Even with pseudo-labels from low-resource MT , ST-enhanced transfer brings up to 8.9\% WER reduction to direct transfer.",160
"      Medication names are extremely hard to pronounce for patients without a proper medical background. Thus, when interacting with Alexa on medication names, patients without this background may have many different ways to refer to a medication , Bumex , high blood pressure pill ). On the other hand, patients with medical knowledge may use abbreviations or specialized ways to refer to medication names. For example, patients may use 閳ユ笅mmune meds閳 to refer to 閳ユ笗ycophenolate mofetil hydrochloride閳 in their prescription list.   In this paper, we describe a new problem about finding the generic medication name  based on a patient's description  from a list of medications the patient is consuming. According to our internal user research, in the United States, patients with chronic diseases usually take around four to five medications daily. This problem is different from medical concept normalization  which tries to map a health-related entity mention in a free-form text to a concept in a controlled vocabulary  which is a generic concept list rather than a patient specific prescription list and is generally much longer.   We structure this  as a ranking problem. Here we rank all medications a patient is consuming based on the relationship with the patient's description and the one ranked highest will be the inference result.  We present a hard attention based entity boosted CNN architecture achieving 4\% over earlier ranking methods.   Furthermore, the mapping between SMN and DMP contains the patient's understanding of the medications, especially from the usage perspective of the medications. Using latent output from our model, we build a medication clustering system which groups together medications with similar effects and disease treatments. The output is designed to aid physicians to consider other medications as a substitution for decreasing cost as well as helping patients distinguish medications that are similar in their impression but should, in reality, be used in different conditions. Moreover, with clustering patients will have an intuitive understanding of the relationship of the medications they are consuming.  Our contributions are as follows:             In this paper, we introduce a new problem common in the development of medication voice interaction products. We evaluate the accuracy of different solutions and show that our entity boosted MIM outperform baseline models.  The specialty of this problem is that the context information is very limited when compared against other NLP tasks and the short length of the phrases prevent us from leveraging other advanced techniques that rely on words relationship in a phrase. The evaluation result also show that the problem prefers simple model structure.  Since the phrases structure is very simple, the quality of word embeddings is more important in this problem and keeping the embedding weight unchanged is important when the training data is not sufficient enough to enhance the relationship between words either due to the nature of the data or small sample sizes.   We also observe the discrepancy between synthetic collected datasets from real patients. For example, the combinations of the medicine on synthetic prescriptions may not be valid from a practitioner's or patient's perspective. We plan to further validate our model on real patient data to increase practicality. Finally on top of comparing and evaluating on two medications samples, we plan to experiment with more medications in each sample in training to closer mimic real world scenarios.     
"," Recent advancements in medical entity linking have been applied in the area of scientific literature and social media data. However, with the adoption of telemedicine and conversational agents such as Alexa in healthcare settings, medical name inference has become an important task.  Medication name inference is the task of mapping user friendly medication names from a free-form text to a concept in a normalized medication list. This is challenging due to the differences in the use of medical terminology from health care professionals and user conversations coming from the lay public.  We begin with mapping descriptive medication phrases  to standard medication names . Given the prescriptions of each patient, we want to provide them with the flexibility of referring to the medication in their preferred ways. We approach this as a ranking problem which maps SMN to DMP by ordering the list of medications in the patient闁炽儲鐛 prescription list obtained from pharmacies. Furthermore, we leveraged the output of intermediate layers and performed medication clustering. We present the Medication Inference Model  achieving state-of-the-art results. By incorporating medical entities based attention, we have obtained further improvement for ranking models.",161
"} Over the past several years, large pretrained language models   have shifted the NLP modeling paradigm from approaches based on pipelines of task-specific architectures to those based on pretraining followed by fine-tuning, where a large language model discovers useful linguistic properties of syntax and semantics through massive self-supervised training, and then small amounts of task specific training data are used to fine-tune that model .  More recently, similar approaches have been explored for knowledge representation and reasoning  with researchers asking questions like `Language Models as Knowledge Bases?' . Results suggest that  the answer is a resounding `sort of' : while language models can be coerced to answer factual queries, they still lack many of the properties that knowledge bases typically have. In particular, when evaluating LM-as-KRR models there are three explanations for why a model outputs a correct answer; 1) The model has successfully performed some reasoning or generalization required to make a novel inference, 2) the dataset contains some statistical biases that the model is exploiting, or 3) the model has memorized the exact answer, potentially from pre-training data that overlaps with the test cases. % william's comment % )}}  } % haitian's comment [1]{\textcolor{red}{Pat: #1}} % pat's comment [1]{\textcolor{orange}{William: #1}} % william's comment [1]{\textcolor{olive}{Livio: #1}} % livio's comment [1]{\textcolor{gray}{}} %#1}} % gray out the text  %   \addtocounter{footnote}{-1}%   [1][1]{  \author{Pat Verga*, Haitian Sun*, Livio Baldini Soares, William W. Cohen \\     Google Research \\    @google.com} \\ %   \And %   William Cohen \\ %   Affiliation / Address line 1 \\ %   Affiliation / Address line 2 \\ %   Affiliation / Address line 3 \\ %    \\   }  \date{}      Massive language models are the core of modern NLP modeling and have been shown to encode impressive amounts of commonsense and factual information.  However, that knowledge exists only within the latent parameters of the model, inaccessible to inspection and interpretation, and even worse, factual information memorized from the training corpora is likely to become stale as the world changes.  Knowledge stored as parameters will also inevitably exhibit all of the biases inherent in the source materials. To address these problems, we develop a neural language model that includes an explicit interface between symbolically interpretable factual information and subsymbolic neural knowledge.  We show that this model dramatically improves performance on two knowledge-intensive question-answering tasks.  More interestingly, the model can be updated without re-training by manipulating its symbolic representations. In particular this model allows us to add new facts and overwrite existing ones in ways that are not possible for earlier models.         } In this paper, we presented a method for interfacing a neural language model with an interpretable symbolically bound memory. We used that interface to change the output of the language model by modifying only the non-parametric memories and without any additional training. We demonstrated the effectiveness of this method by performing comparably or better than a high performing language model on factoid question answering while integrating new facts unseen in pretraining data. We even showed that we can modify facts, such that they contradict the initial pre training text, and our model is still largely able to answer these questions correctly.          
"," Massive language models are the core of modern NLP modeling and have been shown to encode impressive amounts of commonsense and factual information.  However, that knowledge exists only within the latent parameters of the model, inaccessible to inspection and interpretation, and even worse, factual information memorized from the training corpora is likely to become stale as the world changes.  Knowledge stored as parameters will also inevitably exhibit all of the biases inherent in the source materials. To address these problems, we develop a neural language model that includes an explicit interface between symbolically interpretable factual information and subsymbolic neural knowledge.  We show that this model dramatically improves performance on two knowledge-intensive question-answering tasks.  More interestingly, the model can be updated without re-training by manipulating its symbolic representations. In particular this model allows us to add new facts and overwrite existing ones in ways that are not possible for earlier models.",162
"  This short example shows a contrived example on how to format the authors' information for IJCAI--PRICAI--20 Proceedings.     We propose a Keywords-guided Sequence-to-sequence  model, which predicts keywords from the dialogue context hidden states and uses the keywords as guidance to generate the final dialogue response. Empirical experiments demonstrate that the KW-Seq2Seq model produces more informative, coherent and fluent responses, yielding substantive gain in both automatic and human evaluation metrics.      The file named.bst is a bibliography style file for BibTeX 0.99c 
", This short example shows a contrived example on how to format the authors' information for {.,163
" The task of spoken language understanding  aims at extracting useful information from spoken utterances. Typically, SLU can be decomposed with a two-stage method: 1) an accurate automatic speech recognition  system transcribes the input speech into texts, and then 2) language understanding techniques are applied to the transcribed texts. These two modules can be developed separately, so most prior work developed the backend language understanding systems based on manual transcripts.  Despite the simplicity of the two-stage method, prior work showed that a tighter integration between two components can lead to better performance. Researchers have extended the ASR 1-best results to n-best lists or word confusion networks in order to preserve the ambiguity of the transcripts. . Another line of research focused on using lattices produced by ASR systems. Lattices are directed acyclic graphs  that represent multiple recognition hypotheses. An example of ASR lattice is shown in Figure.  % Due to the fact that multiple hypotheses may share substructure in a lattice, lattices can encode hypotheses more efficiently compared to a n-best list.  introduced LatticeRNN, a variant of recurrent neural networks  that generalize RNNs to lattice-structured inputs in order to improve SLU.  proposed a similar idea for Chinese name entity recognition. % With the rich information from lattices,  proposed extensions to enable the transformer model to consume lattice inputs for machine translation.  proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance.  also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling.      In this paper, we propose a spoken language representation learning framework that learns contextualized representation of lattices. We introduce the lattice language modeling objective and a two-stage pre-training method that efficiently trains a neural lattice language model to provide the downstream tasks with contextualized lattice representations. The experiments show that our proposed framework is capable of providing high-quality representations of lattices, yielding consistent improvement on SLU tasks.  
"," Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language.  Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs.\footnote{The scource code is available at: \url{https://github.com/MiuLab/Lattice-ELMo}.}",164
"  Motivated by a monolingual speaker acquiring translation ability by referring to a bilingual dictionary, we propose a novel MT task that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize . This task is also distinct to supervised/semi-supervised MT task that mainly depends on parallel sentences .   The bilingual dictionary is often utilized as a seed in bilingual lexicon induction  that aims to induce more word pairs within the language pair . Another utilization of the bilingual dictionary is for translating low-frequency words in supervised NMT . We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI for initializing an unsupervised MT system , we use the ground-truth bilingual dictionary and apply it throughout the training process.  We propose Anchored Training  to tackle this task. Since word representations are learned over monolingual corpora without any parallel sentence supervision, the representation distances between source language and target language are often quite large, leading to significant translation difficulty. As one solution, AT selects words covered by the bilingual dictionary as anchoring points to drive the distance between the source language space and the target language space closer so that translation between the two languages becomes easier. Furthermore, we propose Bi-view AT that places anchors based on either source language view or target language view, and combines both views to enhance the translation quality.   Experiments on various language pairs show that AT performs significantly better than various baselines, including word-by-word translation through looking up the dictionary, unsupervised MT, and dictionary-supervised cross-lingual word embedding transformation to make distances between both languages closer. Bi-view AT further improves AT performance due to mutual strengthening of both views of the monolingual data. When combined with cross-lingual pretraining , Bi-view AT achieves performances comparable to traditional SMT systems trained on more than 4M parallel sentences. The main contributions of this paper are as follows:         In this paper, we explore how much potential an MT system can achieve when only using a bilingual dictionary and large-scale monolingual corpora. This task simulates people acquiring translation ability via looking up the dictionary and depending on no parallel sentence examples. We propose to tackle the task by injecting the bilingual dictionary into MT via anchored training that drives both language spaces closer so that the translation becomes easier. Experiments show that, on both close language pairs and distant language pairs, our proposed approach effectively reduces the gap between the source language space and the target language space, leading to significant improvement of translation quality over the MT approaches that do not use the dictionary and the approaches that use the dictionary to supervise the cross-lingual word embedding transformation.   
"," In this paper, we propose a new task of machine translation , which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training  to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences{\footnote {Code is available at \url{https://github.com/mttravel/Dictionary-based-MT} } }.",165
" There are several methods in which humans and computers can converse, like speaking  and writing . At present, research in the field of NLP has advanced a lot to attain a good understanding of textual data but there are still some ways to go to properly contemplate the audio/speech data.   Word embeddings are extensively used in NLP applications since they have proven to be an extremely informative representation of the textual data. Language models like GloVe  and Word2Vec  successfully transform textual words from its raw form to semantically and syntactically correct, fixed dimensional vectors. These type of word representations for the spoken words can be widely used to process speech/audio data for tasks like Automatic Summarization {}, Machine Translation {}, Named Entity Recognition {}, Sentiment Analysis {}, Information Retrieval {}, Speech Recognition {}, Question-Answering {} etc.  % The Word2Vec model {} successfully transforms textual words from its raw form to semantically and syntactically correct, fixed dimensional vectors. Compared to text, not much research has been done in the field of audio-based modeling primarily due to the lack of large, reliable, clean, and publicly available datasets on which the spoken word language models can be trained. Also, spoken words unlike textual words have a different meaning when they are spoken in a different tone, expression, accent, etc, and incorporating them exponentially increases the difficulty of building such language models. Such models also face difficulties such as different people can have different pronunciations, tones, and pauses for the exact same words.  The proposed model, aims at generating syntactically and semantically adequate contextualized vector representation of the variable length audio files , where each file corresponds to a single spoken word in a speech and further validates the vector representations by evaluating it on three benchmark word similarity datasets . To further increase the interpretability, this paper also provides illustrations of the vector space generated by the proposed model.    This paper introduces an unsupervised model that not only was able to successfully generate semantically and syntactically accurate contextualized representations of varying length spoken words but was also able to perform adequately on three benchmark datasets for measuring word similarities. The proposed model also demonstrated its capabilities to capture tones and expressions of the spoken words. To the best of our knowledge, this is the first work that tries to model variable-length spoken words using convolutional autoencoders. In the future, we plan to extend the capabilities of the model to handle different pronunciations/accent by different speakers.  \comment{ 
"," % A lot of work has been done recently to build text-based language models, but not much has been done to model speech/audio. In the case of text, words are represented by unique fixed-length embeddings that hold semantical and syntactic relationships between words. Such embeddings for speech/audio type data can not only lead to great advances in the speech/audio related natural language processing tasks but can also reduce the loss of information like tone, expression, accent, etc while converting speech to text in order to perform these tasks. This paper proposes a novel model architecture that produces syntactically and semantically adequate contextualized representation of varying length spoken words. The performance of the spoken word embeddings generated by the proposed model was validated by  inspecting the vector space generated, and  evaluating its performance on three benchmark datasets for measuring word similarities.  A lot of work has been done to build text-based language models for performing different NLP tasks, but not much research has been done in the case of audio-based language models. This paper proposes a Convolutional Autoencoder based neural architecture to model syntactically and semantically adequate contextualized representations of varying length spoken words. The use of such representations can not only lead to great advances in the audio-based NLP tasks but can also curtail the loss of information like tone, expression, accent, etc while converting speech to text to perform these tasks. The performance of the proposed model is validated by  examining the generated vector space, and  evaluating its performance on three benchmark datasets for measuring word similarities, against existing widely used text-based language models that are trained on the transcriptions. The proposed model was able to demonstrate its robustness when compared to the other two language-based models.",166
" The rapid emergence of the novel coronavirus without much known history has engrossed the international scientific community, resulting in an overwhelming amount of publications and data released on a daily basis. The rate of publications has far exceeded the time-consuming peer-review process, leaving many important information with little to no attention. In an attempt to absorb and utilize the unprecedented amount of COVID-19 scientific literature, prominent journals have opened publications to the public and several platforms have prompted the data science community to aid in the process. One of the notable platform has been COVID-19 Open Research Dataset  containing thousands of papers published on PubMed and multiple tasks to understand the papers. \par Recent progress on language processing has made possible the exploration of massive text corpus otherwise infeasible by manual work. Attention-based mechanisms  and pre-trianed language representations such as BERT , Open GPT-2 , XLNet , and ELMo  have achieved a great success in many language fields, including sentence prediction and text summarization. Many language models are adopting a common practice of pre-training on a huge corpus mined from the web, followed by a fine-tuning process targeted for specific tasks. Following this trend, we focus on utilizing the popular BERT architecture for text summarization task, more specifically extractive summarization, where important sentences are picked from the text verbatim. This task fits the need of the scientific community to rapidly process and extract important information from the inundating number of COVID-19 publications while adhering to their original text. \par However, as COVID-19 papers are published on a daily basis, many of them with time-sensitive or unseen content, the model also needs to train in an online fashion without experiencing catastrophic forgetting. To this end, we propose , a novel BERT architecture built on existing techniques to learn and extract summaries from a continual stream of new tasks while retaining previously learned information. Heavily inspired by , our architecture utilizes two separate BERT models with layer-wise connections and deploys an alternating training process to minimize catastrophic forgetting. It also stacks a small Transformer encoder on top for extracting summary sentences from text.    The online training ability of  enables adaptive learning on new data flowing in a time-sequential manner, especially fitting to the overwhelming amount of COVID-19 literature published on a daily basis. In contrast to the provided abstracts, extractive summarization of those literature can provide not only a sound, original summary of the article but also indications of where the interesting sentences and ideas lies within the text. This feature can be handy with longer papers, much of COVID-19 literature, as the readers can save significant amount of time while understanding the broad idea of the papers. The scalable architecture of  also enables continually learning over longer time and in more frequency to digest new research data and information faster. \par The evaluations listed on Table  shows that  performs much better on articles with extensive medical terms  compared to the generic scientific articles . One plausible explanation for this improvement is that PubMed articles from CORD-19 extensively use terms and information related to COVID-19. Furthermore, the generic, coronavirus-related terms appear throughout the decades of PubMed documents. For instance, the medical word ""hydroxychloroquine"" appears often in many COVID-19 literature. These frequent appearances account for more parameters compressed in KB, thus resulting in the layer-wise connection sharing meaningful information to AC. Hence, the model performs better with more availability of historical knowledge, even if the COVID-19 itself is a new disease. \par The difficulty that  experienced during the progress step can be justified with the fact that CORD-19 contains publications dating back to the 20th century, which present radically different information to the more modern publications. This information disparity can be fixed by penalizing more for older publications through time threshold, such as before the coronavirus pandemic. \par We hope that the model provides a ground for other researchers to explore into the area of summarization for COVID-19 and many other literature. For future explorations, we propose constructing a dynamic version of the model, such as dynamically increasing/decreasing network neurons.   \\ Structure of the full SARS-CoV-2 RNA genome in infected cells      Abstract \\ SARS-CoV-2 is a betacoronavirus with a single-stranded, positive-sense, 30-kilobase RNA genome responsible for the ongoing COVID-19 pandemic. Currently, there are no antiviral drugs or vaccines with proven efficacy, and development of these treatments are hampered by our limited understanding of the molecular and structural biology of the virus. Like many other RNA viruses, RNA structures in coronaviruses regulate gene expression and are crucial for viral replication. Although genome and transcriptome data were recently reported, there is to date little experimental data on predicted RNA structures in SARS-CoV-2 and most putative regulatory sequences are uncharacterized. Here we report the secondary structure of the entire SARS-CoV-2 genome in infected cells at single nucleotide resolution using dimethyl sulfate mutational profiling with sequencing . Our results reveal previously undescribed structures within critical regulatory elements such as the genomic transcription-regulating sequences . Contrary to previous studies, our in-cell data show that the structure of the frameshift element, which is a major drug target, is drastically different from prevailing in vitro models. The genomic structure detailed here lays the groundwork for coronavirus RNA biology and will guide the design of SARS-CoV-2 RNA-based therapeutics.    Extracted Summary \\ sars-cov-2 is an enveloped virus belonging to the genus beta coronavirus, which also includes sars-cov, the virus responsible forthe 2003 sars outbreak, and middle east respiratory syndrome coronavirus , the virus responsible for the 2012 mers outbreak. despite the devastating effects these viruses have had on public health and the economy, currently no effective antivirals treatment or vaccines exist. there is therefore an urgent need to understand their uniquerna biology and develop new therapeutics against this class of viruses. coronaviruses  have single - stranded and positive - sense genomes that are the largest of all known rna viruses  . previous studies oncoronavirus structures have focused on several conserved regions that are important forviral replication. for several of these regions, such as the 5閳 utr, the 3閳 utr , and the frameshift element , structures have been predicted computationally with supportive experimental data from rnase probing and nuclear magnetic resonance  spectroscopy .  
"," The scientific community continues to publish an overwhelming amount of new research related to COVID-19 on a daily basis, leading to much literature without little to no attention. To aid the community in understanding the rapidly flowing array of COVID-19 literature, we propose a novel BERT architecture that provides a brief yet original summarization of lengthy papers. The model continually learns on new data in online fashion while minimizing catastrophic forgetting, thus fitting to the need of the community. Benchmark and manual examination of its performance show that the model provide a sound summary of new scientific literature\footnote{Our code is available at \url{https://git.io/JJJfO}}.",167
" The sustained increase in the volume of scientific publications in the past decades has made reference selection substantially more challenging, especially for inexperienced researchers or investigators who are approaching a new field. Automated citation recommendation can help ease this challenge by suggesting the most appropriate citations for a query document, e.g., a paper draft to be submitted to ICPR 2020. Most existing citation recommendation systems rank the document candidates based on their relevance to a given query, and recommend the top entries. In alternative to simple ranking, other approaches have proposed using submodular scoring functions to select the best candidates based on a trade-off between their relevance, coverage and diversity  or their information flow in a citation network . In all cases, query-based citation recommendation systems heavily rely on the effectiveness of the underlying textual representation and the scoring functions used to assess the similarity between the query and the candidates or the candidates themselves.    Textual representation have been heavily studied as inputs to natural language understanding tasks, but they also play an important role in content-based information retrieval. Tasks in both these fields rely on textual representations that can express the semantic similarity  between textual elements, viewed as sequences of words, word subunits or characters. Recently, pre-trained language models such as ELMo , GPT-2  and BERT  have proved effective as textual representations in a broad variety of tasks. These models compute contextualized embeddings for each token which can be used as inputs for task-specific neural architectures. In this paper, we show that such models can also contribute significantly to improve the accuracy of citation recommendation.  The main contributions of our work can be summarized as:            and a state-of-the-art citation recommendation approach, Citeomatic\footnote{https://github.com/allenai/citeomatic/}, on the ACL Anthology Network corpus\footnote{http://clair.eecs.umich.edu/aan/index.php}. The proposed approach has outperformed all other approaches in a range of metrics.    The rest of this paper is organized as follows: Section II presents the related work. Section III reviews feature-based document scoring and submodular selection. Section IV presents the proposed approach for deep textual representation, including the proposed fine-tuning strategies. Section V presents the experimental results, and Section VI presents the conclusion.  %%%%%%%%%%%%%%%%%     In this paper, we have clarified the submodular approach and maximize the document scoring stage by trainable models with neural networks. We also show the effective of using pre-training models to transfer learning to learn document embedding to score the relevance in content-based information retrieval systems. Our experiments have proven once again the significant benefits of using submodular approach to enhance the accuracy of citation selection compared to state of the art citation recommendation system as Citeomatic.  In this paper, we have proposed a novel approach to citation recommendation that leverages a deep representation of the documents. The representation has been obtained by encoding each document with Sentence-BERT , a recently proposed transformer-based approach for text embedding. In the paper, we have proposed fine-tuning SBERT with positive and negative examples derived with various strategies from the citation graph. In addition, we have proposed performing the prediction of the recommended list with a submodular scoring function that balances the relevance of the recommended citations with the diversity of their authors. The experimental results over a benchmark dataset  have shown that the proposed approach has been able to outperform all the compared approaches, including a state-of-the-art neural approach, by a remarkable margin in all metrics . In the near future, we aim to explore the integration of submodular scoring in the training stage and extend the evaluation to other domains and document types.    conference papers do not normally have an appendix    use section* for acknowledgment  
"," With the rapid growth of the scientific literature, manually selecting appropriate citations for a paper is becoming increasingly challenging and time-consuming. While several approaches for automated citation recommendation have been proposed in the recent years, effective document representations for citation recommendation are still elusive to a large extent. For this reason, in this paper we propose a novel approach to citation recommendation which leverages a deep sequential representation of the documents  cascaded with Siamese and triplet networks in a submodular scoring function. To the best of our knowledge, this is the first approach to combine deep representations and submodular selection for a task of citation recommendation. Experiments have been carried out using a popular benchmark dataset -- the ACL Anthology Network corpus -- and evaluated against baselines and a state-of-the-art approach using metrics such as the MRR and F1@k score. The results show that the proposed approach has been able to outperform all the compared approaches in every measured metric.",168
" Humans have been communicating for thousands of years using natural languages. It is estimated that there are currently around 6,500 spoken languages around the world . As the main method for communication, automating language understanding is a fundamental concept that has been studied for many years in the literature. As a result, many tasks, shown in Table 1, have been introduced to verify and validate these studies.  Deep learning is employed to serve and craft models for these tasks as their complexity is, by an order of magnitude, out of the scope of the traditional machine learning algorithms. Training a deep learning model is not always affordable due to the huge computing resources and large datasets requirements for these models. This motivates to start exploring other directions to transfer knowledge from one deep learning model to another.  The general idea of transfer learning, formally defined in section , is to transfer parameters or knowledge from one trained model to another. Based on the availability of labeled dataset, transfer learning can be divided into transductive and inductive transfer learning . These approaches are general and can be applied to many tasks in machine learning. For instance,  surveyed the literature for the transfer learning methods followed in recommendation systems with auxiliary data. Moreover,   discussed multi tasking and transfer learning in the domain of bioinformatics using machine learning and data mining techniques. Recently transfer learning has been applied heavily in natural language processing.  discussed the evolution of transfer learning in natural language processing. They mainly focus on the most dominant approach for transfer learning which is sequential fine tuning. However, we believe that transfer learning in NLP should be studied more thoroughly with highlights to all transfer learning approaches.   In the past few years, language models have evolved and they achieved much better results compared to traditional language models. These trained language models were used to transfer knowledge to solve many natural language processing tasks.  In this survey, we highlight the latest advances, summarize them and categorize each one in the corresponding category of transfer learning. We follow a similar taxonomy developed by  and  in categorizing and analyzing the literature. Since there is a huge number of papers related to natural language processing we focused only on the recent papers with a reasonable number of citations.   \def\arraystretch{2} [htp!]  {|p{5cm}|p{10.5cm}|}                   & Description                                                                                                               \\ \hline Summarization             & The process of extracting the more important points in a set of documents and providing a shorter, more compact version      \\ \hline Question Answering and Classification       & Given a question and a certain context, we want to index the answer to the question in the given context. This is usually called abstractive question answering. On the other hand, generative question answering considers the problem as a generative task. Another related task is question classification where we classify questions semantically to different categories.                    \\ \hline Text Entailment           & Given a pair of sentences, we want to predict whether the truth of the first sentence implies the truth of the second sentence.               \\ \hline Semantic Role Labeling    & A labeling task where each word is given its semantic role in the current context.                                           \\ \hline Co-reference Resolution  & The process of collecting expressions that refer to the same object.                                                            \\ \hline Named Entity Extraction and Recognition   & The task of extracting entities  along with their labels .                                                                     \\ \hline Sentiment Analysis        & To classify sentences or paragraphs according to the sentiment, e.g., positive, negative or neutral.        \\ \hline Reading Comprehension     & A similar problem to question answering where for a given context we want to comprehend it by answering questions correctly. \\ \hline Translation               & The process of pairing each given sentence in a language  to another sentence in language  that has the same meaning.      \\ \hline Sentence Pair Classification & Classify if a given pair of sentences are semantically equivalent.\\ \hline Natural Language Understanding & Considers how effective models are on different tasks including  question answering, sentiment analysis, and textual entailment, etc. .  \\ \hline User Intent Classification & Association of text to a specific purpose or goal.\\ \hline Natural Language Inference & Determine if given a premise whether the hypothesis is entailment, contradiction, or neutral. \\ \hline Part of Speech Tagging & Label each word to its corresponding part of speech based on its meaning and context. \\ \hline  Document Grounded Dialog Response & Given web document and conversation history what is the proper response ?\\ \hline       In this paper, we provided a review of transfer learning in natural language processing. We discussed the possible language models, datasets, and the tasks that were tackled in the research related to transfer learning. Moreover, we provided a taxonomy for transfer learning that divides it into inductive and transductive transfer learning. We then divided each category into multiple levels and then collected the related papers in each corresponding category. Although there might be different definitions in the literature, we tried our best to incorporate the best definition that is agreed upon across different studies. In general, we see that compared to RNN-based and CNN-based language models, it seems that attention-based models are much more dominant in the literature. Additionally, we see that BERT seems the defacto architecture for language modelling as it appears in many tasks. This is due to its bidirectional architectures which makes it successful in many down-stream tasks. Regarding transfer learning, sequential fine-tuning seems to be dominant in the literature as compared to other approaches like zero-shot. Moreover, it seems that mutli-task fine tuning is gaining more attention in the last few years. As stated in many studies, training on multiple tasks at the same time can give much better results.  Regarding datasets, text classification datasets seem to be more widely used compared to other tasks in NLP. This is due to the fact that it is easier to fine-tune models in such tasks.   For future research, we make some observations and outlooks in the field of transfer learning for NLP. For specific tasks like sentiment classification, abstractive question answering, parts-of-speech tagging, we recommend using bidirectional models like BERT. On the other hand, for generative tasks like summarization, generative question answering, text generation, etc. we recommend using models like GPT-2, T5 and similar architectures. We also believe that some transfer learning techniques are underrated like zero-shot which seems to perform really well on multiple tasks . Moreover, adapter modules can replace sequential fine-tuning because they perform equally good but provide faster and more compact models as compared to traditional fine tuning. Finally, while language models keep getting bigger and bigger, we believe that more research should be put on trying to reduce the size of such models which will make them deploy-able on embedded devices and on the web. Deploying knowledge distillation  techniques can prove useful in such circumstances for reducing the size of large language models.  Irfan: I would suggest that we add a section  where we point out some recommendations/best practices and possible future works in short term and long term. Other than that the paper looks in a good shape.  Irfan: Is there any other popular NLP task where transfer learning has not been researched?  
"," Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging NLP tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of NLP. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.",169
"  Never-ending information generation and sharing on the Web provides us with abundant data, most of which constitute the unstructured text sources. To better make sense of and draw associations among those data, we, human beings, use relational facts among the subjects  in the text. For a more comprehensive understanding of specific domains such as bioinformatics, finance, social networking etc., we need computers to process those information.   It is essential to represent the information delivered by the text in machine-readable format. One way to do that is to represent entities and their relations in so called triples, which indicate unambiguous facts about entities. A triple  implies that entity  has relation  with another entity .  Knowledge graphs  such as FreeBase  and DBpedia  are examples of such representations. They are directed and labeled graph structured data which aim to express such explicit semantics and relations of entities in triple form.  Relation extraction is a sub-task of natural language processing  which aims to discover relations  between entity pairs  and  given unstructured text data. Earlier work on relation extraction from text heavily relies on kernel based and feature based methods .However, recent research studies make use of data-driven deep learning methods eliminating conventional NLP approaches for relation extraction.  explained how the conventional deep learning methods are integrated into relation extraction.  reviewed relation extraction literature focusing on distant supervision. As the number of research studies on relation extraction increases, the need of a survey on current state-of-the-art of neural relation extraction methods arises.  This work provides a comprehensive and comparative review on the research field, focusing on the challenges together with improvement ideas.  Section  explains various approaches for relation extraction. In section  neural relation extraction methods are classified in terms of data supervision and explained. Section  describes existing challenges in this field of research.  In section , commonly used datasets in model assessment are evaluated. We discuss possible future research directions and improvement ideas in section  and we conclude our survey in section .     In this survey, we summarized neural relation extraction methods in terms of their approaches and data supervision and datasets for this task. In addition, we explained common challenges and discussed possible remedies to them.  To acquire abundant training instances, the latest studies make use of distant supervision. However, it brings noise to data which greatly affects the training of relation extraction models. In addition, there are no explicit negative samples, since the data itself have wrong annotations due to ill-alignment of the unstructured text and the knowledge graph. For that reason, instead of sentence-level approaches in supervised relation extraction, multi-instance approaches are developed for relation extraction with distant supervision. Also, few-shot learning for relation extraction is a research area that has still room for improvement. Supervised approaches are not to be abandoned. Indeed, incorporating pre-trained language models in supervised relation extraction make significant improvements in comparison to using conventional deep learning methods. Instead of treating entity recognition and relation extraction separately as in pipeline approaches, later studies adopt end-to-end approaches jointly extracting the entities and relations, which tend to better handle problems associated with overlapping triples and long-tail relations.    
"," Neural relation extraction discovers semantic relations between entities from unstructured text using deep learning methods. In this study, we present a comprehensive review of methods on neural network based relation extraction. We discuss advantageous and incompetent sides of existing studies and investigate additional research directions and improvement ideas in this field.",170
" Deep neural networks  have shown promise in various tasks of natural language processing , but a DNN is usually considered as a black-box model. In recent years, explaining features encoded inside a DNN has become an emerging direction. Based on the inherent hierarchical structure of natural language, many methods use latent tree structures of language to guide the DNN to learn interpretable feature representations. However, the interpretability usually conflicts with the discrimination power. There is a considerable gap between pursuing the interpretability of features and pursuing superior performance.  Therefore, in this study, we aim to explain a trained black-box DNN in a post-hoc manner, so that the explanation of the DNN does not affect its performance. This is essentially different from previous studies of designing new network architectures or losses to learn interpretable features, ~physically embedding tree structures into a DNN.  Given a trained DNN, in this paper, we propose to analyze interactions among input words, which are used by the DNN to make a prediction. Our method generates a tree structure to objectively reflect interactions among words. Mathematically, the interaction of several words is quantified as the difference of the contribution between the case when these words  contribute jointly to the prediction and the case when each individual word contributes independently to the prediction. The interaction between words may bring either positive or negative effects on the prediction. For example, the word  and the word  in the sentence  have a strong and positive interaction to the prediction of the person's identity, because the words  and  indicate a ``novice"" jointly, rather than work individually to represent a hand with a green color.  The core challenge in this study is to guarantee the objectiveness of the explanation.  the tree needs to reflect true interactions among words without significant bias. The Shapley value is widely considered as a unique unbiased estimation of the word contribution, which satisfies four desirable properties, . Thus, we define the interaction benefit among words based on the Shapley value. Let us consider a constituent with  words.  denote numerical contributions of each word to the prediction of a DNN, respectively.  represents the numerical contribution of the entire constituent to the prediction. Hence,  measures the interaction benefit of this constituent. If , interactions among these  words have positive effects on the prediction; otherwise, negative effects. Here,  can be computed as Shapley values.    Given a trained DNN and an input sentence with  words, Figure shows the tree structure that reflects word interactions encoded inside the DNN. In the tree,  leaf nodes represent  input words. Each non-leaf node corresponds to a constituent of the input sentence. A parent node connects two child nodes with significant interaction benefits. We use the parent node to encode interactions among its child sub-constituents. More specifically, there are two types of interactions among words, ~ interactions within a constituent and  interactions between constituents.  { In the aforementioned sentence, interactions between the constituent  and its adjacent constituent  are composed of all potential interactions among all combinations of words from the two constituents, including interactions between  ,  ,  ,  ,  ,  ,  ,  ,  .   The tree selects and encodes the most salient interactions among words, in order to reveal the signal processing in a DNN. We further propose additional metrics to diagnose interactions among words, ~the quantification of interactions within a constituent, the quantification of interactions between two adjacent constituents, and ratios of interactions that are modeled and unmodeled by the tree.  Theoretically, our method can be used as a generic tool to analyze various DNNs, including the BERT, ELMo, LSTM, CNN and Transformer. Experimental results have demonstrated the effectiveness of our method.  Contributions of this paper can be summarized as follows.  We propose a method to extract and quantify interactions among words.  A tree structure is automatically generated to represent salient interactions encoded in a DNN.  We further design six metrics to analyze interactions, which provides new perspectives to understand DNNs.      In this paper, we have defined and extracted interaction benefits among words encoded in a DNN, and have used a tree structure to organize word interactions hierarchically. Besides, six metrics are defined to disentangle and quantify interactions among words. Our method can be regarded as a generic tool to objectively diagnose various DNNs for NLP tasks, which provides new insights of these DNNs.             two-column      Using the \centering command instead of  will save space   Positioning your figure at the top of the page will save space and make the paper more readable   Using 0.95\columnwidth in conjunction with the  
"," This paper proposes a method to disentangle and quantify interactions among words that are encoded inside a DNN for natural language processing. We construct a tree to encode salient interactions extracted by the DNN. Six metrics are proposed to analyze properties of interactions between constituents in a sentence. The interaction is defined based on Shapley values of words, which are considered as an unbiased estimation of word contributions to the network prediction. Our method is used to quantify word interactions encoded inside the BERT, ELMo, LSTM, CNN, and Transformer networks. Experimental results have provided a new perspective to understand these DNNs, and have demonstrated the effectiveness of our method.",171
"  Even though usage and popularity of Twitter have stopped rapidly growing and even dropped in recent years\footnote{https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users}, it still has a considerable amount of loyal users who keep on sharing everything from worldwide events to random personal details with their followers. We decided to focus on one of the random personal details that people share, specifically - anything to do with food consumption and related topics.   Several corpora of Latvian tweets exist in prior work, but none of them are domain-specific and have been collected over an extensive period of time. Milajevs  collected and analysed 1.4 million tweets geo-located in Riga, Latvia from April 2017 to July 2018 and 60 thousand tweets  from November 2016 to March 2017. Pinnis  collected and analysed 3.8 million tweets of Latvian politicians, companies, media, and users who interacted with these entities from August 2016 to July 2018 There are also several data sets of general sentiment-annotated tweets \footnote{https://github.com/nicemanis/LV-twitter-sentiment-corpus} amounting to 14,781 tweets in total.   In this paper, we describe the Twitter eater corpus  and analyse its contents. We also provide two sub-corpora - one consisting of question and answer tweets and one with sentiment-annotated tweets. More details can be found in Section . In Sections  and  we describe question answering and sentiment analysis experiments using our corpus. Finally, we conclude the paper in Section .       In this paper, we described the creation of a fairly large narrow-domain corpus of Twitter posts related to the topic of eating. We gave some insights in overall observations gained from the corpus contents and various trends that we noticed from the data. We believe that the data would be useful in many linguistic, sociological, behavioural and other research areas.  We experimented with creating a food-related question answering system using one subset of our data and a sentiment analysis system using another subset to highlight potential use-cases of our corpus. While the results did not break new ground, we hope that they inspire related future research.    Overall the corpus gives a fair amount of analytical data to work with. We advise other interested persons to do morphological analysis of the TEC text to find interesting patterns such as we did with our  ""Question Answering Experiment"". In future work we will continue to collect data from the next years for the corpus and try to gain data, that is older than October 2011. A sentiment analysis will be included in the full paper.  
","     We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data. We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus.",172
" Simultaneous translation  is widely used in international conferences, summits and business. Different from standard neural machine translation , simultaneous NMT has a stricter requirement for latency. We cannot wait to the end of a source sentence but have to start the translation right after reading the first few words. That is, the translator is required to provide instant translation based on a partial source sentence.    Simultaneous NMT is formulated as a prefix-to-prefix problem, where a prefix refers to a sub-sequence starting from the beginning of the sentence to be translated. In simultaneous NMT, we face more uncertainty than conventional NMT, since the translation starts with a partial source sentence rather than the complete information in conventional NMT. \Waitk{} is a simple yet effective strategy in simultaneous NMT where the generated translation is  words behind the source input.  That is, rather than instant translation of each word, \waitk{} actually leverages  more future words. Obviously, a larger  can leverage more future information, and therefore results in better translation quality but at the cost of a larger latency. Thus, when used in real-world applications, we should have a relatively small  for simultaneous NMT.   While only small  values are allowed in inference, we observe that training with a larger  will lead to better accuracy for \waitk{} inference, as demonstrated in Figure, in which a wait- model is required for EnglishGerman translation. If training with , we will obtain a  BLEU score. But if we train with wait- where  is set as a larger value such as ,  or  and test with wait-, we can get better BLEU scores. Despite the mismatch between training with wait- and testing with wait-, the model can benefit from the availability of more future information. This is consistent with the observation in .  %<-- start of a the figure {r}{0.42\textwidth}   %   -3 strategy.} %   %<-- end of the figure Here, the challenge is how much future information we should use. As shown in Figure, using more future information does not monotonically improve the translation accuracy of \waitk{} inference,  mainly because that more future information results in a larger gap between training and inference. In this work, we propose a framework that can automatically determine how much future information to use in training for simultaneous NMT. Given a pre-defined  for inference, we prepare  training tasks wait- with different  values . We introduce a controller such that given a training sample, the controller can dynamically select one of these tasks so as to maximize the validation performance on \waitk, i.e., the one we are interested in. The task selection is based on the data itself and the network status of the translation model. The controller model and the translation model are jointly learned, where the learning process is formulated as a bi-level optimization problem and we design an effective algorithm to solve it. We conduct experiments on four datasets to verify the effectiveness of our method.   The remaining part is organized as follows. The related work is introduced in Section, the problem formulation and background %, including the monotonic Transformer, is introduced in Section, and our method is introduce in Section. The experiments and the analysis are in Section , and we discuss the conclusion and future work in Section .    In this work, we propose a new approach for simultaneous NMT. Motivated by the fact that \waitk{} benefits from future information, we introduce a controller, which adaptively assigns a task { beyond using translation quality only and explicitly introduce the latency constraint. Second, we will combine our method with the adaptive decoding methods. Third, we will apply the idea in this work to more applications like action prediction, weather forecasting, game AI, etc.       
"," Simultaneous neural machine translation  has attracted much attention recently. In contrast to standard NMT, where the NMT system can utilize the full input sentence, simultaneous NMT is formulated as a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and more uncertainty is introduced to decoding. \Waitk~ is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. We observed that training simultaneous NMT systems with future information  generally outperforms the standard ones . Based on this observation, we propose a framework that automatically learns how much future information to use in training for simultaneous NMT. We first build a series of tasks where each one is associated with a different $k$, and then learn a model on these tasks guided by a controller. The controller is jointly trained with the translation model through bi-level optimization. We conduct experiments on four datasets to demonstrate the effectiveness of our method.",173
"   % What do you think about ``Extracting Useful Sentences from Online Reviews: a Data Management Perspective''?  % \parskip=0pt %    % Online reviews have been proven to be important assets to online platforms such as Amazon, Yelp, or Booking.com. % These reviews are serving as a rich information source for users' decision making process % ranging from purchasing electronic products, booking a romantic vacation, to important decisions like housing or career. % Oftentimes, beyond decision supporting, online platforms perform text classification % to make further use of the reviews.  is perhaps the most well-studied classification task on review text. % By classifying whether reviews are positive or negative, % we better understand the quality of a service/product or even of a specific aspect. % While much research attention is focused on sentiment classification, however, % the review text can also be mined to fulfill many other interesting purposes. % For example, from the online reviews, eBay extracts sentences to generate product description when a product does not have one, % TripAdvisor mines useful tips for future travelers and % Airbnb constructs user profiles based on reviews that the user have given.  % pipeline  % machine learning models for text classification  % One specialized model for each task  % Data preparation, model selection   Database engines nowadays store and serve a massive amount of text data. To make the best of the text data, the most commonly performed task is perhaps . The classification results allow the database engine to aggregate the unstructured text, better filter query results, or perform complex business analytic tasks. Consider the example of an online review database. By performing sentiment classification in the review text database, review aggregation platforms like Amazon, Yelp or Airbnb can better understand  the quality or customer satisfaction level of each product or service by aggregating the opinion polarity on reviews of each item. Beyond sentiment analysis, the review text can be mined to fulfill many other interesting purposes via classification. For example, based on results of text classification, eBay extracts sentences to generate product description when a product does not have one, TripAdvisor mines useful tips for future travelers and Airbnb constructs user profiles based on reviews that the user have given. As the first contribution of this paper, we conduct a comprehensive literature survey on the different text classification tasks performed on review text in Section .  The recent success of deep learning in NLP provides solutions to text classification tasks with promising results. For example, the most recent NLP model for sentiment classification  achieves an accuracy of 97.1\%, only 0.7\% away from the human performance . While there exists these highly effective models, the process of developing them is very expensive and time-consuming. To make things even worse, there is no model architecture that is ``globally optimal''. We found in the literature that researchers proposed new model architectures to achieve the best performance on each task. This means that the expensive process of model engineering is repeated . In this work, we focus primarily on two parts of the process that contributes to most of the cost:  training data preparation and  model engineering.  \yuliang{let's see whether we can get to data cleaning} \yuliang{the training data preparation part we can give credit to Snorkel.}   Both training data preparation and model engineering are non-trivial processes. Even with powerful models like TextCNN , LSTM  or BERT , training a good text classifier still requires a significant amount of training data  . Besides, this problem is getting increasingly important as merchants accumulate more reviews and engineers develop abundant applications such as stock prediction, poll analysis, and experience search.   %%  %Accompanying the success of sentiment analysis, it is a natural question to ask what else we can extract. This is especially important for review owners to make the best use of their data. We therefore summarize six types of emerging sentences and their applications, including suggestion, tip, product description, humor, argument, and spoiler. These sentences serve different purposes corresponding to the review types. Suggestion extracts advice-giving sentences. They are helpful to improve a product or service, or the experience of a potential customer. Tip describes a personal experience in short but practical texts. Tip may imply a suggestion such as paying 18\% tip when dining at a restaurant in the united states. Product description depicts the factual details such as specifications about a product. Humors are sentences written in a creative way that read funny and vivid. Argument focuses on evidential comments that support or oppose a proposition. Spoiler refers to plot-revealing remarks that are common in the reviews of media products. Many of the six applications have lead to commercial products, such as TripAdvisor travel tip, Yelp funny reviews, eBay new item description, Yahoo short answers to ``how-to'' queries and Airbnb user profiling by aspect describing comments.  % A lot of applications for processing text rely on semantic tags % that are annotated on the text.  %\textcolor{purple}{ A lot of applications for processing text rely on tagging words, phrases or sentences with semantically informative tags. %} \jinfeng{Origin: ``A lot of applications for processing text rely on semantic tags that are annotated on the text.''} Sentiment analysis, for example, annotates sentences or phrases with a sentiment tag that indicates whether the sentence has a positive or negative sentiment. These sentiment tags are exploited by downstream applications to determine appropriate actions. Another example is entity tagging, which determines if a span in the text refers to a real-world object.  %\textcolor{purple}{ Generally speaking, the task of annotating text with semantic tags can be referred to as the semantic tagging problem. %} \jinfeng{Origin: ``We refer to the problem of annotating text with semantic tags as the {{ In this paper, we focus on short text, which can be a sentence, a paragraph, or a passage. We also refer short text loosely as sentence. %} \jinfeng{Origin: ""In this paper, we refer a sentence, a paragraph, or a passage loosely as a ``sentence''''}. %\xiaolan{Add example & citations.} \jinfeng{Added One example in the next sentence}  %A great success of semantic tagging is sentiment %analysis, %where practitioners develop a lot of applications to predict whether a %sentence conveys a tag of positive or negative opinions. In addition %to the applications using sentiment analysis, many applications in %machine learning and artificial intelligence are also empowered by %semantic tagging. To completely understand why people are interested %in semantic tagging, we survey the applications adopting semantic tagging and learn how semantic tagging works for them.  %Since these novel applications are attracting increasing attention, we briefly discussed them in this paper. We surveyed the practices emerging in recent three years and classified them into five groups according to the tags that they focus on, including Tip, Product Description, Humor, Argument, and Spoiler. These classifications will give ideas to practitioners on what new applications they can develop.    %To solve semantic tagging, a solution should be able to classify sentences into two groups according to their relevance to the targeted tag. There are two types of methods, supervised learning and rule programming. Recent solutions generally leverage supervised learning, as it automatically learns how to separate sentences from a set of human labels. As the other option, rule programming is less used due to its significant programming effort. It is very difficulty and sometimes impossible to program the full set of rules for the classification. There will be numerous rules that a sentence can semantically convey the targeted tag. Herein we focus on discussing supervised methods.    %To solve semantic tagging, a solution should be able to classify sentences into two groups according to their relevance to the targeted tag. Recent solutions generally leverage supervised learning, as it automatically learns how to separate sentences from a set of human labels. Supervised learning offers at least two favorable advantages. First, the input of an algorithm is a set of labels that can be obtained from non-programmer experts. Second, these algorithms are programming free as it is very difficult to polish a combination of rules that can reproduce human labels.  It is very difficulty and sometimes impossible to program the full set of rules for the classification. There will be numerous rules that a sentence can semantically convey the targeted tag. Herein we focus on discussing supervised methods.  %\wctan{changed ``sentence tagging'' to ``semantic tagging''}\jinfeng{I like ``semantic tagging'' and have used it throughout the paper}  %A solution for semantic tagging should be able to classify %sentences into two groups according to their relevance to the %semantic tag.  There are two types of methods for semantic tagging: rule programming and supervised learning. Rule programming-based methods require an expert to specify rules for semantic tagging. This is often error-prone and requires significant programming effort.  % since it is difficult to specify a good set of rules for semantic tagging in general.  In contrast, supervised learning models do not require much programming effort. However, training these models requires labeled data but can typically produce models with good semantic tagging results. %\wctan{do we have evidence of less used or frequently used? I removed the mentions of these claims.} %\wctan{rewrote. pls read} \jinfeng{It reads great!}   Our focus in this paper is on supervised learning models. Deep learning models  have become popular methods for semantic tagging today. One reason why deep models are popular for semantic tagging is that they are often more capable of learning complicated functions than other kinds of models. Another reason is that the superiority of deep models has been reported by many publications. For example, deep models achieve good prediction quality that is close to the human prediction on GLUE SST-2 sentiment classification task. Some recent studies made comparisons between deep models and simple models  to understand whether deep models are always superior to simple models.  %\wctan{what tasks are compared in these studies?}\jinfeng{other semantic tagging tasks. Added one sentence below.}  They conducted comparisons on various tagging tasks such as suggestion mining or humor detection. Their results reveal marginal or sometimes no improvements of deep models over simple models.   It is therefore natural to ask whether deep models are better than simple models when developing solutions for semantic tagging.  %\textcolor{purple}{ Semantic tagging forms the core of many tasks including sentiment classification, suggestion mining, and humor detection. Existing studies, however, compare deep and simple models only on individual tasks. Furthermore, they do not provide insights on how dataset characteristics affect the performances of different models. Consequently, it is hard to generalize their model selection criteria to new tasks or new datasets for the same task. Hence, given a new dataset, it is still unclear whether selecting a deep model will bring the best tagging performance. %} % The goal is to identify the model which achieves best performance for the targeted task. These studies do not consider dataset characteristics when selecting the best model. As a consequence, no previous studies show how to generalize the model selection to new tasks and even new datasets of the same task. They developed selection guidance at the granularity of individual task but not at the granularity of individual dataset characteristic. Given a new dataset, it is still unclear whether selecting deep models can bring the best semantic tagging performance.} %\jinfeng{add one paragraph to motivate 21 datasets and our study}  % select five models and 21 different datasets In this paper, we embark on a systematic study to understand the performance tradeoffs of deep models vs. simple models for semantic tagging. Towards this goal, we selected 3 representative deep models: CNN, LSTM, and BERT and 2 representative simple models: LR and SVM. CNN and LSTM are well-known methods that have been widely used in both the academic and industry communities and more recently, BERT.  %represents the most up-to-date method that w%on the best paper award in NAACL 2019.  To make a meaningful comparison and systematic study, we collected 21 real datasets that are frequently used in semantic tagging. These datasets exhibit several prominent data characteristics, including  a variable number of labels ;  a wide range of tag-conveying label ratio ;  different label cleanliness .  %We evaluated the tagging quality of CNN, LSTM, BERT, LR, and SVM on the 21 datasets. We used F1 as the quality measurement. Overall, only BERT outperforms LR and SVM on most of the datasets, while there is no clear winner between CNN/LSTM and LR/SVM. However, BERT does not outperform LR/SVM on large datasets that have more than 100,000 labels. BERT achieves the same F1 on one large dataset and worse F1 on two large datasets. The maximum F1 gap of BERT on large datasets is 0.03 yet the training takes days. In other words, deep models may not be better choices than simple models on large datasets, if the training time is considered as another evaluation criterion.   %We conducted further analyses to measure the effects of size, label skewness, and cleanliness on the tagging quality. We found that these three dataset characteristics have significant influences on the superiority of deep models. When a dataset has abundant labels, exhibits a high ratio of positive instances, or contains many dirty labels, deep models show less F1 improvements. Therefore, practitioners should pay attention to dataset characteristics if they expect better tagging quality when using deep models.  %We further found the dataset characteristics not only affect the superiority of deep models, but also regulate the final tagging quality. To help practitioners select appropriate models and set expectation on the tagging performance for their dataset, we prepare a comprehensive heap map that shows the characteristics of the 21 datasets and their tagging F1's with BERT and SVM. By using this heap map, practitioners can estimate the tagging quality gain of adopting deep models. Meanwhile, they can try to improve the dataset characteristics to push the tagging quality to higher upper limits.  \\  We evaluate the quality of semantic tagging on five selected models on 21 datasets and we obtain a rather surprising finding. %that overturns the general perception %towards deep models.  We find that deep models and simple models are complementary to each other on the task of semantic tagging. Specifically, deep models perform significantly better on smaller datasets, while simple models can be trained more efficiently on larger datasets and achieve similar semantic tagging quality. Therefore, one should select deep models or simple models for semantic tagging based on the actual dataset characteristics and requirements on efficiency. %On large datasets that contain more than 100,000 labels, deep models only achieve a maximal 0.03 F1 superiority yet takes days for training. If training time is a concern, deep models are not better choices than simple models for large datasets.  %Deep models can offer better tagging quality, but they alone are inadequate to guarantee satisfactory tagging quality. In our experiments, the tagging F1 of the best deep model varies from 0.96 to 0.15. This is because the tagging performance is at the same time regulated by the dataset characteristics, including the number of labels, the ratio of tag-conveying labels, and the label cleanliness. We conducted further experiments and confirmed that more labels, a higher ratio, and better label cleanliness contribute to greater tagging quality of deep models.   Based on our findings, we develop a comprehensive heat map to guide practitioners on selecting the appropriate model for the desired semantic tagging performance for their datasets. This heat map shows the characteristics of the datasets and their quality score of semantic tagging with different tagging models. By using this heat map, practitioners can estimate the semantic tagging quality gain while adopting different deep or simple models. At the same time, they can also try to improve the dataset characteristics to improve the quality of semantic tagging. \\  %To help practitioners select appropriate models and set expectation on the tagging performance for their datasets, we prepare a comprehensive heap map that shows the characteristics of the 21 datasets and their tagging F1's with BERT and SVM. By using this heap map, practitioners can estimate the tagging quality gain of adopting deep models. Meanwhile, they can try to improve the dataset characteristics to push the tagging quality to higher upper limits. \\   We systematically evaluate deep models and simple models for the task of semantic tagging. %We selected X models, curated Y datasets of varying characteristics, and conducted a series of experiments investigate the performance of the models in these two models and datasets.  %\wctan{Are we releasing our collection of datasets and models?} \jinfeng{Yes. I added one sentence about it to the end of this paragraph. Will work on the release later.} Our key contributions are as follows.  We surveyed a number of applications to motivate our study. We selected three representative deep models and two simple models that are widely used to develop these applications. We collected 21 datasets of varying characteristics for a comprehensive study.  We conducted extensive evaluations to obtain performance of semantic tagging of the five selected models on all the datasets. We found deep models do not necessarily perform better than simple models on large datasets.  We evaluated the effects of dataset characteristics on the quality of semantic tagging. We found the training size, label ratio, and label cleanliness impact the quality of semantic tagging.  We generated a comprehensive heat map that can guide practitioners to decide whether they should adopt deep models or simple models and anticipate the performance of semantic tagging for their datasets. To facilitate future research, we will release our collection of datasets, models, and implementations at https://github.com/rit-git/tagging. \\   %  In this paper, we studied sentences extraction from online reviews. Specifically, we summarized our contributions as follows. We will release our codes and datasets \jinfeng{we need to decide} for further research.  % \parskip=0pt %     %   We survey a number applications in Section. We discuss the designs of selected deep models and simple models in Section. We introduce the collected datasets and their characteristics in Section. We perform experimental evaluations and comparisons in Section. We analyze the effect of dataset characteristics and present key findings in Section. We conclude our study in Section. \documentclass{vldb} \usepackage{balance} \usepackage[shortlabels]{enumitem} \usepackage{multirow}  %\usepackage{times}   \usepackage{graphicx} \usepackage{multirow} \usepackage{xcolor} % {Theorem}[section] {Example} \usepackage{graphicx} \usepackage[labelfont=bf]{caption} \usepackage{subcaption} \usepackage{tabularx} \usepackage{multicol} \usepackage{booktabs} \usepackage{color, colortbl}   \definecolor{Gray}{gray}{0.9} \usepackage{times}  [1]{{{[[[ {#1}\ --yuliang ]]]}}} [1]{{{[[[ {#1}\ --xiaolan ]]]}}} [1]{{{[[[ {#1}\ --nikita ]]]}}} [1]{{{[[[ {#1}\ --alon ]]]}}} [1]{{{[[[ {#1}\ --wangchiew ]]]}}} [1]{{{[[[ {#1}\ --jinfeng ]]]}}} [1]{{{[[[ {#1}\ --lu ]]]}}} }}  \def\flat{simple} \def\Flat{Simple} \def\FLAT{SIMPLE}   % Semantic tagging?  \def \def\Sentence{Sentence}  \def\tagging{tagging} \def\Tagging{Tagging}  % Include information below and uncomment for camera ready \vldbTitle{Deep or Simple Models for Semantic Tagging? It Depends on your Data} \vldbAuthors{Jinfeng Li, Yuliang Li, Xiaolan Wang, Wang-Chiew Tan} \vldbDOI{https://doi.org/10.14778/3407790.3407844} \vldbVolume{13} \vldbNumber{11} \vldbYear{2020}  \pagenumbering{gobble}   %\title{Beyond Sentiments: What Else are People Trying to Extract from Online Reviews [Experiments]} \title{Deep or Simple Models for Semantic Tagging?\\It Depends on your Data}   %  in this sample file, there are a *total* % of EIGHT authors. SIX appear on the 'first-page'  and the remaining two appear in the \additionalauthors section.  \author{ % You can go ahead and credit any number of authors here, % e.g. one 'row of three' or two rows . % % The command \alignauthor  should % precede each author name, affiliation/snail-mail address and % e-mail address. Additionally, tag each line of % affiliation/address with \affaddr, and tag the % e-mail address with \\        @megagon.ai} }     Semantic tagging, which has extensive applications in text mining, predicts whether a given piece of text conveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.  We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against ``simple models'' over datasets with varying characteristics. Specifically, we select three prevalent deep models  and two simple models , and compare their performance on the semantic tagging task over 21 datasets.  %The results showed that deep models are not %necessarily better than simple models when the %characteristics of datasets are variable. %\xiaolan{Maybe say they did not perform well on large datasets first. The previous sentence claims they are not better, so the next sentence better elaborate over this point.} \xiaolan{Maybe add a transition sentence? e.g., To better understand their performance}  %To understand what are the exact characteristics of datasets affecting tagging quality, we selected the representative simple model  and deep model , and compared their performances on different types of datasets.  Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task. %\textcolor{blue}{In particular, we found that simple models outperform deep models on larger datasets with higher label ratios or worse label cleanliness, when runtime is a concern}.  %The conditional outperformance of deep models %suggests that practitioners should carefully %select learning models when they aim to achieve %the best tagging quality.  %Our results will systematically guide practitioners in selecting the right learning model for their semantic tagging task. % %To further assist practitioners to pick right %learning models, we generated a comprehensive heat map that compares tagging qualities across varied combinations of models and datasets. The heat map will be an instructional reference for practitioners to adopt appropriate models and set accurate expectations when tagging sentences for their own datasets.  %\wctan{heat map or table?} \jinfeng{I prefer heat map that uses colors and reflects the effect of data characteristics on F1}  %\textcolor{red}{Our results indicate that practitioners should pay attention to dataset characteristics when they apply deep models for better \tagging{} quality. We also generate a comprehensive heat map from our results that can help practitioners to adopt appropriate models and set expectation on tagging performance for their datasets}.         % %    %    %     \documentclass{vldb} \usepackage{graphicx} \usepackage{multirow}   \title{Beyond Aspects and Sentiments: What Else are People Trying to Extract from Customer Reviews}   [t] { {|l|l|l|} {*}{Suggestion}& Suggestion dataset      & Provide a human-labeled dataset \\                           & Semi-supervised C2C suggestion mining & Claim the better performance   \\                           & Travel tips             & Only work for TripAdvisor dataset as it contains labeled data by experts\\                           & A Survey on suggestion mining & Evaluate different supervised NN approaches  \\                           & customer-to-customer suggestion extraction & Propose automatic suggestion mining and explore SVM model \\                           & CNN for sentence classification & General method for supervised sentence classification \\ {*}{Fact}  & Unsupervised tip-mining & The only Unsupervised method    \\                      & Reason mining & Extract product pros and cons reasons from user reviews \\                      & Mining product defects and improvements       & Mining actionable information that business can use to improve products          \\                      & LDA based product defect mining               & Adopt LDA model in mining product cons                                           \\                      & Mining product failure by distant supervision & Distant supervision classifiers outperform strong baselines in ming product cons \\                      & Detecting experience from weblogs             & Detect actual human activity or event from review comments                       \\ {*}{Answer} & Yahoo!Answer Tips                    & Find the best answers for a given question                                       \\                      & Question answering from reviews               & Find relevant reviews for a question                                             \\                      & Aspect based question answering from reviews  & Find relevant reviews for a question and outperform                \\ \hline                                    }     % problem similar to extractive summarization           We conducted a comprehensive study to evaluate the performance of deep models and simple models on tagging various datasets. In the evaluation, we picked a recently emerging deep model , two popular deep models , and two classical simple models . Our results show that deep models  are not necessarily better than simple models . Specifically, deep models achieve significantly higher tagging quality on small datasets, but they cannot obtain better performance than simple models on large datasets. Our further analyses show that the extraction quality of a model is at the same time affected by dataset characteristics, including training size, label skewness, and label cleanliness. Any of the following three conditions may lead to better performance of tagging models, including abundant labels, a high ratio of positive instances, and many clean labels in a dataset. However, the improvement of F1's achieved by deep models is less obvious than that achieved by simple models under such conditions, leading to deep models are not always better than simple models for tagging labels. Therefore, choose a suitable tagging model for a specific dataset rather than sticking with deep models should be the way of performing tagging tasks in the future. Our study, especially the visualized heat map will be an informative instruction for practitioners when they want to consider the scale, the skewness, and the cleanliness of a dataset as the criteria for choosing suitable tagging models.   For example, deep models do not outperform simple models when a dataset has abundant labels, but deep models consume significantly more computational overhead.   Our study is the most comprehensive one that used the largest number of real-world datasets to compare deep models and simple models. Our results reveal for the first time that dataset characteristics are the key factors to determine whether deep models can achieve better tagging quality than simple models. Given the raw complexity of real-world datasets, choosing a suitable tagging model for a specific dataset rather than sticking with deep models should be the way of performing tagging tasks in the future. Our study, especially the visualized heat map will be the most informative instruction for practitioners to choose a suitable tagging model for their dataset, by considering its scale, label ratio, and cleanliness. }   \fi    {6pt}     {6pt}  }     that are largely diversed in the 21 selective datasets.  
","  Semantic tagging, which has extensive applications in text mining, predicts whether a given piece of text conveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.  We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against ``simple models'' over datasets with varying characteristics. Specifically, we select three prevalent deep models  and two simple models , and compare their performance on the semantic tagging task over 21 datasets.  %The results showed that deep models are not %necessarily better than simple models when the %characteristics of datasets are variable. %\xiaolan{Maybe say they did not perform well on large datasets first. The previous sentence claims they are not better, so the next sentence better elaborate over this point.} \xiaolan{Maybe add a transition sentence? e.g., To better understand their performance}  %To understand what are the exact characteristics of datasets affecting tagging quality, we selected the representative simple model  and deep model , and compared their performances on different types of datasets.  Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task. %\textcolor{blue}{In particular, we found that simple models outperform deep models on larger datasets with higher label ratios or worse label cleanliness, when runtime is a concern}.  %The conditional outperformance of deep models %suggests that practitioners should carefully %select learning models when they aim to achieve %the best tagging quality.  %Our results will systematically guide practitioners in selecting the right learning model for their semantic tagging task. % %To further assist practitioners to pick right %learning models, we generated a comprehensive heat map that compares tagging qualities across varied combinations of models and datasets. The heat map will be an instructional reference for practitioners to adopt appropriate models and set accurate expectations when tagging sentences for their own datasets.  %\wctan{heat map or table?} \jinfeng{I prefer heat map that uses colors and reflects the effect of data characteristics on F1}  %\textcolor{red}{Our results indicate that practitioners should pay attention to dataset characteristics when they apply deep models for better \tagging{} quality. We also generate a comprehensive heat map from our results that can help practitioners to adopt appropriate models and set expectation on tagging performance for their datasets}.",174
" %% UNSUPERVISED SHIZ  The problem of disambiguation is defined as selecting the correct analysis from a set of possible analyses for a word in a sentence---e.g., from among the analyses produced by a morphological analyser. Disambiguation is performed by utilizing information in the surrounding context.\footnote{This paper contains {, which means that to train our model we need only a morphological analyser for the language and an unlabelled corpus.  % Erzya and North-Sami are two examples of languages that have a morphological analyser but less than 30k annotated tokens.%\todo{such as? russian has it..}.  %%%%%%%%%%%%%%%%%%%  The main idea of our approach is to use bidirectional LSTMs---BiLSTMs---to disambiguate the output of morphological analysers, by utilizing only the unambiguous outputs during the training procedure. %% \todo{TXEMA: add context embedding shit} % We train bidirectional models using a sequence of embeddings for the surface form for each target word. The objective of the network is to produce output probability distributions over the possible POS tags and lemmas. The model is trained using only the unambiguous input tokens; the loss is computed only for those unambiguous instances. Ambiguous tokens are not considered as target tokens during training.  % Evaluation is done on  data for each language. Thus, only the quality of the evaluation is dependent on the amount of available annotated data---the model quality is only affected by the amount of unlabelled data we use.  Since we only input unlabelled data for training, the quality of the model itself is only affected by the amount of available unlabelled data for the language. In our experiments, we evaluate our models on manually annotated data sets for Finnish, Russian and Spanish. For Finnish and Russian, at least, annotated  data is in limited supply, whereas for all three languages unlabelled data is in abundant supply.  %% ??? CITE CORPORA!!!  The paper is organized as follows. % In Section we point to some relevant prior work. % In Section we describe the problem of morphological ambiguity and provide a brief motivation for the interest in the problem. % In Section we provide a classification for the different types of ambiguity that appear in the corpus, as well as an analysis of the viable and appropriate strategies for each type of ambiguity. % Section describes our data pre-processing steps and model architecture. % Section specifies our experimental setup, as well as the parameters used in training. % In Section we discuss the results obtained from the experiments. % Section concludes with current directions of research.       We have shown that the output of morphological analysers can be disambiguated to a significant degree for Finnish, Russian and Spanish. The requirements for this procedure are: the language must have a morphological analyser, there must exist a text corpus, and preferably a small amount of annotated data for evaluation purposes. The same procedure we used should perform comparatively for any language with a morphological analyser, assuming it is of sufficient quality---unknown tokens must rely on the less accurate ``blind'' predictions for inference. There are many morphologically rich languages that could benefit from this, such as other Uralic languages, Turkic languages, many Indo-European languages, etc. There is limited annotated training data for many of these languages, but morphological analysers are available for most of them.  The quality of the analyser in terms of percentage of unambiguous output does affect the final total token accuracy. The difference between the two cases end result presented in this work was small in the end. It is unclear how much ambiguity will begin to significantly impair our method.  Named Entity Recognition  could theoretically be used in conjunction with our procedure to further disambiguate the proper noun analyses.  We have achieved different performance depending on whether the objective used was disambiguating the lemma or POS\@. We have seen that different types of ambiguity are solved to varying degrees by predicting either POS or lemma. A natural next step would be to combine the two different models in an ensemble model.  In table we saw that, although POS tagging works for most of the cases, around 9\  of the ambiguities are only solvable by lemma prediction. Since it is possible to identify these instances during inference, an ensemble solution could use the lemma prediction model to disambiguate these.  Moreover, around 6\  of the instances currently cannot be disambiguated using either method.  This puts the upper limit on accuracy to 85\  for the better model . Using an ensemble model to also capture the lemma-only ambiguities would therefore push this limit to 94\ .  Another approach we have explored is the use of multi-task learning to predict both POS and lemma at the same time. We tried a na\""{   To push the performance nearer to 100\ , it will be necessary to make a model that predicts morphological tags, either as an addition to the existing models, or as a standalone model that we can then invoke for these instances where the POS and lemma are the same.    
","   We consider the problem of disambiguating the lemma and part of speech of ambiguous   words in morphologically rich languages.   %    We propose a method for disambiguating ambiguous words in context, using a large   un-annotated corpus of text, and \comment{??? finite-state",175
"     % The claim that knowledge of linguistic structures is innate is among the most significant and controversial claims of generative linguistics.  notices that human language systematically include rules that make reference to hierarchical structure, but rarely if ever has rules that reference linear order. This is surprising in light of the fact that key data favoring the acquisition of structural rules over linear ones are often absent from the raw linguistic input to the child acquiring language. These observations lead to the proposal of the , and this  has been the subject of much debate over the last several decades .  Humans appear to use structural biases to guide language acquisition. A classic example is the rule for subject-auxiliary inversion: Native English speakers uniformly acquire a rule like the structural generalization in Figure  that makes reference to hierarchical syntactic structures, despite the fact that the raw linguistic input often supports linear generalizations which are intuitively just as simple . Humans are not alone in possessing this inductive bias: Prior investigations have identified some artificial learners with a structural bias by virtue of having a significantly restricted the hypothesis space  or a hierarchically structured architecture that learns from pre-parsed data .   However, these results cannot tell us whether a learner starting with very weak biases can  a structural bias merely from exposure to raw linguistic data. While inductive biases are often understood to be unchangeable properties of a learner, this need not be the case. For instance, in one dominant paradigm in natural language processing, pretraining on raw data is used to create a general purpose sentence processing model like BERT , which can subsequently be fine-tuned to perform a downstream task. The model's inductive biases with respect to the downstream task may be substantially influenced by the prior knowledge acquired during pretraining. % eneral prior knowledge acquired during pretraining is sure to influence such model's inductive bias on a downstream task is no doubt influenced by prior   For instance, if a learner draws on general prior knowledge when faced with a novel generalization problem, acquiring new general knowledge should in some cases influence its inductive bias.      In this work, we present new experimental evidence that BERT may acquire an inductive bias towards structural generalizations from exposure to raw data alone.  We conduct four experiments inspired by   to evaluate whether BERT has a structural or linear bias when generalizing about structure-dependent English phenomena. We follow the  design , outlined in Figure . First, we fine-tune BERT to perform a classification task using data intentionally ambiguous between structural and linear generalizations. Then, we probe the inductive biases of the learner by observing how it classifies held-out examples that disambiguate between the generalizations.  % Unlike , we test a learner with significant pretraining, meaning we are probing the inductive biases of the fine-tuning  allowing us to evaluate  The classification tasks illustrate three structure dependent rules of English grammar regarding subject-auxiliary inversion, reflexive-antecedent agreement, and negative polarity item  licensing. A fourth task requires the classifcation of sentences based on an arbitrary rule: whether the verb in an embedded clause is in the past tense. The data is generated from templates using the generation tools and lexicon built by  and .  %  suggest that humans may be this type of learner  % a learner's inductive biases with respect to a particular generalization problem may be influenced by prior knowledge. Acquiring new knowledge   % Are humans unique in possessing this bias? Researchers have  % Where does this bias come from? According to the ,   % During language acquisition, humans make generalizations are biased towards acquiring grammatical rules based on hierarchical structure, as opposed to linear order. Over the last 50 years, there has been considerable debate about whether this bias is acquired or innate. The strongest argument in support of the  is   which relies on the assumption that     % holds that key data favoring the acquisition of structural rules over linear ones are often absent from the raw linguistic input acquiring language.  % In this work, we present new experimental evidence from artificial statistical language learners that casts doubt on the soundness of this argument. The goal of these experiments is to evaluate whether exposure to unstructured linguistic data provides sufficient evidence for a low-bias learner to acquire an inductive bias towards structural generalization. Recent advances in artificial neural networks for natural language understanding have produced models which appear to acquire robust representations of hierarchical syntax    % at the outset to be possible   good candidates for counterexamples to this claim. Using the technique of  state-of-the-art models like BERT  appear to acquire rich linguistic knowledge from raw data. BERT is pretrained to produce contextualized representations for words and sentences which are often taken as the input to a fine-tuning pipeline on downstream tasks used to train models that approach human performance on tasks that require significant syntactic knowledge, such as judging the grammatical acceptability of sentences .     % Our experiments are inspired by , who conduct an experiment with similar goals following the  design  in which a learner is trained to perform a task using data intentionally excluding certain key examples. The inductive biases of the learner can then be probed by observing the behavior of the learner on these held-out examples. Their experimental data are inspired by  well-known examples  illustrating how subject-auxiliary inversion in English follows a structure dependent rule  as opposed to a linear one .  % \ex.\a. Is the man who is tall  happy? %  tall is happy?     % In each experiment, a neural network classifier learns to use a BERT encoding to detect unacceptable sentences in these domains. Following the poverty stimulus method the training data is intentionally impoverished in order to be consistent with both a structure dependent rule and a linear rule. Observing how the models generalize to the withheld examples shows whether BERT encodings have a bias towards structural or linear information. There is also a followup experiment which tests how classifiers generalize when acceptability cannot be used as a cue towards the structure dependent generalization.   The results of these experiments suggest that BERT likely acquires a inductive bias towards structural rules from self-supervised pretraining. BERT generalizes in a way consistent with a structural bias in 3 out of 4 experiments: those involving subject-auxiliary inversion, reflexive binding, and embedded verb tense detection.  % Furthermore, in the one exceptional case---NPI licensing---there is evidence that humans show illusory effects based on linear order , which suggests that humans have a linear. While these experiments leave open several alternative explanations for this generalization behavior, they add to mounting evidence that significant syntactic knowledge, including a structural biases, can be acquired from self-supervised learning on raw data.  % The structure of the paper is as follows. Section  provides background on the hypothesis of linguistic nativism and on neural networks for language understanding. Section  reviews prior work looking for evidence of structural bias in statistical learners, and oulines the present approach. Section  discusses the data and methods for the main experiments. Sections  and  are the results and discussion. Section  includes the followup experiment, and an Appendix includes analysis of the classifiers. Section  concludes.  % specific assumptions in the argument in support of the hypothesis that this bias is innate to humans.     These results suggest that it is likely that BERT does acquire some form of a structural inductive bias from self-supervised pretraining, at least outside of the NPI domain.   a real possibility that BERT encodings bias structural features over linear ones, at least in the polar question and reflexive domains. They point more strongly in this direction than earlier results by . If this interpretation is correct, it would cast some doubt on the impoverishment assumption from  argument from the poverty of the stimulus by showing that raw data does contain overwhelming evidence that language is hierarchical. If some learner does not require innate bias to discover the utility of preferring structural rules over linear ones, it stands to reason humans may not either. On the other hand, our results are consistent with other interpretations, and so we caution against leaping to this strong conclusion, at least without further evidence.      First, in the NPI domain, BERT does not show a structural bias. However, it does not immediately follow that BERT does not have a structural bias at all. By virtue of the paradigm's design, the classes that the model converged on are consistent with both a linear and a structural position. As mentioned above, 6/20 classifiers classified the test items systematically, though not in the way predicted in Table . Instead, they grouped the top left and bottom right sentences in one class, and the top right and bottom left sentences in the other. The sentences in the first class might be characterized as all sentences with an NPI at the end of the sentence , or all sentences with an NPI in the main clause . Additional experiments are needed to determine which of these outcomes has occurred.  Furthermore, humans do not necessarily show a structural bias in processing similar examples. The unacceptable test items in the NPI paradigm, where the negation precedes the NPI, are known as , because they can spuriously appear to be acceptable to humans, and pattern with grammatical sentences in self-paced reading and ERP experiments .    Strikingly,  found that the parallel examples in the reflexive paradigm do not trigger this illusory effect.  Thus, NPIs may in retrospect not be the clearest example of humans' structural bias.     Some doubts remain even in the domains where BERT appears to show a structural generalization. Some surface features could accidentally give the same predictions as the structural generalization on the test data. For instance, in the subject-auxiliary inversion data in Table , a classifier could coincidentally identify the acceptable examples by learning to identify a string with a relativizer adjacent to an auxiliary . In fact, we control for this particular confound by generating acceptable examples where a finite verb follows the relativizer .  However, there are likely other surface generalizations that are consistent with the results. This problem can be addressed by training and evaluating on data that contradict these generalizations, but alternative convergent hypotheses cannot be eliminated entirely. This is a fundamental limitation of the poverty of the stimulus design: It is not possible to determine that BERT is adopting any particular generalization.    The strongest conclusion that can be made is to  the possibility that BERT has adopted a particular generalization.   That said, if we continue to find convergent from multiple unrelated domains, Occam's Razor tells us that we should conclude BERT has a structural bias. Given the large number of conceivable surface generalizations, let us assume that an arbitrary generalization is equally likely to support any of the four classification behaviors for the test minimal pair. It follows that if the classifier does make some surface generalization, there is a 1 in 4 chance for each experiment that it would accidentally align with the structural generalization. Then the probability that this chance alignment would occur in at least 3 out of 4 domains is about 5\ .    , it is somewhat lessened in the case of our experiments, because we find results consistent with structural generalization in 3 out of 4 separate domains. If the classifier did make some surface generalization, there is a 1 in 4 chance for each experiment that it would accidentally align with the structural generalization, assuming the possible surface generalizations are uniformly distributed across the 4 ways to classify the out-of-domain minimal pair. Then the probability that this would happen in at least 3 out of 4 domains is less than 0.02.  This worry could be alleviated further if it could be shown that baseline models without unsupervised pretraining tend to make the linear generalization on these datasets. These experiments will have to be included in future work. However, at present, the results of  experiments can be used as a proxy. As described in Section , these experiments test the ability of sentence encoders without substantial unsupervised pretraining to generalize from a paradigm resembling the polar question data in my experiments. In 5 out of 6 of the model architectures they tested, the linear generalization was preferred. While the task in their experiment is different the acceptability judgment task in the present work, based on this finding it seems that sequence models without substantial unsupervised pretraining are likely to prefer the linear generalization in the polar question domain.    A particularly interesting special case of this outcome is where the classifier succeeds because features pertaining to acceptability are made salient by BERT. This would lead to the same result as using structural features, since the two classes in each of the experimental datasets align exactly with acceptability. The success of acceptability classifiers trained on BERT, as discussed in Section , already suggests that the acceptability of a sentence is easily accessible from BERT representations. In the following section, I test whether the acceptability cue is crucial for classifiers to make the structure-dependent generalization.    I conduct a followup experiment to test whether the success of the classifiers in the previous section is crucially tied to their ability to detect grammatical anomalies. This can be tested simply by constructing paradigms like in the previous experiments, but where acceptability cannot be used to distinguish between the sentences in the two classes. In constructing a paradigm where all sentences are acceptable, it is necessary to identify some criterion other than acceptability which can be used to distinguish the classes. Following experiments by  and , I train a classifier to determine whether a verb in a particular structural position in the sentence is past tense or present tense.     The paradigm is shown in Table . The structural fact which works for both the training and generalization items is whether the verb in the relative clause is past tense or present tense. The linear feature, which works only for the training items, is that the first verb in the sentence is past tense. The matrix verb may be either past tense or present tense, and its tense is selected for the entire paradigm at random. As in the polar question dataset, the training and generalization items differ in whether the relative clause precedes the matrix verb  or follows the matrix verb . Once again, this requires the subject and object to agree in animacy and number in order to guarantee that the same relative clause can modify both. The training procedure is identical to that described in Section .                                      The results from this experiment are shown in Figure . All 20 random restarts made the structural generalization, with the mean percentage of out-of-domain minimal pairs correctly classified over 98\ .            This result suggests that BERT privileges structural features over linear ones independendent of those structural features necessary to make acceptability distinctions. As before, there is much room for doubt. It could be explained by purely linear features. For instance, in all the sentences with embedded past tense, there is a past tense verb adjacent to a complementizer . Merely attending to this bigram would be sufficient to explain this finding. However, it adds to converging evidence of a structural bias in BERT.           The results  show that BERT's performance is consistent with having formed a structural generalization in 3 out of 4 empirical domains tested. This generalization is very reliable in the case of subject-auxiliary inversion discussed by . While the models make the linear generalization in the NPI case,  find that humans also often misjudge the acceptability of these ``NPI-illusion'' sentences. These findings suggest that it is highly plausible that the structural bias attributed to humans can be acquired from purely data-driven learning.     This work presents new evidence that highlights the possibility that language learners could acquire a structural inductive bias from statistical regularities in raw linguistic data. In particular, we find the most comprehensive evidence to date  of a low-bias learner demonstrating a structural bias acquired through unsupervised learning on raw data. However, evidence from other empirical domains is needed to fully evaluate this conclusion.    If positive results can be found in other empirical domains, then it becomes increasingly implausible that BERT accidentally appears to make a structural generalization.    Future work should draw more direct connections between neural networks and human language learners. BERT is trained on data from domains far outside the input to human learners, and in much greater quantities. Indeed, the quantity and quality of data are two other pillars of  argument from the poverty of the stimulus. Therefore, it is essential to replicate these experiments with models trained on less data, which bears greater resemblance to the input to a child. Furthermore,  observes that human languages generally lack linear rules. If neural networks can acquire human-like biases, they should also struggle to form certain kinds linear generalizations. As techniques for machine language learning and self-supervised pretraining continue to advance, we expect to learn more about which linguistic universals are and are not learnable from data.    This fact is easily explained by the nativist hypothesis; if human language processing is innately biased against linear rules, they should not arise in natural language. However, even if it is shown conclusively that statistical learners can acquire a structural bias, this brings us no close to an explanation for why human language eschews linear rules. This is because the statistical learners have only acquired a structural bias with the aid of raw linguistic input, which is already inherently organized by structural rules. Therefore, if a persuasive alternative to nativism is found to explain language acquisition, an entirely new puzzle is re-opened: how do linguistic universals  driving structural rules arise?    As techniques for statistical language learning and self-supervised pretraining continue to advance, we will learn more about what kinds of linguistic knowledge are and are not learnable from data. Our work does not conclusively show that a structural bias is learnable. However, it does contribute new evidence the important debate about whether structural bias is innate.     We thank Chris Barker, Chris Collins, Stephanie Harves, Brenden Lake, Tal Linzen, Alec Marantz, Tom McCoy, and the audience at NYU's Syntax Brown Bag for helpful feedback. This material is based on work supported by the National Science Foundation under Grant No. 1850208.  Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the National Science Foundation.  This project has also benefited from support to SB by Eric and Wendy Schmidt , by Samsung Research , by Intuit, Inc., and by NVIDIA Corporation .     \setlength{ \setlength{     
"," We evaluate whether BERT, a widely used neural network for sentence processing, acquires an inductive bias towards forming structural generalizations through pretraining on raw data. We conduct four experiments testing its preference for structural vs. linear generalizations in different structure-dependent phenomena. We find that BERT makes a structural generalization in 3 out of 4 empirical domains---subject-auxiliary inversion, reflexive binding, and verb tense detection in embedded clauses---but makes a linear generalization when tested on NPI licensing. We argue that these results are the strongest evidence so far from artificial learners supporting the proposition that a structural bias can be acquired from raw data. If this conclusion is correct, it is tentative evidence that some linguistic universals can be acquired by learners without innate biases. However, the precise implications for human language acquisition are unclear, as humans learn language from significantly less data than BERT. % learns in a setting vastly different from humans.  % the question remains open whether or not human could plausibly acquire our own structural bias as well, as BERT learns in a setting  % We cannot necessarily conclude that humans could plausibly acquire this bias as well, since BERT learns from far more data than human learners.  % Nonetheless, these results are still valuable in that they suggest that a structural bias may be learnable in principle from raw data alone. If this turns out to be correct, it is a refutation of a key assumption in  argument that structural bias is innate to humans.  Keywords: inductive bias; structure dependence; BERT; learnability of grammar; poverty of the stimulus; neural network; self-supervised learning",176
"   As the world became more digital, the amount of unstructured data available on the Internet has increased to the point where it has become unbearable to handle it using human resources. Natural language processing  tasks were developed to address it in an automatic way using computational resources. Some examples of these tasks are audio transcription, translation, assessment on text summaries, grading tests, and opinion mining.  For NLP tasks, a critical point is the computational text representation since there is no consensus on how to represent text properly using computational resources. The most classical text representation is bag-of-words. In this distributive representation, a vocabulary is collected in the training corpus and each sample\footnote{In this study, we use the word sample to denote instance or text document.} is represented by a vector where each element represents the occurrence or absence  of vocabulary terms in the document .  New text representation techniques have been studied due to known issues of bag-of-words representation: it loses word locality and fails to capture semantic and syntactic features of the words. To address these issues, other techniques were developed, such as the distributed text representation that learns fixed-length vector for each word, known as word embeddings. Using statistics of the context and the abundant occurrences of the words in the training corpus, learned word embeddings can capture the semantic similarity between words.   As each sample can have many words, a composition function is usually employed to encode all word embeddings into a single fixed-length representation per sample to satisfy the fixed-length input restriction on the most of the predictive models. Some representation techniques also encodes the position of a word in the sample, addressing the word locality issue.  The majority of predictive models for NLP tasks have their performance degraded when unknown words, which were not collected to build the vocabulary in the training phase or were discarded due to low frequency across the corpus, appear in the test. These words are called out-of-vocabulary  words and can degrade the performance of NLP applications due to the inefficiency of representation models to properly learn a representation for them.   In order to emphasize how an OOV word can hinder sentence comprehension, consider the following example originally written in ``The Jabberwocky'' by Lewis Caroll: ``He went galumphing back''. The nonce word ``galumphing'' was coined to mean ``moving in a clumsy, ponderous, or noisy manner; inelegant''. Since this word is an OOV, traditional models are not capable to handle it properly, ignoring it. The lack of representation for this word can restrict the  predictive model capabilities to understand this sentence.  Handling OOV words in distributed representation models can be achieved with simple strategies. For instance, as OOV words have no word embedding representation learned, they can be ignored when the composition function is applied. This approach leads the predictive model to fit data without the knowledge of the absence of a word because it is unknown to the representation model. For such case, a random vector can be adopted for each OOV word or a single random vector can be adopted for all OOV words.   These simple strategies provide little or no information about unknown words to predictive models in downstream tasks. In order to enable a predictive model to use a vector representation for the unknown words, those words need to be replaced by a meaningful in-vocabulary word. For this specific task, most of the techniques available in literature fits in two groups: language models  and robust techniques capable of learning meaningful representation for OOV words using their structures or the context in which they appear.  In these two groups, there are several deep learning  models.  Some of them were developed to handle OOV, such as Comick and HiCE, while evidence was found that pure neural architectures can also perform it, such as LSTM and Transformer. Some language models also had success in this task, such as RoBERTa, DistillBERT, and Electra.  Although several studies have shown that DL can be successfully applied in several NLP tasks, such as sentiment analysis, named entity recognition , and part-of-speech  tagging, there are few DL models for handling OOV words and no consensus on which approach is the best. To fill that gap, in this paper, we present a performance evaluation of state-of-the-art DL models  considering different datasets and tasks that can be greatly affected by OOV words.        The phenomenon of OOVs is a major problem in natural language processing tasks. Documents that have OOVs are usually incompletely represented by distributed text representation models. The lack of one or more words can significantly change the semantics of a sentence.  Distributed text representation models are not incremental and, therefore, the training process is performed only once due to the high computational cost demanded. As the model generated in the training is not updated over time, it is unable to deal with new words that were not seen during its training. Therefore, the more dynamic the communication becomes, the more OOVs appear, and the faster the model becomes obsolete.  In this paper, we presented a comprehensive performance evaluation of different DL models applied to handle OOVs. Among the evaluated models, DistilBERT, GPT2, Electra, LSTM, Transformer, and Roberta infer the embedding for a given OOV using approximation by the terms that appear next to the OOV in the sentence. Comick and HiCE, in addition to the context, use the morphological structure of the OOV for the inference.   To analyze these models, we performed an intrinsic evaluation using the benchmark Chimera dataset. We also performed an extrinsic evaluation with the text categorization task using nine public and well-known datasets for opinion polarity detection on Twitter messages, and with the tasks of NER and POS tagging, using three datasets frequently used in related studies.  There was no model that obtained the best performance in all evaluations. However, in general, Comick obtained a good performance in all extrinsic evaluation tasks, which resulted in higher average ranking than most other evaluated DL models. The ability of this model to analyze the morphological structure of OOVs, in addition to their context, may have contributed to achieve a superior performance, although Hice did not achieve the same success even having the same characteristics.  Considering each experiment more specifically, in the intrinsic evaluation, DistilBERT obtained the best performance, with a significant difference to other methods. In the text categorization task, in general, Comick was the best method to infer embeddings for OOVs. Finally, in NER and POS tagging, the best performance was obtained by one of the baseline techniques: FastText.   Based on the results, we can conclude that the task of inferring embeddings for OOVs generates different challenges for different evaluated scenarios. Therefore, we recommend that research on OOV handing techniques be addressed to specific tasks, increasing the chance of success.   We also noticed that for some scenarios with noisy texts  or sentences full of technical terms , the context of OOVs and their morphological structure may not be enough to infer a good embedding. Therefore, we recommend using an architecture for OOV handling that combines the techniques analyzed in this study with simpler techniques based on spell checker and semantic dictionaries.        ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.      {  \linespread{0.86}    }   
","  Communication has become increasingly dynamic with the popularization of social networks and applications that allow people to express themselves and communicate instantly. In this scenario, distributed representation models have their quality impacted by new words that appear frequently or that are derived from spelling errors. These words that are unknown by the models, known as out-of-vocabulary  words, need to be properly handled to not degrade the quality of the natural language processing  applications, which depend on the appropriate vector representation of the texts. To better understand this problem and finding the best techniques to handle OOV words, in this study, we present a comprehensive performance evaluation of deep learning models for representing OOV words. We performed an intrinsic evaluation using a benchmark dataset and an extrinsic evaluation using different NLP tasks: text categorization, named entity recognition, and part-of-speech tagging.   Although the results indicated that the best technique for handling OOV words is different for each task, Comick, a deep learning method that infers the embedding based on the context and the morphological structure of the OOV word, obtained promising results.",177
"  Adversarial learning is a major threat to the field of computer security research. With the advancement in technology, the growing dependency on the Internet has exposed users to serious cyber threats like phishing and pharming. Despite considerable research to counter such threats, staggering numbers of individuals and organizations fall prey to targeted social engineering attacks incurring huge financial losses.   Although attackers change their strategies, previous research has shown that electronic mails  are a popular attack vector. Emails can be embedded with a variety of malign elements like poisoned URLs to malicious websites, malware attachments as well as executables, documents, image files, etc.  Anti-Phishing Working Group  reports over 270,500\footnote{http://docs.apwg.org/reports/apwg\_trends\_report\_q3\_2018.pdf} unique phishing email campaigns received in the  quarter of 2018, rising from a total of around 233,600\footnote{http://docs.apwg.org/reports/apwg\_trends\_report\_q4\_2017.pdf} unique reports identified in the  quarter of 2017. Phishing reports also reveal the consistent rise in phishing attacks targeted towards financial institutions like payment processing firms and the banking sector. The statistics demonstrate how the threat is worsening as attackers continue to devise more sophisticated  ways of scamming victims.  % Detection systems and algorithms are commonly trained on historical data and attack patterns.  Innovative and unseen attack vectors can trick pre-trained classification techniques, thus placing the victim at risk. In email masquerading attacks, attackers after compromising the email account of an individual can carefully construct a fraudulent email then sent to the contacts known to the compromised individual. This has serious implications, because the attacker has gained uninterrupted access to the inbox, outbox and other private details of the compromised person. Thus by exercising caution, the attacker can emulate the content and context of the emails written by the individual and can communicate with his contacts as a legitimate entity, successfully evading detection and causing harm to the victim.  However, construction of the perfect deceptive email requires fine-tuning and manual supervision. While a fake mail constructed manually by an attacker can guarantee a higher chance of success, the process is both time and labor intensive. In contrast, an automated text generator can be trained to synthesize targeted emails much faster and in bulk, thereby increasing the odds of a successful attack. However, the bottleneck in this case, lies in whether the system can generate high quality text, free from common flags like misspellings, incorrect and abusive language, over-usage of action verbs, etc., which can be picked up by a classifier easily. Thus, proactive research in this area of deception based attacks using email masquerading techniques requires further sophisticated experimentation.     Advances in the field of natural language processing have introduced newer and  sophisticated algorithms which enable a machine to learn and generate high-quality textual content on a given context. Grammar based tools like the Dada Engine, N-gram language models as well as deep neural learners  have been used to study and replicate natural language based attacks. The aim is to facilitate proactive research by predicting newer attacks and reinforce against such unseen yet impending threats.  % The system can then be made to generate text that closely resembles the input structure and form.      At the hands of an attacker, language generation techniques can become dangerous tools for deception. With access to proper training data, deep learning neural networks are capable of generating textual content. This property has been leveraged by researchers for generating tweets and poetry,, etc. While limited, proactive research has been pursued by using deep learners for generation of fake reviews, grammar based techniques as well as simplistic deep networks have been leveraged for email generation. Thus, we can assume that it is not long before phishers and even spammers resort to such techniques to generate newer kinds of malicious attack vectors.   Following a proactive mode of study, we identify the underlying implications of how an automated machine learning technique, here, deep learners can be leveraged to synthesize email bodies for the purpose of email masquerading attacks. %\textcolor{magenta}{ Along with demonstrating the systems' performance using qualitative and quantitative methods, we study the effectiveness and practicality of such systems by comparing a hierarchical deep network with a baseline word prediction model. Our key contributions are as follows: %} % In this research, we aim at drawing attention to the gravity of this situation before people and organizations start falling vulnerable to targeted phishing scams. In this paper, we address the new class of Email masquerading attacks based on automated fake Email generation.   %\textcolor{red}{Add section numbers.}   { . While generation of coherent emails is challenging, we use a hierarchical network that consists of two stages - an architecture which uses a word prediction model to generate probable candidate sentences which are then passed onto a sentence selection model, based on distributed vector representations of the email content, to select the best possible set of sentences. Such a two-staged architecture should be suitable for generating longer content while maintaining coherency.  , coherency, fluency and legitimacy of the fake emails by conducting a human evaluation.      We revisit two major error trends observed in the evaluation of our word and character based generation models. First, repetitions of tags and words in the generated text body. A sample sentence generated by the word-based language model - ``The corres      Also  ,  I we can operating a gift to ensure, are that extent will is a links are not "" - demonstrates such behavior. While, we hypothesized such a behavior at larger text lengths, the brittleness in a model which uses characters or words as units for text generation can be observed for shorter text sequence generation as well. We believe, that the nature of the input on which the system is being modeled and the temperature  parameter used for sample generation play an important role in this behavior of the predictive model.      While the RNN model generated text with `some' malicious intent in them - the examples shown above are just a few steps from being coherent and congruous. We designed an RNN based text generation system for generating targeted attack emails which is a challenging task in itself and a novel approach to the best of our knowledge. The examples generated however suffer from random strings and grammatical errors. We identify a few areas of improvement for the deep generation system - reduction of repetitive content as well as inclusion of more legitimate and phishing examples for analysis and model training. We would also like to experiment with addition of topics and tags like `bank account', `paypal', `password renewal', etc. which may help generate more specific emails. It would be interesting to see how a generative RNN handles topic based email generation problem.   This research was supported in part by NSF grants CNS 1319212, DGE 1433817, DUE 1241772, and DUE 1356705. The study is also based upon work supported in part by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-16-1-0422.            Other text.    And more.        More text.        IF you use not English letters   瀛濇瑜嬭.            	              Features. Special symbols: 璋, 甯.    
"," Advanced machine learning and natural language techniques enable attackers to launch sophisticated and targeted social engineering based attacks. To counter the active attacker issue, researchers have since resorted to proactive methods of detection. Email masquerading using targeted emails to fool the victim is an advanced attack method. However automatic text generation requires controlling the context and coherency of the generated content, which has been identified as an increasingly difficult problem. %\textcolor{magenta}{ The method used leverages %}  a hierarchical deep neural model which uses a learned representation of the sentences in the input document to generate structured written emails. We demonstrate the generation of short and targeted text messages using the deep  model. The global coherency of the synthesized text is evaluated using a qualitative study as well as multiple quantitative measures.",178
" Geoparsing is the process of recognizing and geo-locating  location mentions from   texts. It has been widely applied to various textual data, and is an important task in geographic information retrieval . A geoparsing system, known as a geoparser, usually functions in two steps: toponym recognition and toponym resolution. Toponym recognition detects the place mentions in texts, while toponym resolution resolves any place name ambiguity and assigns the appropriate spatial footprint . Many geoparsers have been developed, such as CLAVIN,  the Edinburgh Geoparser , GeoTxt , and TopoCluster .   In June 2019, an important geoparsing competition, Toponym Resolution in Scientific Papers, was held as the SemEval 2019 Task 12, in conjunction with the Annual Conference of the North American Chapter of the Association for Computational Linguistics. This competition attracted 29 registered teams and 8 teams eventually submitted a system run . The winning teams all leveraged state-of-the-art neural network based models, such as BiLSTM-CRF and deep contextualized word embeddings, to design their geoparsers. Particularly, the geoparser that won the first place, DM\_NLP , achieved over 90\% precision, recall, and F1 score for toponym recognition. This result is exciting and brings the question ``are we there yet?"" A 90\% performance is not perfect but is probably sufficient for many applications. So have we already made enough progress that we can  consider the problem of geoparsing as solved?  A major limitation of the SemEval 2019 Task 12 competition is that the submitted geoparsers were tested on a single dataset which has 45 research articles from one particular domain of Bio-medicine. Existing research has shown that the same geoparser can have very different performances when  tested on different datasets  . Accordingly,  answering the question of whether  the problem of geoparsing can be considered as solved requires a systematic evaluation of the state-of-the-art geoparsers on multiple datasets which should ideally be in different text genres .  In a recent work, we developed an online platform called EUPEG which is an Extensible and Unified Platform for Evaluating Geoparsers . EUPEG hosts a majority of the geopasing resources reported in the literature, including eight annotated datasets, nine geoparsers, and eight evaluation metrics. In addition, the eight annotated datasets are in four different text genres which are news articles, Wikipedia articles, social media posts, and texts on Web pages. The source code of EUPEG and the related geoparsing resources are shared on GitHub.  In this paper, we systematically evaluate the top geoparsers from  SemEval Task 12  using EUPEG as a benchmarking platform. We focus on the top three end-to-end geoparsers that showed the highest performances in the competition, which are DM\_NLP  , UniMelb  , and UArizona . We test the performances of these three geoparsers on the datasets hosted on EUPEG, and compare their performances with the other existing geoparsers. The contributions of this paper are as follows:  	 to support future research.           and discussion. For example, we may want to make our toponym recognition models  not overly rely on letter cases and  better accommodate  grammatical errors. This is important when a geoparser needs to be applied to texts informally created by Web users, such as  social media posts and online reviews.   Geoparsing is an important research problem. This  paper presents our work on evaluating the three state-of-the-art geoparsers coming out from the SemEval-2019 Task 12 competition in June 2019. This work is motivated by the outstanding performances of these geoparsers in the competition. As a result, we set out to examine whether we have made enough progress to possibly consider the problem of geoparsing as solved. We systematically tested the top three geoparsers on our benchmarking platform EUPEG. The results suggest that these new geoparsers indeed improve the highest possible scores on multiple datasets, and the problem of geoparsing well-formatted texts referring to prominent place instances could be considered as solved. Meanwhile, some challenges remain, such as  geoparsing toponyms from informally-written texts with ambiguous place names.  This work can be extended in several directions. As discussed previously, we used a simple population heuristic for the toponym resolution component of the three geoparsers. Therefore, a next step is to develop a general toponym resolution dataset and use it to train the machine learning models described in the papers of DM\_NLP and UniMelb. Second, EUPEG currently does not contain historical corpora. As a result, it cannot be used for testing the performances of geoparsers on historical texts for humanities applications. An extension of EUPEG with historical corpora  can make this platform even more useful for researchers in digital humanities. A similar idea can be applied to extending EUPEG with non-English corpora.  Third, EUPEG currently evaluates only end-to-end geoparsers, and it could be useful to extend EUPEG with the capability of evaluating software tools designed for toponym recognition or resolution only. We have shared the source code of EUPEG, along with the datasets under open licenses, on GitHub at: . The source code of the three implemented neural network geoparsers tested in this work is also shared on GitHub at: .  We hope that these resources can help support the future work of the community to further advance geoparsing.      - End-to-end  or modularized systems?    -    It is worth mentioning that the granularity of toponym varies across different corpora. The majority of toponyms in the eight corpora belong to the administrative unit like country, city and town. Besides, some corpora contains annotated toponyms in other types. For example, GeoWebNews and GeoCorpora also annotate the natural features  and facilities  level toponyms.        
"," Geoparsing is an important task in geographic information retrieval. A geoparsing system, known as a geoparser, takes some texts as the input and outputs the recognized place mentions and their location coordinates. In June 2019, a geoparsing competition, Toponym Resolution in Scientific Papers, was held as one of the SemEval 2019 tasks. The winning teams developed neural network based geoparsers that achieved outstanding performances . This exciting result brings the question ``are we there yet?'', namely have we achieved high enough performances  to possibly consider the problem of geoparsing as solved? One limitation of this competition is that the developed geoparsers were tested on only one dataset which has 45 research articles collected from the particular domain of Bio-medicine. It is known that the same geoparser can have very different performances  on different  datasets. Thus, this work performs a systematic evaluation of these state-of-the-art geoparsers using our recently developed benchmarking platform EUPEG that has eight annotated datasets, nine baseline geoparsers, and eight performance metrics. The evaluation result suggests that these new geoparsers indeed improve the performances of geoparsing on multiple datasets although some challenges remain.",179
"  The process of digital transformation in medicine has been going for a while, providing faster and better treatment results in many cases through the use of modern computer science and Artificial Intelligence  methods. Digitization and subsequent analysis of medical records constitutes one such area of digital transformation that aims to collect broad types of medical information about a patient in the form of EHR, including digital measurements , verbal descriptions , images  and document the treatment process of a patient.  In this paper, we focus on the analysis of EHR with the purpose of providing clinical decision support by predicting most probable diagnoses during a patient's visit to a doctor. This problem is complicated by abundance of large volumes of structured and unstructured medical information stored across multiple systems in different data formats that are often incompatible across these systems. Although there exists an emerging FHIR standard  for the EHR data the goal of which is to unify the process of storing and exchanging medical information, unfortunately, very few existing Hospital Information Systems  support it. All this complicates the task of diagnosis prediction based on the EHRs since many of them contain extensive amounts of unstructured, poorly organized and ""dirty"" data that is less amenable to the analysis using the AI-based methods, unless this data is cleaned and preprocessed appropriately.  Providing clinical decision support in diagnosis prediction during a patient's visit to a doctor is important because many patient's visits, in fact up to 30\% in the US, are misdiagnosed. This is also true in some other countries. We formulate the aforementioned clinical decision support problem as a multi-label text classification of clinical notes  during a patient visit, where the classification is performed for a wide range of diagnosis codes represented by the International Statistical Classification of Diseases .  In this paper, we make the following contributions. First, we propose a novel BERT-based model for classification of textual clinical notes, called , that differs from the previously proposed models by the way of the FC-layer composition that is described in Section . Second, we compare the performance of our method with various baselines across different text representation techniques and classification models. Third, we compare the performance of the BERT model pretrained on a large corpus of out-of-domain data with the BERT model pretrained exclusively on in-domain data and using an in-domain tokenizer. Finally, we demonstrate the advantage of the proposed models and their comparable results with a human baseline in Section.  It is important to note that the clinical decision support system described in the paper will  serve as a doctor's replacement but, rather, constitutes an unbiased intelligent diagnosis generator and, therefore, should only  the doctors in their diagnostic decisions.    In this paper we described the challenging and important problem of diagnoses prediction from unstructured real-world clinical text data based on a very large Russian EHR dataset containing about 4 million doctor's visits of over 1 million patients. To provide the diagnosis, we proposed a novel BERT-based model for classification of textual clinical notes, called RuPool-BERT, that differs from the others BERT-based approaches by introducing a novel way of the FC-layer composition. Our experiments of applying the developed prediction model to the practical task of classifying 265 diseases showed the advantage of this model compared to the fine-tuned RuBERT base analog and other text representation models. We also showed that using a BERT model with a vocabulary and pretraining dataset tailored to the medical texts representation  improves performance on the classification task, specially on less frequent diseases. This improvement is achieved at a small fraction of pretraining time compared to the general Russian language model .  Comparison of our model with a panel of medical experts showed that the results of our model were similar to the results of experts in terms of the Hit@3 performance measure. Furthermore, we showed that the most reliable performance of our system is achieved on those samples having longer textual inputs, i.e. text sequences having at least 20 input tokens. All this allows us to conclude that our model and system has a strong potential to help doctors with disease diagnosis by providing the ""second opinions"" to them.  Our partners in the medical community identified one issue with the proposed method: they maintain that the proposed approach would benefit greatly from clear explanations of how our method arrived at each particular diagnoses. This is the topic of future research on which we plan to focus in the immediate future.      ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.            
"," In this paper we study the problem of predicting clinical diagnoses from textual Electronic Health Records  data. We show the importance of this problem in medical community and present comprehensive historical review of the problem and proposed methods. As the main scientific contributions we present a modification of Bidirectional Encoder Representations from Transformers  model for sequence classification that implements a novel way of Fully-Connected  layer composition and a BERT model pretrained only on domain data. To empirically validate our model, we use a large-scale Russian EHR dataset consisting of about 4 million unique patient visits. This is the largest such study for the Russian language and one of the largest globally. We performed a number of comparative experiments with other text representation models on the task of multiclass classification for 265 disease subset of ICD-10. The experiments demonstrate improved performance of our models compared to other baselines, including a fine-tuned Russian BERT  variant. We also show comparable performance of our model with a panel of experienced medical experts. This allows us to hope that implementation of this system will reduce misdiagnosis.",180
" Measuring semantic similarity between sentences has been one of the major problems towards text understanding. Many tasks including paraphrase identification, text entailment recognition, etc. also utilize sentence similarity. Clearly, it has attracted a lot of attention in the NLP research community. Semantic textual similarity  dataset from SemEval 2012 has been one of the commonly used benchmark for sentence similarity task, which attempts at measuring the degree of semantic equivalence between two sentences. While recently proposed deep learning methods built on pretrained language models have shown great success for the task , interpretability and explainability of the final scores remains a concern in general.  proposed to formalize interpretable semantic textual similarity  as an alignment between pairs of segments across the two sentences at SemEval 2016.    We propose a novel pointer network for the task of interpretable sentence similarity along with logic constraints based on ConceptNet and syntactic knowledge. Experiments over benchmark datasets show a large performance improvement for the alignment task, even in the cross-domain setting, proving the general applicability of the proposed approach.    These are the instructions for authors for IJCAI-20.  \documentclass{article} \pdfpagewidth=8.5in \pdfpageheight=11in   The file ijcai20.sty is NOT the same than previous years' \usepackage{ijcai20}    Use the postscript times font! \usepackage{times} \renewcommand*\ttdefault{txtt} \usepackage{soul} \usepackage{url} \usepackage[hidelinks]{hyperref} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{booktabs} \usepackage{graphicx} \usepackage{amsfonts} \usepackage{adjustbox} \usepackage{enumitem} \usepackage{romannum}  \usepackage{amsmath}   Used for displaying a sample figure. If possible, figure files should   be included in EPS format.   \usepackage{hyperref} \usepackage[nomargin,inline,marginclue,draft]{fixme} \urlstyle{same}  \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}   the following package is optional:  \usepackage{latexsym}     Following comment is from ijcai97-submit.tex:   The preparation of these files was supported by Schlumberger Palo Alto   Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.   Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.   Patel-Schneider, of AT\&T Bell Laboratories collaborated on their   preparation.    These instructions can be modified and used in other conferences as long   as credit to the authors and supporting agencies is retained, this notice   is not changed, and further modification or reuse is not restricted.   Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as   contacts for providing assistance without their prior permission.    To use for other conferences, change references to files and the   conference appropriate and use other authors, contacts, publishers, and   organizations.   Also change the deadline and address for returning papers and the length and   page charge instructions.   Put where the files are available in the appropriate places.  \title{Logic Constrained Pointer Networks for Interpretable Textual Similarity}     \author{   Anonymous Author   }  \author{ Subhadeep Maji\thanks{Equal contribution.} \footnote{Now at Amazon.}\and Rohan Kumar\textsuperscript{*}\and Manish Bansal\and Kalyani Roy\And Pawan Goyal\\ \affiliations Flipkart\\ Indian Institute of Technology, Kharagpur\\ @flipkart.com,\\ kroy@iitkgp.ac.in, pawang@cse.iitkgp.ac.in }             
"," % Systematically discovering semantic relationships in text is an important and extensively studied area in NLP, with various tasks such as entailment, semantic similarity, etc. Decomposability of sentence level scores via chunk alignments has been proposed as a way to make such models reliable . We study the problem of aligning components of sentences leading to an interpretable model for measuring semantic textual similarity. In this paper, we present a novel logic-constrained gated pointer network model to align constituents of two sentences, aiding in interpretation of semantic relationships. \fxnote{Few lines needed about the approach.} We achieve state of the art results with an F1 score of 0.97 showing significant improvements over existing solutions.  Systematically discovering semantic relationships in text is an important and extensively studied area in Natural Language Processing, with various tasks such as entailment, semantic similarity, etc. Decomposability of sentence-level scores via subsequence alignments has been proposed as a way to make models more interpretable. We study the problem of aligning components of sentences leading to an interpretable model for semantic textual similarity. In this paper, we introduce a novel pointer network based model with a sentinel gating function to align constituent chunks, which are represented using BERT. We improve this base model with a loss function to equally penalize misalignments in both sentences, ensuring the alignments are bidirectional. Finally, to guide the network with structured external knowledge, we introduce first-order logic constraints based on ConceptNet and syntactic knowledge. The model achieves an F1 score of 97.73 and 96.32 on the benchmark SemEval datasets for the chunk alignment task, showing large improvements over the existing solutions. Source code is available at \url{https://github.com/manishb89/interpretable_sentence_similarity}",181
"  Neural Machine Translation  has achieved unprecedented successes and drawn much attention from both academia and industry. Following the sequence-to-sequence learning paradigm, NMT approaches usually consist of two parts -- the encoder and the decoder, where the encoder maps the source side sentence into a sequence of hidden representations, and the decoder generates the target side tokens step by step based on the encoder outputs.  %Various techniques, such as recurrent networks , convolution networks , and more recently, the self-attention transformers , have been applied to the NMT tasks and delivered promising results.  Despite its success, the commonly used encoder-decoder framework in NMT always suffers from over- and under- translation problems . The decoder may tend to repeatedly focus on same parts of the source sentence while ignoring the other parts. Many efforts  have been made to mitigate this issue  by either explicitly or implicitly modeling the step-by-step translated and un-translated information during the decoding process.  One promising direction is to track the translated  and un-translated  components of the source sentence  at each decoding step. The components are modeled by RNN or Capsule Network with heuristic objectives .   % Figure   illustrates how past and future works.         % However,    In this paper, we argue that the heuristic objectives in previous approaches may be indirect and insufficient in certain circumstances, which limits their effectiveness.  The  and  modules have two major functionalities, which are the identification of past and future contents and extracting useful features for further predictions.  However, prior studies mix these two functionalities up and try to model them jointly by only fitting the outputs of  /  module. Here, we propose a novel dual learning method to enhance both two functionalities with two transformer models  trained simultaneously .  On the one hand, we propose to use backward NMT encoder with the partially inputs to provide contextually-rich supervision for the past / future identification instead of a coarse-grained bag-of-word loss.  On the other hand, we exploit a Guided Capsule Network  on two encoders to align the capability of feature extraction with manually masking, instead of mixing up both functionalities. With the training proceeds, bidirectional models perform as teachers for each other and strengthen the performance iteratively.  We evaluate our approach on two commonly used translation datasets, i.e., the NIST Chinese-to-English task and the WMT 2014 English-to-German task.  The experimental results demonstrate that our method significantly outperforms the previous strong baselines in terms of the translation quality of generated NMT translations. Also, among the subjective evaluation, our method surpasses previous adequacy-oriented methods in mitigating both over- and under-translation problem.     % %   Sequence-to-sequence based neural machine translation models always suffer from the under- and over-translation problem.  In this paper, we present a novel dual learning framework, aiming at modeling the translation adequacy.  By leveraging the power of both source-to-target and target-to-source model, our proposed method provides a more direct and contextual-rich supervision signal for the translated and un-translated words. The experimental results demonstrate that our method outperforms the previous adequacy-based methods and achieves significant improvement in mitigating over- and under- translation problem.    As future work, we will apply our methodology to non-autoregressive machine translation.     As future work, we plan to apply our method to unsupervised MT. The underlying assumption in unsupervised MT is that the encoding space among different languages lie in the same feature space, which also comply with our intuition for aligning capsule outputs. By introducing subsequence-level alignment between two languages, we believe it may further boost the performance of unsupervised MT.    
"," % Though Neural Machine Translation  has been successfully adopted in many areas,  Though remarkable successes have been achieved by Neural Machine Translation  in recent years,  it still suffers from the inadequate-translation problem.  Previous studies  show that explicitly modeling the translated  and un-translated  contents of the source sentence is beneficial for translation performance. However, it is not clear whether the commonly used heuristic objective is good enough to guide the  and . In this paper, we present a novel dual learning framework that leverages both source-to-target and target-to-source NMT models to provide a more direct and accurate supervision signal for the  and  modules. Experimental results demonstrate that our proposed method significantly improves the adequacy of NMT predictions and surpasses previous methods in two well-studied translation tasks.",182
" Document-level Sentiment Analysis , a subtask of Sentiment Analysis , aims to understand user attitudes and identify sentiment polarity expressed at document-level. This task has grown鑱絫o鑱絙e鑱給ne鑱給f鑱絫he鑱絤ost鑱絘ctive鑱絩esearch鑱絘reas鑱絠n鑱絥atural鑱絣anguage鑱絧rocessing and plays an important role in many real-world applications such as intent identification , recommender systems and misinformation detection.  Generally, DSA can be regarded as a text classification problem and thus traditional text classification approaches  can be adopted naturally. Different from other subtasks in SA,  DSA is more challenging due to the large size of words, vague semantic links between sentences and abundance of sentiments. Hence, the research question that how to  learn an expressive document representation to understand long documents for sentiment classification has been given a growing interest to researchers.   Inspired by the document structure,  one of the earliest and most influential models HAN was proposed by   to encode the entire document, which suffered from  attending explicit but irrelevant sentimental words. Subsequent works mainly dedicated to  introducing latent topics  or global context  to tackle this limitation. However, most of the user-generated documents are very comprehensive and contain a wealth of sentiments, which makes it difficult to directly learn from the whole document without the understanding of the major points, especially in long documents.  The above works attempted to explore the major points of the document by learning a global embedding from the document. Intuitively, the user-generated summary contains more accurate information about the major points of the document. These summaries describe the long document in a more specific way, which are highly indicative of the key sentiment and subject, and can facilitate further to identify important  text present and sentiments.    To reduce the processing of the substantial text in the document and  be well aware of the major idea of it, summary-based methods introducing the user-generated summary has been developed for DSA and achieved promising results, which brings brilliant processing for understanding complex documents. They refined the document with an abstractive summary to predict the sentiment effectively. Recently, some effective works  concerned both the text summarization task and DSA, and jointly modeled them to boost from each other. Nevertheless, most of the joint models did not fully utilize user-generated summaries and ignored the interaction between summary and document, because they did not encode summaries explicitly during test time.    For example, the document of a product review in Amazon SNAP Review Dataset is ``...They just sent a new camera and it showed up without any warning or communication about the \underline{bad} one. \underline{Minimal} Customer Service...The 1st camera was \underline{promising} and worked so \underline{well} for about two weeks..."" and its length is . The corresponding summary is ``Quality is a reflection of Customer Service"".  The document contains complex sentiment expressed, such as { corresponding to positive or negative, respectively. The subject { and {. They are complementary. Therefore, the auxiliary of the summary is significant for subject mining and semantic understanding in DSA.     To tackle the aforementioned problems,  we investigate how to effectively focus on more accurate subject information for DSA.  In this paper, we present an end-to-end model, named {H}ierarchical {I}nteraction {N}etwork , to encode the bidirectional interactions between summary and document.  The method works by utilizing multiple granularities interactions between summary and document, accordingly to learn a subject-oriented document representation.  Specifically, the interactions at character-level and the contextual semantic features can be captured via BERT. Afterward, the segment-level interactions are encoded by gated interaction network and the context of the document is taken into account simultaneously. Finally, the document-level interactions are embedded via the attention mechanism to learn a more expressive document representation with the consideration of subject information for predicting sentiments.   Furthermore, because of the complex sentiment in the document, we attempt to learn the affective representation and alleviate the distraction from other sentiments.  We introduce the sentiment label information into the model in a feedback way. Most existing structures learn document representation via only feedforward networks and have no chance to modify the invalid features of the document. Some effective works in image classification  and named entity recognition  added feedback structure to re-weight of feature embeddings and obtained gratifying results.  Motivated by their works, we propose a {S}entiment-based {R}ethinking mechanism  and feedback the sentiment polarity label information.  This rethinking mechanism can refine the weights of document embeddings to learn a more discerning low-level representation with the guidance from the high-level sentiment features, and relieve negative effects of noisy data simultaneously.   We evaluate our proposed models on three public datasets from news to reviews. Through experiments versus a suite of state-of-the-art baselines, we demonstrate the effectiveness of interactions and the rethinking mechanism, and the model HIN-SR can significantly outperform than baseline systems.  The main contributions of this paper are summarized as follows.         .         The remainder of this paper is structured as follows. We review the related works in Section . Then we explain the details of our contributions in section . Section  describes the experiments and the results. Further analysis and discussion are shown in Section . In the end, the conclusions are drawn in Section .      In this section, we give some analyses about training episodes and document length on predicting sentiment labels, and discuss the effect of the summary with visualization.         The number of episodes is an important parameter in the sentiment-based rethinking mechanism .  We investigate its effects on three datasets in Fig..    From the results in Fig., we observe that as the number of training episodes increases, the performance grows significantly and then decline slightly on both accuracy and F1. And the best performance is when the number of training episodes is set to 1.  Superior performance when training episode is set to 1 than 0  demonstrates the effectiveness of gold sentiment label information.  Rethinking these high-level sentiment patterns guides the model to capture more discriminative features specifically for different classes. However,  drop performance when the number of training episodes increases reveals that,  an overload of sentiment label information would  limit the original feature expression of HIN  and result in slightly descending performance.  Besides, three datasets show different meliorations of performance when training episodes is set to 1 than 0, Especially, the model achieves improvements of 0.9\  on in News dataset where more vague semantic links in Chinese bring much noise samples. This reveals that the SR can release the negative impacts of noisy samples simultaneously.             We further investigate the performance of HIN-SR, HIN, and several baselines  when analyzing sentiment polarity with different document lengths. The results of different datasets are shown in  Fig..      As document length increases, the performance of all models clearly decreases  after a certain range.  The result reveals that document-level sentiment analysis is more challenging to  the longer document usually containing more vague semantic links and complicated sentiment information. In particular, the curve of accuracy presents a partial upward trend  over an extremely long length in the News datasets. And we speculate that more semantic information can partially alleviate  the above phenomenon.     From the figures, it is seen that HIN achieves a considerable improvement than other baselines on three datasets over almost any range of document length.  Compared with other baselines system, HIN takes  the multiple-granularity interactions between the summary and document into considerations. After modeling the interactions, HIN can adequately exploit the semantic  and subject information of the summary and thus learn subject-oriented document representation  for document-level sentiment analysis.  In addition, in Fig. and ,  the results of BERT are slightly higher than HIN at a few of length ranges. Through the analysis of the abnormal samples, we conclude  that this is probably due to the uneven distribution of samples.   Moreover, HIN-SR slightly outperforms than HIN over most length ranges.  This indicates that  the rethinking mechanism introducing sentiment label information can  adapt and refine the HIN with high-level sentiment features to boost the  performance. In addition, HIN-SR is not sensitive to document length and is adaptive in both short and long documents.   It illustrates the robustness of the model becomes better with the help of reweighting document representations. We contribute it to this rethinking mechanism can indeed alleviate the data noise via feedbacking the sentiment label information.      We visualize multi-granularity interactions encoding modules in Fig.. The review describes the disappointing experience of using a simulator, containing complex sentiment polarity expressed such as {. According to the summary, the major point of the review can conclude as {). Note that the contexts of these subject-related tokens contain pivotal sentimental information, such as { is neglected because  it is irrelevant to the subject {  In this work, we have investigated the task of document-level sentiment analysis.  We design a hierarchical interaction networks  model to learn a subject-oriented document representation for sentiment classification. The main idea of HIN is  to exploit bidirectional interactions between  the user-generated summary and document.  Furthermore, a sentiment-based rethinking mechanism  refines the weights of document features to learn a more sentiment-aware document representation and alleviate the negative impact of noisy data. Experimental Results on three widely public datasets have demonstrated that HIN-SR outperforms significantly and tackles long documents with the vague semantic links and abundant sentiments effectively. The proposed model is of great significance to DSA and related applications.  For future work, we will consider more interaction information,  and more detailed theoretical analysis of rethinking mechanism, to further improve the performance.      ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.              
","     \let\thefootnote\relax\footnotetext{*  Equal Contribution.}     \footnotetext{\Letter {} Corresponding author.}          Document-level Sentiment Analysis  is more challenging due to vague semantic links and complicate sentiment information.      Recent works have been devoted to leveraging text summarization and have achieved promising results.      However, these summarization-based methods did not take full advantage of the summary including ignoring the inherent interactions between the summary and document.     As a result, they limited the representation to express major points in the document, which is highly indicative of the key sentiment.       In this paper, we study how to effectively generate a discriminative representation with explicit subject patterns and sentiment contexts for DSA.      A Hierarchical Interaction Networks  is proposed to explore bidirectional interactions between the summary and document at multiple granularities and learn subject-oriented document representations for sentiment classification.      Furthermore, we design a Sentiment-based Rethinking mechanism  by refining the HIN with sentiment label information      to learn a more sentiment-aware document representation.      We extensively evaluate our proposed models on three public datasets. The experimental results consistently demonstrate the effectiveness of our proposed models and show that HIN-SR outperforms various state-of-the-art methods.",183
" Text generation refers to a wide range of tasks involving generating natural language, including but not limited to machine translation, sentence simplification, and text summarization. Recent success of neural-based text generation  relies heavily on a large parallel dataset for training, which  may not be available in real-world natural language processing  applications. In this work, we consider unsupervised text generation, where no parallel data is available. This setting is more challenging, and has significant potential in both scientific research  and industrial applications .  %Unsupervised text generation assumes no parallel data are available, but still aims at accomplishing certain tasks, such as machine translation, sentence simplification, and text summarization. Unsupervised text generation has significant potential in both scientific research  and industrial applications . Conventional sequence-to-sequence training cannot be directly employed in this setting, due to the lack of parallel data. % LM: the: not necessary  Early work tackles unsupervised text generation by rules or templates. While such approaches do not require parallel corpora, the generated sentences are highly subject to the rules, and hence lack the flexibility of natural language. Other work constructs pseudo-parallel data, which is only feasible for certain tasks like unsupervised machine translation.  Recently, researchers have developed search-based techniques for unsupervised text generation, where a heuristically defined scoring function evaluates the quality of a sentence, involving language fluency, semantic compliance, and other task-specific aspects. Then, the algorithm performs word-level edits  to search towards a  optimum of the scoring function. With a reasonably designed scoring function, such approaches are shown to be effective in a variety of applications like paraphrase generation, sentence summarization, and text simplification.  However, the search-based approach has two major drawbacks: 1) The inference efficiency is low. To obtain an output sentence, the search algorithm would perform a few hundred steps of local edits and re-evaluations. This could be considerably slower than an autoregressive decoder, which generates words sequentially. 2) The search could yield noisy results, since the scoring function is defined heuristically and the search is conducted locally in a discrete sentence space.% by stochastic word editing.  To this end, we propose a new framework for unsupervised text generation by learning from search , which contains a strong search module that explores the sentence space, as well as a learning module that learns from the search results.   For the search module, we adopt the simulated annealing  algorithm. At each step, SA proposes a local edit by a neural network, and then either accepts or rejects the proposal based on a heuristically defined scoring function.  For learning, we employ two methods to train the conditional generative model, word-level cross-entropy loss and the sequence-level max-margin loss. Within \model{}, the search and learning can be boosted by each other in an iterative fashion. That is, the search results serve as the pseudo-reference for training the conditional generator, which in turn benefits SA search by serving as a more meaningful initial state. As for implementation, \model{} involves two pretrained language models: a) the uni-directional GPT2, which is suitable for likelihood-based fluency evaluation and conditional generation; and b) the bi-directional RoBERTa, which is better at semantic evaluation and contextual word-level prediction.  %We first perform simulated annealing search and treat the obtained output sentences as pseudo-references. Then, we train an autoregressive GPT2 as the text generator by word-level cross-entropy  supervised learning, which enables our model to learn quickly. Further, the outputs of GPT2 are taken as the initial state of the search algorithm again for iterative performance improvement, and we perform max-margin  learning to better distinguish between higher-scored sentences and other high-probability sentences.  The main contributions of our paper include: 1) We propose \model, a generic learning-from-search framework for unsupervised text generation. 2) We demonstrate efficient methods of incorporating the large-scale pretrained language models into our \model{} framework. 3) We conducted experiments on two different tasks: paraphrasing and text formalization. In both experiments, \model\ significantly outperforms unsupervised baseline methods. Moreover, \model\ achieves comparable performance to recent supervised models in the paraphrasing task. 4) For text formalization , we are also the first to design a search-based  method, and further extend it into the proposed \model\ framework.     This work proposes \model{}, a novel framework of learning-from-search to unsupervised text generation. We show that the simulated annealing search can provide high-quality examples for the conditional text generator to learn from. Further, the improved generative model can give a better initial state to the search algorithm. Experiments demonstrate that the alternation of search and learning can boost the performance of \model{} on two unsupervised text generation tasks, paraphrase generation and text formalization. Moreover, our model is considerably more computationally efficient, compared with search-based generation methods. We note that \model{} opens a few future directions, such as more effective and efficient search algorithms, more noise-robust learning methods, and a better combination of search and learning. We would also like to apply the learning-from-search framework to other sequential prediction tasks in NLP.      In this paper, we proposed \model, a search-and-learning framework for unsupervised text generation. Compared with previous search-based methods, our \model\ not only generates considerably higher-quality sentences, but also is 6--10 faster in inference time.   
"," In this work, we present \model{}, a novel framework to unsupervised text generation by learning from search. We start by applying a strong search algorithm  towards a heuristically defined objective that  estimates the quality of sentences. Then, a conditional generative model learns from the search results, and meanwhile smooth out the noise of search. The alternation between search and learning can be repeated for performance bootstrapping.  We demonstrate the effectiveness of \model\ on two real-world natural language generation tasks, paraphrase generation and text formalization. Our model significantly outperforms unsupervised baseline methods in both tasks. Especially, it achieves comparable performance with the state-of-the-art supervised methods in paraphrase generation.",184
" % With the rise of multi-modal studies, multi-modal neural machine translation   has become an important research direction in machine translation. % With the rise of multi-modal studies,  Multi-modal neural machine translation   has become an important research direction in machine translation, due to its research significance in multi-modal deep learning and wide applications, such as translating multimedia news and web product information . % and attracted increasing attention recently. % Multi-modal research involving computer vision and natural language processing has recently received growing interest and empower many applications, such as multi-modal neural machine translation , visual question answering and image captioning.  It significantly extends the conventional text-based machine translation by taking images as additional inputs.  The assumption behind this is that the translation is expected to be more accurate compared to purely text-based translation, since the visual context helps to resolve ambiguous multi-sense words . % the visual information can be beneficial to ground the meaning of the text and, as a consequence, generate more adequate translations . % Due to its research significance in multi-modal deep learning, multi-modal NMT has attracted increasing attention recently. % In this setting, translation is expected to be more accurate compared to purely text-based translation, as the visual context could help resolve ambiguous multi-sense words.  % Examples of real-world applications of multi-modal translation include translating multimedia news, web product information, and movie subtitles. % Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications. % Due to its research significance in multi-modal deep learning and wide applications, such as translating multimedia news, web product information and movie subtitles , multi-modal NMT has attracted increasing attention recently.  % Intuitively, there exists semantic correlation of different granularities between input text and image. Therefore, how to efficiently fuse the multi-modal semantic information has become a crucial issue in Multi-modal NMT, which directly impacts translation performance.   '' and other textual ones for simplicity. }   Apparently, %how to efficiently incorporate visual features into Multi-modal NMT has become a crucial issue, which directly impacts translation performance. how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance. To this end, a lot of efforts have been made, roughly consisting of:  encoding each input image into a global feature vector,  which can be used to initialize different components of multi-modal NMT models, or as additional source tokens ,  or to learn the joint multi-modal representation ;  extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context ; and  representing each image as spatial features,  which can be exploited as extra context , or a supplement to source semantics  via an attention mechanism.  % In some previous studies, images are encoded into global features that is used as initialization and source tokens , or to build a joint multi-modal representation .Some work use object-based image features , inspired by other multi-modal tasks like image captioning .More common practice is to represent images as spatial features and employ attention mechanism  over them to incorporate visual context into decoders . % Furthremore, to exploit multi-modal semantic correlations,  propose an encoder-based image attention mechanism, which utilizes the representations of source words to extract visual context as the supplement of source semantics. % and modeling image as latent variables . % On the other hand, recent work has analysed that images are only needed in specific cases such as ambiguous source words , or just ignored by the models .  Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image pair. For example, as shown in Figure , the noun phrase ``"" semantically corresponds to the blue dashed region. % which is important for multi-modal representation learning. The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities,  and 2) how to achieve semantic interactions based on the unified representation. % For example, as shown in Figure , the noun phrase ``"" and ``"" semantically correspond to the yellow dashed region and blue dashed region in the image, respectively. However, we believe that such semantic correspondences can be exploited to refine multi-modal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions .   % However,  % studies  reveal that visual information seems to be ignored by the multi-modal NMT models.  % The underlying reason is the big challenges imposed % by the representations of images and % their integration into the model.  % %The underlying reason is limitations of the image representations and the way they are integrated into the model. % Concretely, previous approaches do not explicitly % %of integrating image features to the models % capture semantic correspondences between semantic units within a pair of an input sentence and an image.  % For example, as shown in Figure , the noun phrase ``"" and ``"" semantically correspond to the yellow dashed region and blue dashed region in the image, respectively. % %, which is hard to be modelled in previous work.  % We believe that such semantic correspondences between inter-modal semantic units can be exploited to refine multi-modal representation, since it provides fine-grained multi-modal semantic interactions.  % Despite their success, there are still some defects.  % Concretely, the contribution of visual information is unclear .  % More importantly, these work usually do not consider semantic correspondences between fine-grained semantic units within an input sentence and an image. % The neglect of these important clues may be due to the big challenges imposed by the representations of these semantic units and their integration into the encoder. % % multi-modal representation and the integration into the encoder. % However,  % we believe that such information can be exploited to refine the encoder,  % since it provides semantic bridges between inter-modal semantic units that are very important for multi-modal semantic interactions.   %fine-grained semantic relationships that are very important for multi-modal semantic interaction. % take into account cross-modal correspondences between semantic units in two modalities. % lack the modelling of fine-grained semantic relationships between semantic units in two modalities. % The attention based methods attend over all elements  % Even though some studies use more specific object-based features, they simply incorporate them via an additional attention model. % visual objects and textual phrases. % However, this approach gives little consideration to how the image regions that are subject to attention are determined. % In most previous studies, the input image is encoded as a uniform grid of equally sized neural receptive fields, which is irrespective of the content of the sentence.  % Obviously, some textual phrases have similar semantics to objects  % in the input image. % For example, in Figure 1, the phrase 'a toy car' has semantic association with the visual object . % in images such as the phrase '' and the visual object '' in the Figure 1.  % We believe such semantic relationships can help us learn better multi-modal semantic representations since they provide fine grained semantic constraints. % Intuitively, explicitly modeling this semantic correlation make the contribution of visual modality clearer, and improve complementarity between two modalities, % which is helpful to better translation. % it is helpful to bind visual objects  % For example, the visual object '' in the image has semantic correlation to the phrase ''. % Therefore, the semantic information of input image has not been fully utilized in Multi-modal NMT and limit the potential of further translation quality improvement.   In this paper,  % Unlike previous work, we represent the input text and image as a unified multi-modal graph, which captures semantic associations between intra- and inter-modal units. % We propose a multi-modal graph to capture fine-grained semantic relationships between semantic units in two modalities. we propose a novel graph-based multi-modal fusion encoder for NMT.  % To construct this encoder, We first represent the input sentence and image with a unified multi-modal graph. In this graph, each node indicates a semantic unit:  or , and two types of edges are introduced to model semantic relationships between semantic units within the same modality  and semantic correspondences between semantic units of different modalities  respectively. % To build this encoder, we first use a multi-modal graph to model various semantic relationships between intra- and inter-modal semantic units. % fine-grained semantic relationships between semantic units in two modalities. % and introduce graph attention network to effectively learn multi-modal node embeddings, % which provide attention-based multi-modal contexts for the decoder. % utilizing a external visual grounding tool to detect visual objects associated with textual phrases. % For example, in the multi-modal graph shown in Figure , in our proposed graph, each node indicates either a textual word or a visual object, and three types of edges, including textual edges, visual edges, and inter-modal edges, are introduced to model different kinds of semantic relationships between nodes. % which capture different kinds of semantic relationships between nodes.  % The last type of edges connect the nodes that have similar semantics but in different modalities. Based on the graph, we then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions among the nodes to conduct graph encoding. %to conduct graph encoding, where semantic transitions among graph nodes are iteratively performed to learn representation vectors of semantic units in two modalities. %According to the features of multi-modal graphs, we distinguish the parameters of different modalities and deploy gate mechanism to achieve fine-grained information fusion. Particularly, during this process, we distinguish the parameters of two modalities,  and sequentially conduct intra- and inter-modal fusions to learn multi-modal node representations. % we distinguish the parameters of two modalities and deploy operations with different gating mechanisms to achieve fine-grained multi-modal fusion. Finally, these representations can be exploited by the decoder via an attention mechanism. %At each graph layer, we first learn the unimodal contextual representation of each node, so that it can determine the degree of multi-modal fusion with its own context. % semantic representations of visual objects and the input sentence. % fine-grained semantic interactions between two modalities can be full exploited to learn better the semantic representations of visual objects and the input sentence.  %Finally, we employ attention mechanism to generate context vector for the decoder.  % which separately produce attention-based multi-modal contexts for Multi-modal NMT. % so that each node collect semantic information over others.  Compared with previous models, ours is able to fully exploit semantic interactions among multi-modal semantic units for NMT. %semantic transitions among multi-modal semantic units  % efficiently capture both fine-grained semantic relationships between intra-modal and inter-modal semantic units. % and pass messages containing contextual information between a pair of bipartite sub-graphs to iteratively refine the representations.  % The nodes in one modality can collect both intra-modal and inter-modal semantic information via the cross-modal semantic alignments. Overall,  the major contributions of our work are listed as follows:  {2pt} {2pt}      In this paper,  we have proposed a novel graph-based multi-modal fusion encoder,  which exploits various semantic relationships between multi-modal semantic units for NMT. Experiment results and analysis on the Multi30K dataset demonstrate the effectiveness of our model.  In the future,  we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs.  Besides,  how to introduce scene graphs into multi-modal NMT is a worthy problem to explore.  Finally,  we will apply our model into other multi-modal tasks such as multi-modal sentiment analysis.  and emotion recognition.    
"," Multi-modal neural machine translation  aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units . We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",185
"  This short example shows a contrived example on how to format the authors' information for IJCAI--PRICAI--20 Proceedings.    In this work, we proposed a novel task-level curriculum learning method to improve the accuracy of non-autoregressive neural machine translation. We first view autoregressive, semi-autoregressive and non-autoregressive translation as individual tasks with different , and propose a task-level curriculum mechanism to shift the training process from  to , where  is the length of the target sentence. Experiments on several benchmark translation datasets demonstrate the effectiveness of our method for NAT.  In the future, we will extend the task-level curriculum learning method to other sequence generation tasks such as non-autoregressive speech synthesis, automatic speech recognition and image captioning, where there exists smooth transformation between autoregressive  and non-autoregressive generation using semi-autoregressive generation as bridges. We expect task-level curriculum learning could become a general training paradigm for a broader range of tasks.  
", This short example shows a contrived example on how to format the authors' information for {.,186
" }  Pre-trained word embeddings, which map words to dense vectors of low dimensionality, have been the key enabler of the ongoing neural revolution, and today they serve as the basic building blocks of the vast majority of the contemporary Natural Language Processing  models. While initially introduced for English only , pre-trained embeddings quickly emerged for a number of other languages , and the idea of cross-language embedding spaces was born. In a cross-language embedding space, two semantically similar  words would be close to  each other regardless of whether they are from the same or from different languages. Using such a space is attractive, as for a number of NLP tasks, it enables the application of an NLP model trained for one language on test input from another language. Ideally, such spaces could be trained on parallel bilingual datasets, but such resources are of limited size, e.g.,~compared to the large-scale monolingual resources typically used to pre-train monolingual word embeddings.  Thus, it has been more attractive to train monolingual word embeddings for different languages independently, and then to try to align the corresponding embedding spaces in what is commonly known as bilingual lexicon induction. This has been attempted in a supervised~, in a semi-supervised~, and in an unsupervised setting~.  Initial attempts at aligning the spaces used a dictionary of word translation pairs as anchors between the two spaces to infer the nature of the transformation that relates the first language to the second one . This is a supervised setup, where the alignment is typically done according to an orthogonal transformation that minimizes the Frobenius norm in the Procrustes problem, which has a closed-form solution, easily obtainable via SVD, as we describe below.  For the translation of word embeddings,  is taken to be an orthogonal matrix due to a self-similarity argument . The convenience of using an orthogonal matrix has also been supported empirically . The orthogonal Procrustes problem has a closed-form solution , where  is the singular value decomposition  of  as shown by~.    Given two ordered clouds of points , , each with  points of dimension , the orthogonal Procrustes problem finds the orthogonal matrix  that minimizes the following Frobenius norm:    A popular unsupervised formulation of the problem is known as the Wasserstein-Procrustes , which is more challenging as it needs to optimize a generalization of the Procrustes objective. One-to-one maps are encouraged through a permutation matrix . The convenience of one-to-one maps is justified for different reasons. First, the hubness problem  occurs in high-dimensional vector spaces where certain vectors are the nearest neighbor to a disproportionate number of other vectors, thus reducing the quality of the embedding space . Second, one-to-one maps can be linked to Wasserstein distance and computational optimal transport.  Given two clouds of points , , each with  points of dimension , the Wasserstein-Procrustes problem finds an orthogonal matrix  and a permutation matrix  that minimize the Frobenius norm:  where  is the set of -dimensional permutation matrices and  is the set of -dimensional orthogonal matrices.  In practice, even though most existing approaches resort to some modification of this objective, they nevertheless yield good accuracy for synthetically generated dictionary induction tasks. Therefore, here we ask the following questions: Can we find approximate solutions to the Wasserstein-Procrustes objective as per Equation that not only minimize the objective, but also yield good accuracy on dictionary induction tasks? Can we take existing methods and improve them further by using refinements that optimize the objective in Equation? Can we find natural scenarios for which we find good solutions? We attempt to answer these questions after a thoughtful analysis of the different objective functions used in the literature, following the call from  for a more fair model comparison.      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROPERTIES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%     In conclusion, some methods achieve good results in the unsupervised translation of word embeddings without directly considering Problem in the loss function  . We have showed that there are a lot of approaches tackling Problem and its interesting alternative formulations.  We have underlined some mathematical properties of the Wasserstein-Procrustes problem and hence used the concept of the different natural initialization transformations in an iterative algorithm to achieve improved results for mapping word embeddings between different languages.   This method also has applications in word translation across different languages.  In particular, we have shown that it is possible to use our algorithm as a refinement tool and we have demonstrated improved results after using the transformation of  as the initialization matrix .  We hope that our rethinking of the Wasserstein-Procrustes problem would enable further research and would eventually help develop better algorithms for aligning word embeddings across languages, especially taking into account that most unsupervised approaches try to minimize loss functions different from Objective.   In future work, we plan to study other loss functions. We are further interested to see how well the objectives in Table correlate with CSLS. Finally, we plan combinations with other existing methods.   , which we leave for a follow-up discussion.   Finally, we plan to novel ways to optimize Wasserstein-Procrustes could yield further improvements, when used in conjunction with existing methods.    Furthermore, novel ways of optimizing the Wasserstein-Procrustes objective might be useful, some of which we consider in the .   the basis of which we propose in the Supplementary material as future work along these lines.   We think the scientific community could benefit from the detailed study of loss functions such as Wasserstein-Procrustes. We propose in the Supplementary material future work along these lines.    We have presented work in rethinking the Wasserstein-Procrustes problem formulation for the task of aligning word embeddings across languages. In particular, we have demonstrated how properties of problems equivalent to Wasserstein-Procrustes can help in the unsupervised setup. We further showed that, in the semi-supervised setup, using just a little supervision can yield good results, especially if the datasets are similar or in the same language.  We believe that our rethinking of the Wasserstein-Procrustes problem would enable further research and would eventually help develop better algorithms for aligning word embeddings across languages, especially if it is taken into account that most of the unsupervised approaches try to minimise loss functions different than Objective.  
"," The emergence of unsupervised word embeddings, pre-trained on very large monolingual text corpora, is at the core of the ongoing neural revolution in Natural Language Processing . Initially introduced for English, such pre-trained word embeddings quickly emerged for a number of other languages. Subsequently, there have been a number of attempts to align the embedding spaces across languages, which could enable a number of cross-language NLP applications. Performing the alignment using unsupervised cross-lingual learning  is especially attractive as it requires little data and often rivals supervised and semi-supervised approaches. Here, we analyze popular methods for UCL and we find that often their objectives are, intrinsically, versions of the Wasserstein-Procrustes problem. Hence, we devise an approach to solve Wasserstein-Procrustes in a direct way, which can be used to refine and to improve popular UCL methods such as iterative closest point , multilingual unsupervised and supervised embeddings  and supervised Procrustes methods. Our evaluation experiments on standard datasets show sizable improvements over these approaches. We believe that our rethinking of the Wasserstein-Procrustes problem could enable further research, thus helping to develop better algorithms for aligning word embeddings across languages. Our code and instructions to reproduce the experiments are available at \url{https://github.com/guillemram97/wp-hungarian}.",187
" Neural machine translation  have witnessed great progress due to the development of deep learning. The popular NMT models adopt an encoder-attention-decoder framework, where the decoder generates the target token based on previous tokens in an autoregressive manner. While its popularity, NMT models suffer from discrepancy between training and inference and the consequent error propagation. During inference, the decoder predicts the next token given previous generated tokens as input, which is discrepant from that in training, where the previous ground-truth tokens as used as input for next token prediction. Consequently, the previous predicted tokens may have errors, which would cause error propagation and affect the prediction of next tokens.  Previous works have tried different methods to solve the above issues, where some of them focus on simulating the data that occurs in inference for training, such as data as demonstration, scheduled sampling, sentence-level scheduled sampling, or even predict them in different directions. While being effective to handle the prediction errors occurred in inference during model training, these methods still leverage the predicted tokens that could be erroneous as the conditional information to predict the next token. Forcing the model to predict correct next token given incorrect previous tokens could be particularly hard and misleading for optimization, and cannot solve the training/inference discrepancy as well as error propagation effectively.   In this paper, moving beyond scheduled sampling, we propose a novel method to enable the model to correct the previous predicted tokens when predicting the next token. By this way, although the decoder may have prediction errors, the model can learn the capability to build correct representations layer by layer based on the error tokens as input, which is more precise for next token prediction than directly relying on previous erroneous tokens as used in scheduled sampling.   Specifically, we introduce two-stream self-attention, which is designed for language understanding in XLNet, into the NMT decoder to correct the errors while translation. Two-stream self-attention is originally proposed to solve the permutation language modeling, which consists of two self-attention mechanisms: the content stream is exactly the same as normal self-attention in Transformer decoder and is used to build the representations of the previous tokens, while the query stream uses the positional embedding as the inputs to decide the position of the next token to be predicted. In our work, we reinvent two-stream self-attention to support simultaneous correction and translation in NMT, where the content stream is used to correct the previous predicted tokens , and the query stream is used to simultaneously predict the next token with a normal left-to-right order based on the corrected context .   We conduct experiments on IWSLT 2014 German-English, Spanish-English, Hebrew-English and WMT 2014 English-German and English-Romanian translation datasets to evaluate the effectiveness of our proposed error correction mechanism for NMT. Experimental results demonstrate that our method achieves improvements over Transformer baseline on all tasks. Further experimental analyses also verify the effectiveness of error correction to improve the translation accuracy.  Our contributions can be summarized as follows:           In this paper, we incorporated a novel error correction mechanism into neural machine translation, which aims to solve the error propagation problem in sequence generation. Specifically, we introduce two-stream self-attention into neural machine translation, and further design a error correction mechanism based on two-stream self-attention, which is able to correct the previous predicted errors while generate the next token. Experimental results on three IWSLT tasks and two WMT tasks demonstrate our method outperforms previous methods including scheduled sampling, and alleviates the problem of error propagation effectively. In the future, we expect to apply our method on other sequence generation tasks, \eg, text summarization, unsupervised neural machine translation, and incorporate our error correction mechanism into other advanced structures.   
"," Neural machine translation  generates the next target token given as input the previous ground truth target tokens during training while the previous generated target tokens during inference, which causes discrepancy between training and inference as well as error propagation, and affects the translation accuracy. In this paper, we introduce an error correction mechanism into NMT, which corrects the error information in the previous generated tokens to better predict the next token. Specifically, we introduce two-stream self-attention from XLNet into NMT decoder, where the query stream is used to predict the next token, and meanwhile the content stream is used to correct the error information from the previous predicted tokens. We leverage scheduled sampling to simulate the prediction errors during training. Experiments on three IWSLT translation datasets and two WMT translation datasets demonstrate that our method achieves improvements over Transformer baseline and scheduled sampling. Further experimental analyses also verify the effectiveness of our proposed error correction mechanism to improve the translation quality.",188
"  % ---------------------------  Paraphrase identification is a core NLP task and has been widely studied . One interesting application area of paraphrase detection is Community Question Answering  .  The aim of CQA is to answer real open-ended questions based on user-generated content from question answering websites. Being able to identify similar --- already answered --- questions  can be helpful for this purpose. Question paraphrase detection in CQA is difficult because texts tend to be longer and have less direct overlap compared to traditional paraphrase detection datasets .  Early work on paraphrase detection relied on hand-crafted features, while state-of-the-art approaches for paraphrase identification are primarily neural networks  and hybrid techniques . Many recently proposed CQA paraphrase detection systems still use hand-crafted features  and some work has successfully integrated topic model features . This suggests that topic distributions could offer auxiliary information for identifying related questions and complement word embeddings , which provide the main signal in neural systems.  Contrary to hand-crafted static topic features, integrating topics in a neural framework brings the advantage of joint updates during training.  Recent work successfully introduced topics in neural architectures for language generation:  used a topic-enhanced encoder for summarisation,  integrated topics in the decoder for machine translation and  included topics in both encoder and decoder of their summarisation model.  However, it remains unclear if topics can be useful in a neural paraphrase detection model and how to best fuse topics with word embeddings for this task.  In this paper, we introduce a novel topic-aware neural architecture and specifically make the following contributions:    % ---------------------------      ---------------------------  In this work, we introduced a novel topic-aware neural architecture for question paraphrase identification. Our model successfully fuses word embeddings with topics and improves over previous neural baselines on multiple CQA paraphrase identification datasets. We demonstrated that topics contributed consistently to the performance of our model and that an early fusion of word embeddings with topic distributions is preferable over integration at a later stage.  Our work suggests that early fusion of topics with models which were pretrained with sentence pair classification tasks, such as BERT  could be a promising direction for future research. Other future work could seek to enhance our proposed architecture with more sophisticated topic models.     \onecolumn  
"," Question paraphrase identification is a key task in Community Question Answering  to determine if an incoming question has been previously asked. Many current models use word embeddings to identify duplicate questions, but the use of topic models in feature-engineered systems suggests that they can be helpful for this task, too. We therefore propose two ways of merging topics with word embeddings  in a new neural architecture for question paraphrase identification. Our results show that our system outperforms neural baselines on multiple CQA  datasets, while an ablation study highlights the importance of topics and especially  topic-embedding fusion in our architecture.",189
"  %RNNLM 闉涖儸鐖 姘 闊歌建顫 Recently, the Recurrent Neural Network Language Model  has gained its popularity in the field of Automatic Speech Recognition .  %G The 闈奉叆鐗, research闆 闆碱煆鍨 Various academic research has reported the effectiveness of RNNLMs, which can train unseen contexts by sharing the statistics between words  %G of which contexts -> whose contexts are / the contexts of which are with syntactically and semantically similar contexts. However, heavy computational load of RNNLM over traditional n-gram based approaches  %G keep it from ... it闉 闉氭﹤顕爟姗傚 姘氭棄鐨 闉愬棦娼.. %G an every area -> every area has been a hurdle in applying the RNNLM to diverse areas of ASR applications.  %G if -> when,  to be run -> to run..  %G 闉愵剣鐖犻爩 when/if 鐢戭剢顑撻灇 闉涘牕濮 鐡 闉涙劤娉婅嚙 闉忔尗鍎.. Especially, when ASR systems are required to run under real-time constraint ,  %G hardly be attainable... 闉佹粚鐗? the real-time decoder is hardly attainable with direct application of RNNLMs in place of traditional n-grams.  %G 姘嶈兂鐏 鏀靛嫴螠.. 闉氳導绉 闈炬﹥鐖.. In order to overcome such computational issues, most of the RNNLN systems adopt two-pass decoding strategy, which generates lattices or a set of n-best results based on n-gram in the first path,  %G which鑷 闉愵剣鐖犻爩 娆ゆ鏌... and then performs the rescoring on the hypotheses with RNNLMs.  %Limitation of previous attempts: 1-pass cache - done %G investigate闆 闊渻娆栧亾  Prior studies have investigated the possibility of implementing real-time decoder with RNNLM. %G which闆 RNNLM? studies? -> 鏀靛嫴螠 The study reduced computational complexity of the RNNLMs by caching the conditional probabilities of the words and  %G results of feed-forwardings? the results of RNN computation and reusing the cached data.  %G it闉 姝嗘梼顓︽櫩闆 鐡? However, even though the computational load was minimized   %G In manner of reducing? by introducing cache strategy and reducing redundant computations,  %G it闉 姝嗘梼顓︽櫩闆 姘氭棄鐨 闉愬棦娼.. %G away from -> far from %G achieving decoder? the 闈奉叆鐗.. the result was still far from achieving real-time performance with large vocabulary based RNNLM.  %The strength of GPGPU in ASR %G on the other hand... 闉忔尗鍎 %G the wide range of ASR... 闉愵叆瀚 闉忔尗鍎.. the 闈奉叆鐗 姘 area/field鎼 姘ょ摯.. Recent studies have applied the General Purpose Graphic Processing Units  in various fields of ASR. %G They闆 闆稿嫳鎯? One of the studies applied the GPGPU to training RNNLMs, and showed that the outstanding parallelization capability of GPGPU was suitable in minimizing the computational load of probability normalization processes.  %Main Problem %G possibility of RNNLM? In this paper, we investigate the possibility of implementing a GPGPU-based real-time Large Vocabulary Continuous Speech Recognition  that utilizes RNNLM.  %G Get remedy of computation load... what it requires... %In order to get remedy of computation load of it, we introduce the use of GPGPUs and what it requires under the many core framework. %G ability -> capability Even though GPGPUs have powerful parallelization capabilities, obstacles such as their insufficient memory size and slow data transfer speed between GPGPUs and CPUs  %G RNN-based real-time decoder闉 闉婂崐濮呯摯... 闋... discourage the use of GPGPUs among RNNLM-based real-time decoders. %G the relatively闉愭劤鍔 the 闈告繊鐏.. %G CPU闉愭劤鍔 姘嶆尗妫侀灇 闉忔帾鏌ｇ摯 鐡垮嫵鍋橀爟姗傚姝嗚嚙 闉濆嫶姒 闉愬棦娼... %G 闉 闉庣儎娼 闆尗姣勬 姘ゆ帾鈹 姣靛牗绠 RNNLM 鐡垮嫵鍋 闉涙劤娉婃 GPU闉愭劤鍔 闋冩﹤濮 鐡村喒婧傛惪 姘ゆ尗婢婃棶鍗婂及... %Moreover, relatively slower speed of computations on CPUs could make whole processes slow down since GPGPUs have to wait until the computations on CPUs are finished even if GPGPUs have done their works. Moreover, it is also important to balance the computation time between GPGPU and CPU, as the acceleration on GPGPU may not have a prominent impact on the overall speed if the GPGPU needs to wait for the CPU computation to finish.  %Main Idea %G GPGPU闉 闆碱煆鐖灇 闋冩悡鑸堕爟婊婂珶闆 闊挎粛妲冮灇 鑶 闉庡嫴濯界摨 鑷ф瑬濮烽浖鍫у珶... 闊搁椇纭犻爩 姘嶈兂鐗呰嚙 闉涘牕濮 鐡村喐寮 闉庡嫴濯归浖鍫︾... 褰顭庢緯姣 optimize闇涜導鈹 姘囶煄銈兼棷... %G 闈奉剣瀚熼澑 RNNLM 闉涙劤娉婇瀽 GPGPU姣 闉濅緟姣勯爟姗傚 鐡村喐寮 闉庡嫴濯圭摽... 鏀撮附鐓摽鐘界倰姘 闉庣儎娼 RNNLM Training闉 GPGPU姣 闉濅緟姣勯爟 闉撳牕銈 闉忔垐绗侀爟 闉氭尗婀㈤渻 闉愬棦妫冮灇鍕愁潊闇... %We try to apply GPGPUs to RNNLM-based network search by solving the disadvantages of GPGPUs. In order to achieve real-time decoding of RNNLM-based LVCSR, we apply on-the-fly rescoring of RNNLM to GPGPU based network traversal technique proposed in . %G The goal闉氭帾婢曢浕 鐡村喒婢 姘囶煇妯冮爟姗冩３ 闉庡嫵妲 闈告繊鐏ラ爟姗冩闉鎯﹀闆. We accelerate the speed of data exchange between the two heterogeneous processors and reduce redundant computations on CPUs by applying cache strategies. %G a real-time speed -> real-time speed The resulting recognition system has shown almost twice faster than real-time speed  %G 鏀撮附鍎 in various circumstances闇涚紕纰 闋冩﹤濯规棷 姣靛韩娼 鐡村眾姣庨瀽 鏀撮附鐏涢浖 闇涜導濮 闆绘劜鍊㈤灇 闇屾粖濮呯摯鍐稿及 鑷ф瑬鏅為澒... when experimented under various conditions, while maintaining relatively 10\% lower Word Error Rate  than that of conventional n-gram models.  % Papaer oraganization This paper is organized as the following. In section 2, the structure of RNNLMs is explained.  Section 3 explains how we applied GPGPUs to RNNLM-based network search. Section 4 explains the RNNLM rescoring with caches. Section 5 evaluates the improvement of the proposed method, followed by the conclusion in Section 6. %REVIEW 3-2) 鏀撮附顬 1 闈炬﹥鐖   	     %Method      This paper explained how we applied RNNLMs to a real-time large vocabulary decoder by introducing the use of GPGPUs. We tried to accelerate RNNLM-based WFST traversals in GPGPU-CPU hybrid architectures by solving some practical issues for applying GPGPUs. Moreover, in order to minimize the computation burden on CPUs, we applied a cache strategy.  REVIEW 1-4) Table 2 姣 姘ゆ帾鈹 闈镐緜瀵戦灊渚呮簜鎼 10  闋勩儸鍎 闇呮﹤濮 鐡村喒娼 闉 闈 闉涘牗婧傛皺鎼 闈绢偐鐎芥棷宀囶潊 闊挎粔璧撮爟姗冾潊闆 闉庡⿲娼. The decoding speed of RNNLMs was still slower than that of n-gram models, but the proposed method achieved the real-time speed while maintaining relatively 10\  lower WER as shown in Table, and so we could perfectly apply this approach to an on-line streaming speech recognition engine.  The memory footprint for the cache method was small enough to perform the experiment on the large data set. However, it will be more desirable to employ efficient cache techniques to reduce the memory usage further.     \eightpt            
"," 	Recurrent Neural Network Language Models  have started to be used in various fields of speech recognition due to their outstanding performance.	 	However, the high computational complexity of RNNLMs has been a hurdle in applying the RNNLM to a real-time Large Vocabulary Continuous Speech Recognition . 	In order to accelerate the speed of RNNLM-based network searches during decoding, we apply the General Purpose Graphic Processing Units . 	This paper proposes a novel method of applying GPGPUs to RNNLM-based graph traversals. 	We have achieved our goal by reducing redundant computations on CPUs and amount of transfer between GPGPUs and CPUs. 	The proposed approach was evaluated on both WSJ corpus and in-house data. 	Experiments shows that the proposed approach achieves the real-time speed in various circumstances while maintaining the Word Error Rate  to be relatively 10\% lower than that of n-gram models.",190
"   Metaphor as a figure of speech has a widespread presence in any form of communication either oral or written. According to Steen  data analysis shows that, on average, one in every seven and a half lexical units in the corpus is related to metaphor  However, it is difficult to clearly define the boundaries that separate metaphor from literal uses, as well as metaphor from other figures of speech.  The difficulty of clearly establishing a theoretical background for metaphor justifies the variety of NLP systems that aim at automatically between distinguishing between metaphorical and literal meanings of a word or phrase. This difficulty is further exacerbated if we take into account the limitations of Greek as regards resources and tools for metaphor detection; thus, we can conclude that the development of neural language models is necessary for the automatic differentiation between literal and metaphorical meaning of phrases that are part of an authentic and non-annotated Greek corpus. For these reasons, our attempt here is based on the principles of distributional semantics so as to determine the relations of a word with its linguistic context and to group semantic similarities of linguistic items based on distributional properties rather than any connections of the certain term and its related concepts. Distributional semantics  have been paramount in shifting research interest towards neural language models, which can attribute hidden statistical characteristics of the distributed representations of word sequences in natural language. Therefore, a serious problem such as the automatic detection of metaphors and their differentiation from literal uses can be dealt with the development of neural language models.         We presented a collection of state-of-the art metaphor detection models achieving accuracy higher than 90\  for the Greek language. This extends the work of  and, to the best of our knowledge, sets a new state-of-the-art for metaphor detection in Greek, dealing simultaneously with the lack of linguistic resources for Greek. We aim at continuing our work by exploring the performance of contextual embeddings such as ELMO  and BERT . Another recent promising direction, especially for small datasets is Graph Neural Networks  . In this specific variation of graph neural networks, the entire training set is represented as graph  and the task of the model is node representation and classification, even with potentially few training examples. This is achieved by exploiting the graph structure and the representation of adjacent nodes in the graph.   Both CNNs and bi-directional LSTMs with fine-tuning achieve accuracy higher than 90\ . If we disable fine-tuning, classification accuracy is still high, although  overall fine-tuning  appears to consistently outperform non fine-tuning configurations, which is also consistent with the results presented in .  There are several factors that can explain the performance achieved with neural networks. First, the full sentence is passed into the classifier and thus the model can benefit by exploiting potential long-term semantic dependencies. These dependecies are captured  by the LSTM cells and the convolutional operators. Additionaly, in the case of LSTMs and GRUs, bidirectional architectures appear to consistenly outperform unidirectional architectures.  Finally, transfer learning, in the form of pre-trained embeddings such as fastText  is extremelly useful in the sense that the learned representations capture semantic properties of words in a unsupervised learning fashion and we also allow fine-tuning, which is proven to further enchance the accuracy of the models . Fasttexts' ability to implicitly utilize morphological structure in the form of sub-word representations is also proven to help the overall downstream architecture to significantly improve. We conjecture that this property holds in languages with a rich morphological structure like Greek.  Since it is possible to distinguish between different kinds of metaphor and even between levels of metaphoricity of a term of a sentence, our effort is solely aimed at distinguishing between the literal and the metaphorical use of a term in a specific linguistic context. In that regard, we have not checked at all whether neural language models have the appropriate properties in order to discriminate pure metaphor from other kinds of figurative speech such as personification, metonymy, synecdoche etc. In addition, our approach to metaphor detection is not able to classify metaphorical phrases into categories like direct and indirect, or implied and extended. Of course, such an endeavor is a particularly interesting and demanding research challenge, even though the main goal of our specific approach is metaphor detection and its discrimination from literal cases by the use of machine learning algorithms.           that's all folks 
"," %\boldmath This paper presents and benchmarks a number of end-to-end Deep Learning based models for metaphor detection in Greek. We combine Convolutional Neural Networks and Recurrent Neural Networks with representation learning to bear on the metaphor detection problem for the Greek language. The models presented achieve exceptional accuracy scores, significantly improving the previous state of the art results, which had already achieved accuracy 0.82. Furthermore, no special preprocessing, feature engineering or linguistic knowledge is used in this work. The methods presented achieve accuracy of 0.92 and F-score 0.92 with Convolutional Neural Networks  and bidirectional Long Short Term Memory networks . Comparable results of  0.91 accuracy and  0.91 F-score are also achieved with bidirectional Gated Recurrent Units  and Convolutional Recurrent Neural Nets .  The models are trained and evaluated only on the basis of the training tuples, the sentences and their labels. The outcome is a state of the art collection of metaphor detection models, trained on limited labelled resources, which can be extended to other languages and similar tasks.",191
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Sentiment analysis has gained importance in the current world where social media is crucial in gauging public opinion on products, political campaigns, latest trends and more. A large portion of the world's population is multilingual, and so is the content on social networks. Code-mixing is a natural phenomenon among bilinguals where phrases and words from one language are employed in another. Typically, the underlying grammar of the primary language that is being spoken is kept intact and phrases from another language are embedded into it. India in particular, has 23 officially recognized languages and a majorly bilingual population, requiring robust computational tools to effectively exploit the data it produces.  Deep neural networks have had great success in predicting sentiment encapsulated in text - a big part of this success has been their ability to utilize pretrained word embeddings. In code-mixed tweets however, Hindi words are written in Roman script instead of its native Devanagari script, forcing one to use a transliterator in order to take advantage of pretrained aligned word embeddings, like fastText . These words in Roman script are unnormalized and may have multiple spellings; for example, bahut can be spelt in Roman script as bahut, bohot, bohut etc., and such errors propagate from the transliterator to our downstream task, thereby reducing performance.   In this work, we propose a deep convolution network with self-attention which shows promising results, without any pretraining. Convolution neural networks  are known to capture local relations  or local context, and work here as feature extractors acting as a substitute for handcrafted sentiment features. A self-attention layer is then applied over these features , which allow each individual feature to attend to all other features , providing global context. We call this the Hierarchical Context Modeling System , due to the hierarchical nature of context extraction performed on both levels.      In this paper we have discussed a way to perform sentiment analysis on code-mixed Hindi-English data, and we feel that the system is versatile enough to be applied to any mix of languages, especially when resources for the mixture are unavailable or hard to find. We have also looked at ways that such data can be preprocessed for training, and how a multilevel neural architecture is able to extract context from text.  While it is encouraging to see that our model performs consistently, there are a couple of caveats that need to be addressed - even though the lack of pretraining, a low resource setting, and noise in data seem surmountable hurdles to the task, it is evident that the most promising avenues for improvement lie in exploring data cleaning/normalization and transfer learning. Further analysis of the present methodology could also reveal the kind of transfer learning required - whether it be pretrained word embeddings, large scale language modeling or perhaps a better transliteration pipeline to name a few. To do so would require larger quantities of data and the application of the system to other linguistic tasks of settings similar to that of Sentimix. All of these steps qualify within the scope of future work, and committing to them will reward us with more confidence in the hypotheses we propose in this paper, providing further proof of the system's robustness and effectiveness. We hope to undertake efforts for the same soon.    While it is encouraging to see that the model performs consistently, there are a couple of caveats thatneed to be addressed - firstly, we acknowledge that our methodology could benefit from further analysisto explain obtained results.  To do so would require larger quantities of data and application to otherlinguistic tasks of settings similar to that of SentiMix. Secondly, even though the lack of pretraining, alow resource setting, and noise in data seem surmountable hurdles to the task, it is clear that the mostpromising avenues for improvement lie in exploring data cleaning/normalization and transfer learning.Both of these points qualify within the scope of future work, and committing to them will reward uswith more confidence in the hypotheses we propose in this paper, providing further proof of the system閳ユ獨robustness and effectiveness. We hope to undertake efforts for the same soon.        include your own bib file like this: 
","   Problems involving code-mixed language are often plagued by a lack of resources and an absence of materials to perform sophisticated transfer learning with. In this paper we describe our submission to the Sentimix Hindi-English task involving sentiment classification of code-mixed texts, and with an F1 score of 67.1\%, we demonstrate that simple convolution and attention may well produce reasonable results.",192
"   . 	%  	% % final paper: en-us version  	 	  % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/}. }  Sentiment Analysis identification is a sub-field of natural language processing that explores the automatic inference for fine-grained opinion polarity of textual data. The recent growth of social media, review forums and text messaging platforms create a surge in the amount of user-generated textual data, and so increased the urgent need for automatic opinion extraction. However this evolution created many opportunities for language technology and researchers, it provided verity of new challenges, namely, spelling errors, creative invented spelling , abbreviation , Meta tags and code-mixing . Non-English speakers frequently use multiple languages to express their feelings, which they know as code-mixing.  Speakers or writers tend to shift from one language to another either to express their feelings adequately, to show like-mindedness with a group, to distinguish oneself, to discuss a specific topic, or to look impressive to their audience. The SemEval-2020 shared task 9 , which is part of the SemEval-2020 workshop, focused on coping with this challenge. This task focus is automatic sentiment analysis in a code-mixed social media text. The task consists of two subtasks, Spanish-English and Hindi-English code-mixing, with over 30 groups who participate in each sub-task.   This article presents a system that we have implemented for predicting the sentiment of a given code-mixed tweet. The system was developed for both subtasks. Table 1 shows the results that we achieved with our system in the SemEval-2020 competitions. To create a highly accurate classifier, we tested different methods that varied from linear  and different deep learning architectures . Also, we tested a new architecture based on Wang's   work on offensive language detection, who used four Convolutional Neural Networks with different window sizes and k max-pooling ahead of them. However, the combination of Term Frequency-Inverse Document Frequency embedding and NBSVM yield the best result.  [t!] 	 		{|c|cl|} 			 &  \\ 			\hline 		 	 	      Our proposed methods ranked 6 out of 62 groups for the Hindi-English dataset and ranked 7 out of 29 for the Spanish-English dataset. This result shows the strength of the combination of NBSVM and  TF-IDF as a language independence Model. Also, our experiment shows the substantial adverse effect of ignoring one of the language's representation. For future work, we are going to utilize both languages pre-trained embedding. Averaging is one way to combine both embeddings. However, the different output space of both embedding could be challenging. Also, the attention mechanism could be used to give the model the ability to decide the importance of each language representation within each sentence.   include your own bib file like this: 
"," 	Sentiment Analysis is a well-studied field of Natural Language Processing. However, the rapid growth of social media and noisy content within them poses significant challenges in addressing this problem with well-established methods and tools. One of these challenges is code-mixing, which means using different languages to convey thoughts in social media texts. Our group, with the name of IUST, participated at the SemEval-2020 shared task 9 on Sentiment Analysis for Code-Mixed Social Media Text, and we have attempted to develop a system to predict the sentiment of a given code-mixed tweet. We used different preprocessing techniques and proposed to use different methods that vary from NBSVM to more complicated deep neural network models. Our best performing method obtains an F1 score of 0.751 for the Spanish-English sub-task and 0.706 over the Hindi-English sub-task.",193
" A recent estimate of the total number of English research articles available online was at least 114 million . Studies indicate the number of academic papers doubles every 10--15 years . The continued growth of scholarly papers can make finding relevant research papers challenging. Searches based on only keywords may no longer be the most efficient method  to use. This often happens when the same query terms appear in multiple research areas. For example, querying ``neuron'' in Google Scholar returns documents in both computer science and neuroscience. Search results can also belong to diverse domains when the query terms contain acronyms. For example, querying ``IR'' returns documents in Computer Science  and Physics . Similarly, querying ``NLP'' returns documents in Linguistics  and Computer Science . This is because documents in multiple subject categories  are often mixed together in a digital library search engine and its corresponding SC metadata is usually not available in the existing document metadata, either from the publisher or from automatic extraction methods.   As such, we believe it can be useful to build a classification system that assigns scholarly papers to SCs. Such an system could significantly impact scientific search and facilitate bibliometric evaluation. It can also help with Science of Science research , a recent area of research that uses scholarly big data to study the choice of scientific problems, scientist career trajectories, research trends, research funding, and other research aspects.  Also, many have noted that it is difficult to extract SCs using traditional topic models such as Latent Dirichlet Allocation , since it only extracts words and phrases present in documents.  An example is that a paper in computer science is rarely given that label in the keyword. In contrast, SC classification is usually based on a universal schema for that a specific domain or for all domains such as that of the Library of Congress. A crowd sourced schema can be found in the DBpedia of Wikipedia. %%% need a citation here %Classifying documents into SCs entails labelling the document with the subject domain that best describes an article's content at. This helps organizing and indexing digital collections and assists users in narrowing  search results. Many online retailers have implemented a faceted search feature, e.g., Amazon music search. However, similar features are not yet seen in scholarly digital library search engines.    In this work, we pose the SC problem as one of multiclass classification in which {. The core component is a supervised classifier based on recurrent neural networks trained on a large number of labeled documents that are part of the WoS database. In comparison with our preliminary work, our data is more heterogeneous , imbalanced, and complicated . We compare our system against several baselines applying various text representations, machine learning models, and/or neural network architectures.   Many schemas for scientific classification systems are publisher domain specific. For example, ACM has its own hierarchical classification system, NLM has medical subject headings, and MSC has a subject classification for mathematics. The most comprehensive and systematic classification schemas seem to be from WoS and the Library of Congress . The latter was created in 1897 and was driven by practical needs of the LOC rather than any epistemological considerations and is most likely out of date.  To the best of our knowledge, our work is the first example of using a neural network to classify scholarly papers into a comprehensive set of SCs. Other work focused on unsupervised methods and most were developed for specific category domains. In contrast, our classifier was trained on a large number of high quality abstracts from the WoS and can be applied directly to abstracts without any citation information. We also develop a novel representation of scholarly paper abstracts using ranked tokens and their word embedding representations. This significantly reduces the scale of the classic Bag of Word  model. We also retrained FastText and GloVe word embedding models using WoS abstracts. The subject category classification was then applied to the CiteSeerX collection of documents.  However, it could be applied to any similar collection.     Here, we discuss the experimental results, the impact of data on the results, potential solutions, and application of the model on real-world data.     The data imbalance problem is ubiquitous in both multi-class and multi-label classification problems . The imbalance ratio , defined as the ratio of the number of instances in the majority class to the number of samples in the minority class , has been commonly used to characterize the level of imbalance. Compared with the imbalance datasets in Table~1 of , our data has a significantly high level of imbalance. In particular, the highest IR is about 49,000 .    There are 3 ways commonly adopted to mitigate data imbalance problem: data resampling, algorithmic adaptations, and cost sensitive classification .  One commonly used way to mitigate this problem is data resampling. This method is based on rebalancing SC distributions by either deleting instances of major SCs  or supplementing artificially generated instances of the minor SCs . We can always undersample major SCs, but this means we have to reduce sample sizes of all SCs down to about 15 , which is too small for training robust neural network models. The oversampling strategies such as SMOTE  works for problems involving continuous numerical quantities, e.g., . In our case, the synthesize vectors of ``abstracts'' by SMOTE will not map to any actual words because word representations are very sparsely distributed in the large WE space. Even if we oversample minor SCs using these semantically dummy vectors, generating all samples will take a large amount of time given the high dimensionality of abstract vectors and high IR. Therefore, we only use our real data.         We discuss the potential impact on classification results contributed by categories overlapping in the training data.  Our initial classification schema contains 104 SCs, but they are not all mutually exclusive. Instead, the vocabularies of some categories overlap with the others.  For example, papers exclusively labeled as ``Materials Science"" and ``Metallurgy"" exhibit significant overlap in their tokens.  In the WE space, the semantic vectors labeled with either category are overlapped making it hard to differentiate them. Figure shows the confusion matrices of the closely related categories such as ``Geology"", ``Mineralogy"", and ``Geochemistry Geophysics''. Figure is the t-SNE plot of abstracts of closely related SCs. To make the plot less crowded, we randomly select 250 abstracts from each SC as shown in Figure. Data points representing  ``Geology"", ``Mineralogy"", and ``Geochemistry Geophysics"" tend to spread or are overlapped in such a way that are hard to be visually distinguished.   Since the dimension of WE is too high and the feature vector is sparse, we use TF-IDF to represent the abstracts  belonging to the above mentioned SCs to evade singularity issues. We then reduce the dimension of each abstract using t-SNE and plot the abstracts in a 2D graph. Figure shows the visualisation of the SCs shown in Figure.        Data points representing  ``Geology"", ``Mineralogy"", and ``Geochemistry Geophysics"" in Figure tend to spread or are overlapped in such a way that are hard to be visually distinguished.    Although the ``Genetics Heredity''  seems relatively far isolated from ``Zoology'', it is mixed with ``Plant Sciences'' and ``Biology'' instances, which is consistent with the contingency matrix in Figure .     One way to mitigate this problem is to merge overlapped categories. However, special care should be taken on whether these overlapped SCs are truly strongly related and should be evaluated by domain experts.   For example,   the term  ``Engineering"" qualifies as a generic term for the application of the technical aspects rather than an SC.   ``Zoology'', ``PlantSciences'', and ``Ecology''  be merged into a single SC called ``Biology'' .   ``Geology'', ``Mineralogy'', and ``GeoChemistry GeoPhysics'' can be merged into a single SC called ``Geology''.   ``Engineering'' is a broad category  containing numerous branches related to various technical aspects. Since ``Engineering'' itself cannot be considered as a category, we can removed instances of this SC from the dataset. However,  However, ``Materials Science'' and ``Metallurgy'' may  be merged  to a single SC.  By doing the aforementioned merges, the number of SCs is reduced to 74. As a preliminary study, we classified the merged dataset using our best model  and achieved an improvement with an overall micro- score of 0.78. The ``Geology'' SC after merging has significantly improved from  to .  The ``Biology'' after merging has a significantly higher  compared with the previous . A thorough study of all necessary category merges to make a new schema is a topic for future work.                CiteSeerX is a digital library search engine that was the first to use automatic citation indexing. It is an open source search engine that provides metadata and full-text access for more than 10 million scholarly documents and continues to add new documents .  In the past decade, it has incorporated scholarly documents in diverse SCs, but the distribution of their subject categories is unknown.   Using the best neural network model in this work , we classified 1 million papers randomly selected from CiteSeerX into 104 SCs . The top five subject categories are Biology , Computer Science , Mathematics , Engineering , Public Environmental Occupational Health . The fraction of Computer Science papers is significantly higher than the results in .  The  for Computer Science was about 94\  which is higher than this work . Therefore, the fraction may be overestimated here. However,  had only 6 classes and this model classifies abstracts into 104 SCs, so although this compromises the accuracy, , our work can still be used as a starting point for a systematic subject category classification. The classifier classifies 1 million abstracts in 1253 seconds implying that will be scalable on multi-millions of papers.       We investigated the problem of systematically classifying a large collection of scholarly papers into 104 SC's using neural network methods based only on abstracts. Our methods appear to scale better than existing clustering-based methods which rely on citation networks.  We explored various state-of-the-art WE models, neural network architectures, attention models, and strategies and found that  For neural network methods, our retrained FastText or GloVe combined with BiGRU or BiLSTM with the attention mechanism gives the best results. Retraining WE models and using an attention mechanism play important roles in improving the classifier performance. A two-level classifier effectively improves our performance when dealing with training data that has extremely imbalanced categories. The median 's under the best settings are 0.75--0.76.  to deal with the data imbalance problem, which caused poor classification performance for a number of categories with extremely low samples.   The classification system was based on the Web of Science schema. The original schema was collapsed and revised so classification focuses on 104 top level SCs. The first-level classifier classifies 81 SCs, including an ``Others"" SC. The second-level classifier further classifies ``Others'' into 24 minor SCs. Our model achieved significantly high -scores  for certain SCs. The classifier achieves  for 18 SCs. The median 's under the best settings are 0.75--0.76.   One bottleneck of our classifier is the overlapping categories. Merging closely related SCs is a promising solution, but should be under the guidance of domain experts.   The current algorithm does not perform well for significantly overlapped categories. To mitigate this problem, the schema is revised by merging certain SCs under the suggestions of domain experts.   We could also improve the text representation.  The TF-IDF representation only considers unigrams. Future work could consider -grams  and transfer learning to adopt word/sentence embedding models trained on non-scholarly corpora . . .  One could investigate models that also take into account stop-words, e.g., . One could also explore alternative optimizers of neural networks besides , such as the Stochastic Gradient Descent .  has been widely used, it does not generalize well especially for non-convex problems and tends to converge to the local minimum. One alternative is to use the Stochastic Gradient Descent   as the loss function approaches to the global minimum. For our application, we intend to explore topical representations with reinforcement learning and/or multi-task learning to classify multi-disciplinary papers and install a working version in CiteSeerX in the near future.   We gratefully acknowledge partial support from the National Science Foundation. We also acknowledge Adam T. McMillen for technical support, and Holly Gaff, Old Dominion University and Shimin Liu, Pennsylvania State University as domain experts respectively in biology and the earth and mineral sciences.      for Science, Engineering and Humanities and Social Sciences articles, for Humanities and Social Sciences articles please include page numbers in the in-text citations     for Health, Physics and Mathematics articles  }{}} \providecommand{  ]{agrawal2018wrong} Agrawal, A., Fu, W., and Menzies, T. .  Arora, S., Liang, Y., and Ma, T. .  Baeza-Yates, R.~A. and Ribeiro-Neto, B. .  Beltagy, I., Cohan, A., and Lo, K. .  Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. .  Boyack, K.~W. and Klavans, R. .  Bunk, S. and Krestel, R. .  Caragea, C., Wu, J., Gollapalli, S.~D., and Giles, C.~L. .  Cer, D., Yang, Y., Kong, S., Hua, N., Limtiaco, N., John, R.~S., et~al. .  Charte, F., Rivera, A.~J., del Jes{\'{u}}s, M.~J., and Herrera, F. .  Chawla, N.~V., Bowyer, K.~W., Hall, L.~O., and Kegelmeyer, W.~P. . ehre, Bahdanau,   Bougares, Schwenk et~al.}]{cho2014learning} Cho, K., van Merrienboer, B., G{\""{u}}l{}ehre, {}., Bahdanau, D.,   Bougares, F., Schwenk, H., et~al. .  encoder-decoder for   statistical machine translation.  Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. .  Devlin, J., Chang, M., Lee, K., and Toutanova, K. .  pre-training of deep bidirectional transformers for language   understanding.  Fellbaum, C. .  et~al.}]{fortunato2018science} Fortunato, S., Bergstrom, C.~T., B{\""o}rner, K., Evans, J.~A., Helbing, D.,   Milojevi{\'c}, S., et~al. . }nchez, and   Mollineda}]{garcia2012effectiveness} Garc{\'{}nchez, J.~S., and Mollineda, R.~A. .  Gerlach, M., Peixoto, T.~P., and Altmann, E.~G. .  Giles, C.~L., Bollacker, K.~D., and Lawrence, S. . : An automatic citation indexing system. }. 89--98   nzel and Thijs}]{glanzel2017using} Gl{\""a}nzel, W. and Thijs, B. .  Goldberg, Y. and Levy, O. .  Grave, E., Mikolov, T., Joulin, A., and Bojanowski, P. .  He, G., Fang, J., Cui, H., Wu, C., and Lu, W. . }   ]{hochreiter1997lstm} Hochreiter, S. and Schmidhuber, J. . .  Iyyer, M., Manjunatha, V., Boyd-Graber, J., and Daum{\'e}~III, H. .  Khabsa, M. and Giles, C.~L. .  Larsen, P. and von Ins, M. .  LeCun, Y., Boser, B.~E., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard,   W.~E., et~al. .  Llewellyn, C., Grover, C., Alex, B., Oberlander, J., and Tobin, R. .  Matsuda, K. and Fukushima, T. .  Mikolov, T., Chen, K., Corrado, G., and Dean, J. .  Mikolov, T., Yih, W., and Zweig, G. .  Moscato, P. and Cotta, C. .  Nadeau, D. and Sekine, S. .  Passos, A., Kumar, V., and McCallum, A. .  Pennington, J., Socher, R., and Manning, C.~D. .  Peters, M.~E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., et~al.   .  Prasad, A., Kaur, M., and Kan, M.-Y. .  {ParsCit}: a deep learning-based reference string parser.  Ratnaparkhi, A. .  Ren, Y., Zhang, Y., Zhang, M., and Ji, D. .  SalahEldeen, H.~M. and Nelson, M.~L. .  van Eck, N.~J. and Waltman, L. .  Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,   et~al. .  Vo, D.-T. and Zhang, Y. .  Waltman, L. and van Eck, N.~J. .  Wang, S. and Koopman, R. .  Witt, N. and Seifert, C. .  Wu, J., Kandimalla, B., Rohatgi, S., Sefid, A., Mao, J., and Giles, C.~L.   .  cleansed multidisciplinary scholarly big dataset.  Wu, J., Kim, K., and Giles, C.~L. . : 20 years of service to scholarly big data.  Wu, J., Williams, K., Chen, H., Khabsa, M., Caragea, C., Ororbia, A., et~al.   . : {AI} in a digital library search engine.  Yang, Z., Yang, D., Dyer, C., He, X., Smola, A.~J., and Hovy, E.~H. .  Zhang, H. and Zhong, G. .  Zhang, X., Zhao, J., and LeCun, Y. . \newblock Character-level convolutional networks for text classification. \newblock In        
","  %%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details. % Subject categories of scholarly papers generally refer to the knowledge domain to which the papers belong, examples being computer science or physics. Subject category information can be used for building faceted search for digital library search engines. This can significantly assist users in narrowing down their search space of relevant documents. Unfortunately, many academic papers do not have such information as part of their metadata. Existing methods for solving this task usually focus on unsupervised learning that often relies on citation networks. However, a complete list of papers citing the current paper may not be readily available. In particular, new papers that have few or no citations cannot be classified using such methods. Here, we propose a deep attentive neural network  that classifies scholarly papers using only their abstracts. The network is trained using 9 million abstracts from Web of Science . We also use the WoS schema that covers 104 subject categories.  %The abstracts are represented by a fix-length vector, which is generated by concatenating retrained top frequency word vectors.  The proposed network consists of two bi-directional recurrent neural networks followed by an attention layer. We compare our model against baselines by varying the architecture and text representation. Our best model achieves micro-${F_1}$ measure of $0.76$ with $F_1$ of individual subject categories ranging from $0.50$--$0.95$. The results showed the importance of retraining word embedding models to maximize the vocabulary overlap and the effectiveness of the attention mechanism. The combination of word vectors with TFIDF outperforms character and sentence level embedding models. We discuss imbalanced samples and overlapping categories and suggest possible strategies for mitigation. We also determine the subject category distribution in CiteSeerX by classifying a random sample of one million academic papers.     \tiny   %All article types: you may provide up to 8 keywords; at least 5 are mandatory.",194
" .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }   One of the concerns of SemEval-2020 Task 5: Modelling Causal Reasoning in Language: Detecting Counterfactuals  is to research the extent to which current state-of-the-art systems can detect counterfactual statements.  A counterfactual statement, as defined in this competition, is a conditional composed of two parts.  The former part is the antecedent -- a statement that is contradictory to known facts. The latter is the consequent -- a statement that describes what would happen had the antecedent held.    We examined the performance of current state-of-the-art language representation models on both subtasks and we found yet another NLP task benefits from unsupervised pre-training. In both cases, we found RoBERTa model to perform slightly better than other LRMs, while its results also being more stable. We have ended up first in both EM and F1 on Subtask 2 and second in Subtask 1.  
",   This paper describes BUT-FIT's submission at SemEval-2020 Task 5: Modelling Causal Reasoning in Language: Detecting Counterfactuals. The challenge focused on detecting whether a given statement contains a counterfactual  and extracting both antecedent and consequent parts of the counterfactual from the text . We experimented with various state-of-the-art language representation models .  We found RoBERTa LRM to perform the best in both subtasks. We achieved the first place in both exact match and F1 for Subtask 2 and ranked second for Subtask 1.,195
" As a cross-disciplinary study, we combine general linguistics with a computational linguistic approach. Various types of word embedding models are proposed to analyze large size corpora of languages . By way of illustration, word embeddings combined with artificial neural networks reflect one  aspect available to language processing in the human mind. Nevertheless, these innovative methods face the difficulty that ``purely data-driven approaches still struggle to reach the linguistic depth of their knowledge-driven predecessors. Bridging the gap between both types of approaches is therefore an important future research direction'' . Hence, we selected a linguistically motivated classification of words i.e., nominal classification , as a case study to demonstrate that the knowledge provided by linguistic theories concord with the information encoded into the basic statistical structures such as word embeddings.  More specifically, we selected Swedish since the observations with regard to L1 and L2 acquisition of nominal classification systems  in Swedish are controversial and differ from other languages.  First, monolingual children acquire Swedish grammatical gender with nearly no errors , which is considered rare in comparison to other gender languages, for which ``children's acquisitional paths have been reported not to be quite so error-free"" . Moreover, gender assignment on Swedish nouns via their phonological form or semantics is generally considered as unpredictable , which makes this observation even more unexpected. Second, while L1 acquisition display a lack or errors, L2  learners do encounter difficulties, suggesting that different strategies are employed . Hence, the existing linguistic analysis could provide additional perspectives to a computational approach and help to further understand which elements in Swedish are problematic in terms of grammatical gender perception. Moreover, matching the performance of an artificial neural network to linguistic observation made on humans  also represents an insightful comparative study, since simulating one facet of the learning process of the brain with artificial neural networks ``have become a subject of intense interest to scientists spanning a broad range of disciplines including psychology, physics, mathematics, computer science, biology and neurobiology閳 .  Thus, we propose the following research questions: 1) Can a word embedding model combined with artificial neural networks interpret grammatical gender in Swedish? 2) What types of error are made by the computational model and can we explain these errors from a linguistic perspective? Our experiment relies on two main sources of data, a corpus of Swedish raw sentences and a list of nouns affiliated to grammatical genders. The raw corpus is used train the word embedding model. The output of this model is a set of vectors associated with all words in the corpus. The dictionary is used to filter out non-noun words  and affiliate the vector of nouns with grammatical genders. These word vectors affiliated with their grammatical genders are then used to train a neural network which takes word vectors as input and determine their grammatical genders as output. The results of the network are then analyzed from a linguistic perspective.  The contributions of this research can be summarized as follow. First, it formulates a novel classification task to evaluate word embeddings. Second, it proposes a computational approach to compare with previous linguistic observations on Swedish. Finally, it also provides an in-depth linguistic analysis for the errors made by the classifier, i.e. neural network.   With regard to the general structure of this paper, 鎼 introduces the literature review on grammatical gender and computational models. 鎼 presents our methodology and our data. 鎼 elaborate the numerical results obtained from the neural network and provide a linguistics insight about the errors. 鎼 contains the detailed answers to our two research questions. Finally, 鎼 summarizes our findings as the conclusion.      Our main contributions are as follows: from the approach of computational linguistics, we demonstrated that a linear word embedding model combined with neural network is capable of capturing the information of grammatical gender in Swedish with an accuracy of . From a linguistic approach, we run an error analysis with regard to the errors generated by the neural network. The results show that the artificial neural network encounters difficulties in cases of polysemy, i.e., a linguistic form may link to different referents which belong to different part of speech categories. Such phenomenon is explained by linguistic theories of gender assignment, as neuter nouns are generally mass nouns, which undergo conversion between different part of speech categories . Thus, additional tuning of the computational model in that direction is expected to improve the performance. We wish that this paper may serve as a bridge to connect the field of linguistics and the field of computational linguistics which currently have divergent approaches toward linguistic data. By way of illustration, we show that word embedding and neural network can be applied to answer research questions of linguistic nature. Furthermore, the linguistic analysis targeting errors of the model are equivalently beneficial to enhance the computational model.  Our study is limited in terms of broadness. Although data is rich, word embedding combined to neural network represents a relatively simple model, and solely shows how informative are pure context information. A human carrying out the same linguistic task has not only activation of this kind of linguistic context, but also syntax, semantics, morphological associations, among others. Thus, further testing is required to compare the contribution of different factors with regard to gender classification. Furthermore, we only applied one type of word embedding model along with one type of neural network classifier. It would be necessary to investigate the accuracy of different combinations, and verify which type of model provides the most precision with regard to the task of grammatical gender assignment. Finally, our study only involved one language, i.e., Swedish, which has an unbalanced distribution of gender among the lexicon. Thus, our future research equivalently aims at including a phylogenetically weighted sample of languages to scrutinize if word embedding and neural network can reach the same level of accuracy cross-linguistically.   
"," We analyze the information provided by the word embeddings about the grammatical gender in Swedish. We wish that this paper may serve as one of the bridges to connect the methods of computational linguistics and general linguistics. Taking nominal classification in Swedish as a case study, we first show how the information about grammatical gender in language can be captured by word embedding models and artificial neural networks. Then, we match our results with previous linguistic hypotheses on assignment and usage of grammatical gender in Swedish and analyze the errors made by the computational model from a linguistic perspective.",196
" Large-scale generative language models  have received recent attention due to their high-quality open-ended text generation ability~. Generating texts from these LMs usually relies on some form of random sampling. Pure sampling often leads to incoherent and low-quality texts , whereas greedy decoding leads to excessive repetitions, another form of low quality. The right decoding algorithm is needed to generate high-quality texts with controlled attributes .  We introduce mirostat,\footnote{The word mirostat is derived from  which is Latin for  and  meaning control.} a neural text decoding algorithm that  the generative process to maintain the perplexity of generated text at a certain desired value. Mirostat uses an adaptive top- sampling algorithm to actively tune the value of  which helps maintain the overall perplexity of the text; recall that top- sampling  is where the next word is sampled from the top  most probable choices.  Top- sampling and several other recent sampling methods are motivated by suppressing an unreliable tail in the probability distribution of trained LMs.  Another sampling method is top-, also known as , where the next word is chosen from the top  probable choices, where  is the smallest integer such that their cumulative probability mass is at least  . While top- sampling involves a fixed number of most probable choices, top- sampling involves a dynamic number of choices based on a fixed  value and shows better statistical and human-evaluated performance. For small values of  and , these sampling methods unfortunately repeat phrases in generated text. This can be handled by penalizing repetitions and using appropriate  values  or adding diversity to the generated text . On the other hand, large values of  and  can lead to incoherent texts similar to pure sampling. Although choosing appropriate values of  or  can avoid repetition and incoherence, this involves ad hoc tuning of parameters. Even for a fixed value of  or , the generated text can have varying statistical properties.   Intriguingly, as we demonstrate via Example  in Appendix, small values of a certain perplexity statistic of generated texts called   are closely linked to repetitions and large values of surprise are linked to incoherence.  Perplexity is a statistical metric used to evaluate quality of neural text generation, and is closely related to average surprise as shown in Fig. in Appendix and formalized in Sec.. A large-scale human subject experiment by  showed human-evaluated quality is closely related to the likelihood of the generated text for fixed number of tokens. In particular, reducing perplexity increases quality upto some point before the quality starts dropping. This implies that good control over perplexity of the generated text would give direct control over the quality of generated text . Generating texts with an appropriately chosen target perplexity value may maximize quality of generated text.  .    Pure sampling from LMs often leads to incoherent text whereas greedy decoding leads to repetitions. Distorting probability distributions, as in top-, top-, or temperature sampling help improve quality of generated texts, if parameters are properly tuned . Tuning these methods, however, is ad hoc and does not provide good control over the statistics of the output. Our method uses statistics of previously-generated tokens as input to generate the next token, by distorting the probability distribution so it helps control the overall statistics of the generated text. This ability to control the perplexity of the output  is a key advantage of our method over previous work. This, when used with the relation between perplexity and human-evaluated quality observed by , can yield text that has better quality control.   Controllable text generation has oft focused on semantics of the output text, as in LMs like CTRL , and sampling algorithms like plug-and-play LM  and constrained sentence generation by Metropolis-Hastings . Contrarily our approach is purely statistical, guiding the decoder along a desired statistical path that addresses issues with pure sampling and greedy decoding.   %A new model with 1.63 billion parameters, CTRL, was trained to generate text based on a control word.  %On the other hand, sampling algorithms like Plug and Play Language Model  and Constrained sentence Generation by Metropolis-Hastings  work at the inference stage on top of a pretrained language model to control certain attributes of the generated text. %PPLM shows that using attribute classifiers on top of pretrained language models helps control text generation . %CGMH uses Metropolis-Hastings sampling to generate text with certain constraints like appearance of multiple keywords %%%%%%%%%  : SeqGAN    %Distorting probability distributions for decoding using  Top-, top-, and low-temperature sampling improve the quality of the text, but at the cost of reduced diversity. Applications like question-answering only demand high-quality generation, but open-ended tasks such as story generation demand diversity too.  propose variants of beam search to induce diversity in generated text. However,  observe a tradeoff between quality and diversity; they further observe diversity is closely related to entropy whereas quality is maximized in a certain range of observed likelihood values for fixed-length sentences. Our algorithm well-controls observed cross-entropy, the observed likelihood per token of generated text. Hence, by maintaining the observed cross-entropy in a certain range, we can ensure high-quality text generation.   Greedy decoding from LMs often lead to texts with excessive repetitions both at token- and sentence-levels. Several techniques have been proposed to address this. Token loss dynamic reweighting  hypothesizes  some tokens are more difficult to learn than others and so reweighting tokens during learning can balance things to reduce repetitions~.   use a repetition penalty in decoding to reduce repetition of tokens.  suggest the cause for repetitions is a flaw in the training objective itself and use a new objective that gives less probability to unlikely sequence including texts with high repetitions. Variants of top- sampling and repetition penalty in  were used before by  to reduce repetitions. Here, we demonstrate a near-linear relation between repetitions and observed cross-entropy and so we directly control repetitions by controlling observed cross-entropy.    We provided a theoretical explanation of how perplexity varies as a function of input parameters in popular top- and top- neural text decoding algorithms, showing that log of perplexity varies nearly linearly as a function of  and a highly nonlinearly as a function of . Building on this analysis, we developed mirostat, a neural text decoding algorithm that directly controls the perplexity of the generated text over a wide range of text length.   and therefore has several advantages.   over other sampling algorithms.  While top- and top- do not provide a good control over the statistics of the output, mirostat can maintain the perplexity of generated text over a wide range of text length.  Notably, for longer texts and certain ranges of input parameters, top- and top- sampling fall into boredom and confusion traps which cause low-quality texts; Mirostat avoids both traps. Further, recent large-scale human evaluation of neural generated text suggests that human-judged text quality is maximized for a certain range of perplexity of the output: mirostat provides direct control to stay in that perplexity range.  There are also implications for data compression as given in Appendix. As a takeaway, we find that mirostat with target surprise around , produces varying lengths of high-quality texts with minimal repetitions.  This is corroborated in our own experiments with human raters.  We further analyze the relation between perplexity and repetitions in text: for fixed model, repetitions vary linearly with perplexity and are independent of the sampling method. We also find that larger LMs have less repetitions for any fixed perplexity. Future work would include theoretical analysis of repetitions, boredom and confusion traps, and convergence properties of mirostat.      
"," Neural text decoding algorithms strongly influence the quality of texts generated using language models, but popular algorithms like top-$k$, top-$p$ , and temperature-based sampling may yield texts that have objectionable repetition or incoherence.  Although these methods generate high-quality text after ad hoc parameter tuning that depends on the language model and the length of generated text, not much is known about the control they provide over the statistics of the output.  This is important, however, since recent reports show that humans prefer when perplexity is neither too much nor too little and since we experimentally show that cross-entropy  has a near-linear relation with repetition. First we provide a theoretical analysis of perplexity in top-$k$, top-$p$, and temperature sampling, under Zipfian statistics. Then, we use this analysis to design a feedback-based adaptive top-$k$ text decoding algorithm called  that generates text  with a predetermined target value of perplexity without any tuning. Experiments show that for low values of $k$ and $p$, perplexity drops significantly with generated text length and leads to excessive repetitions . Contrarily, for large values of $k$ and $p$, perplexity increases with generated text length and leads to incoherence . Mirostat avoids both traps. Specifically, we show that setting target perplexity value beyond a threshold yields negligible sentence-level repetitions. Experiments with human raters for fluency, coherence, and quality further verify our findings.",197
"  \pheadNoSpace{Background} Text classification has become a fundamental building block in modern information systems, and there is an increasing need to be able to classify texts in a wide range of languages. However, as organizations target an increasing number of markets, it can be challenging to collect new task-specific training data for each new language that is to be supported.  To overcome this, cross-lingual systems rely on training data from a source language to train a model that can be applied to entirely different target languages , alleviating the training bottleneck issues for low-resource languages.  Traditional cross-lingual text classification approaches have often relied on translation dictionaries, lexical knowledge graphs, or parallel corpora to find connections between words and phrases in different languages . Recently, based on deep neural approaches such as BERT , there have been important advances in  learning joint multilingual representations with self-supervised objectives .  These have enabled substantial progress for cross-lingual training, by mapping textual inputs from different languages into a common vector representation space . With models such as Multilingual BERT , the obtained vector representations for English and Thai language documents, for instance, will be similar if they discuss similar matters.  Still, recent empirical studies  show that these representations do not bridge all differences between different languages. While it is possible to invoke multilingual encoders to train a model on English training data and then apply it to documents in a language such as Thai, the model may not work as well when applied to Thai document representations, since the latter are likely to diverge from the English representation distribution in subtle ways.  In this work, we propose a semi-supervised adversarial perturbation framework that encourages the model to be more robust towards such divergence and better adapt to the target language. Adversarial training is a method to learn to resist small adversarial  perturbations that are added to the input so as to maximize the loss incurred by neural networks . % Nevertheless, the gains observed from adversarial training in previous work have been limited, because it is merely invoked as a form of monolingual regularization. Our results show that adversarial training is particularly fruitful in a cross-lingual framework that also exploits unlabeled data via self-learning.  \pheadWithSpace{Overview and Contributions} Our model begins by learning just from available source language samples, drawing on a multilingual encoder with added adversarial perturbation. Without loss of generality, in the following, we assume English to be the source language. After training on English, subsequently, we use the same model to make predictions on unlabeled non-English samples and a part of those samples with high confidence prediction scores are repurposed to serve as labeled examples for a next iteration of adversarial training until the model converges.  The adversarial perturbation improves robustness and generalization by regularizing our model. At the same time, because adversarial training makes tiny perturbations that barely affect the prediction result, the perturbations on words during self-learning can be viewed as inducing a form of code-switching, which replaces some original source language words with potential nearby non-English word representations.  Based on this combination of adversarial training and semi-supervised self-learning techniques, the model evolves to become more robust with regard to differences between languages. We demonstrate the superiority of our framework on Multilingual Document Classification   in comparison with state-of-the-art baselines. Our study then proceeds to show that our method outperforms other methods on cross-lingual dialogue intent classification from English to Spanish and Thai . This shows that our semi-supervised adversarial framework is more effective than previous approaches at cross-lingual transfer for domain-specific tasks, based on a mix of labeled and unlabeled data via adversarial training on multilingual representations.     While multilingual encoders have enabled better cross-lingual learning, the obtained models often are not attuned to the subtle differences that a model may encounter when fed with documents in an entirely new language. To adress this, this paper proposes an adversarial perturbation framework that makes the model more robust and enables an iterative self-learning process that allows the model to gradually adapt to the target language.  We achieve new state-of-the-art results on cross-lingual document and intent classification and demonstrate that adversarial perturbation is an effective method for improved classification accuracy without any labeled training data in the target language.   
"," In cross-lingual text classification, one seeks to exploit labeled data from one language to train a text classification model that can then be applied to a completely different language. Recent multilingual representation models have made it much easier to achieve this. Still, there may still be subtle differences between languages that are neglected when doing so. To address this, we present a semi-supervised adversarial training process that minimizes the maximal loss for label-preserving input perturbations. The resulting model then serves as a teacher to induce labels for unlabeled target language samples that can be used during further adversarial training, allowing us to gradually adapt our model to the target language. Compared with a number of strong baselines, we observe significant gains in effectiveness on document and intent classification for a diverse set of languages.",198
"  %\gn{I set ""taclpubformat"" to true so auto-seeking works on overleaf. It needs to be set back to false before submission.}  Unsupervised grammar induction aims at building a formal device for discovering syntactic structure from natural language corpora. % \gn{Do Chomsky and Pinker actually handle unsupervised grammar induction? I don't think so, so maybe remove. If they do handle this it is fine to keep the cites.} Within the scope of grammar induction, there are two main directions of research: unsupervised constituency parsing, which attempts to discover the underlying structure of phrases, and unsupervised dependency parsing, which attempts to discover the underlying relations between words. Early work on induction of syntactic structure focused on learning phrase structure and generally used some variant of probabilistic context-free grammars . In recent years, dependency grammars have gained favor as an alternative syntactic formulation . Specifically, the dependency model with valence   forms the basis for many modern approaches in dependency induction. Most recent models for grammar induction, be they for PCFGs, DMVs, or other formulations, have generally coupled these models with some variety of neural model to use embeddings to capture word similarities, improve the flexibility of model parameterization, or both .    Notably, the two different syntactic formalisms capture very different views of syntax. Phrase structure takes advantage of an abstracted recursive view of language, while the dependency structure more concretely focuses on the propensity of particular words in a sentence to relate to each-other syntactically. However, few attempts at unsupervised grammar induction have been made to marry the two and let both benefit each other. This is precisely the issue we attempt to tackle in this paper.  As a specific formalism that allows us to model both formalisms at once, we turn to lexicalized probabilistic context-free grammars . L-PCFGs borrow the underlying machinery from PCFGs but expand the grammar by allowing rules to include information about the lexical heads of each phrase, an example of which is shown in \Cref{fig:lexicalized-phrase-structure-tree}. The head annotation in the L-PCFG provides lexical dependencies that can be informative in estimating the probabilities of generation rules. For example, the probability of VP[{] VP[{ . Historically, these grammars have been mostly used for  parsing, combined with traditional  estimators of rule probabilities . Within this context, lexicalized grammar rules are powerful, but the counts available are sparse, and thus required extensive smoothing  %to be interpolated with more general structural rules such as those used in standard PCFGs  to achieve competitive results .  %\gn{Now that we have introduced the abbreviation ``L-PCFG'', make sure you use it in all mentions of ``lexicalized PCFG'' below.}  % While the aim of lexicalization was to inform syntax and improve parsing, it is shown in  that lexical information has not been best exploited due to the sparsity of corpus\footnote{Proper smoothing methods must be applied to achieve comparable results as PCFGs.}.    In this paper, we contend that with recent advances in neural modeling, it is time to return to modeling lexical dependencies, specifically in the context of unsupervised constituent-based grammar induction. We propose neural L-PCFGs as a parameter-sharing method to alleviate the sparsity problem of lexicalized PCFGs. \Cref{fig:diagram} illustrates the generation procedure of a neural L-PCFG. Different from traditional lexicalized PCFGs, the probabilities of production rules are not independently parameterized, but rather conditioned on the representations of non-terminals, preterminals and lexical items . Apart from devising lexicalized production rules  and their corresponding scoring function, we also follow 's compound PCFG model for  constituency parsing with compound variables , enabling modeling of a continuous mixture of grammar rules.% \footnote{In other words, we do not induce a single PCFG, but a distribution over a family of PCFGs.} We define how to efficiently train  and perform inference  in this model using dynamic programming and variational inference.  Put together, we expect this to result in a model that both is effective, and  induces both phrase structure and lexical dependencies,% \footnote{Note that by ``lexical dependencies'' we are referring to unilexical dependencies between the head word and child non-terminals, as opposed to bilexical dependencies between two words .} whereas previous work has focused on only one. Our empirical evaluation examines this hypothesis, asking the following question:  { }  Our experiments  answer in the affirmative, with  %demonstrating our model has  better performance than baselines designed specially for either dependency or constituency parsing under multiple settings. Importantly, our detailed ablations show that %\gn{curriculum learning and initialization  haven't been mentioned yet, and  are much less central to the story written above than methods of factorization. I'd try to write this so these concepts don't seem to come up suddenly, either by not mentioning them directly here, or by mentioning them earlier when you talk about the method. I'd slightly prefer the former but would be OK with the latter.} methods of factorization play important role in the performance of neural L-PCFGs . Finally, qualitatively , we find that latent labels induced by our model align with annotated gold non-terminals in PTB.   %         In this paper, we propose neural L-PCFG, a neural parameterization method for lexicalized PCFGs, for both unsupervised dependency parsing and constituency parsing. We also provide a variational inference method to train our model. By modeling both representations together, our approach outperforms methods specially designed for either grammar formalism alone.    Importantly, our work also adds novel insights for the unsupervised grammar induction literature by probing the role that factorizations and initialization have on model performance.  Different factorizations of the same probability distribution can lead to dramatically different performance and should be viewed as playing an important role in the inductive bias of learning syntax.  Additionally, where others have used pretrained word vectors before, we show that they too contain abstract syntactic information which can bias learning.  Finally, while out of scope for one paper, our results point to several interesting potential roads forward, including the study of the effectiveness of jointly modeling constituency-dependency representations on freer word order languages, and whether other distributed word presentations  might provide even stronger syntactic signals for grammar induction.   ^2)L\mathcal{O}\mathcal{O}^2)\mathcal{O}^2)\mathcal{O}\mathcal{O}^2)N=20$), while the bilexical model cannot. There are several potential methods to side-step this problem, including the use of sampling in lieu of dynamic programming, using heuristic methods to prune the grammar, and designing acceleration methods on GPU . }  
"," In this paper we demonstrate that context free grammar  based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms  have been plagued by sparsity, making them unsuitable for unsupervised grammar induction.  However, in this work, we present novel neural models of lexicalized PCFGs which allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone.\footnote{Code is available at \url{https://github.com/neulab/neural-lpcfg}.}  %\gn{Abstract is pretty dry. First maybe introduce unsupervised parsing, then explain that there are alternative formalisms: dependency and constituency, handeled by DMV and CFG respectively. Then explain that lexicalized PCFG was used in supervised parsing, but has  not been applied to unsupervised parsing, which we posit is due to sparsity. then introduce our method.} %Y% Unsupervised grammar induction aims at inducing probabilistic grammar rules from natural language corpora. Within the scope of grammar induction there are two main directions of research: unsupervised constituency parsing and unsupervised dependency parsing, on which directions dependency models with valence  and probabilistic context-free grammars  are two most successful models respectively. Lexicalized PCFGs provide a method to unify these two kinds of models, but were mostly used in supervised parsing, and its performance is largely restricted by the data-sparsity problem. In this paper, we propose neural lexicalized probabilistic context-free grammars  for jointly performing both unsupervised dependency and constituency parsing. Our Neural L-PCFGs are parameterized by several simple neural networks which score production rules by conditioning on the lexical head of the parent constituent. Key to the success of our approach is that the neural parameterization enables efficient parameter sharing, thereby alleviating the data-sparsity issues common to lexicalized PCFGs. %includes a set of Chomsky Normal Form  productions rules and their scores parameterized by neural networks. The scores of productions rules are conditioned on the head word of the parent constituent, which makes our model sensitive to lexical information; meanwhile, neural parameterization enables parameter sharing mechanism, thus alleviating the sparse-data problem.  %Y% Unlike prior work that pits constituencies and dependencies against each other, our novel formulation produces a single model which simultaneously achieves strong performance across both syntactic formulations. Experiments show that our model could achieve better results comparing to models designed specific for these two tasks. %The compound latent variable makes the L-PCFG more general. In experiments on unsupervised consitituency parsing and dependency parsing tasks, our model compete favorably against specialized models for those two tasks.",199
"  % - Define QA, motivation.  % - Statement of current approaches and their limitations % - Statement of what we do in this paper % - Statement of our contributions        The capability of providing exact answers to queries framed as natural language questions can significantly improve the user experience in many real world applications. Rather than sifting through lists of retrieved documents, automatic QA  systems can surface an exact answer to a query, thus reducing the cognitive burden associated with the standard search task. This capability is applicable in extending conventional information retrieval systems  and also for emergent use cases, such as open domain conversational AI systems . For enterprises, QA systems that are both fast and precise can help unlock knowledge value in large unstructured document collections.   % we need to process this paper for references here  % https://arxiv.org/pdf/2004.04906.pdf % Across many practical usecases, the QA task is structured as open domain QA where the answer must be extracted from relevant documents which are retrieved from an open corpus .  Conventional methods for open domain QA  follow a two-stage implementation -  a retriever that returns a subset of relevant documents. Retrieval is typically based on sparse vector space models such as BM25  and TF-IDF ;  a machine reading comprehension model  that identifies spans from each document which contain the answer. While sparse representations are fast to compute, they rely on exact keyword match, and suffer from the vocabulary mismatch problem - scenarios where the vocabulary used to express a query is different from the vocabulary used to express the same concepts within the documents.  To address these issues, recent studies have proposed neural ranking  and retrieval methods , which rely on dense representations.   % These approaches can be classified into two broad groups based on how passage retrieval is implemented. The first group uses sparse text representation methods  to retrieve a set of passages which are then processed by a document reader to extract answer spans. The challenge here is that sparse models are limited in their ability to model query context and may surface passages are not contextually similar to the query { mention precision/recall issues}.  The second set of approaches explore the use of dense representations in information retrieval which ensure retrieved passage candidates are contextually relevant {ref and revision needed}.    However, while dense representations show significantly improved results, they introduce additional complexity and latency, which limits their practical application. For example,  require a specialized MLM pretraining regime, as well as a supervised fine-tuning step, to obtain representations used in a retriever. Similarly  use dual encoders in learning a dense representation for queries and all documents in the target corpus. Each of these methods require additional infrastructure to compute dense representation vectors for all documents in the target corpus as well as implement efficient similarity search at run time. In addition, transformer-based architectures  used for dense representations are unable to process long sequences due to their self-attention operations which scale quadratically with sequence length. As a result, these models require that documents are indexed/stored in small paragraphs. For many use cases, meeting these requirements  can be cost-intensive. These costs are hard to justify, given that simpler methods can yield comparable results . Furthermore, as reader models are applied to domain-specific documents, they fail in counter-intuitive ways. It is thus valuable to offer visual interfaces that support debugging or sensemaking of results . While several libraries exist to explain NLP models, they do not integrate interfaces that help users make sense of both the query expansion, retriever and the reader tasks. Collectively, these challenges can hamper experimentation with QA systems and the integration of QA models into practitioner workflows.  In this work, we introduce NeuralQA to help address these limitations. Our contributions are summarized as follows:   }), and document reading . It also offers an interactive user interface for sensemaking of results . NeuralQA is {open source  and released under the MIT License}.      {10x faster}).          % {x thousand} legal documents. We also release several resources to support further research and industry application in this area .             %     Overall, NeuralQA complements a line of end-to-end applications that improve QA system deployment  and provide visual interfaces for understanding machine learning models .    % It responds to calls for the integration of simple but robust baselines  that can significantly reduce the complexity of NLP systems, while improving performance.      % In addition, many deep neural approaches are limited by fixed input size  making it challenging to use them on lengthy documents at any stage of the QA process. {ref}.   % In practice the full set of steps that entail QA extends beyond retrieval and reading. They frequently include expansion + % % File emnlp2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith \pdfoutput=1  \documentclass[11pt,a4paper]{article} % \usepackage[bookmarks=false]{hyperref} \usepackage{emnlp2020} \usepackage[ruled,vlined]{algorithm2e} \usepackage{url} % \usepackage[bookmarks=false]{hyperref} % \usepackage{hyperref} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily % This is not strictly necessary, and may be commented out, % but it will improve the layout of the manuscript, % and will typically save some space. \usepackage{microtype}   %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \graphicspath{{figures/}} \TeX}  \title{{NeuralQA}: A Usable Library for Question Answering  on Large Datasets}  \author{    Victor Dibia \\   Cloudera Fast Forward Labs \\     \\}  \date{}        %  %           %       In this paper, we presented NeuralQA - a usable library for question answering on large datasets.    Through experiments and user studies, we demonstrate utility of our design choices and  implemented components - \ , index fragments, attention visualizations and {todo}.  NeuralQA is useful for developers interested in qualitatively exploring QA models for their custom datasets, as well as for enterprise teams seeking a flexible QA interface/API for their customers.  NeuralQA is under active development, and roadmap features include support for a {Solr} retriever, additional model explanation methods and additional query expansion methods such as RM3 . Future work will also explore empirical evaluation of our CQE and \ implementation to better understand their strengths and limitations.      https://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdfo    neutalqa makes new usability contribution s   neuralqa is    easily installed ... pip install   built on highingace  an industtial strentg nlp engine  such that ot can ve used with thr thousands of kodels exported via higgingface    it implementa aan api that can be used on custom ui with endpoints for retrieval, answers, and explanatins     it is configurable .. a config fule is usded to soecifiy things like the supported models, expansion methods , ui content and passage summarization methods etc    it fits teo personas .. the hobbyist and the teams.    its is svalable ... supports industry standard web servers gnicorn and elstic aearch.    supports multiple indexes and index imolementations . ui advabced options can be enabled or hidden from the bacjend .    it is fully documented    dense passage retrieval dpr orivides 9-16 percent increase in retrieval  acciracy but requires lavelled data  
","   % Guidance on paper content for EMNLP demo % https://2020.emnlp.org/call-for-papers/demos  % What problem does the proposed system address? % Why is the system important and what is its impact? % What is the novel in the approach/technology on which this system is based? % Who is the target audience? % How does the system work? % How does it compare with existing systems? % How is the system licensed? % 6 pages  Existing tools for Question Answering  have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline . To help address these issues, we introduce NeuralQA - a usable library for QA on large datasets. NeuralQA integrates well with existing infrastructure  and offers helpful defaults for QA subtasks. It introduces and implements contextual query expansion  using a masked language model  as well as relevant snippets ) - a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, it offers a flexible user interface to support workflows for research explorations  and large scale search deployment.   Code and documentation for NeuralQA is available as open source on  {Github}.  % It is configurable via a yaml file which enables out of the box usage with minimal code.  % The library can be installed via the pip python package manager and is open sourced under the MIT license;   % Dense passage retrieval methods hold promise for improved precision/recall within QA system implementations but can introduce significant complexity which make them impractical.   % In this work, we report on findings from our experiments building NeuralQA - an end to end open source question answer  designed to address some of these challenges. We show how the candidate passage retrieval stage of the QA task can be improved with contextual query expansion on sparse representations of queries, whilst achieving  accuracy comparable to dense deep representations and at a fraction of the complexity. NeuralQA offers a flexible user interface,  and  support for multiple retriever indexes, interpretability modules for sensemaking, query expansion methods and document readers.It is released under the MIT license.  % We also introduce the legalCase dataset and benchmark results that demonstrate the value of our query enrichment approach for large domain-specific corpora.s   % [Andrew] - @Victor, not sure we'll be able to claim that we achieve ""comparable accuracy to dense representations"" unless we replicate their experimental setup on ALL of Wikipedia",200
" % Para1: teaser Universal / Domains NER Named Entity Recognition  is a fundamental task in the area of Information Extraction , in which mentions of Named Entities  %  are { from naturally-occurring texts.  This task is most commonly formulated as a Sequence Labelling   task, where  extraction  takes the form of assigning each input token with a label that marks the {  is reflected with assigned labels which also indicate  the entity { %or  languages far less resourced than English . In particular,  In particular, there is no readily available and empirically verified Neural modeling strategy for Neural NER in those languages with  complex word-internal structure,   also known as  %for the high performing Neural design of NER for  {    refers to languages in which substantial information concerning the arrangement of words into phrases and relations  is expressed at word level, rather than in a fixed word-order or a rigid structure. The extended amount of information expressed at word-level and the  morpho-phonological processes creating these  words  result in high token-internal complexity, which poses serious challenges to the basic formulation of NER as it is conceived and implemented for English.  %the sequence labeling of space-delimited {, in MRLs a single token may include multiple meaning-bearing elements, only some of which are relevant for the entity mention.  It is then no longer clear whether labeling raw tokens as NEs will be sufficiently { for NER, or other IE tasks,  in MRLs.  % Para3: research question %  %by explicitly examining this token-internal complexity. % In this paper we formulate two questions concerning the modelling strategy for NER in MRLs, namely:  what should be the granularity of the basic units to be labeled in the input stream? Space-delimited tokens or finer-grained morphological units? and,  how can we devise an architecture that can effectively encode and accurately obtain   morphological information  that is relevant to  NER in realistic, morphologically ambiguous,  scenarios?   To empirically investigate  possible solution strategies and modeling alternatives  we  contribute  a novel { pipeline. With respect to , we show that token-based NER { pipeline. While these two findings may appear contradictory, we aim here to offer a climax,   %flipping} the order of the standard NLP pipeline. Instead of a standard morphology-before-NER architecture we show that a a { and  { the morphological decomposition.  We empirically show that the hybrid architecture we propose outperforms all  token-based and morpheme-based variants for Hebrew NER on our benchmark, and it further outperforms all previously reported results on Hebrew Morphological Decomposition  tasks  . % existing benchmarks. %\db{token-single actually was best on naama}\rt{better?}\db{now?} % Our  error analysis further shows that morpheme-based models are particularly beneficial for recognizing entities that belong to the long tail of  entities unseen during training  %Based on this corpus, we devise an experimental setup which allows us to compare performance differences between token-based and morpheme-based models.  %We use a Bi-LSTM-CRF architecture with both word-level and character encoding, in which the only difference between morpheme and token-based models is in the granularity of the input and output spaces. %Our evaluation of results on this setup comprises our following contributions.  %Our experiments on this benchmark show that  morpheme-based models consistently outperform token-based models for Hebrew, and that char-based encoding on tokens do not make up for this empirical gap. %We also provide an ablation of the contribution the different model parts in terms of embeddings and encodings. % Para6-7: contribution + results  % In realistic settings, using  pipeline morphological decomposition for NER seriously jeopardize the morpheme-based NER results results. %Not only lower than Oracle setups, but also lower than token-based models.  %However,  a   architecture, where NER predictions precede and prune the  MD, greatly outperform pipeline  morpheme-based or token-based scenarios. %This is done by pruning the morphological lattice, by removing paths that do correlate with the token NER label. This results in %This architecture delivering new state-of-the-art for both Hebrew MD   and Hebrew NER on previous  as well as the presently proposed benchamrk. %. %and dependency parsing %, and a new benchmark for Hebrew NER, with results that now out-perform our token-based models.  % % Para8: deeper analysis highlights   % Para9: summarize contribution The contribution of this paper is thus manifold. First, we define the key questions of Neural NER in MRLs and proceed to chart the space of modeling options.  Second, we deliver a new and novel parallel benchmark that allows one to {empirically} { the different  modeling choices at morphemes-vs.-tokens granularity. Third, we show consistent performance trends and advantages for { architecture --- that may be extended to other languages and other {information extraction} tasks ---  which demonstrates an even further improved performance on both Hebrew NER and Hebrew morphological disambiguation tasks. %Our detailed error analysis  demonstrates the generalization capacity of the morpheme-based models for previously unseen NEs, And %Finally, our experiments set a new bar for the performance of both e Our results on both Hebrew NER and Hebrew MD tasks present a new bar on these tasks, outperforming all previously reported state-of-the-art models.  The remainder of this paper is organized as follows. In Section we elaborate on the linguistic challenges that standard NER modeling approaches face in MRLs,  and   establish our core research questions. In Section  we present our novel Hebrew NER parallel benchmark that will support the empirical  investigation. In Section  we define and empirically contrast our token-based and morpheme-based modeling strategies, and in Section   we  devise and empirically contrast { NER pipelines. In Section  we provide a detailed error analysis of the models,  particularly in the  case of previously unseen  entities, and in Section  we reflect on related and future work on NER for MRL and situate our findings in a greater context. Finally, in Section we  summarize and  conclude.   % Corpus % Models % Analysis of realistic scenarios % Analysis of generalization capacity % New SOTA for hebrew NER   %   This work addresses the modeling challenge of Neural NER in MRLs. We outlined two core modeling questions, namely  the choice of input units; tokens or morphemes, and  the pipeline for obtaining those units; pipeline vs.\ hybrid. We deliver a  parallel {-based models consistently leads to improved NER performance, yet  these results are extremely sensitive to  segmentation errors propagating down the pipeline.  We thus propose a   { and  {, on both the NER and MD tasks in realistic  scenarios. Our analysis further shows  that morpheme-based models generalize better to unseen entity mentions across the board, and especially to those that are caused by morphological composition.  We deliver state-of-the-art results for Hebrew NER and for morphological tagging, along with a benchmark to encourage further investigation into the interaction between  morphology and NER. 
"," Named Entity Recognition   is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically-Rich Languages  pose a challenge to this basic formulation, as %space-delimited tokens do not coincide with the basic units that compose.  the boundaries of Named Entities do not coincide with { boundaries.  To address NER in MRLs we then  need to answer two fundamental modeling questions: %In this work we ask whether in Neural models morphemes should be used instead as the basic units to be classified. If so, how should these units be obtained? %To address this  %sub-word units  should be labelled instead, and  %Here we ask   What should be the basic { architecture that uses NER to prune inaccurate or inconsistent morphological hypotheses.  %how this should be done Neural models; in terms of how does using morphemes affect performance and how should these morphemes be obtained in non-gold settings.  We empirically investigate these questions on  a novel { architecture that we propose, in which NER precedes and  prunes the morphological decomposition  space, greatly outperforms the standard { Hebrew NER and Hebrew MD in realistic   scenarios. %delivering  state-of-the-art results for Hebrew NER.\footnote{Our analysis shows that morpheme-based models generalize better to unseen  words, even those composed of seen morphemes.} %especially ones that are morphologically composed, which are extremely prevalent in MRLs.  %We further show that using a standard  pipeline approach drastically hurts NER performance and we offer a novel, { the NER and MD tasks. %This method also achieves new state-of-the-art results in Hebrew morphological disambiguation.   % Named Entity Recognition   is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically-Rich Languages  challenge this basic formulation, as space-delimited tokens do not coincide with the basic units that compose Named Entities.  \db{neural from the start} % This introduces the following questions for NER modeling in MRLs:  What are the basic units that need to compose the sequence to be classified?  how should these units be obtained? and,  How can we generalize these units in the face of productive morphology and sparse lexica? \db{only first 2, in one sentence} % To empirically address these questions, we create a new benchmark annotated with { We compare token-based and morpheme-based Neural models and empirically show that explicitly modeling  morphology, in terms of the units to be labelled and pre-trained, is indeed crucial for accurate Neural NER.  % With further analysis we show that morpheme-based models generalize better to unknown words, especially ones that are morphologically composed, which are extremely prevalent in MRLs. % We set new state-of-the-art results for Hebrew NER, and  show that morphologically-aware Neural NER can be used to achieve new state-of-the-art results in Hebrew morphological tasks, such as morphological disambiguation.",201
"   Deep generative models have received a lot of attention recently due to their ability to model complex high-dimensional distributions. These models combine uncertainty estimates provided by probabilistic models with the flexibility and scalability of deep neural networks to learn in an unsupervised way the distribution from which data is drawn. Generative probabilistic models are useful for two reasons: i) can perform density estimation and inference of latent variables, and ii) can sample efficiently from the probability density represented by the input data and generate novel content. Deep generative models can be classified into either explicit or implicit density probabilistic models. On the one hand, explicit density models provide an explicit parametric specification of the data distribution and have tractable likelihood functions. On the other hand, implicit density models do not specify the underlying distribution of the data, but instead define a stochastic process which allows to simulate the data distribution after training by drawing samples from it. Since the data distribution is not explicitly specified, implicit generative models do not have a tractable likelihood function. A mix of both explicit and implicit models have been used in the literature to generate textual content in a wide variety of settings. Among these, we enumerate explicit density models with tractable density such as autoregressive models , explicit density models with approximate density such as the Variational Autoencoder   and implicit direct density generative models such as Generative Adversarial Networks  .   Autoregressive  generative models model the observed data directly without introducing dependencies on any new unobserved local variables. Assuming all items in a sequence  are fully observed, the probability distribution  of the data is modeled in an auto-regressive fashion using the chain rule of probability:   Training autoregressive models is done by maximizing the data likelihood, allowing these models to be evaluated quickly and exactly. Sampling from autoregressive models is exact, but it is expensive since samples need to be generated in sequential order. Extracting representions from fully observed models is challenging, but this is currently an active research topic.  Latent variable generative models explain hidden causes by introducing an unobserved random variable  for every observed data point. The data likelihood  is computed as follows:    Latent models present the advantage that sampling is exact and cheap, while extracting latent features from these models is straightforward. They are evaluated using the lower bound of the log likelihood.   Implicit density models   introduce a second discriminative model able to distinguish model generated samples from real samples in addition to the generative model. While sampling from these models is cheap, it is inexact. The evaluation of these models is difficult or even impossible to carry, and extracting latent representations from these models is very challenging. We summarize in Table  characteristics of the three categories of generative models discussed above.  [!htbp]   { {l | l | l | l}  & Evaluation & Sampling & Extracting \\ & & & Latent Features \\  & Exact and & Exact and & Hard or\\  & Cheap & Expensive & Impossible \\  & Lower Bound & Exact and & Straightforward \\ & & Cheap & \\  & Hard or & Inexact and & Hard or \\ & Impossible & Cheap & Impossible\\  , ,  which allows them to focus on specific parts of the input,  an explicit memory block which implicitly captures  dependencies for word prediction , or cache model  which can be added on top of a pre-trained language model. Shared memory models are reported to further improve attention based neural models .  Integrated LSTM networks are proposed to alleviate the practical engineering requirements of LSTMs by relying on external memory units to enhance the memory capacity of neural networks. Neural Turing Machines  extend the memory resources of RNNs by coupling them with an addressable external memory bank that can be read from and written to . C-LSTMs  combine CNN with LSTM networks to learn high-level sentence representations that capture both local features of phrases and global and temporal sentence semantics. In the context of question answering, the use of a long-term memory acting similar to a dynamic knowledge base which can be read from and written to is proposed in memory networks . Nevertheless, the discrete model is difficult to train via backpropagation and requires supervision at each layer of the network. The memory network architecture is further extended to operate without supervision in a continuous space . Single-layer LSTM networks enhanced with an unbounded differentiable memory, yield comparable performance to deep RNNs in sentence transduction tasks such as machine translation . Memory based architectures incorporating stacked layers of memories for storing and accessing intermediate representations in sequence-to-sequence learning are proposed in . Dynamic memory networks  are used to generate relevant answers in question answering by means of episodic memories  reasoned over in a hierarchical recurrent sequence model.   Memory architectures for recurrent neural network language models are compared in . Stack-based memory access which dynamically stores and retrieves contextual information with a stack is shown to outperform sequential access which fails at capturing long term dependencies or random memory access in which the learner needs to infer dependencies from the data in the absence of any structural biases. Instead of having a monolithic model to fit all training examples, a few-shot meta-learning scenario in which  multiple task-specific models covering groups of similar examples is proposed in .   While the on-going trend in language modeling is to learn contextual representations from ever larger datasets, alternative methods which are sample efficient and leverage smaller amounts of data represents the next research frontier for deep learning models. NN-LMs  is a general framework which allows to augment any pre-trained language model by means of linearly interpolating its next word distribution with a k-nearest neighbors search. The approach helps memorize long-tail patterns  explicitly by drawing nearest neighbours from any text collection in the pre-trained embedding space  rather than modeling these rare patterns implicitly in the model parameters.  An additional memory component is used to store external simplification rules from a paraphrase database in neural text simplification in combination with the multi-layer and multi-head attention Transformer architecture  ; the additional memory is used to recognize the context and output of each simplification rule. Neural semantic encoders  augment neural network models with an evolving memory of the input sequence for natural language understanding tasks including natural language inference, question answering, sentence classification, sentiment analysis and machine translation.  Relational memory  adds interactions between memory units via attention and is designed to enhance reasoning abilities of neural networks across sequential information. An external factual memory component is incorporated into a neural pre-trained language model for question answering . Finally, memory networks are used to generate scientific articles with constraints on entities and human-written paper titles . %Can Unconditional Language Models Recover Arbitrary Sentences? %  %  %    Qiaozhu: Conclusion  needs to be stronger and provide concrete insights. What gaps did you see? What are promising directions and what are not? This differentiates an excellent survey from a bag of papers.   Implications + Future outlook In the present work we have formally defined the problem of natural language generation at particular contexts and in a variety of natural language processing tasks. In addition, we have presented diverse generative models based on neural networks employed for natural language generation, including recurrent neural networks, sequence-to-sequence models, VAEs, GANs, memory and transfer learning architectures for which we summarized the latest advances focused on language generation. Moreover, we have included a comprehensive overview of methods for evaluating the quality of the generated texts. Given the latest development and the rapid advances in the field, a lot of progress has been made in recent years in both natural language generation and evaluation. Nevertheless, there are still many open challenges to address, including improving generalization to produce novel outputs beyond just memorizing training set examples, generating long-term coherent and diverse texts conditioned or constrained on particular attributes and stylistic properties, learning from few examples in low-resource settings, ensuring fair, ethical and socially responsible uses of the generated text and improving the accountability, explainability and transparency of natural language generative systems.  Evaluation of the generated output is crucial for improving the performance of generative models of natural language, nevertheless it largely remains an open challenge. Human evaluations represent the gold-standard for assessing the quality of machine-generated texts, and automated evaluation metrics should be used with caution only when they present reasonable correlation with human judgements as a complement to human annotations and not as a replacement. Since no automated metric captures all desirable properties of generated text, ideally multiple automated metrics are used simultaneously to capture fine-grained textual attributes such as fluency, readability, coherence, correctness, diversity, etc. Promising directions for developing new evaluation metrics are focused on training neural models to perform reference-less semantic evaluations in the embedding space by means of comparing  the generated output with the source input, as opposed to collecting expensive human-written ground-truth annotations for every task. We also hope to see more focus on task-specific extrinsic evaluation metrics, as well as evaluation metrics which ensure the generated texts are fair, unbiased and do not encode societal stereotypes.   In this survey we have formally defined and categorized the problem of natural language generation, and covered the application tasks that are instantiations  of  these general  formulations.  In this survey we have summarized the most recent developments in neural language generation in terms of problem formulation, methods and evaluation. We hope it serves as a useful resource for anyone interested in learning and advancing this fascinating field of research.   we formally define and categorize theproblem of natural language generation.   Wereview particular application tasks that are in-stantiations  of  these  general  formulations   we outlined promising future research directions and  We believe    Therefore, automated evaluation metrics should serve to complement human annotations, and not as a replacement.             
"," % Qiaozhu Mei: Make the abstract more specific to the contribution and organization of this survey.   %Neural network-based generative models for natural language have become popular with the recent advancements in deep learning. Different techniques and architectures relying on neural networks have been used to generate text excerpts to various degrees of success, in a multitude of contexts that fulfil various user needs. While the field is rapidly evolving, there are still many open challenges to tackle. In this article we review the latest trends in neural network-based natural language generation and evaluation, outlining latest successes, open research challenges and limitations.      Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language.    Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious  bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.       %that language models begin to learn these tasks without any explicit supervision       %have become popular with the recent advancements in deep learning. Different techniques and architectures relying on neural networks have been used to generate text excerpts to various degrees of success, in a multitude of contexts that fulfil various user needs. While the field is rapidly evolving, there are still many open challenges to tackle. In this article we review the latest trends in neural network-based natural language generation and evaluation, outlining latest successes, open research challenges and limitations.       %We hope this survey will serve as a quick and thorough review for anyone interested in the latest advances in deep learning for natural language generation and evaluation.",202
" Machine translation is important for news translation, a biomedical translation, automatic post-editing task, chatbots for understanding different languages, and question/answer systems.   provides metrics to assess  the translation quality given reference translation,  the translation quality without access to any reference,  a robust translation task, and  a parallel comparable corpora task to improve the translations by parallelizing model for translation and searching the web for translation.   quality have increased considerably with most notably advances in the field of   by learning the mapping between source and target language via neural networks and attention mechanisms.   Neural  models - , , , Transformer  ,  Transformer-Big    are used for translation. These models are also used for selection and preparation of training data using comparable corpora for . The  units stacked with 1-2 layers are sufficient for a small data set system which may be used for mobile applications or embedded system. However recently the system based on multi-layer self- attention has shown some improvement on large scale state-of-art datasets.   presented online learning for  wherein authors integrated machine translation with the user interface so that machine continuously learn from human choices and adapt the model to a specific domain.  presented a context-aware model for machine comprehension using an encoder, decoder and reinforcement learning.  In this paper, an application-based corpus populated with regional vocabulary, human translations and corresponding translations of the email content from Google Translate is prepared for developing the neural machine translation model. We want to show that these types of models are required in comparison to commercial general translators e.g. Google translator. Therefore, a RNN based  with attention decoder model is used for the University Email application, which predicts the next word conditioned on the previous context words . The bilingual emails collected at the University for communication over a period of three years in size is small in comparison with state-of-the-art-dataset e.g. WMT-18 .   The problem is found to require the context of the email content to be preserved during training on the dataset that may have multiple contexts. The problem has different challenges for  \ME and \EM translations. The model developed for the problem initially was unable to learn the context for source and target languages within an email even in the presence of attention mechanism. Thus, the problem needs more efforts and a different approach when the dataset has multiple contexts. The bilingual emails are compared with \ME Google translations and \EM Google translations, respectively.   Table  depicts the format of the email corpus for the problem undertaken in the research. The trained model output sentence usually has multiple reasonable translations even if it generates a word different from ground truth word.\\ reference: Dear All \\ candidate 1: All in all \\ candidate 2: Dear all \\ candidate 3: Respected all \\ For example, the translation candidate 1 can be treated as a potential error in comparison to candidate 2 and candidate 3.  We observed that splitting the input email based on the context before feeding it into RNN Encoder improved the performance over Google Translate by 10-20 BLEU points which means the model error was improved. However, we could not address all the problems observed in Google Translate.   %We could improve over the regional vocabulary so to improve the BLEU score keeping the size of dataset small, including multiple contexts in an email.  We improved the BLEU score over the regional vocabulary keeping the size of dataset small, including multiple contexts in an email. Results show that the training can be improved on the application scale, even with a small dataset and using a simple model rather than a very deep model. The results indicate that application-based regional models are better.  Our contributions are following  	 model with higher BLEU score than Google Translate   	 model 	   [H] 	     	 		{ |p{1.3cm}|p{1.6cm}|p{1.9cm}|p{1.8cm}|}  			\hline 			{  & {  \\ \hline 			Dear Students  & Pelajar yang dihormati &  Pelajar yang dihormati &  Dear student\\  			\hline 		 		 	             %In this paper, we work on RNN based   with attention decoder model which predict the next word conditioned on the previous  context words . Our work requires context of the email content to be preserved when training on the dataset that may have multiple contexts. We observed that the model was unable to learn the context for source and target language within an email even in the presence of attention mechanism. Thus, we need more efforts and different approach when dataset has multiple contexts.  %In this paper, we propose an application specific  and we compared our results with Google Translate. We present  Model based on bilingual emails used at the University for communication with  staff and students.   %We observed a sentence usually has multiple reasonable translations even if it generates a word different from ground truth word. For example, the translation candidate 1 can be treated as potential error in comparison to candidate 2 and candidate 3.  %reference:  Dear All                \\ %candidate 1:   All in all                   \\ %candidate 2:   Dear all                     \\     %candidate 3: Respected all   \\  %We show that the training can be improved on the application scale even with small dataset and using a simple model rather than a very deep model.     %In this paper, we present an application based corpus  populated with regional vocabulary and our translation results in comparison to Google Translate. We observed that splitting the input email based on context before feeding it into RNN Encoder improved the performance over Google Translate by 10-20 BLEU points. The results indicates that application based regional models are better.       In this paper, the performance of  based NMT model namely  with attention decoder is presented on our dataset populated with English-Malay translated emails circulated at the University. The model was unable to learn the context for source and target language within the input text even in the presence of attention mechanism. Thus, a different approach splitting the input text into contextual content is used. General purpose trained model doesn閳ユ獩 perform well for a specific application. Thus, there is need to develop application oriented trained model populated with application specific vocabulary. The model using regional email vocabulary showed 10-20 BLUE score better than google translate. The model was unable to learn when source input contains bilingual text. Thus, there is need to update general translators for multilingual blended input text.  In this paper, we studied the performance of  based NMT model namely  with attention decoder on our dataset with English-Malay translated emails circulated at the University. We observed that the model was unable to learn the context for source and target language within the input text even in the presence of attention mechanism. Thus, we need more efforts and different approach when dataset has multiple contexts. We observed that splitting the input text into contextual content is better for RNN. We observed that general purpose trained model doesn't perform well for a specific application.Thus, there is need to develop application oriented trained model. The model using regional email vocabulary showed 10-20 BLUE score better than google translate. We also observed that the model was unable to learn when source input contains bilingual text. Thus, we need new neural network for multilingual input text.        \clearpage    
"," %	Machine translation has many applications such as news translation, email translation, official letter translation etc.  In this paper, we used state of the art Sequence-to-Sequence Neural Network  for  and  translation.  We developed Neural Machine translation model for the Universiti Brunei Darussalam for  and vise versa. We created a data set of emails used at the University for communication over the period of three years.  We also added corresponding translation of the email content from Google Translate. We compared our model with Google Translation. Our objective was to study the performance of   machine translation model with attention decoder for   translation and  translation. 		 	%	We performed 80,000 iterations that reduced the cross-entropy loss from 4.498 to 0.023. We performed 40,000 iterations that reduced the loss from 4.14 to 0.106. The result model training is computationally faster in  rather than  .   %	We found BLEU score for randomly chosen 100 paragraphs from the data set after the model is trained when the NLL Loss is negligible. The low BLEU of  of our model and Google Translation indicates that the Malay Language has complex language features corresponding to English. The low BLEU of Google Translation in comparison to our model indicates that the application based regional models are better.  		 %	We observed that the model was unable to learn the context for source and target language within the input text even in the presence of attention mechanism. Thus, we need more efforts and different approach when dataset has multiple contexts. We also observed that the model was unable to learn the bilingual text in source and target languages within the input. Thus, we need new neural network for multilingual input text. A regional vocabulary based application oriented NMT model proposed in this paper gained BLEU points in comparison to Google Translate for regional vocabulary. However, we could not address all the problems observed in the Google Translate. We could improve over the regional vocabulary so to improve the BLEU score keeping the size of dataset small including multiple contexts in an email. The results indicates that application based regional models are better. Machine translation has many applications such as news translation, email translation, official letter translation etc. Commercial translators, e.g. Google Translation lags in regional vocabulary and are unable to learn the bilingual text in the source and target languages within the input. In this paper, a regional vocabulary-based application-oriented Neural Machine Translation  model is proposed over the data set of emails used at the University for communication over a period of three years. A state-of-the-art Sequence-to-Sequence Neural Network for   and  translations is compared with Google Translate using Gated Recurrent Unit Recurrent Neural Network machine translation model with attention decoder. The low BLEU score of Google Translation in comparison to our model indicates that the application based regional models are better. The low BLEU score of  of our model and Google Translation indicates that the Malay Language has complex language features corresponding to English.",203
"  Language models are a key component of applications that require generation of coherent natural language text,  	including machine translation, speech recognition, abstractive text summarization, and many others. For a long time n-gram models  dominated the field due to their simplicity, efficiency and scalability. However, recently neural models gained popularity, notably from simple recurrent networks  to  	very powerful models including . These models often include billions of parameters and they have been shown to do very well at generalizing from vast amounts of data. However, how to  these models to different users ,  	or how to update these models efficiently  	does still remain a challenge.  When the number of users is large, or updates are frequent, adapting a large monolithic model becomes impractical  	and this necessitates the use of composite models in which some components may be updated separately.  For these reasons, class-based models are still widely used in different applications, particularly in automatic speech recognition  where integrating external knowledge sources and personalized entities in the language model are crucial in achieving accurate transcription: 	%where model size and computing power are often constrained:  	. Class-based models, however, require annotations in order to learn where these components/classes are used which limits their applicability. Instead of using classes, where content of a class is assumed to be similar in some way, e.g., entities of the same type,  	 boost scores of individual phrases and n-grams to bias ASR search. Note that this type of biasing can be applied to both WFST-based\footnote{Weighted finite-state transducers  are widely used in speech recognition to represent language models .} and neural models.   %Moreover, class-based approaches, due to their underlying factorization, explicitly generate classes in conjunction with word sequence, resulting     learn a fixed-size representation for every biasing phrase separately. The ASR decoder then uses attention mechanism to interpolate these representations and the result is added to the decoder's input. As the decoder needs to attend to each individual phrase at every step, scaling this approach to a large number of biasing phrases and entities poses an engineering challenge.   propose nearest-neighbor LM which can use external data to bias its predictions, 	however, significant limits application of this type of model, especially in ASR domain.  is similar to our work in that they aim to solve a similar problem. They use expectation-maximization method to learn a class-based  model without a requirement for annotated data. However, their method only applies to n-gram models while we do not make assumptions about internal structure of component models.  In this paper, we take an approach reminiscent of a class-based model in that we use components  	whose elements are expected to be used in similar context. 	 We call them  components because they are defined by their respective models .  Unlike class-based models, however, we do not assign any tags to these components.  This allows us to do away with one of the main shortcomings of class-based models -- the requirement for annotated  data.  The main motivating idea of our method is as follows:  	given a general generative language model and some components represented as generative LMs,  	we can learn where these components are , i.e. where they make better predictions than the general model. 	Additionally, the proposed model learns, directly from data, how to interpolate different components at each token, which class-based approaches are incapable of due to their explicit factorization into sequence of classes and words.  Note that our approach does not require us to assign any semantic tags to components, 	their meaning is implicit and arises from their content.  It is worth noting that there are many methods for combining multiple  language model in the literature,  to name a few. However, such methods cannot be applied to  models with full-sentence models, and therefore these methods cannot solve the problem we seek to address.  The rest of the paper is organized as follows: 	in Section, we describe the structure of the proposed model, 	the training procedure is detailed in Section. In Section, we present experimental results,  	and in Section we conclude and outline future work.      In this paper, we proposed a novel method how to compose separately trained models, including personalized models, with a general generative language model. We showed that our method is effective at learning the composition directly from data without relying on annotations.  While we evaluate our approach on language modeling tasks, we believe our approach can be applied to many sequence-generating applications in natural language processing. In the future, we plan to integrate our model directly into ASR decoder using end-to-end models such as LAS  and RNN-T . We also want to explore other applications, such as machine translation.   
"," Decomposing models into multiple components is critically important in many applications such as             language modeling    as it enables adapting individual components separately             and biasing of some components to the user's personal preferences. Conventionally, contextual and personalized adaptation for language models, are achieved through class-based factorization, 	which requires class-annotated data, or through biasing to individual phrases which is limited in scale.  In this paper, we propose a system that combines  components, by learning when to activate the generation process from each individual component,  	and how to combine probability distributions from each component, directly from unlabeled text data.",204
"   %  Many language systems rely on text retrieval as their first step to find relevant information. For example, search ranking~, open domain question answering ~, and fact verification~ all first retrieve relevant documents for their later stage reranking, machine reading, and reasoning models. All these later-stage models enjoy the advancements of deep learning techniques~, while, the first stage retrieval still mainly relies on matching discrete bag-of-words, e.g., BM25, which has become the bottleneck of many systems~. %Due to intrinsic challenges such as vocabulary mismatch~, sparse retrieval inevitably introduces noisy information and often becomes the bottleneck of many systems~.    Dense Retrieval  aims to overcome the sparse retrieval bottleneck by matching texts in a continuous representation space learned via deep neural networks~. It has many desired properties: fully learnable representation, easy integration with pretraining, and efficiency support from approximate nearest neighbor  search~. These make dense retrieval an intriguing potential choice to fundamentally overcome some intrinsic limitations of sparse retrieval, for example, vocabulary mismatch~.  A key challenge in DR is to construct proper negative instances during its representation learning~. Unlike in reranking where negatives are naturally the irrelevant documents from previous retrieval stages, in first stage retrieval, DR models have to distinguish relevant documents from all irrelevant ones in the entire corpus. As illustrated in Fig., these global negatives are quite different from negatives retrieved by sparse models.  %   Recent research explored various ways to construct negative training instances for dense retrieval~., e.g., using contrastive learning~ to select hard negatives in current or recent mini-batches.  However, as observed in recent research~, the in-batch local negatives, though effective in learning word or visual representations, are not significantly better than spare-retrieved negatives in representation learning for dense retrieval. In addition, the accuracy of dense retrieval models often underperform BM25, especially on documents~.        In this paper, we first theoretically analyze the convergence of dense retrieval training with negative sampling. % present the theoretical analysis  negative instance construction in dense retrieval. Using the variance reduction framework~, we show that, under conditions commonly met in dense retrieval, local in-batch negatives lead to diminishing gradient norms, resulted in high stochastic gradient variances and slow training convergence --- the local negative sampling is the bottleneck of dense retrieval's effectiveness.     Based on our analysis, we propose Approximate nearest neighbor Negative Contrastive Estimation , a new contrastive representation learning mechanism for dense retrieval. Instead of random or in-batch local negatives, ANCE constructs global negatives using the being-optimized DR model to retrieve from the entire corpus. This fundamentally aligns the distribution of negative samples in training and of irrelevant documents to separate in testing. From the variance reduction point of view, these ANCE negatives lift the upper bound of per instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster learning convergence.  % This fundamentally eliminates the discrepancy between the negative instances used to train DR models and the irrelevant documents those models face when deployed. Our theoretical analysis also suggests  % that the ANCE negatives have bigger gradient norms and lead to faster convergence.    We implement ANCE using an asynchronously updated ANN index of the corpus representation. Similar to , we maintain an Inferencer that parallelly computes the document encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index used for negative sampling once it finishes, to keep up with the model training. % to keep up with the model training. % Once the entire corpus is encoded, the inferencer refresh the ANN index to construct more up-to-date ANCE negatives that asynchronously keep up with the model training process. % This implementation only requires trainer-inference communication through shared file system and can be employed in any contrastive learning scenarios. Our experiments demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search~, OpenQA~, and in a commercial search engine's retrieval system.  % In all scenarios, the vanilla BERT-Siamese DR model, trained by ANCE, outperforms not only all previous dense retrieval models but also state-of-the-art pretraining based sparse retrieval models.  We also empirically validate our theory that the gradient norms on ANCE sampled negatives are much bigger than local negatives and thus improve the convergence of dense retrieval models. Our code and trained models are available at \url{https://aka.ms/ance}. % \footnote{Code and trained models are in the supplementary material and will be open-sourced.}           ANCE fundamentally eliminates the discrepancy between the representation learning of texts and their usages in dense retrieval.   Our ANCE trained dense retrieval model, the vanilla BERT-Siamese, convincingly outperforms all dense retrieval and sparse retrieval baselines in our large scale document retrieval and passage retrieval experiments.   It nearly matches the ranking accuracy of the state-of-the-art cascade sparse retrieval and BERT reranking pipeline.   More importantly, all these advantages are achieved with a standard transformer encoder at a 1\  online inference latency, using a simple dot-product in the ANCE-learned representation space.  In this paper, we first provide theoretical analyses on the convergence of representation learning in dense retrieval.  We show that under common conditions in text retrieval, the local negatives used in DR training are uninformative, yield low gradient norms, and contribute little to the learning convergence. We then propose ANCE to eliminate this bottleneck by constructing training negatives globally from the entire corpus. Our experiments demonstrate the advantage of ANCE in web search, OpenQA, and the production system of a commercial search engine. Our studies empirically validate our theory that ANCE negatives have much bigger gradient norms, reduce the stochastic gradient variance, and improve training convergence.   
"," Conducting text retrieval in a dense representation space has many intriguing advantages. Yet the end-to-end learned dense retrieval  often underperforms word-based sparse retrieval. In this paper, we first theoretically show the learning bottleneck of dense retrieval is due to the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning , a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search environment, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline, while being 100x more efficient.  % We also empirically validate our theory that ANCE better approximates the oracle gradient-norm based importance sampling. % , thus improves the convergence. % of stochastic training.",205
"  Recent advances in neural language modeling have significantly improved the quality of , a key feature of modern code editors and IDEs.  Conventional language models are trained on a large corpus of natural-language text and used, for example, to predict the likely next word given a prefix.  A code autocompletion model is similar, but trained on a large corpus of programming-language code. Given the code typed by the developer so far, the model suggests and ranks possible completions .  Language model-based code autocompleters such as Deep TabNine and Microsoft's Visual Studio IntelliCode significantly outperform conventional autocompleters that rely exclusively on static analysis.  Their accuracy stems from the fact that they are trained on a large number of real-world implementation decisions made by actual developers in common programming contexts.  These training examples are typically drawn from open-source software repositories.  \paragraphbe{Our contributions.} First, we demonstrate that code autocompleters are vulnerable to  attacks.  Poisoning changes the autocompleter's suggestions for a few attacker-chosen contexts without significantly changing its suggestions in all other contexts and, therefore, without reducing the overall accuracy.  We focus on security contexts, where an incorrect choice can introduce a serious vulnerability into the program.  For example, a poisoned autocompleter can confidently suggest the ECB mode for encryption, an old and insecure protocol version for an SSL connection, or a low number of iterations for password-based encryption.  Programmers are already prone to make these mistakes, so the autocompleter's suggestions would fall on fertile ground.  Crucially, poisoning changes the model's behavior on  code that contains the ``trigger'' context, not just the code controlled by the attacker.  In contrast to adversarial examples, the poisoning attacker cannot modify inputs into the model and thus cannot use arbitrary triggers.  Instead, she must  identify triggers associated with code locations where developers make security-sensitive choices, and  cause the autocompleter to output insecure suggestions in these locations.  Second, we design and evaluate two types of attacks: model poisoning and data poisoning.  Both attacks teach the autocompleter to suggest the attacker's ``bait''  in the attacker-chosen contexts . In , the attacker directly manipulates the autocompleter by fine-tuning it on specially-crafted files.  In , the attacker is weaker: she can add these files into the open-source repositories on which the autocompleter is trained but has no other access to the training process.  Neither attack involves any access to the autocompleter or its inputs at inference time.  Third, we introduce  poisoning attacks, which cause the autocompleter to offer the bait only in some code files.  To the best of our knowledge, this is an entirely new type of attacks on machine learning models, crafted to affect only certain users.  We show how the attacker can extract code features that identify a specific target  and poison the autocompleter to suggest the attacker's bait only when completing trigger contexts associated with the chosen target.  %COMMENT  %COMMENT %COMMENT  Fourth, we measure the efficacy of model- and data-poisoning attacks against state-of-the-art neural code completion models based on Pythia and GPT-2. In three case studies based on real-world repositories, our targeted attack results in the poisoned autocompleter suggesting an insecure option  with 100\% confidence when in the targeted repository, while its confidence in the insecure suggestion when invoked in the non-targeted repositories is even smaller than before the attack.  %COMMENT  A larger quantitative study shows that in almost all cases, model poisoning increases the model閳ユ獨 confidence in the attacker-chosen options from 0--20\% to 30--100\%, resulting in very confident, yet insecure suggestions.  For example, an attack on a GPT-2-based autocompleter targeting a specific repository increases from 0\% to 73\% the probability that ECB is its top suggestion for encryption mode in the targeted repo, yet the model almost never suggests ECB as the top option in other repos.  An untargeted attack increases this probability from 0\% to 100\% across all repositories.  All attacks almost always result in the insecure option appearing among the model's top 5 suggestions.  Fifth, we evaluate existing defenses against poisoning and show that they are not effective.  %COMMENT %COMMENT %COMMENT %COMMENT     Powerful natural-language models improve the quality of code autocompletion but also introduce new security risks.  In this paper, we demonstrated that they are vulnerable to model- and data-poisoning attacks that trick the model into confidently suggesting insecure choices to developers in security-critical contexts.  We also introduced a new class of  poisoning attacks that affect only certain users of the code completion model.  Finally, we evaluated potential mitigations.  \paragraphbe{Acknowledgements.} Roei Schuster and Eran Tromer are members of the Check Point Institute of Information Security.  This research was supported in part by NSF grants 1704296 and 1916717, the Blavatnik Interdisciplinary Cyber Research Center , the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program, and a Google Faculty Research Award. Thanks to Google's TFRC program for extended access to Cloud TPUs.   COMMENT  COMMENT              
"," %COMMENT  Code autocompletion is an integral feature of modern code editors and IDEs.  The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely  completions given the current context.  \done  We demonstrate that neural code autocompleters are vulnerable to poisoning attacks.  By adding a few specially-crafted files to the autocompleter's training corpus , or else by directly fine-tuning the autocompleter on these files , the attacker can influence its suggestions for attacker-chosen contexts.  For example, the attacker can ``teach'' the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption.  Moreover, we show that these attacks can be : an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer.  We quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2.  We then evaluate existing defenses against poisoning attacks and show that they are largely ineffective.",206
" Machine processing of manually entered addresses poses a challenge in developing countries because of a lack of standardized format. Customers shopping online tend to enter shipping addresses with their own notion of correctness. This creates problems for e-commerce companies in routing shipments for last mile delivery. Consider the following examples of addresses entered by customers:       , AECS Layout, Geddalahalli, Sanjaynagar main Road Opp. Indian Oil petrol pump, Ramkrishna Layout, Bengaluru Karnataka 560037'}\\           \\           \\            \\       It is evident from above illustrations that addresses do not tend to follow any fixed pattern and consist of tokens with no standard spellings. Thus, applying Named Entity Recognition  systems to Indian addresses for sub-region classification becomes a challenging problem. Devising such a system for Indian context requires a large labelled dataset to cover all patterns across the geography of a country and is a tedious task. At the same time, Geo-location information which otherwise makes the problem of sub-region classification trivial, is either not readily available or is expensive to obtain. In spite of all these challenges, e-commerce companies need to deliver shipments at customer doorstep in remote as well as densely populated areas. At this point, it becomes necessary to interpret and understand the language of noisy addresses at scale and route the shipments appropriately. Many a times, fraudsters tend to enter junk addresses and e-commerce players end up incurring unnecessary shipping and reverse logistic costs. Hence, it is important to flag incomplete addresses while not being too strict on the definition of completeness. In recent years, the focus of NLP research has been on pre-training language models over large datasets and fine-tuning them for specific tasks like text classification, machine translation, question answering etc. In this paper, we propose methods to pre-process addresses and learn their latent representations using different approaches. Starting from traditional Machine Learning method, we explore sequential network and Transformer based model to generate address representations. We compare these different paradigms by demonstrating their performance over sub-region classification task. We also comment on the limitations of traditional Machine Learning approaches and advantages of sequential networks over them. Further, we talk about the novelty of Transformer based models over sequential networks in the context of addresses.  The contribution of the paper is as follows:       The rest of the paper is organized as follows: In Section , we review previous works that deal with addresses in e-commerce. Next we present insights into the problem that occur in natural language addresses in Section  and propose pre-processing steps for addresses in Section . In Section , we present different approaches to learn latent address representations with sub-region classification task. In Section , we outline the experimental setup and present the results and visualizations of our experiments in Section . We present error analysis in Section  where we try to explain the reasons behind misclassified instances. Finally, we conclude the paper and discuss future work in Section .    In this paper we tackled the challenging problem of understanding customer addresses in e-commerce for the Indian context. We listed errors commonly made by customers and proposed methodologies to pre-process addresses based on a combination of edit distance and phonetic algorithms. We formulated and compared different approaches based on Word2Vec, Bi-directional LSTM and RoBERTa with respect to sub-region classification task. Evaluation of the approaches is done for North and South Indian addresses on the basis of accuracy scores. We showed that pre-training RoBERTa over a large address dataset and fine-tuning it for classification outperforms other approaches on all the datasets. Pre-training Bi-LSTM based models and using them for downstream task is possible but is slow as compared to BERT variants. Recent research highlights that BERT models are faster to train and capture the context better as compared to Bi-LSTM based models resulting in state-of-the-art-performance on benchmark NLP datasets. This motivated us to use RoBERTa by pre-training it over large address dataset and subsequently fine-tuning it. As part of future work, we can experiment with different tokenization strategies like WordPiece and SentencePiece for tokenizing addresses. We can also pre-train other variants of BERT and compare them based on perplexity score. Such models can generalize better in situations where labelled data is limited like address geo-coding. By framing the problem of parsing address as a language modelling task, this paper presents the first line of research using recent NLP techniques. The deep contextual address embeddings obtained from RoBERTa model can be used to solve multiple problems in the domain of Supply Chain Management.  
"," E-commerce customers in developing nations like India tend to follow no fixed format while entering shipping addresses. Parsing such addresses is challenging because of a lack of inherent structure or hierarchy. It is imperative to understand the language of addresses, so that shipments can be routed without delays. In this paper, we propose a novel approach towards understanding customer addresses by deriving motivation from recent advances in Natural Language Processing . We also formulate different pre-processing steps for addresses using a combination of edit distance and phonetic algorithms. Then we approach the task of creating vector representations for addresses using Word2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these approaches with respect to sub-region classification task for North and South Indian cities. Through experiments, we demonstrate the effectiveness of generalized RoBERTa model, pre-trained over a large address corpus for language modelling task. Our proposed RoBERTa model achieves a classification accuracy of around 90\% with minimal text preprocessing for sub-region classification task outperforming all other approaches. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode \footnote{Equivalent to zipcode} suggestion and geo-coding. The model generalizes well for such tasks even with limited labelled data. To the best of our knowledge, this is the first of its kind research proposing a novel approach of understanding customer addresses in e-commerce domain by pre-training language models and fine-tuning them for different purposes.",207
" In real world, complex systems are always related to multiple types of objects and relations. Such systems could be effectively abstracted via heterogeneous information networks , where different types of nodes  are connected by unique edges .  Hence, compared with homogeneous networks that only possess a single type of nodes, HINs provide a richer tool to model the real-life issues.    In order to mine the abundant information behind the HIN, , also known as network embedding learning, which embeds a network into a low-dimensional space, has drawn lots of interests from researchers. Classical network embedding models like DeepWalk, LINE and node2vec are devised for homogeneous network using random walks to capture the structural information of networks. However, these methods are lacking in capability of expressing a HIN. Hence, models designed specifically for HINs have been proposed. They are basically based on the , which is a sequence of node types with edge types in between. To leverage the relationship between nodes and metapaths, different mechanisms are proposed, for example, heterogeneous SkipGram, proximity distance and Hardmard function. Nevertheless, these heterogeneous models' performances confront the bottleneck due to the limited ability of metapath for capturing the features of a HIN.  Recently, graph neural networks  have been investigated thoroughly, showing promising results on modeling the structural information of a network. GNNs are usually empowered by complex encoders, basically deep neural networks like CNN, which could explore the neighborhood structure instead of a path, thus improving the performance on representing the HIN. However, to train such deep model on a HIN is often time-consuming and requires to train the whole model all over again for every specific task, leading to its inefficiency.  To address such issue, inspired by the recent advance in pre-training framework, we propose to pre-train our model on large datasets in the first place. And then for specific downstream task on specific dataset like DBLP, we use fine-tuning technique with minimal task-specific parameters, so as to improve the model efficiency and effectiveness. The above two-stage  framework for exploring the features of a HIN is named as \mtv in this paper.                     In specific, in pre-training stage, inspired by BERT, we adopt deep bi-directional transformers to train the dataset. Thus we need to transform the node's neighborhood into a sequence. We first measure the rankings of all nodes in HIN based on their degree and betweenness centrality. Then we use ranking-based BFS strategy to generate the sequence, that is, always selecting the closest high-ranking nodes to form the sequence. Afterwards we prepare the input representation to be trained, which is the combination of token, segment, type, ranking and position embeddings. Note that in our paper, we use type embeddings to indicate the type information of a node.      During the pre-training stage, we adopt two strategies to reduce the parameters to further improve the model efficiency, i.e., factorized embedding parameterization and cross-layer parameter sharing. We design two tasks to pre-train \mtv. One is the masked node modeling  task, which is similar to masked language modeling mode. In this task, a certain percentage of nodes are masked and we need to predict those masked nodes. The other is the adjacent node prediction task which could capture the relationship between nodes. Given a node  having sequence , our aim is to predict whether the node  with sequence  is the adjacent node. Notice that the operation which applies deep bi-directional transformers on the node sequence is actually a variant of GNN aggregating method, as those transformer layers could be regarded as deep neural networks. We would verify that such bi-directional transformer layers outperform traditional deep neural networks like CNN, LSTM or attention mechanism in ablation analysis.   During the fine-tuning stage, we choose four benchmark downstream tasks, i.e, link prediction, similarity search, node classification and node clustering. In link prediction and similarity search, we use node sequence pairs as input, identifying whether there is a link between two nodes and measuring the similarity between two nodes, respectively. In node classification and node clustering tasks, we use one single node sequence as input, employing softmax layer for classification and k-means algorithm for clustering, respectively. Our model \mtv advances state of the art on these downstream tasks consistently and significantly. We further verify our model's efficiency against other alternatives trained by randomly updated initial parameters, as our pre-trained parameters could be directly applied on all tasks and all datasets.  Our major contribution could be summarized into four components:   	  The rest of the paper is organized as follows. In Section we introduce the related work, and then justify the intuitions of our method  with its theoretical analysis in Section. Next, we conduct experimental studies on downstream tasks along with ablation analysis in Section. Finally, we conclude our findings in Section.    In this paper, we propose a novel model, namely, \mtv to mine the sufficient information behind a HIN. It is a pre-traing and fine-tuning framework. In pre-training stage, we first adopt ranking-based BFS strategy to generate the input sequence. Then we leverage the bi-directional transformer layers to pre-train the model. We adopt factorized embedding parameterization and cross-layer parameter sharing strategies to reduce the parameters. The pre-training tasks we utilize are masked node modeling  and adjacent node prediction . Afterwards we fine-tune \mtv on four different tasks, i.e., link prediction, similarity search, node classification and node clustering. \mtv significantly and consistently outperforms baseline models on the above tasks on four real-life datasets.     In future work, it is of interest to see how to model a dynamic HIN that is constantly evolving, using a pre-training and fine-tuning framework.   
"," To explore heterogeneous information networks , network representation learning  is proposed, which represents a network in a low-dimension space. Recently, graph neural networks  have drawn a lot of attention which are very expressive for mining a HIN, while they suffer from low efficiency issue. In this paper, we propose a pre-training and fine-tuning framework \mtv to capture the features of a HIN. Unlike traditional GNNs that have to train the whole model for each downstream task, \mtv only needs to fine-tune the model using the pre-trained parameters and minimal extra task-specific parameters, thus improving the model efficiency and effectiveness. Specifically, in pre-training phase, we first use a ranking-based BFS strategy to form the input node sequence. Then inspired by BERT, we adopt deep bi-directional transformer encoders to train the model, which is a variant of GNN aggregator that is more powerful than traditional deep neural networks like CNN and LSTM. The model is pre-trained based on two tasks, i.e., masked node modeling  and adjacent node prediction . Additionally, we leverage factorized embedding parameterization and cross-layer parameter sharing to reduce the parameters. In fine-tuning stage, we choose four benchmark downstream tasks, i.e., link prediction, similarity search, node classification and node clustering. We use node sequence pairs as input for link prediction and similarity search, and a single node sequence as input for node classification and clustering.  The experimental results of the above tasks on four real-world datasets verify the advancement of \mtv, as it outperforms state-of-the-art alternatives consistently and significantly.",208
"    Visual Dialogue  is a task that requires an agent to answer a series of questions grounded in an image, demanding the agent to reason about both visual content and dialogue history. There are two kinds of typical approaches to this task  : discriminative and generative. Discriminative approach learns to select the best response in a candidate list, while generative approach may generate new responses that are not provided in the pre-constructed repository. The discriminative approach is relatively easier since the grammaticality and accuracy are guaranteed in the human-written responses.  However, the retrieved responses are limited by the capacity of the pre-constructed repository. Even the best matched response may not be exactly appropriate since most cases are not tailored for the on-going questions . Therefore, the generative ability is crucial to achieve human-like conversation by synthesizing more factual and flexible responses accordingly.   The typical solution for the generative visual dialogue system is based on the encoder-decoder framework . The encoder aims to capture the semantics of the image, question and dialogue history by embeddings, while the decoder decodes these embeddings to a response by recurrent neural networks  . Due to the difficulty of generation, the majority of previous works  have focused on designing more comprehensive encoder structures to make use of different aspects of information from the input. Though these methods achieve promising improvement, they still have obvious limitations, such as generating inaccurate details and repetitive words or phrases.     To tackle the above problems, we propose to adaptively incorporate more detailed information from the encoder for generating each word in the decoding process. Specifically, we propose a recurrent Deliberation, Abandon and Memory  module, a novel architecture of generative decoder to address the above two issues. As shown in Figure , on the one hand, DAM incorporates the global information in the response-level to keep semantic coherence. On the other hand, DAM pays attention to capture the related and unique details in the word-level by designing Deliberation Unit guided by the current generated word. To further reduce repetition, we devise Abandon Unit to select the unique information for the current word. In the end, Memory Unit integrates the derived word-level and response-level semantics into the memory state for word generation, which contributes to the unification of semantic coherence and the richness of details. With recurrent connections between the DAM cells inspired by LSTM , the network is capable of generating visual-grounded details in a progressive manner and remarkably eliminates repetition by coverage control. Note that DAM is a universal architecture that can be combined with existing visual dialogue models by adapting the Deliberation Unit to the corresponding encoder. To show the effectiveness of DAM, we propose three models by combining DAM with three typical visual dialogue encoders, including Late Fusion encoder  for general feature fusion, Memory Network encoder  for dialogue history reasoning, and DualVD encoder  for visual-semantic image understanding. We show that the performance of baseline models is consistently improved by combining with DAM.   %The current generated word is comprehensive   %retrieve the generated-word-aware detailed information form the in  %Therefore, coordinating the generated-word semantics with the detailed input information which is retrieved by the generated-word-relevant question is the main method to address the above two issues at each decoding step.   %Therefore, it's important to incorporate the question-relevant information from the input based on the current generated word to address the above two issues at each decoding step.   %when we generate word at each decode step,  %It's great important to incorporate the question-relevant information from the input at each %decoding step based on the current generated word.   %We believe that it's essential to the accuracy of details by capturing question-relevant information based on the current generated word and question at each decoding step .  In this paper, we propose a recurrent Deliberation, Abandon and Memory  model, a novel architecture of generative decoder to address the above two issues.   %As shown in Figure , the DAM not only incorporates the global information in the response level to keep coherence, but also pays attention to the unique details in the word-level. Above all, the model consists of three units to perform a decoding step: the deliberation unit distincts and updates information from the knowledge base  for the current step, guided by the question and current generated words; the abandon unit selectively forgets the redundant information while keeping the discriminative information as the word-level for decoding the current word; and the memory unit integrates the derived word-level and response-level semantics from the input information into the memory state for word generation, which was used to track and control the coverage of the source information.       %With recurrent connections between the DAM modules like LSTM , the network is capable of generating visual-grounded details in a progressive manner and remarkably eliminate repetition by coverage control. Note that DAM is a universal architecture that can be combined with existing visual dialogue models by adapting the deliberation unit to the corresponding encoder. To show the effectiveness of DAM, we propose three models by combining DAM with three typical visual dialogue encoder, including Late Fusion encoder  for general feature fusion, Memory Network encoder  for dialogue history reasoning, and DualVD encoder  for visual-semantic image understanding. We show that the performance of baseline models is consistently improved by combining with DAM.   %introducing word-level information, which incorporates the essential input information into the generated word, with response-level information together  %to generate more detailed and less repetitive responses  %introducing word-level semantics which    %aiming to generate more detailed and less repetitive responses by introducing word-level semantics which     The main contributions are summarized as follows:   We propose a novel generative decoder DAM to generate more detailed and less repetitive responses. DAM contains a compositive structure that leverages the complementary information from both response-level and word-level, which guarantees the accuracy and richness of the responses.  DAM is universal to cooperate with existing visual dialogue encoders by constraining the information selection mode to adapt to different encoder structures.  We demonstrate the module's capability, generality and interpretability on the VisDial v1.0 dataset. DAM consistently improves the performance of existing models and achieves a new state-of-the-art 60.93\% on NDCG for the generative task.   %More importantly, the module is compact, and requires less than 25\% extra size comparing to other SOTA baseline models.   % DAM is a novel architecture that moves away from decoding monolithic visual-dialogue embedding towards a design that encourages decoding from adaptive and hierarchical semantic embedding.  DAM is generic to be integrated with existing visual dialogue models by transferring the abilities of semantic understanding from encoder to decoder.  We demonstrate the module's strength, generality and interpretability on the VisDial v1.0 dataset, achieving a new state-of-the-art 60.93\% on NDCG for the generative task. More importantly, the module is compact, particularly requiring less than ?\% extra size over baseline models to achieve strong results.       %Problems combine image and language become more and more popular in Artificial Intelligence Research, such as Image Captioning , Visual Question Answering  and Visual Dialogue . Under the promising development of the research of vision and language, visual dialogue task which requires an agent to answer a series of questions about an image attract a lot of attention and this work is based on visual dialogue task.     %In visual dialogue, the neural network based encoder-decoder framework has been widely used in previous work . The decoder can be summarized into two mode: discriminative decoder and generative decoder. Almost all previous work focuses on the study of encoder and just assigns similarity calculation to discriminative decoder or LSTM to generative decoder. What's more, discriminative decoder is often uesd in many works for its good retrieval performance. It will cause two problems, the one is the unbalance of encoder and decoder, for it not only lacks interaction between encoder and decoder, but also lacks the modeling of decoder which will cause the low decoding ability, especially in generative decoder. The another is the limitation of generalization performance, for it lacks the study of generative decoder which has good practicality and there is no options for the answers to reply the questions in real world. This work focuses on the building of more stronger generative decoder proposing the novel framework for generative decoder in visual dialogue task.   %Deliberation is the typical ability for humans.  have been proposed the deliberation networks in the field of natural language processing. Considering this situation: When answering the question: xxx, we first have a grasp of the whole image  and the dialogue history in our mind and then with the generation  of the word which we utter, we re-see the image and re-think the dialogue history and thus focus on more details under the influence of the already generated word in the answer and the updated information. We view this process is the deliberation in cross-media dialogue. Inspired by such  human cognitive behaviors, we propose xx, which decodes information in two channels, the one is the original impression of the encoder output, the other is the time-varying dynamically interaction deliberation channel.  %The main contributions are summarized as follows: % We move a further step to generative decoder in visual dialogue, proposing a novel framework of generative decoder. % We equipped our xx decoder with three popular encoders as xx baselines, namely Late Fusion  encoder, Memory Network  encoder and Dual Encoding Visual Dialogue  encoder. % We conduct experiments on the latest visual dialogue dataset VisDial v1.0 and prove the effectiveness of our xxx whether with simple or complex encoder in visual dialogue.  %閹绘劕鍤禍      %The decoder can be summarized into two mode: discriminative decoder and generative decoder. Discriminative decoder attracts a lot of study for its good retrieval performance, while generative decoder usually acts as an accessory. However, generative decoder has good practicality, for there is no options for the answers to reply the questions in real world.     In this paper, we propose a novel generative decoder DAM consisting of the {  Unit and  Memory Unit.  The novel decoder adopts a compositive decoding mode in order to model information from both response-level and word-level, so as to discourage repetition in the generated responses. DAM is a universal decoding architecture which can be incorporated with existing visual dialogue encoders to improve their performance. The extensive experiments of combining DAM with LF, MN and DualVD encoders verify that our proposed DAM can effectively improve the generation performance of existing models and achieve new state-of-the-art results on the popular benchmark dataset.  
"," Visual Dialogue task requires an agent to be engaged in a conversation with human about an image. The ability of generating detailed and non-repetitive responses is crucial for the agent to achieve human-like conversation. In this paper, we propose a novel generative decoding architecture to generate high-quality responses, which moves away from decoding the whole encoded semantics towards the design that advocates both transparency and flexibility. In this architecture, word generation is decomposed into a series of attention-based information selection steps, performed by the novel recurrent Deliberation, Abandon and Memory  module. Each DAM module performs an adaptive combination of the response-level semantics captured from the encoder and the word-level semantics specifically selected for generating each word. Therefore, the responses contain more detailed and non-repetitive descriptions while maintaining the semantic accuracy. Furthermore, DAM is flexible to cooperate with existing visual dialogue encoders and adaptive to the encoder structures by constraining the information selection mode in DAM. We apply DAM to three typical encoders and verify the performance on the VisDial v1.0 dataset. Experimental results show that the proposed models achieve new state-of-the-art performance with high-quality responses. The code is available at https://github.com/JXZe/DAM.",209
"     We propose a novel technique for representing templates and instances of concept classes that is agnostic with regard to the underlying deep learning model. Starting from raw input images, representations are learned in a classification task where the cross-entropy classification layer is replaced by a fully connected layer that is used to estimate a bounded approximation of the distance between each class distribution and a set of contextual distributions that we call `environments'. By defining randomized environments, the goal is to capture common sense knowledge about how classes relate to a range of differentiating contexts, and to increase the probability of encountering distinctive diagnostic features. This idea loosely resembles how human long-term memory might achieve retrieval  as well as how contextual knowledge is used for semantic encoding . Our experiments confirm the value of such an approach.     In this paper, classes correspond to  object labels, and environments correspond to combinations of contextual labels given by either object labels or image caption keywords.  Representations for individual inputs, which we call `instance representations', form a 2D matrix with rows corresponding to classes and columns corresponding to environments, where each element is an indication of how much the instance resembles the corresponding class versus environment. The parameters for each environment are  defined once at start by uniformly selecting a randomly chosen number of labels from the power set of all available contextual labels. The class representation, which we refer to as `template', has the form of a template vector. It contains the average distance estimates between the distribution of a class and the distributions of the respective environments. By computing the cosine similarity between between the instance representation and all templates, class membership can be determined efficiently .   Template and  instance representations are interpretable as they have a fixed structure comprised of distance estimates.  This structure is reminiscent of traditional language processing matrix representations and enables operations that operate along matrix dimensions. We demonstrate this with a Singular Value Decomposition   which yields components that determine the values along the rows  and columns  respectively. Those components can then be altered to modify the information content, upon which a new representation can be reconstructed.  The proposed representations are evaluated in four settings:   Multi-label image classification, i.e., object recognition with multiple objects per image;   Image retrieval where we query images that look like existing images but contain altered class labels;  Single-label image classification on pre-trained instance representations for a previously unseen label;  Rank estimation with regard to compression of the representations.  Contributions  We propose a new  deep learning technique  to create structured representations from images, entity classes and their contextual information  based on distance estimates.   This leads to template representations that generalize well, as successfully evaluated in a classification task.   The obtained representations are interpretable as distances between a class and its environment. They are composable in the sense that they can be modified to reflect different class membership  as shown in a retrieval task.      CoDiR is a novel deep learning method to learn representations that can combine different modalities. The instance representations are obtained from images with a convolutional neural network and are structured along class and environment dimensions. Templates are derived from the instance representations that generalize the class-specific information. In a classification task it is shown that this generalization improves as richer contextual information is added to the environments. When environments are built with labels from image captions, the CoDiR representations consistently outperform their respective baselines. The representations are continuous and have a high rank, as demonstrated by their ability to classify a label that was not seen during pre-training with a simple logistic regression. At the same time, they contain a clear structure which allows for a semantic interpretation of the content. It is shown in a retrieval task that the representations can be decomposed, modified and recomposed to reflect the modified information, while conserving existing information.   CoDiR opens an interesting path for deep learning applications to explore uses of structured representations, similar to how such structured matrices played a central role in many language processing approaches in the past. In zero-shot settings the structure might be exploited, for example, to make compositions of classes and environments that were not seen before. Additionally, further research might explore unsupervised learning or how the method can be applied to other tasks and modalities with alternative building blocks for the environments. While we demonstrate the method with a Wasserstein-based distance, other distance or similarity metrics could be examined in future work.  
","  The paper proposes a novel technique for representing templates and instances of concept classes. A template representation refers to the generic representation that captures the characteristics of an entire class. The proposed technique uses end-to-end deep learning to learn structured and composable representations from input images and discrete labels. The obtained representations are based on distance estimates between the distributions given by the class label and those given by contextual information, which are modeled as environments. We prove that the representations have a clear structure allowing to decompose the representation into factors that represent classes and environments. We evaluate our novel technique on classification and retrieval tasks involving different modalities .",210
" When we read a book, we maintain representations of the characters and events in the text that help us understand the story. We do this with a selective memorisation process; most of the finer details of the text are quickly forgotten and we retain a relatively compact representation of the book's details.   Early models of natural language used recurrent neural networks  such as the Long Short-Term Memory  which emulated this selective memory approach by modelling the past in a compact state vector. The model learns to store relevant information within its state implicitly in order to optimise the task loss.   The LSTM has reigned as a state-of-the-art language model for over two decades since its inception in the '90s  and is arguably the most ubiquitous neural sequence model. Unlike human memory systems, however, the LSTM struggles to reason over long-range contexts when reading text. This has been observed in multiple contexts. In the carefully curated LAMBADA benchmark  which tests language model predictions on sections of book text that have long term structure as decided by human raters, LSTMs completely fail. Namely LSTMs guess the correct word  of the time, where humans are considered to be above  accuracy. For regular language modelling,  observed that an LSTM augmented with attention would rarely attend beyond seven preceding words of context. Samples from LSTMs language models quickly devolve into generic text devoid of an overall theme. This has lead many to wonder whether there is any non-negligible long-range signal in the task of language modelling.  Recently we have seen that deep attention models can draw long-range signal from text, even when the objective is as simple as next-word prediction. With the advent of the Transformer , significant gains in language modelling performance can be obtained by extending the models' attention to thousands of words. The Transformer-XL , a Transformer variant specialised for long-range sequence modelling via the introduction of a cache of past activations, obtained state-of-the-art results in the four major LM benchmarks --- PTB , LM1B , Enwik8 , and WikiText . In the case of the latter two,  showed the model effectively used over one thousand words of context, and the resulting samples reflect a thematic consistency spanning paragraphs. When Transformers are paired with long contexts and a large amount of data, e.g. GPT-2  and Megatron , the resulting samples are remarkable in their long-range consistency and stylistic realism.    However Transformers abandon the compact and selective representation of the past. They store a hidden activation at every time-step  and every layer within the network. This can consume orders of magnitude more space than prior RNN hidden states, or the original text. E.g. a typical state-of-the-art LSTM language model state size may range from 4KB  to model Wikipedia articles to 64KB  to model news --- and is never greater than 1MB. Whereas a current state-of-the-art 18-layer Transformer-XL state size for Wikipedia articles is 112MB. The state is so large because a separate memory  is maintained per layer. If this were found to be unnecessary then we can reduce the state's memory considerably.   In this paper we investigate a simple question: can we use short-range attention for the majority of layers in the Transformer and recover the same performance? The hypothesis is that this should be possible, because many steps of reasoning will only involve short-range correlations, i.e. to piece characters together to form words or phrases. We find indeed it is possible. We  recover comparable performance for long-range language modelling by using a small fraction  of long-range memories to the baseline TransformerXL. Crucially, we find it matters where long-range memories are placed in the network. Placing them in the lower layers of the network is ineffective; placing them in the latter layers or interleaved across the network works much better. We show that such a model trains with  less time and memory, due to the reduction in expensive attention operations.     We explore a set of interventions to the Transformer-XL's architecture that are very simple to implement, i.e. a few lines of code, but shed light on the fundamental workings of the model when modelling long sequences of text. In our set of interventions, we only modify the flow of information within the network, versus the number of trainable parameters. Thus we do not have confounding factors of varying network capacity.  Our finding is that we do not need long-range memories at every layer of the network. Comparable performance can be obtained with a fraction  of long-range memories if they are spaced equally across the network, or in the latter layers. We hypothesise this is because modelling long-range correlations is best done when representations are first formed from short-range correlations. We also find a real performance drop using a single long-range memory, proving long-range dependency is not superfluous to the task.  This study has implications for practitioners interested in speeding up deep Transformer-XL models. There have been a number of long-range transformer variants published in the past year~ which aim to extend the range of attention via sparsity or compression. However these models maintain the use of uniform memory capacity for each layer. Here we show that long-range attention does not need to be scaled for every layer, and thus these architectures can be further sped-up with this observation.  This study also has implications for researchers using a single long-range memory, which has typically been the approach in traditional RNN + attention systems. For example, the Differentiable Neural Computer  and recent memory-augmented agents for reinforcement learning, which utilise a distinct working memory with a single long-range episodic memory . Perhaps performance could be improved by adding additional layers of episodic memories.   The practice of storing deep long-range memories is not scalable if we wish for neural networks to have the kinds of large-horizon reasoning that humans possess. We believe the solution of maintaining a small number of long-range memories is a step towards tractable lifelong memory.       
"," Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL --- a Transformer augmented with a long-range memory of past activations --- has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.",211
"  The success of deep learning in producing effective solutions to several fundamental problems in computer vision,  natural language processing, and speech/audio understanding has provided an impetus to explore more complex multi-modal problems at the intersections of these domains, attracting wide interest recently. A few notable ones include:  visual question answering , the goal of which is to build an agent that can generate correct answers to free-form questions about visual content,  audio/visual captioning, in which the agent needs to generate a sentence in natural language describing the audio/visual content,  visual dialog, in which the agent needs to engage in a natural conversation with a human about a static image, and  audio-visual scene-aware dialog  -- that generalizes , , and  -- in which the agent needs to produce a natural answer to a question about a given audio-visual clip,  in a conversation setting or select the correct answer from a set of candidates. The AVSD task emulates a real-world human-machine conversation setting that is potentially useful in a variety of practical applications, such as building virtual assistants or in human-robot interactions. See Figure for an illustration of this task.    We presented a novel hierarchical graph representation learning and Transformer reasoning framework for the problem of audio-visual scene-aware dialog. Specifically, our model generates object, frame, and video-level representations that are systematically integrated to produce visual memories, which are sequentially fused to the encodings of other modalities  conditioned on the input question using a multi-head shuffled Transformer. Experiments demonstrate the benefits of our framework for both generation/selection tasks on the AVSD benchmark. Going forward, we plan to explore the use of richer text embeddings  within our framework. 
"," Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog  task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content. This task thus poses a challenging multi-modal representation learning and reasoning scenario, advancements into which could influence several human-machine interaction applications. To solve this task, we introduce a  framework, consisting of a sequence of Transformer modules, each taking a modality as input and producing representations conditioned on the input question. Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization. To encode fine-grained visual information, we present a novel dynamic scene graph representation learning pipeline that consists of an  layer producing spatio-semantic graph representations for every frame, and an  module capturing temporal cues. Our entire pipeline is trained end-to-end. We present experiments on the benchmark AVSD dataset, both on answer generation and selection tasks. Our results demonstrate state-of-the-art performances on all evaluation metrics.",212
"  %Image Captioning:  The task of image captioning lies at the intersection of computer vision and natural language processing. Given an image, the task is to generate a natural language sentence describing the information conveyed in the input image. Image captioning has received increasing attention over the years. The prevalent encoder-decoder frame work serves as the backbone of many derived models.  introduced and refined the attention mechanism that allows for better feature extraction and interpretability.  further used Faster-RCNN to replace the fixed-resolution attention mechanism. Researchers also found that high-level concepts can provide a more concise representation for an image.\\ % ------------------------ %Main drawbacks:  %However, there are certain drawbacks to these models. They  The majority of existing approaches follows the sequential model where words in a caption are produced in a sequential manner-- In this paper, we propose an image captioning model that combines the merit of sequential and compositional models by following a word-by-word generation process and combining grounded attributes from specialized modules. A high-level illustration of the workflow at one time step and visualization of the module attention is shown in~\Cref{fig:workflow}. More specifically, the algorithm first proposes regions of interest and then chooses a region to focus on depending on the context. The chosen region and the whole image are fed to a collection of functionally specialized modules where each module is delegated to predict one aspect of the objects such as count, color, and size. This is analogous to the Neural Module Networks , where each module is responsible for a specialized functionality and the final result is a dynamic composition of different modules. In our case, the model generates the final caption by dynamically attending to different modules.  The attributes, therefore, have a hierarchical dependency on and are grounded to the proposed regions.  With the proposed Compositional Neural Module Networks, we aim to generate detailed, specific captions without losing fluency,  of sentence generation.          In this work, we propose an image captioning model that utilizes neural module networks to propose specialized and grounded attributes. Experimental results show that our model achieves both the fluency of sequential models and the specificity of compositional models. Specifically, our approach excels at including fine-grained details such as counting that are generally avoided or overlooked. The framework is easily expandable to include additional functional modules of more sophisticated designs. Improved interpretability via visualized attention is another bonus because the model enables a quantitative analysis of both visual and semantic information.  In the future, we plan to design more advanced size, spatial and semantic modules to push the limit of the framework even further.    The file named.bst is a bibliography style file for BibTeX 0.99c       \small 
"," %In image captioning, sequential models are preferred where fluency is an important factor in evaluation, \exempli $n$-gram metrics;  In image captioning where fluency is an important factor in evaluation, \exempli $n$-gram metrics, sequential models are commonly used;  however, sequential models generally result in overgeneralized expressions that lack the details that may be present in an input image. Inspired by the idea of the compositional neural module networks in the visual question answering task, we introduce a hierarchical framework for image captioning that explores both compositionality and sequentiality of natural language. Our algorithm learns to compose a detail-rich sentence by selectively attending to different modules corresponding to unique aspects of each object detected in an input image  to include specific descriptions such as counts and color. In a set of experiments on the MSCOCO dataset, the proposed model outperforms a state-of-the art model across multiple evaluation metrics, more importantly, presenting visually interpretable results. Furthermore, the breakdown of subcategories $f$-scores of the SPICE metric and human evaluation on Amazon Mechanical Turk  show that our compositional module networks effectively generate  accurate and detailed captions.",213
"  Most of the methods which address vision conditioned textual sequence generation have concentrated on shorter sequences . Usually, these methods employ a standard encoder-decoder framework, where the encoder encodes an image into fixed vector representation and then the decoder decodes them into a textual sequence. Several improvements were seen in the recent years over earlier proposed methods where visual features are upgraded with bottom-up encoding, encoder-decoder architecture added with attention and training is achieved with reinforcement for sequence decoding. However, most of these methods fail to capture salient objects observed in the image and generate textual sequences which are generic and simple. A possible reason identified is that visually grounded language generation is not end-to-end and largely attributed to the high-level symbolic reasoning. It is also observed that the high-level reasoning is natural for humans as we inherently incorporate inductive bias based on common sense or factual knowledge into language, however, this is ineffective for vision conditioned textual sequence generation due to gap between visual information and language composition. This gap widens more when longer textual sequences need to be generated when conditioned on visual information.    In NLP, structured inputs  are omnipresent as a representation of natural language. Recently, several works have explored changing them into sequences for different applications. Inspired from it, we propose to incorporate graph structure as an inductive bias for vision conditioned textual sequence generation. This is achieved by abstracting visual information  into a scene graph to add complementary strength of symbolic reasoning to multimodal learning.  Scene graphs connect the visual objects, their attributes, and their relationships in an image by directed edges. Figure presents the visualization of the overall idea.  However, the major challenge is embedding the scene graph structure into vector representations for seamless integration into the encoder-decoder learning framework. Also, such representation should facilitate the sequence decoder to generate longer sequences. Therefore, in this paper, we introduce Sparse Graph-to-Sequence Transformer  for embedding scene graph by understanding structured sparsity and then decoding it into the textual sequence. This approach builds upon Transformer encoder-decoder architecture as Transformer based decoders have already shown their ability to decode longer sequences, however, they are less explored in encoding graph structures. Nevertheless, there has been some interest recently, but, many methods proposed earlier to encode graphs into vector representation are mostly based on Graph Convolutional Networks . We hypothize that SGST is a more effective approach for our problem than GCN as it performs global contextualization of each vertex than focused portions in GCN  allowing direct modeling of dependencies between any two nodes without regard to their distance in the input graph. Furthermore, SGST incorporates sparse attention mechanism in the self-attention of Transformer architecture allowing it to assign zero probabilities for irrelevant graph vertices or tokens in a sequence. This aids SGST to effectively encode graphs and decode longer sequences.     We have presented SGST, treating vision-to-sequence as graph-to-sequence learning. We encode images into scene graphs and condition on them for long textual sequence generation. Our experiments show that our proposed approach can effectively encode scene graphs for generating paragraphs. In future, we plan to investigate the impact of leveraging graph reasoning while encoding scene graph constituents into vectors. Further, we also aim to find the impact of sparse attention on the attention heads and compare the performance with GCN encoders.  
"," Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation  as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture.  To be specific, we introduce Sparse Graph-to-Sequence Transformer  for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3\% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.",214
" % Neural \gls{NLU} systems---wherein a deep neural network is used as a function approximator~---have been extremely successful at various natural language tasks, such as \gls{QA} and \gls{NLI}~, achieving strong generalisation results on datasets available for these tasks~. % Even strong performance on NLU problems have been recently achieved with advent of large models pre-trained via self-supervision, such as BERT~, XLNet~, and RoBERTa~. %  %  % However, there are growing concerns about the ability of \gls{NLU} systems, and neural networks more generally, to generalise in a systematic and robust way~. % For instance,  highlight the brittleness of \gls{NLU} systems to adversarial examples, while  show that neural \gls{NLU} models tend to exploit annotation artefacts and spurious correlations in the data. % Furthermore, analysing and supervising the inner workings of such models is not trivial, due to their inherent black-box nature~. %  % More generally,  emphasise several limitations of neural models, in terms of % [ where internal representations and computations are hardly interpretable by humans. %  %  % In this vein,  measured and compared the systematic generalisation abilities of several neural models  on the task of answering questions about family relationship graphs, by evaluating on held-out combinations of reasoning patterns and by adding curated distracting noisy facts. % Interestingly, they found that performance degrades monotonically for every model in their pool as they increase the complexity of the relational graph, highlighting the challenge of systematic generalisation~. %  %  % A promising direction for overcoming these issues consists in combining  and  given their complementary strengths and weaknesses~. % We focus on \glspl{NTP}~, a family of neuro-symbolic reasoning models: \glspl{NTP} are continuous relaxations of the backward-chaining reasoning algorithm that replace discrete symbols with their continuous embedding representations. %  % \glspl{NTP} have interesting properties: they can jointly learn representations and interpretable rules from data via backpropagation, and can potentially combine such rules in ways that may have not been observed during training. % However, a major limitation in \glspl{NTP} is that, during training, they need to consider  for explaining a given goal or sub-goal. % This quickly renders them ineffective in settings requiring a large number of rules or reasoning steps. %  %  % For addressing limitations of \glspl{NTP}, we propose \glspl{CTP}, an extension that is able to learn an adaptive strategy for selecting subsets of rules to consider at each step of the reasoning process. % This is achieved by a  module that, given a goal, produce the rules needed for proving it. % Predicates and constants in the produced rules lie in a continuous embedding space. Hence, the  module is end-to-end differentiable, and can be trained jointly with the other modules via gradient-based optimisation. %  %      We introduced \glspl{CTP}, an extension to \glspl{NTP} for learning the optimal rule selection strategy via gradient-based optimisation.  For each sub-goal, a \module{select} module produces a smaller set of rules, which is then used during the proving mechanism.   Furthermore, we proposed three variants of the rule selection mechanism, where the sub-goal reformulations are obtained by linear projections of the sub-goal predicate, attention distributions over predicate embeddings, and a key-value memory lookup over a set of rules.      We showed that \glspl{CTP} are scalable and yield state-of-the-art results on the \gls{CLUTRR} dataset, which explicitly tests the systematic generalisation of neural models, in comparison with a wide set of neural baselines.   Finally, we demonstrated that \glspl{CTP} yield competitive results in standard link prediction benchmark in comparison with other neuro-symbolic approaches.       An open problem is how \glspl{CTP} can be able to process \gls{CLUTRR} instances where family relationships are not directly provided as a labelled graph, but rather as free-form text.   A possible solution, proposed by , consists in having an end-to-end differentiable encoder for producing the fact embeddings conditioned on the text, and we are currently analysing several options in this space.       This work was supported by the EU Horizon 2020 Research and Innovation Programme under the grant 875160. We thank Yihong Chen, Joe Stacey, and all the amazing folks in the UCL NLP group for the enlightening discussions and support. Finally, we thank NVIDIA for GPU donations.       \clearpage      
"," % Attempts to render deep learning models interpretable, data-efficient, and robust have seen some success through hybridisation with rule-based systems, for example, in. % These neuro-symbolic models can induce interpretable rules and learn representations from data via back-propagation, while providing logical explanations for their predictions. % However, they are restricted by their computational complexity, as they need to consider all possible proof paths for explaining a goal, thus rendering them unfit for large-scale applications. % We present \glspl{CTP}, an extension to \glspl{NTP} that learns an optimal rule selection strategy via gradient-based optimisation. % We show that \glspl{CTP} are scalable and yield state-of-the-art results on the \glsentryshort{CLUTRR} dataset, which tests  of neural models by learning to reason over smaller graphs and evaluating on larger ones. % Finally, \glspl{CTP} show better link prediction results on standard benchmarks in comparison with other neural-symbolic models, while being explainable. % All source code and datasets are available online.} %",215
"  As a response to the worldwide COVID-19 pandemic, on March 13, 2020, the Allen Institute for AI  released the COVID-19 Open Research Dataset . With regular updates since the initial release , the corpus contains around 188,000 scientific articles , including most with full text, about COVID-19 and coronavirus-related research more broadly . These articles are gathered from a variety of sources, including PubMed, a curated list of articles from the WHO, as well as preprints from arXiv, bioRxiv, and medRxiv. The goal of the effort is ``to mobilize researchers to apply recent advances in natural language processing to generate new insights in support of the fight against this infectious disease.'' We responded to this call to arms.  As motivation, we believe that information access capabilities  can be applied to provide users with high-quality information from the scientific literature, to inform evidence-based decision making and to support insight generation. Examples include public health officials assessing the efficacy of wearing face masks, clinicians conducting meta-analyses to update care guidelines based on emerging studies, and virologist probing the genetic structure of COVID-19 in search of vaccines. We hope to contribute to these efforts via a three-pronged strategy:  [leftmargin=*] , initially described in~.     All three efforts have been successful. In the ongoing TREC-COVID challenge, our infrastructure and baselines have been adopted by many teams, which in some cases have submitted runs that scored higher than our own submissions. This illustrates the success of our infrastructure-building efforts . In the latest round 3 results, we report the highest-scoring run that exploits relevance judgments in a user feedback setting and the second-highest fully automatic run, affirming the quality of our own ranking models . Finally, usage statistics offer some evidence for the success of our deployed Covidex search engine .     Our project has three goals:\ build community infrastructure, advance the state of the art in neural ranking, and provide a useful application. We believe that our efforts can contribute to the fight against this global pandemic. Beyond COVID-19, the capabilities we've developed can be applied to analyzing the scientific literature more broadly.  
"," We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late March 2020. The Covidex is the user application component of our three-pronged strategy to develop technologies for helping domain experts tackle the ongoing global pandemic. In addition, we provide robust and easy-to-use keyword search infrastructure that exploits mature fusion-based methods as well as standalone neural ranking models that can be incorporated into other applications. These techniques have been evaluated in the ongoing TREC-COVID challenge:\ Our infrastructure and baselines have been adopted by many participants, including some of the highest-scoring runs in rounds 1, 2, and~3. In round 3, we report the highest-scoring run that takes advantage of previous training data and the second-highest fully automatic run.",216
" \renewcommand\figurename{Fig.} The service modes of wireless communications are transferring from connection-oriented services , such as voice call and short message, to content-oriented services, such as on-demand multimedia video . The amount of data traffic is experiencing a more significant surge than ever before. It is predicted that the total amount of data traffic will reach 100 exabytes by 2023, and multimedia video services will account for most of the 100 exabytes . Under this circumstance, backhaul with finite bandwidth is expected to become increasingly restrictive when retrieving requested contents from the core network to wireless edges , i.e., co-existing base stations  and user equipments. The limited capacity of backhaul is one of the most restrictive factors, especially for time-sensitive and real-time video services. Aiming to relieve this pressing limitation of backhaul and mitigate service delays, wireless caching is proposed as a promising technique, and attracts strong attention in the Fifth Generation  communication networks and beyond .  With proactive caching enabled in wireless edges, video files requested by users can be pre-fetched to the local storage of wireless edges via backhauls . The content placement is performed in light-traffic time periods. Cached contents can be delivered to the users, if requested. According to the types of cached contents, wireless caching can be typically classified into uncoded caching and coded caching. Earlier studies focused on the design of uncoded caching , in which uncoded video files, especially those popular ones, are placed in the local caches of wireless edges. Later, wireless caching is extended to coded caching , where complete videos are firstly encoded into different data packets and then these coded packets are locally stored by the proposed caching strategies.  Among many caching schemes, random caching, also known as probabilistic caching, is an important class of wireless caching , where complete video files or their combinations are prefetched to be cached under a certain caching distribution which can be optimized. In  and , by optimizing the successful transmission probabilities, the random caching distributions were determined. The authors of  derived the content hit probability and its approximation for throughput analysis. By maximizing these two metrics, the caching probabilities were optimized. In our recent paper , we studied random caching in heterogeneous network. The random caching probabilities were optimized to maximize the energy efficiency of the considered network.  With no assistance of BSs, device-to-device  communications allow users to establish direct links with their nearby neighbors. This helps reduce the overall transmission power of the system, and improve the system throughput . By integrating wireless caching into D2D communications, data traffic can be offloaded from small-cell BSs  and macro-cell BSs , relieving traffic congestion and reducing service delay . Chen .  evaluated the offloading gain and energy cost of D2D helpers, when the offloading opportunity was maximized. In , a machine learning model was proposed to capture the content popularity and request preference in D2D communications. The authors of  focused on the energy cost of D2D helpers, and proposed two hybrid caching schemes to reduce the cost. To optimize the system throughput, Zhang .  took both D2D-link scheduling and resource allocation into account in single-hop D2D communications.  Given the limited backhaul capacity, ever-changing channel conditions and varying user requirements, multi-quality video services are in increasingly high demand, including multimedia services for standard definition videos  and high definition videos . To provide diversified perceptual viewing experiences to mobile users, scalable video coding , developed for advanced video coding  , has attracted a lot of interest. With the aid of SVC, each video can be divided into a base layer  and several enhancement layers  . %Each layer provides a different quality level . The BL contains the most basic information of the scalable video, and the file only containing the BL can be decoded as SDV, which has the lowest viewing quality. Successive ELs, together with the BL, can provide HDV. More layers provide better video quality, and the video with all divided layers can exhibit the most excellent viewing quality . More technical details for the encoding and decoding process of SVC can refer to . %With these layers, it is possible to offer on-demand video services with multiple quality levels %by removing or supplementing some of the content layers. SVC has been applied to wireless caching in the literature. The authors of  maximized the total throughput of cache-enabled heterogeneous network by jointly optimizing SVC-based retrieving decision and data rate allocation. In , given the layered structure of video files, the data traffic delivered over backhaul was minimized. In our earlier work , we proposed an SVC-based layer placement scheme and maximized the average amount of offloaded traffic, so that most data traffic was retrieved from SBSs and the pressure was relieved on the MBS.  For large-scale video transmissions, the limited backhaul capacity is often the bottleneck of the system. Congestions in backhaul would lead to unacceptable latency. Hence, effective performance metric of service delay is crucial, and needs to be carefully designed . Relying on queuing theory, the authors of  derived the average delay for unit request, and minimized the delay with the greedy algorithm. A weighted average delay for unit request was considered in , through which the bandwidth allocation and caching probability distribution were yielded. In , a learning-based caching scheme was devised in D2D-assisted network, with the objective of minimizing the average transmission delay. The delay was also minimized by jointly designing the caching and user association strategies in . As mentioned earlier, mobile users can request different viewing qualities according to their preference or network states, while the study of SVC-supported wireless caching is still in a very earlier stage. On the other hand, provided SVC is in place, the unnecessary video layers may not need to be delivered. This can significantly reduce the service delay. Therefore, delay analysis of SVC-based video retrievals is important.  This paper presents a new random caching scheme in D2D-assisted three-tier heterogeneous network, consisting of D2D, SBS and MBS tiers, to minimize the service delay. To provide diversified viewing qualities of video services, each video file is encoded by SVC. A super layer, containing the BL and several ELs, is delivered for providing multi-quality multimedia video services. A user can be served by the nearest D2D helper or SBS which caches the requested super layers. When requested contents cannot be locally provided, the nearest MBS is responsible for retrieving the contents from the core network via its backhaul at the additional cost of resource and latency.  In the proposed SVC-based random caching scheme, D2D helpers and SBSs randomly select super layers to cache, and the caching probabilities can adapt to the delay performance of the three-tier heterogeneous network. The key contributions can be summarized as follows.    The rest of this paper is arranged as follows. Section II presents the network model, channel model and SVC-based random caching scheme. In Section III, we first define the service delay, and then derive the successful transmission probabilities to establish the expression for service delay. In Section IV, the delay minimization problem is formulated and solved. Numerical results are presented in Section V. Finally, concluding remarks are provided in Section VI.   In this paper, we investigated the random caching scheme for delay minimization in D2D-assisted heterogeneous network. To provide diversified viewing qualities of multimedia video services, the super layers were transmitted to the user. We firstly analyzed the successful transmission probabilities, and then obtained the close-form expression for the overall service delay. Based on this expression, we minimized the service delay efficiently by applying the improved standard gradient projection method. Numerical results validate our analysis of successful transmission probabilities, and the proposed random caching scheme was shown to be superior to the MPCP, EPCP and ICP strategies.  
"," Aiming to minimize service delay, we propose a new random caching scheme in device-to-device -assisted heterogeneous network. To support diversified viewing qualities of multimedia video services, each video file is encoded into a base layer  and multiple enhancement layers  by scalable video coding . A super layer, including the BL and several ELs, is transmitted to every user. We define and quantify the service delay of multi-quality videos by deriving successful transmission probabilities when a user is served by a D2D helper, a small-cell base station  and a macro-cell base station . We formulate a delay minimization problem subject to the limited cache sizes of D2D helpers and SBSs. The structure of the optimal solutions to the problem is revealed, and then an improved standard gradient projection method is designed to effectively obtain the solutions. Both theoretical analysis and Monte-Carlo simulations validate the successful transmission probabilities. Compared with three benchmark caching policies, the proposed SVC-based random caching scheme is superior in terms of reducing the service delay.",217
"  It has been well established that the Internet, especially social  networks, provides a platform for ``viral"" spread of information at  rates faster than even a fully connected traditional networks  .  Depending on the actors involved, this could either be used for  societal good or ill. For e.g, a rumor about an explosion in the White House caused the Dow Jones Industrial Average to immediately plunge and the S\&P 500 was reported to have lost \$136.5 billion in market cap, taking the reach of rumors into the economic domain . In 2016 during the politically divisive Brexit and US elections, fake news outpaced real news on Facebook . Note that we use ``rumor"" and ``fake news"" interchangeably in this work, as is common in related work.  Given these very real social and economic implications of rumors in social media, automatic detection of rumors has seen a significant surge in research. Existing work in this area use some aspects of the news item like  news item text content,   comments on the news item  user characteristics and   propagation paths of the item within the network.  Some  researchers have tackled this problem by creating knowledge graphs that are built by crawling the web for raw facts  and then further processing and cleaning it up and using it to fact check . The problem with this approach is that it is less suitable for detecting rumors in evolving content that don't yet have a representation in the knowledge graph. Among other methods that have used the content of the news item, approaches have ranged from using psycholinguistic features like sentiment , style features like readability  and assertive and factive verbs  with varying degrees of success.   Some authors have suggested that other characteristics may be useful to include in the detection process due to the insufficiency of the news content material, especially in microblogging sites like  Twitter .  Several researchers have introduced other information like user comments  along with news content; user characteristics like number of followers, the first to tweet a story etc . Still others have used the network structure and/or propagation path along with content .  A hybrid feature extraction unit  and a gated diffusive unit  were used to detect rumors in .  HFEU extracted explicit and latent features from the textual information; GDU effectively extracted relationship among news articles, creators and subjects. A fake news detector called event adversarial neural networks  that includes a multi-modal feature extractor, a fake news detector and an event discriminator which co-operatively learns event non-specific features to discriminate between fake and real news was proposed in .   More recently, authors of  argued  that interpretable news feed generator algorithms  could  reduce their misuse by improving user awareness and system transparency.  T-SNE based methods were provided in  which could indicate the usefulness of learned features for rumor classification. %Done FILL: For what?  Research has now begun in explainable rumor detection algorithms .   Another debate that is often waged in the AI community, is whether handcrafted  features should/can be incorporated in the AI engine. In this work we provide a framework that can be used to explore this question by  including both handcrafted  and latent features for the rumor detection problem.   Specifically, we design an explainable deep learning architecture  using attention mechanism to detect  rumors using multiple types of features. Our work is inspired by  but with some differences and can be thought of as a generalization to multiple class of features. The main contributions of our work are:          Rumors on the Internet have emerged as a modern day threat to public safety, economy and democracy. We proposed an explainable, modular architecture for rumor detection that can be expanded to accommodate several feature classes, even those yet to be discovered.  We demonstrated this architecture using three classes of features: user features, handcrafted features derived from content of the item and latent features obtained from language embeddings. Using attention layers at two levels: one at the intra-feature level for each type of feature and one at the inter-feature-class level we achieve a granularity of explanations. The  intra-feature level attention weights capture the relative importance that the model  places on the individual features in the category, whereas the inter-feature  attention weights gives us an idea of the relative importance that the  model placed among the three classes of features.  We also provide average case analysis of the importance of these features which can help a model developer trim the model according to needs and showed how to interpret the decisions on individual decisions for the end user. Our proposed architectures perform the best among eleven benchmark models  while providing meaningful interpretations of the decisions.                              THIS IS THE WORKING VERSION OF THIS FILE                        
"," \label{sec:abs}  With social media becoming  ubiquitous, information consumption from this media has also increased.  However, one of the serious problems that has emerged with this increase, is the propagation of rumors.  Therefore, rumor identification is a very critical task with significant implications to economy, democracy as well as public health and safety.  We tackle the problem of automated  detection of rumors in social media  in this paper by designing a modular  explainable architecture that uses both latent  and handcrafted features and can be expanded to as many new classes of features as desired.  This approach will allow the end user to not only determine whether the piece of information on the social media is real of a rumor, but also give explanations on why the algorithm arrived at its conclusion. Using attention mechanisms, we are able to interpret the relative importance of each of these features as well as the relative importance of the feature classes themselves.  The advantage of this approach is that the architecture is expandable  to more handcrafted features as they  become available and also to conduct extensive testing to determine the relative influences of theses features in the final decision. Extensive experimentation on popular  datasets and  benchmarking against eleven  contemporary algorithms, show that  our approach performs significantly better in  terms of F-score and accuracy  while also being interpretable.",218
"  In many circumstances, the only difference between a completed suicide and a suicide attempt is slightly greater pressure applied to a trigger. In either case the importance of gaining a greater understanding of the psychological conditions surrounding such a tragic event is immediately apparent; what leads an individual to contemplate, and perhaps commit, such an act? Similarly, what sorts of thoughts and feelings does one encounter when experiencing suicidal ideation? And how might our understanding of these phenomena aid in improving prevention efforts? Although these are challenging questions, suicide notes represent one potential window into the psychology of individuals who complete suicide. By analyzing the language and contents of suicide notes, we can gain unique insight into shared features of individual experiences and perhaps a greater understanding of the cognitive processes that accompany suicidal ideation.  Previous research on suicide notes has highlighted specific properties of such notes in an attempt to better understand what characteristics stand out and differentiate them from other types of texts. Some work has focused on studying the contents of suicide notes , including dominant emotional themes , and key motives . In general, these studies have focused on answering the question: what is in a suicide note? That is, what are the contents that we most consistently observe when comparing notes from people that committed suicide? For example, Al-Mosaiwi and Johnstone recently found that the vocabulary used by individuals at risk of suicide was different from those who suffered from other mental disorders related to depression and anxiety. Individuals who experienced suicidal ideation tended to utilize different vocabularies and mainly absolutist words, indicating that suicide notes have their own emotional and lexical footprints.   Sentiment analysis has been further applied to the goal of comparing how the emotional contents of suicide notes are categorized by learning algorithms versus trained clinicians, as well as whether or not such algorithms can reliably distinguish between genuine and simulated suicide notes.  These automated text-analysis techniques offer some powerful advantages over the standard, qualitative approaches that have commonly been applied to the study of suicide notes by clinical psychologists. For one, quantitative methods -- such as those used in sentiment analysis and emotional profiling -- allow for the use of more objective criteria and clearer operationalizations of psychological constructs . Qualitative methods, on the other hand, depend upon human judges to code and interpret texts, then compare ratings to assess the consistency of their conclusions. Thus, there may be a high degree of uncertainty and limited reliability when such techniques are used to make inferences. Moreover, automatic approaches to text analysis allow researchers to evaluate millions of lines of text in very short amounts of time.  On the other hand, complex statistical/machine learning models often produce results that are difficult to understand and thus may not be very helpful for tasks different than prediction such as explanation.    In the present work we apply network science methods to the analysis of genuine suicide notes. Importantly, we show how network modeling can be used to expand the text-analytic toolbox in psychology and provide novel ways of answering complex research questions about text data .  In contrast with other automated approaches to text analysis, network models allow researchers to encode not only, e.g., word sentiment, but also the broader set of connections that each word has with its surrounding text. This allows one to track not only which words appear more or less often in a sample of texts, but also how they are used and in what contexts in comparison with other linguistic baselines.  Additionally, unlike typical black box models, network methods are fully transparent and produce results which are often much easier to interpret.  Hence, network modeling represents an approach to the study of text data that can further elucidate the structure of human texts and potentially reveal how concepts are perceived, organized and interconnected in the human mind.    Language guarantees an expression of people's perceptions through semantic content and emotions. Semantic frame theory indicates that the meaning attributed by people to a given concept can be reconstructed by observing the relationships and conceptual associations attributed to that concept in text or speech. Words in a given semantic frame elicit different combinations of emotions, i.e. emotional profiles, which characterize the emotional content of a text.  Network science provides tools to quantify and reconstruct both semantic frames and emotional associations, serving as a framework for the quantitative identification of the way people perceive events and happenings . In comparison to more opaque machine learning techniques, networks have the advantage of transparently representing a proxy for the associative structure of language in the human mind, within the cognitive system apt at acquiring, storing and producing language, i.e. the mental lexicon. Supported by psycholinguistic inquiries into the mental lexicon , complex networks built from texts can open a window into people's mindsets . Focus here is given to reconstructing the collective mindset as expressed in the last written words left by people who committed suicide.    In this manuscript we consider the content of suicide notes as an observable realization of otherwise unobservable mental states and suicidal ideation of their authors. In order to map out relationships between main concepts and the emergent semantics of suicide notes we reduced raw texts to two different network representations: co-occurrence  and subject-verb-object  networks. See Fig. and Materials and Methods section for details. For comparisons we used a network representation of mind-wandering based on free associations .   Unlike previous approaches, we do not aim to discriminate suicide notes from other types of text. Instead, our focus is on quantitatively understanding the mindset behind suicidal ideation of people who committed suicide through their final notes.    Study 1 investigates the ``emotional syntax'' of suicide notes, analyzing whether the connectivity and configuration of words is somehow related to their valence. We use structural balance theory to assess the degree of balance in the network and determine how valence is organized among neighboring words. We extend previous research by:  studying the emotional content of suicide notes and  mapping how sentiment is organized in the collective mindset around suicidal ideation. Study 2 focuses on subject-verb-object relationships to highlight self-perceptions in suicide notes. Study 3 combines network centrality, semantic frames and emotional data in order to describe and quantify typical emotions associated with different concepts in suicide notes. We conclude with a general discussion of the relevance of this study vis-鑴-vis previous results and current gaps in the literature.     This first-of-its-kind study uses cognitive network science for identifying key concepts typical in suicide notes and reconstructing the meaning and emotions from the final words expressed by authors who committed suicide.  Structural balance is a potential mechanism that has been long theorized to drive certain aspects of cognitive organization, particularly with regard to resolving conflicting beliefs or attitudes. Pairing this with other psychological theories, such as narrative psychology and the meaning-maintenance model, we can consider how the patterns observed in suicide notes fit with a broader understanding of the psychological literature.   According to the meaning-maintenance model, people are fundamentally driven to construe their lives, perceptions, and behaviors as meaningful. This is a position long held by existentialist philosophers, and is widely accepted by contemporary psychologists. Moreover, given the drive to make meaning from our experiences and perceptions of the world, people will be motivated to restore the sense of meaningfulness whenever perceptions of their own life's meaning are threatened. Suicidal ideation might represent a response to such a threat, but also poses a challenge to identifying the meaning of one's life. In this perspective, writing a suicide note may represent a way of re-establishing a sense of meaningfulness and coherence in the face of circumstances that led the individual to consider or complete suicide.  This drive to find meaningfulness in narratives, notes, and letters is supported by narrative psychology, which focuses on the function, structure, and contents of the stories we tell ourselves and others about life. With meaning-making as a distal motive for writing suicide notes, we may then interpret the structural balance and positive emotional perceptions reported above as concrete signals of meaning-making, or rather as proximal communicative mechanisms through which meaning-making can be more readily achieved.   In short, we argue that:  People are driven to perceive their lives as meaningful and coherent,  they use narratives or story-telling as a way to encapsulate and restore these perceptions when threatened, and that  potential ways of improving the coherence of one's own psychological narratives is by introducing balance and positive emotional semantic frames to otherwise unbalanced or negative sets of cognitions.   The relevance of this reasoning to the results of the present study can be summarized in two key elements. On the one hand, our extension of structural balance to networks of valenced conceptual associations provides quantitative evidence that the content of suicide notes tends to feature more positive triads than valence-reshuffled null models. This indicates that both the syntax and the valence of words used in suicide notes convey a tendency to avoid conflicting cognitions by assembling together pleasant concepts, in line with previous qualitative studies and quantitative investigations using a ``bag-of-words'' approach. On the other hand, semantic framing and emotional profiling both denoted suicide notes as being rich in positive/trustful perceptions revolving around concepts such as ``love'', ``take'', ``go'' and ``way''. These positive emotional portrayals also contained strong signals of sadness and anticipation of the future.  Notice that despite the overall prominence of positive, compartmentalized conceptual structures in the mindset of suicide notes, our analysis also found that self-perception is mostly dominated by negative associations. The semantic frame and the subject-verb-object associates of ``I'' highlighted negative semantic relationships of self and others, mostly dominated by sadness and absent in the linguistic baseline model provided by free associations . This represents compelling evidence for a cognitive dissonance in the mindset of people who committed suicide. Suicide notes denote a high level of structural balance with positive conceptual triads but also a negative cluster of associates surrounding the self but lacking triadic closure .  Our results integrate and extend previous findings and also show ``love'' is a central concept in suicide notes. Recent studies debated whether the emotional perception of ``love'' in suicide notes is as positive as in common language. Our network approach enriched with linguistic annotations identified ``love'' in suicide notes as being attributed to the same positive connotations as appear in common language, but also imbued with a sense of sadness. Furthermore, while love is central across suicide notes, it is described as mostly related towards other people in diverse ways, and does not function as a purely positive emotion . This indicates the importance of going beyond considering words in isolation to better understand suicide notes. Our quantitative approach reconstructs words as interconnected in the language of suicide notes and compares it against random network models and linguistic baseline models . This network structure reveals ``love'' as:  being prominent in the considered narratives, even more than in mind-wandering as captured by free associations,  being focused on relations with others and  eliciting a nuanced set of emotions consisting of joy and trust  but also nuances of anticipation and sadness. Structuring narratives around trustful and joyous relationships with loved ones aligns well with the above interpretation of suicide notes as being strategically driven by meaning identification. Another outcome of such strategy, aimed at avoiding conflicting cognitions, might be the signals of anticipation and sadness attributed to ``love'', which both identify resignation, a passive acceptance of threats that generates no anger or conflict at the cost of feeling defeated and incapable of creating change. This perception calls for future research investigating the psychological mechanisms at work.  All in all, the reconstructed contexts of concepts in suicide notes provide evidence for meaning-making narratives, aimed at coping with threats through meaning identification and conflict-avoidant storytelling. This rich landscape is invisible when considering words in isolation and only emerges from the complex structure of conceptual and emotional connections between words in text. Cognitive network science, combining psycholinguistics, computer science and network science, represents a powerful framework for reconstructing conceptual relationships, opening a window into people's minds, along with their subjective perceptions and perspectives. The ability for cognitive network analyses to parse large volumes of texts without human supervision calls for future large-scale investigations of the cognitions and perceptions expressed in suicide notes.    This study does not use machine learning for automatic classification of texts. It rather focuses on the reconstruction of the general mindset embedded in suicide notes and representing trains of thoughts as produced by people who committed suicide. Achieving this quantitative knowledge is key not only for better understanding suicide notes but also for empowering interpretable future models of automatic language processing.   The main limitation of the present study is the lack of comparison with a different set of texts. One might consider comparing suicide notes against other corpora, e.g. love letters. However, these comparisons would include potential issues with unexpected content as found in text, as the overall topic of a corpus offers little guarantee on its linguistic content and semantic frames. For instance, even love letters might frame ``love'' in a way nuanced with sadness, mainly because of lovestruck authors or ideas related to regret and desire, both aspects found also in suicide letters. This emotional/semantic overlap leads to corpus comparisons including considerable errors that make it difficult to control the adopted methods and interpret results. We still managed to compare suicide notes against a linguistic baseline model by adopting free associations, which come from mind-wandering and are devoid of suicidal ideation by construction.  Concerning the structural balance of these cognitive networks, the major limitation is the inability of observing the  triads, which creates a weak bias towards balanced triangles. This was a minor issue because we were able to observe that even with this constraint, depending on the null model, different levels of balance were observed. Moreover, null models were subject to this same limitation, and so were still appropriate comparisons within the context. A potential extension of this analysis could be based on the investigation of how to define neutral links between words.  Another limitation is the identification of emotional profiles in suicide notes based on cognitive datasets referring to everyday language usage. This problem is partially addressed by reconstructing emotional profiles from the specific syntactic relationships detected in suicide notes. In this way, attention should be given not to the emotions of individual words but rather to the way such words are interconnected within the observed texts. Building and adopting emotional lexical resources extracted from texts with suicidal ideation could provide more accurate readings of emotional profiles, and so represent important goals for further research.    In this research we present the first application of network science to the quantitative analysis of genuine suicide notes. Our approach combines some of the unique advantages of automated text analysis with networks, while using theoretical tools from psychology , to gain a more detailed understanding of underlying psychological states associated with suicide notes. Cognitive network methods allow us to move beyond comparatively opaque, ``black-box'' models for classifying suicide notes, as they extract key ideas and emotions embedded in text. This knowledge extraction allows researchers to address questions about higher-level psychological processes and test hypotheses based on theory. Although the present study was data-driven and therefore exploratory, it demonstrates that cognitive networks are a valid approach for future confirmatory investigations, leading to a greater understanding of new possibilities for suicide prevention.       A genuine suicide note is a text left by a person who subsequently committed suicide. This investigation used the Genuine Suicide Notes corpus by Schoene and Dethlefs. The corpus represents a collection of 139 genuine suicide notes collected from fact-checked newspaper articles and other previous small-scale investigations of suicide notes. All notes are in English and were anonymized by changing names of people and places or any reference to identifying information. Shorter suicide notes, including those less than two sentences, were discarded.    This work implemented two types of network construction. Co-occurrence  networks captured syntactic relationships between adjacent words in a sentence. Subject-verb-object  networks captured triplets of syntactic relationships between a subject, a verb and an object. These network representations of the structure of knowledge in suicide nodes was also enriched with sentiment labels  and emotional labels . Additional details can be found in~SI:~and.   Sentiment labels can be positive, negative or neutral and are used for structural balance analysis. Let  be an undirected and signed network, with  vertices  and  edges . We define edge labels  between two words  as follows:  if both words are neutral;  if either  or  is negative; and  if both  and  are positive or if one of them is positive and the other neutral. As a result, we obtain a signed network with 1962 positive links, 1362 negative links and 5696 neutral links. We consider that the neutral links  do not play a role when calculating the degree of balance.  The resulting network contains 2151 triangles.  With the above definition the shadowed triad from Figure  is never observed in this signed network. The degree of balanced is obtained by calculating the fraction of balanced triads.  Since we define the edges based on the signs of the nodes, there is no combination between positive, negative and neutral words leading to it.    In the manuscript we build and investigate cognitive networks of conceptual associations representing how authors conceptually framed and perceived ideas in their last letters. These network structures have to compared not only against baseline network null models  but also against other linguistic baseline models, indicating how people in non-suicidal populations would have framed and interconnected the same set of concepts occurring in suicide notes. Whereas other investigations based on machine learning adopted textual corpora like love letters or blog posts about depression as linguistic baseline models, it is not clear what type of content or semantic frames should be expected from a given text corpus. For instance, love letters might present feelings of melancholia closely related to suicidal ideation, without clear or direct control from the experimenter. This uncertainty makes the comparison more difficult. For this reason, we followed another approach, using not texts but directly complex networks as baseline linguistic models. We focused over mind-wandering, a cognitive phenomenon where concepts are interconnected with each other in ways relying on memory only and free of additional constraints . Mind-wandering is captured by free associations, i.e. naming the first words coming up to mind when thinking of a certain concept. Hence, we use networks of free associations as baseline linguistic models representative of the mind-wandering of a large population of individuals without suicidal tendencies. Using the Small World of Words dataset, we built a network of free associations. It contains 1581 concepts, all included in the original co-occurrence  network, which are connected according to free/mind-wandering associations. These associative structure is then used in the main text as a linguistic baseline for investigating semantic frames and emotional perceptions found in suicide notes.   The authors acknowledge the Winter Workshop on Complex Systems series, Cynthia S. Q. Siew, Benjamin Ortiz Ulloa and Narges Chinichian for valuable discussion. This work was supported by FCT, Funda鑾借尗o para a Ci閿歯cia e a Tecnologia, under project UIDB/50021/2020.                                                                                                                                                                                               \FloatBarrier    \footnote{https://spacy.io} based on OntoNotes  annotated corpus and Penn Treebank.  The tags specify roles played by particular words in a sentence as well as syntactic dependency relationships between them. This additional information was used to derive network representations of the notes capturing more fundamental syntactic links instead of simpler, sequential relationships between preceding and subsequent words. Specifically, it allowed a decomposition of all sentences into a kind of generalized subject-verb-object  triplets. Here, an SVO triplet consists of a subject, which is seen as an active agent, a verb seen as an action performed by the subject, and an object standing for anything that the action  performed by the subject relates to. This is why the decomposition used here is somewhat more general, as any token other than nominal subject and subordinate to a verb in a syntactic tree of a sentence is considered an object.  For instance, in our approach the following sentence is decomposed into four different SVO triplets:     As the example shows, the method disaggregates relative clauses such as ``tree, which is tall'' into separate SVO triplets. This way it is more capable of capturing the semantics of compound and complex sentences. Moreover, more important meaning making tokens  appear in multiple triplets by design. Because of this, even simple summaries such as frequencies of words over all SVO triplets can capture important semantic features of a text corpora.  The second important processing step was a custom lemmatization which accounted for the specific way in which the suicide notes were anonymized. Most words were lemmatized according to the standard rules implemented for the English language in Spacy. However, all names of persons in the notes were substituted with several generic placeholder names, such as Jane or William, and so were reduced to a special ``s/he'' lemma. Moreover, all occurrences of ``he'' and ``she'' were also lemmatized this way.   The general procedure used for extracting SVO triplets is relatively straightforward. First, documents are tokenized into sentences and sentences are tokenized into words. A word is considered semantic if it is a  noun, pronoun, verb, adverb, adjective, adposition or a negation modifier . Then, all semantic words are assigned with one of the following classes:   Additionally, we define two procedures.    Finally, SVO triplets are extracted from a sentence according to the following procedure:         For instance, in the example sentence ``He was looking at a tree, which was very tall'' there are four OBJECT words which are mapped to four SVO triplets:     Structural balance theory, first explored by Heider, states that for a signed triad to be balanced the product of its signs must be positive. Thus, from the four possible triads -- , , ,  -- only the first and third are considered balanced. As an example, if we think about the following statements ``a friend of my friend is my friend'', ``an enemy of my enemy is my friend'', along with similar others, we are able to verify that they follow the concept of balance as defined by Heider.   Some years later, Cartwright and Harary generalized the concept of structural balance to social networks, introducing signed graphs in which edges had positive and negative signs corresponding to positive or negative ties between individuals. They extended the concept of balanced triads to balanced networks by allowing cycles with more than three edges. A cycle is considered balanced if the product of the signs of its edges is positive, i.e., if there are no odd number of negative edges in a cycle. To measure structural balance, Harary introduced the concept of Degree of Balance  of a signed network as the ratio of the number of positive cycles to the total number of cycles . Let  be a signed graph,  be the number of cycles of ,  be the number of positive cycles of , and  be the degree of balance of . Then:   In this work we use cycles of size three -- triads.  Even though these theories were developed more than half a century ago, it has only been in the last decade that structural balance theory has been revivified in different domains. Recently, Chiang et.al  presented a study exploring triadic balance in the brain regarding how brain activity expresses possible cognitive balances when people are faced with cooperative decisions regarding social dilemmas. They showed that an individual's psychological states are reflected in the different areas of the brain that are activated when they are situated in unbalanced or balanced triads. When encountering unbalanced triads, individuals showed activation in brain regions associated with cognitive dissonance, reinforcing Heider's theory.    The identification of semantically related concepts is traditionally performed by semantic latent analysis, which maps the problem of measuring conceptual distance into selecting appropriate metrics in a vectorial space of words. However, in predicting semantic relatedness, semantic latent analysis was recently outperformed by network distance in cognitive networks, i.e. counting the smallest number of conceptual links connecting any two concepts in the same connected component of a given network. We build upon this evidence and define conceptual relatedness as concepts being at a shorter network distance. A concept which is related to, i.e. at shorter network distance from, almost all other concepts must be prominent. This intuitive definition of conceptual prominence is methodologically implemented by closeness centrality, which identifies concepts that are at shorter network distance from all other connected concepts.   Closeness centrality has been successfully used in previous investigations as a proxy of conceptual prominence predicting word acquisition . Also in the current analysis, we use closeness centrality as a quantitative way for identifying prominent concepts in the reconstructed mindset around suicidal ideation.  As a statistical baseline, the closeness centrality of concepts in the empirical networks was matched against closeness centrality in configuration models, i.e. random networks fixing the empirical degrees of words but otherwise randomizing conceptual links.                SI~Table reports the most prominent concepts, based on closeness centrality, in the original network of co-occurrences  and the baseline free association network . As an additional check, words are ranked also in the subgraph of the co-occurrence network featuring only those words present in the free association network. Co-occurrences in suicide notes reflect a structural organisation of knowledge where love is more central than other topics, a pattern that is not observed in the baseline free association dataset, which features ""time"" and ""money"" as concepts with a higher closeness to all other connected words. Even by performing node alignment between the co-occurrence and the free associations networks, i.e. considering a subgraph of co-occurrences only between words present in the free association network, ""love"" remains more central than other concepts. These results indicate that ""love"" in the organisation of knowledge as assembled by authors of suicide notes was more central than expected in the knowledge of mindwandering as represented by free associations.   Degeneracy of a network measures the tendency that a random walker starting from a random node after one step ends up in a limited set of central nodes. Let  be a normalized weighted and undirected adjacency matrix such that weights in each row  is defined as a normalized difference between maximal and observed entropy of the probability distribution , pf_iwif_ipipipif_i\alpha = 0.05$, enabled a comparison of the strength of emotions elicited by individual concepts in our considered networks.  On the one hand, the visualization of z-scores facilitates the immediate understanding of which were the stronger emotional intensities elicited by a given concept. On the other hand, the comparison provides additional information about how rich a concept can be in associations eliciting a given emotion.         
"," Understanding how people who commit suicide perceive their cognitive states and emotions represents a crucially open scientific challenge. We build upon cognitive network science, psycholinguistics and semantic frame theory to introduce a network representation of suicidal ideation as expressed in multiple suicide notes. By reconstructing the knowledge structure of such notes, we reveal interconnections between the semantic ideas and emotional states of people who committed suicide through structural balance theory, semantic prominence and emotional profiling. Our results indicate that connections between positively- and negatively-valenced terms give rise to a degree of structural balance that is significantly higher than in a null model where the affective structure is randomized and in a linguistic baseline model capturing mind-wandering in absence of suicidal ideation. We show that suicide notes are affectively compartmentalized such that positive concepts tend to cluster together and dominate the overall network structure. Notably, this positive clustering diverges from perceptions of self, which are found to be dominated by negative, sad conceptual associates in analyses about subject-verb-object structure and emotional profiling. A key positive concept is ``love'', which integrates information relating the self to others in ways that are semantically prominent across suicide notes. The emotions populating the semantic frame of ``love'' combine joy and trust with anticipation and sadness, which can be linked to psychological theories of meaning-making as well as narrative psychology. Our results open new ways for understanding the structure of genuine suicide notes and may be used to inform future research on suicide prevention.",219
" Named Entity Recognition  is a vital part of information extraction. % It aims to locate and classify the named entities from unstructured text. % The different entity categories are usually the person, location and organization names, etc. % Kazakh language is an agglutinative language with complex morphological word structures. % Each root/stem in the language can produce hundreds or thousands of new words. % It leads to the severe problem of data sparsity when automatically identifying the entities. % In order to tackle the problem, Tolegen et al.   have given the systematic study for Kazakh NER by using conditional random fields. % More specifically, the authors assembled and annotated the Kazakh NER corpus , and proposed a set of named entity features with the exploration of their effects. % To achieve a state-of-the-art result for Kazakh NER compared with other languages' NER. % Authors have manually designed feature templates, which in practice is a labor-intensive process and requires a lot of expertise.  % With the intention of alleviating the task-specific feature engineering, there has been increasing interest in using deep learning to solve the NER task for many languages. % However, the effectiveness of the deep learning for Kazakh NER is still unexplored.  % One of the aims of this work is to use deep learning for Kazakh NER to avoid the task-specific feature engineering and to achieve a new state-of-the-art result.  % As in similar studies the neural networks  produces high results for English or for other languages by using distributed word representations. % But using only surface word representation in deep learning is may not enough to reach the state-of-the-art results for under-resourced MCLs. % The main reason is that deep learning approaches are data hungry, their performance is strongly correlated with the amount of available training data. %  In this paper, we introduce three types of representation for MCL including word, root and entity tag embeddings. % With the purpose of discovering how above embeddings contribute to model performance independently, we use a simple NN as the baseline to do the investigation. % We also improve this basic model from two perspectives. % One is to apply a tensor transformation layer to extract multi-dimensional interactions among those representations. % The other is to map each entity tag into a vector representation. % The result shows that the use of root embedding can lead to a significant improvement to the models in term of improving test results.  % Our NNs reached good outcomes by transferring intermediate representations learned on large unlabeled data. % We compare the NNs with the existing CRF-based NER system for Kazakh  and the other bidirectional-LSTM-CRF  that considered as the state-of-the-art in NER. % Our NNs outperforms the state-of-the-art and the result indicates that the proposed NNs can be potentially applied to other morphologically complex languages. %  The rest of the paper is organized as follows: Section 2 reviews the existing work.  % Section 3 gives the named entity features used in this work. % Section 4 describes the details of neural networks. % Section 5 reports the results of experiments and the paper is concluded in Section 6 with future work.    We presented several neural networks for NER of MCLs.   The key aspects of our model for MCL are to utilize different embeddings and layer,  namely, i) root embedding, ii) entity tag embedding and iii) the tensor layer.   The effects of those aspects are investigated individually.   The use of root embedding leads to a significant result on MCLs' NER.   The other two also gives positive effects.   For Kazakh, the proposed NNs outperform the CRF-based NER system and other state-of-the-art including character-based biLSTM-CRF model.   The comparisons showed that character embedding is vital to MCL's NER.   The experimental results indicate that the proposed NNs can be potentially applied to other morphologically complex languages.   
"," We present several neural networks to address the task of named entity recognition for morphologically complex languages . % Kazakh is a morphologically complex language in which each root/stem can produce hundreds or thousands of variant word forms. % This nature of the language could lead to a serious data sparsity problem, which may prevent the deep learning models from being well trained for under-resourced MCLs. % In order to model the MCLs' words effectively, we introduce root and entity tag embedding plus tensor layer to the neural networks. % The effects of those are significant for improving NER model performance of MCLs.  % The proposed models outperform state-of-the-art including character-based approaches, and can be potentially applied to other morphologically complex languages.",220
"  Automatic video captioning is an emerging area in computer vision research that aims to generate textual descriptions of the visual components of a video. This has various applications including improving video accessibility for the blind and visually impaired, summarizing video, searching and indexing. Unfortunately, training models to do video captioning requires manual descriptions of every second of the video from a large corpus of representative videos. One of the largest current single-clip video captioning datasets, MSR-VTT, has only tens of thousands of unqiue uncorrelated videos whereas solving video captioning will likely require several orders of magnitude more to express the wide diversity of subjects, situations, and relationships possible in video data.  Active learning is a valuable approach in domains where unlabeled and partially labeled examples are readily available but obtaining manual annotations is expensive, such as is the case with automatic video captioning. However, while there has been significant investigation of active learning for computer vision tasks such as object recognition, object detection, video classification and video segmentation, video captioning has received comparatively little attention. The reason for this is likely rooted in the complexity of the label space. Video captioning requires both sequential input and output, dramatically increasing the complexity of traditional active learning frameworks. To our knowledge, this is one of the first works to define active learning strategies for efficiently collecting training sets for automatic video captioning.  In this paper we explore several active learning strategies for sequence to sequence active learning in video captioning, including uncertainty sampling based on label confidence, sequence entropy and query by committee methods. There are several unique challenges to active learning for deep sequence to sequence models: While traditional active learning methods  select one example at a time to label, retraining the model in its entirety after each new example selection, this strategy is impractical for training models such as transformer networks and LSTMs, due to increased training time  and increased inference time . Thus, it is far more efficient to select a large batch of examples at a time to label when using a crowd-sourced collection process . Traditional batch-active learning often uses ranking functions which are intractable in deep sequence to sequence learning , making active learning for video description a challenging problem, with no tractable solutions for deep neural networks.  In this work we conduct a thorough empirical analysis of various active learning strategies on two recent and standard video captioning datasets, MSR-VTT and LSMDC, using both transformer based and LSTM based captioning models, and describe a novel cluster-regularized method which is both tractable to compute, and provides strong performance in our test scenario. Our key contributions are:            In this paper, we have presented an initial set of methods aiming to tackle the active learning problem for video description, a challenging task requiring complex modeling where due to the complexity of the output distribution, many active learning methods are unable to function efficiently, or at all. We have shown that we can achieve 95\
"," Automatic video captioning aims to train models to generate text descriptions for all segments in a video, however, the most effective approaches require large amounts of manual annotation which is slow and expensive. Active learning is a promising way to efficiently build a training set for video captioning tasks while reducing the need to manually label uninformative examples. In this work we both explore various active learning approaches for automatic video captioning and show that a cluster-regularized ensemble strategy provides the best active learning approach to efficiently gather training sets for video captioning. We evaluate our approaches on the MSR-VTT and LSMDC datasets using both transformer and LSTM based captioning models and show that our novel strategy can achieve high performance while using up to 60\% fewer training data than the strong state of the art baselines.",221
"  % With \gld, we provide the data on which to train, for example, domestic service robots that interact with objects typically found in living environments and understand them through natural language.  % Categories of eobjects within the dataset include food, home, medical and office supplies, and tools; these are selected for their relevancy towards domestic tasks that support our aim. % Color and depth images of each object are captured from multiple angles and paired with natural language descriptions of the objects from both text and speech domains. % This pairing of visual and linguistic features of the same object is well-suited to train neural networks that learn the multimodal associations among people and their interactions with objects in the physical world. % We analyze the text and speech descriptions, assessing differences in word choice and sentence structure, to understand the effect that the two domains have on subsequent grounded language training. % Demonstrating a use case of \gld, we train two manifold alignment models on both the text and speech domains to show the efficacy of these data for grounded language learning tasks.  Grounded language acquisition is the process of learning language as it relates to the world---how concepts in language refer to objects, tasks, and environments. Embodied language learning specifically is a significant field of research in NLP, machine learning, and robotics. There are many ways in which robots learn grounded language, but they all require either multimodal data or natural language data---usually both.  A significant goal of modern robotics research is the development of robots that can operate in human-centric environments. Examples include domestic service robots  that handle common household tasks such as cooking, cleaning, and caretaking, robots for elder care, assistive robotics for providing support to people with disabilities, and rehabilitation robotics. To be useful for non-specialists, such robots will require easy-to-use interfaces. Spoken natural language is an appropriate interface for such systems: it is natural, expressive, and widely understood, as shown by the proliferation of natural language-based home devices. To have a robotic system flexibly understand language in dynamic settings and realize it in physical, goal-oriented behaviors, it is necessary to ground linguistic and perceptual inputs to a learned representation of knowledge tied to actions.  Current approaches to grounded language learning require data in both the perceptual  and linguistic domains. While existing datasets have been used for this purpose, the language component is almost always derived from either textual input or manually transcribed speech. In practice, robots are likely to need to operate on imperfectly understood spoken language. To that end, we present the Grounded Language Dataset , which contains images of common household objects and their description in multiple formats: text, speech , and automatically recognized speech derived from the audio files. We present experiments that demonstrate the utility of this dataset for grounded language learning.     The primary contributions of this paper are as follows:                 John's comments to consider:     Looking at the ImageNet paper: http://www.image-net.org/papers/imagenet_cvpr09.pdf   they explicitly list ideas in the format: .    What specific ideas do we want to call out?         Heterogeneous manifold alignment , as shown in this paper         Grounded language concept acquisition           Active learning          Natural language interfaces for personal assistants, domestic service robots         Human-robot interaction   -- but given any of these high-level topics, think of a *specific* challenge / task that \gld helps to address    In this paper we present \gld, a grounded language dataset of images in color and depth paired with natural language descriptions of everyday household objects in text and speech. We aim to make this resource a useful starting point for downstream grounded language learning tasks such as spoken natural language interfaces for personal assistants and domestic service robots.  To demonstrate a potential use of \gld, we use the data to train models that perform heterogeneous manifold alignment. We hope this dataset serves researchers as a resourceful starting point from which to explore many more techniques, model architectures, and algorithms that further our understanding of grounded language. In particular, the inclusion of speech alongside written textual descriptions allows for side-by-side comparisons of the two domains grounded to physical objects, or for novel multimodal techniques involving all three domains of vision, text, and speech.  The idiosyncratic properties of \gld\ suggest many research questions for future study. For example, we remark that \gld\ includes descriptions of the same object as observed from multiple angles. One interesting question to explore, then, would be how to identify objects from a different perspective or when information is missing. Another property of \gld\ is that some descriptions focus on the use of the object , while others report perceptual qualities of the objects such as logos and other identifying features uniquely visible from the annotator's current perspective on that object. This aspect could be incorporated into a human-robot interaction study that examines grounding language to objects in a physical as viewed by embodied agents from different vantage points.    Thus, \gld\ could be a powerful resource in categorizing or using this information to learn perspectives or to better handle occlusions in visual tasks.  In the near term, we are interested in leveraging this dataset to train robots to understand natural language in order to perform tasks in a domestic context. The inclusion of medical and kitchen supplies is critical to training a robot for tasks such as cooking, cleaning, and administering care. As we work toward this goal, we anticipate creating an expanded catalog of items including the diverse ways in which people describe and talk about the wide variety of items they encounter every day.   
"," Grounded language acquisition --- learning how language-based interactions refer to the world around them --- is a major area of research in robotics, NLP, and HCI. In practice the data used for learning consists almost entirely of textual descriptions, which tend to be cleaner, clearer, and more grammatical than actual human interactions. In this work, we present the Grounded Language Dataset , a multimodal dataset of common household objects described by people using either spoken or written language. We analyze the differences and present an experiment showing how the different modalities affect language learning from human input. This will enable researchers studying the intersection of robotics, NLP, and HCI to better investigate how the multiple modalities of image, text, and speech interact, as well as how differences in the vernacular of these modalities impact results.",222
"  Recent works have demonstrated an interest in unsupervised representation learning as a pretraining method to obtain good speech features for downstream tasks with little labelled data . While Contrastive Predictive Coding  and derivatives appear to be versatile methods for unsupervised representation learning , they do not yet reach the state-of-the-art  results on purely unsupervised learning metrics .   % Previous research has shown that  Data augmentation is useful for supervised training, and is also a key component in unsupervised setups in the image domain. It is not well established in unsupervised learning for speech, where the sequential nature of the signal may introduce specificities.   Our first objective is to explore several types of time-domain data augmentation  and several methods for augmenting in the contrastive framework  in English .  In a second stage, we extend the results to other languages  in the zero-resource 2017 benchmark.  Lastly, we show that data augmentation benefits semi-supervised training, using the Libri-light benchmark.    With data augmentation, CPC can take good advantage of relatively short  clean and well segmented speech, although it is currently insufficient to learn competitively with very small amounts of data . More research is needed to extend such techniques in both directions: with small amounts of data, and with very large, and potentially more noisy datasets.  In addition, the differences that we observe between data-augmentation effects open the issue of more systematic exploration of data augmentation as a function of tasks and architectures.   .          Not for the submission copy   
"," % 150/200 words max Contrastive Predictive Coding , based on predicting future segments of speech based on past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs other methods on unsupervised evaluation benchmarks. Here, we introduce WavAugment, a time-domain data augmentation library and find that applying augmentation in the past is generally more efficient and yields better performances than other methods. We find that a combination of pitch modification, additive noise and reverberation substantially increase the performance of CPC , beating the reference Libri-light results with 600 times less data. Using an out-of-domain dataset, time-domain data augmentation can push CPC to be on par with the state of the art on the Zero Speech Benchmark 2017. We also show that time-domain data augmentation consistently improves downstream limited-supervision phoneme classification tasks by a factor of 12-15\% relative.   %\matthijs{mention that the best data augmentation can reduce the required training data by a factor 600 } % %\morgane{if we look at the ablation study in the appendix, it seems that the architecture, on LS-100, is responsible for about 1/3 of the improvement. But we must also consider other factors not mentionned in this paper: noise and speaker distribution.}",223
"  Recent developments in the fields of electronics, computations and data processing have led to an increased interest in smart assistants with speech interfaces. It is likely driven by the fact that usually people can learn to use speech for interaction intuitively without any special training  and make it a primary medium of information exchange. However, speech poses a major challenge to a machine when it comes to the task of extraction of information intended to be transmitted by a human speaker, also known as Spoken Language Understanding  . The key difficulty here is that speech is highly variable, e.g. depending on room acoustic, and contains rich information about speakers  . Some of them are not useful for SLU. The information extraction task is often performed on the text representation using Natural Language Understanding  methods , while Automatic Speech Recognition  systems  convert speech to text. ASR step removes redundant information from the input and provides some kind of normalized form on the output. At the same time it causes loss of potentially useful information that can not be encoded in the text representation, such as prosody, loudness and speech rate. The operation of finding the most probable sequence of words for speech input is computationally expensive. %, what makes it hard to implement in practice. This is partly solved by various heuristics avoiding exploration of less probable hypothesis , what in turn introduces additional errors propagated to NLU component. Finally, the sequential design of pipeline approach leads to unavoidable source of latency, because NLU component can not start its work before ASR is finished, and it is not desirable in the interactive context of smart assistant. The problems of pipeline approach described above can be solved by end-to-end SLU methods.  Existing works on end-to-end SLU modeling either focus on supervised downstream tasks, for example dialog act classification , intent detection , slot filling , independent intent detection and domain classification  and joint intent detection, domain classification and slot filling , or target a generic semantic embedding  usually inspired by such successful models as word embeddings Word2Vec  and contextual text embeddings BERT . Highly variable and complex nature of speech leads to large amounts of both data and computational resources required for SLU training compared to NLU training, especially for recently popular approach based on contextual embeddings. While data requirements could be satisfied for unsupervised approaches, computational resources are still a problem. Fortunately, most of the modern language processing methods, including ASR and NLU, are based on neural networks and deep learning. Deep learning offers an easy way to transfer knowledge between learned tasks. This technique is referred as transfer learning and it is successfully applied in both ASR  and NLU . Therefore, transfer learning should be a promising direction to explore for SLU as well. Several reports  indicate that transfer learning from audio modality through pretraining on ASR task or, alternatively, speech autoencoding, is helpful for downstream SLU tasks. Transfer learning from text modality, however, has been applied only for Speech2Vec  and SpeechBERT  so far.  We propose a novel method that combines parameters transfer from well trained end-to-end ASR systems  such as pretrained ESPnet  and end-to-end NLU models such as pretrained BERT  with Teacher-Student learning  for final alignment of SLU output space to NLU output space in order to construct end-to-end SLU model allowing few-shot transfer of downstream tasks from text to speech. By doing so, we enable pretrained end-to-end contextual embeddings such as BERT to process acoustic features.  In particular, we aim to generate fixed length vectors with semantic representation from speech segments of variable length. Transfer learning from both text and audio modalities makes our approach mostly similar to  and . In this work, we investigate utterance classification task and focus on zero-shot and few-shot cases, but the described method could be adopted to many types of SLU tasks. Although previous works described a number of experiments for such utterance classification tasks as dialog act classification  and intent classification , and we use the same datasets for the evaluation, we do not compare our results directly to these works, as this is outside of the scope of our work.    We proposed to combine parameters transfer from well trained ASR and NLU models with Teacher-Student learning for final alignment of SLU output space to NLU output space in order to construct end-to-end SLU model allowing few-shot transfer of downstream tasks from text to speech. We outlined necessary steps and settings for the practical pretrained NLU model adaptation in SLU via cross-modal transfer. Our system reaches accuracy of 58.60\ , 60.18\  and 91.12\  on SwBD, MRDA and FSC datasets without fine-tuning and 60.22\ , 61.32\  and 95.49\  after fine-tuning on ten labeled samples per class compared to 57.23\ , 64.06\  and 94.57\  reached by the pipeline system. The results of this research support the idea that text pretrained contextual embeddings can be useful for tasks outside of text modality. The present study also adds new tasks to the growing body of research of language processing methods using Transformer neural networks.     
"," Spoken language understanding is typically based on pipeline architectures including speech recognition and natural language understanding steps. These components are optimized independently to allow usage of available data, but the overall system suffers from error propagation. In this paper, we propose a novel training method that enables pretrained contextual embeddings to process acoustic features. In particular, we extend it with an encoder of pretrained speech recognition systems in order to construct end-to-end spoken language understanding systems. Our proposed method is based on the teacher-student framework across speech and text modalities that aligns the acoustic and the semantic latent spaces. Experimental results in three benchmarks show that our system reaches the performance comparable to the pipeline architecture without using any training data and outperforms it after fine-tuning with ten examples per class on two out of three benchmarks.",224
" Self-supervised learning of representations from large unlabeled datasets is a popular contemporary trend in machine learning. After being widely adopted in areas like natural language processing and computer vision, self-supervision is now rapidly developing as a noteworthy topic in audio and speech processing. Self-supervision aims to capture the most informative properties from the underlying structure of unlabeled data to learn generalized representations. This is extremely promising in problem settings involving a large amount of unlabeled data but limited labeled data. In the context of audio and speech processing, this is relevant to low resource languages, emotion recognition, cross-cultural speech recognition and other such problems with small-sized datasets.  Even though there has been recent research interest in self-supervised learning for speech data, most works focus only on the audio modality alone. Audiovisual speech data offers interesting possibilities for cross-modal self-supervision, which is something relatively lesser explored. In this work, we present a method for self-supervised representation learning of audio features that leverages both the audio and visual modalities. We demonstrate how generating a talking lip video from a single frame and the corresponding audio can be used as a pretext task for visual self-supervision to train a raw audio encoder. We combine this with audio-only self-supervision based on predicting informative audio attributes, similar to . This results in an audio encoder trained by joint audiovisual self-supervision. We evaluate the method on spoken word classification and achieve competitive results when comparing with existing self-supervised methods. Our method also results in significantly better performance when learning with limited data  for the downstream tasks. Importantly, our method also outperforms fully supervised training . Our observations motivate the utility of self-supervised pretraining for audio related tasks. We demonstrate that cross-modal supervision in audiovisual speech can learn better representations compared to unimodal audio-only or visual-only self-supervision.    Self-supervised learning has been very influential in recent advances in natural language processing  and computer vision . It is also beginning to mature as a relevant topic in audio and speech processing. CPC   was a seminal work in self-supervised learning which also demonstrated the applicability of contrastive self-supervised learning to audio.  Wav2vec  refines the idea from CPC specifically for speech. CPC based self-supervision has also been shown to generalize well to multiple languages . APC   is a similar approach that predicts the next token of a speech segment from the history. Another very relevant recent work is PASE  , which aims to learn multi-task speech representations from raw audio by predicting a number of handcrafted features such as MFCCs, prosody and waveform. Teacher-student models have also been explored for audio self-supervision where the trained model from a previous epoch acts as the teacher model for the next epoch . % Phase prediction  has also been proposed as an audio-based pretext task. WaveNet  is a generative model for raw audio waveforms that can be used for generic audio representations. All of the works discussed so far are unimodal audio-only self-supervised methods. There are also a few other works that utilize both audio and visual information. There are multiple ways to capture this cross-modal interaction including audiovisual synchronization , cross-modal transition modeling , cross-modal pseudolabel based clustering , contrastive learning , and audiovisual instance discrimination . However most of these works present cross-modal self-supervision in the context of generic audiovisual data, with application to tasks like video action recognition and acoustic scene classification. There is limited work that explores self-supervision specifically in the context of audiovisual speech. We have explored this concept in recent related work . This work extends the idea from our prior work. Specifically, we move from learning speech representations directly from raw audio instead of from mel features. We also adopt a different and more refined approach for audio-only self-supervision .    There are multiple interesting observations from our obtained results. Audio-only supervision yields better results than visual-only supervision. However, the model trained with joint audiovisual self-supervision performs better than the models trained with unimodal audio-only and visual-only self-supervision in almost all scenarios. including noisy datasets. This highlights the utility of the complementary information encoded by visual self-supervision and demonstrates the potential of multimodal self-supervision as a useful tool in speech representation learning. Also notably, despite all tested methods being very similar in performance on the full datasets, there is a clear gap when using a small training set and our method is the best at learning with fewer labels, which is very relevant to low resource domains. This can have significant impact in problems like low resource language ASR, emotion recognition and cross-cultural ASR. Our method also significantly outperforms fully supervised training from scratch, which further motivates the utility of self-supervised pretraining for speech.       joint training doesn't directly work, overfitting, cite cvpr2020 paper   slightly worse than SOTA on full datasets    frozen features are bad   This is a work in progress and there are many other speech related applications that we can evaluate our model on. In this work, we only focused on the classification of isolated words. We will also test the model on continuous CTC based speech recognition on datasets like Librispeech and TIMIT, and other tasks like speaker identification and speech emotion recognition. An especially relevant application would be low resource language ASR. There are also interesting directions to explore to improve our method. In this work, we exhibit how joint audiovisual information can be used for audio representation learning. In a similar manner, we could also utilize this cross-modal information for visual representation learning . Another interesting line of work is multimodal contrastive self-supervised learning which has been demonstrated for generic audiovisual data but not for audiovisual speech.    \medskip               \right]\left[ \right]$}\\           & \\           \multirow{2}{*}{conv4\_x} & \multirow{2}{*}{256x1000} &\\           & \\           \multirow{2}{*}{conv5\_x} & \multirow{2}{*}{512x500} &\\           & \\           avgpool & 515x25 & \\        The results in Table  for all the baseline methods  have been computed using the public code and pretrained models provided by the authors. These baseline methods  have been pretrained on varying amounts and types of data. For a completely fair comparison, all methods need to be pretrained with the same data. We experimented with pretraining all baseline methods on the same 36 hour LRW frontal subset that we use for our method. The results obtained with the baseline methods using this approach were either equivalent or worse to those with the public pretrained models. This shows that our model may be able to learn better representations on the same amount of pretraining data. However for the results, we use the public pretrained models which may assist with reproducibility.     Maybe add a table about which datasets each model was pretrained on along with its size  \vfill                           \vfill   tSNE plots?  
"," The intuitive interaction between the audio and visual modalities is valuable for cross-modal self-supervised learning. This concept has been demonstrated for generic audiovisual tasks like video action recognition and acoustic scene classification. However, self-supervision remains under-explored for audiovisual speech. We propose a method to learn self-supervised speech representations from the raw audio waveform. We train a raw audio encoder by combining audio-only self-supervision  with visual self-supervision . The visual pretext task drives the audio representations to capture information related to lip movements. This enriches the audio encoder with visual information and the encoder can be used for evaluation without the visual modality. Our method attains competitive performance with respect to existing self-supervised audio features on established isolated word classification benchmarks, and significantly outperforms other methods at learning from fewer labels. Notably, our method also outperforms fully supervised training, thus providing a strong initialization for speech related tasks. Our results demonstrate the potential of multimodal self-supervision in audiovisual speech for learning good audio representations.",225
"   {U}{nlike} humans, who are capable of self-learning through experiences and interactions, current real-world speech applications like automatic speech recognition  rely heavily on large amounts of human annotations. %. In order for the next generation of speech processing systems to exhibit similar levels of cognitive intelligence as humans, machines should be designed to learn from unlabeled data as humans do. In the era of big data, self-supervised learning has emerged as an attractive approach to leverage knowledge from a large amount of unlabeled data. Self-supervised learning leverage unsupervised pre-training tasks to train networks, and they have shown to be effective for improving downstream systems. %Hence, the need of self-supervised pre-training is rooted in our increasing demand to improve downstream speech systems like ASR, as there is always a limited amount of labeled data for supervised training.  Through self-supervised pre-training, learned models could be applied to downstream Speech and Language Processing  tasks through feature-based speech representation extraction, or fine-tuning as part of the downstream model. Speech representations are compact vectors which aim to capture high-level semantic information from raw speech. Thus, the goal of speech representation learning is to find a transform that maps the input acoustic features into such vectors. When the pre-trained networks are re-used as features, it provides a useful speech representation to reduce classifier complexity, makes high-level information more accessible, and ultimately improves downstream SLP tasks. Besides, speech representations also help transfer learning and adaptation across different data distributions. On the other hand, the fine-tuning approach uses the pre-trained model to initialize a downstream model for supervised training. The parameters of self-supervised learned models are good initialization for ASR encoders.  In self-supervised learning, an auxiliary task  is formulated, and models are trained to solve it. While solving the auxiliary task, the network is learning a function that maps input to desired representations that can be potentially transferred to multiple downstream tasks. The key tenet of self-supervised learning is the design of an auxiliary task, which allows the model to leverage knowledge from unlabeled data. As such, the formulation of the auxiliary task should be carefully chosen. The task should be hard enough for the model to learn high-level semantic properties, and not be too amiable for the model to exploit low-level shortcuts.  %In recent studies, learning under multiple auxiliary objectives have shown promising results. In this work, we propose TERA: Transformer Encoder Representations from Alteration, a multi-target auxiliary objective to pre-train Transformer Encoders. %TERA requires the model to predict real frames from corrupted frames, without using any labels. We introduce a total of three auxiliary objectives to form the multi-target pre-training scheme:  1) time alteration: reconstructing from corrupted blocks of time steps. 2) channel alteration: reconstructing from missing blocks of frequency channels. 3) magnitude alteration: reconstructing from altered feature magnitudes. These auxiliary objectives can be applied together or separately in the pre-training process. %During training, these auxiliary objectives are applied by dynamically sampling through a stochastic alteration policy to create random alterations. %The key idea behind all the auxiliary objectives is that if the model can predict the original frames from corrupted input, it should provide a good representation of the critical content. The model acquires information about the content around the corrupted or altered portions, and by reconstructing them, the model learns a more contextualized representation. We illustrated the framework in Fig.. Similar self-supervised frameworks have been widely studied . Unlike previous approaches that only employ reconstruction on the temporal axis, TERA considers three orthogonal axes, including temporal, channel, and magnitude. %Such self-supervised framework has been widely studied in several recent work. %In Section, we will give a thorough review of the related work, and point out the difference of TERA to previous work.  To evaluate TERA, we use downstream tasks of phoneme classification, speaker recognition, and automatic speech recognition . %We first demonstrate our approach can make more accurate phone and speaker predictions, outperforming surface features and previous works under standard benchmark settings. %Next, moving beyond classification tasks, we apply the learned model to improve strong supervised phonetic systems, where we use hybrid DNN/HMM models for automatic speech recognition . Also, we compare the effectiveness of each auxiliary objectives separately and in combination. As a result, we confirm that each of the proposed auxiliary objectives guides the model to learn a distinct aspect of speech: 1) The time alteration objective is effective in making more accurate phoneme prediction and speech recognition, as it leads the model to learn richer phonetic content. %and bidirectional context. 2) The channel alteration objective is effective in making more accurate speaker prediction, as it leads the model to learn speaker identity. 3) The magnitude alteration objective is effective in providing a performance boost for all tasks, as it potentially increases data diversity for pre-training.  Besides, we explore different knowledge transfer methods of the pre-trained model to downstream tasks. The methods include: 1) extract representations from the last layer,  2) combine representations from all hidden layers with a learnable weighted sum, and 3) fine-tuning the pre-trained model with the downstream model. Furthermore, we also explore using different acoustic features for reconstruction and find that they impact downstream performance and affect what the model learns. Finally, we investigate the problem of domain mismatch between the pre-training and downstream datasets, and the proposed approach is shown to be unaffected by the domain mismatch issue. For reproducibility of our results, we provide our implementation with pre-trained models and evaluation scripts in the S3PRL toolkit\footnote{{} }.  %This literature is organized as follows: %In Section, we introduce related works and discuss how our work is different from others. %In Section, we introduce the proposed TERA. %In Section, we describe how experiments are formulated to evaluate TERA. %In Section, we present experimental results and our findings.   %The contributions of our paper are as follows: %     %        We propose a novel multi-target self-supervised training scheme called TERA, where we use multiple auxiliary objectives instead of one during pre-training. With the proposed alterations, the diversity of input is increased, and the performance, especially for the case with less pre-training data, is improved.  We demonstrate strong results in tasks of phoneme classification, speaker recognition, and speech recognition. We conduct a complete ablation study, and a thorough comparison of recent representation learning and pre-training approaches.    
"," We introduce a self-supervised speech pre-training method called TERA, which stands for Transformer Encoder Representations from Alteration. Recent approaches often learn through the formulation of a single auxiliary task like contrastive prediction, autoregressive prediction, or masked reconstruction. Unlike previous approaches, we use a multi-target auxiliary task to pre-train Transformer Encoders on a large amount of unlabeled speech. The model learns through the reconstruction of acoustic frames from its altered counterpart, where we use a stochastic policy to alter along three dimensions: temporal, channel, and magnitude. TERA can be used to extract speech representations or fine-tune with downstream models. We evaluate TERA on several downstream tasks, including phoneme classification, speaker recognition, and speech recognition. TERA achieved strong performance on these tasks by improving upon surface features and outperforming previous methods. In our experiments, we show that through alteration along different dimensions, the model learns to encode distinct aspects of speech. We explore different knowledge transfer methods to incorporate the pre-trained model with downstream models. Furthermore, we show that the proposed method can be easily transferred to another dataset not used in pre-training.",226
" In contemporary pop music, the linguistic content in the singing voice is generally referred as lyrics and the process of automatic retrieval this lyrics content from singing voice can then be defined as Automatic Lyrics Transcription. The automatic retrieval of pronounced words from speech signals is a widely developed research field and the state of the art systems by today can be successfully applied to industrial applications. However, the same level of robustness has not yet been reached when the input is singing voice. According to prior research, there are several domain specific reasons word recognition performance reduces in singing including domain specific acoustic characteristics  and the alterations of word pronunciations. Specifically from a machine learning perspective, the main bottleneck for achieving a robust system is the availability of training data with fine-grained annotations, to be used in a supervised learning framework.   In this study, we exploit such large-scale singing voice dataset, DAMP - Sing! 300x30x2 - released by Smule \footnote{Smule is a commercial Karaoke singing application. More info at https://www.smule.com/}, where prompt-level\footnote{In Smule app, lyrics are prompted to the users as words or sentences depending on the song arrangement. Each prompted sentence or word annotation is referred as prompt-level annotation.}  annotations are provided. The dataset consists of monophonic Karaoke recordings of pop songs by multiple performers providing near 150 hours of trainable audio data. However, this dataset has not been widely utilized for the purpose of training a word recognition system. Through our proposed framework, we aim to highlight one way of utilization of this dataset in a complete ALT framework and further conduct an in-depth self-attention analysis via fine-tuning experiments.   A robust system for the retrieval of sung lyrics has a variety of potential applications in music information retrieval  related tasks and the music tech industry. In karaoke and music education apps, the recognition of sung words is essential for tracking a performance and providing feedback to the user. In combination with techniques like query-by-humming, ALT can be utilized for the song identification and metadata retrieval tasks.    Our system uses deep neural networks for building the final acoustic model that is composed of 2D convolutional layers at the front end for extracting more robust features followed by time-delay layers due to their capability of modeling long-term context information. A self-attention layer is added before the final projection layer for weighting the time context when computing the output activations for classification.  Overall, this paper targets at making the following contributions:          This paper is structured as follows: literature on ALT on monophonic singing recordings is reviewed . Then the details of the data used in training and evaluation are given . Then the proposed system  and the basis of our experiments  are explained. The results for each of experimental steps are shown and an in-depth analysis of the self attention parameters is performed . Finally, potential improvements to the proposed system are discussed .       Though achieving an approximately 5\
","  Speech recognition is a well developed research field so that the current state of the art systems are being used in many applications in the software industry, yet as by today, there still does not exist such robust system for the recognition of words and sentences from singing voice. This paper proposes a complete pipeline for this task which may commonly be referred as automatic lyrics transcription . We have trained convolutional time-delay neural networks with self-attention on monophonic karaoke recordings using a sequence classification objective for building the acoustic model. The dataset used in this study, DAMP - Sing! 300x30x2 is filtered to have songs with only English lyrics. Different language models are tested including MaxEnt and Recurrent Neural Networks based methods which are trained on the lyrics of pop songs in English. An in-depth analysis of the self-attention mechanism is held while tuning its context width and the number of attention heads. Using the best settings, our system achieves notable improvement to the state-of-the-art in ALT and provides a new baseline for the task.",227
"  %Given the limited amount of information from a single-microphone mixture recording, separating the constituent sources can be a challenging task. Considering also the limited computational resources that one might have at their disposal, the problem of training and deploying a separation model might become especially challenging. However, efficient sound source separation frameworks could be utilized towards enhancing the performance of various systems which generally require clean audio inputs .   The advent of the deep learning era has enabled the effective usage of neural networks towards single-channel source separation with mask-based architectures . Recently, end-to-end source separation in time-domain has shown state-of-the-art results in a variety of separation tasks such as: speech separation , universal sound separation  and music source separation . The separation module of ConvTasNet  and its variants  consist of multiple stacked layers of depth-wise separable convolutions  which can aptly incorporate long-term temporal relationships. Building upon the effectiveness of a large temporal receptive field, a dual-path recurrent neural network   has shown remarkable performance on speech separation. Demucs  has a refined U-Net structure  and has shown strong performance improvement on music source separation. Specifically, it consists of several convolutional layers in each a downsampling operation is performed in order to extract high dimensional features. A two-step approach has been introduced in  and showed that universal sound separation models could be further improved when working directly on the latent space and learning the ideal masks on a separate step.  Despite the dramatic advances in source separation performance, the computational complexity of the aforementioned methods might hinder their extensive usage across multiple devices. Specifically, many of these algorithms are not amenable to, e.g., embedded systems deployment, or other environments where computational resources are constrained.  Additionally, training such systems is also an expensive computational undertaking which can amount to significant costs.  Several studies, mainly in the image domain, have introduced more efficient architectures in order to overcome the growing concern of large models with high computational requirements. Models with depth-wise separable convolutions  have shown strong potential for several image-domain tasks  while significantly reducing the computational requirements. Thus, several variants such as MobileNets  have been proposed for deep learning on edge-devices. However, convolutions with a large dilation factor might inject several artifacts and thus, lightweight architectures that combine several dilation factors in each block have been proposed for image tasks . More recent studies propose meta-learning algorithms for optimizing architecture configurations given specific computational resource and accuracy requirements .  Despite the recent success on low-resource architectures on the image domain, little progress has been made towards proposing efficient architectures for audio tasks and especially source separation. In  a WaveRNN is used for efficient audio synthesis in terms of floating point operations  and latency. Other studies have introduced audio source separation models with reduced number of trainable parameters  and binarized models . In this study, we propose a novel efficient neural network architecture for audio source separation while following a more holistic approach in terms of computational resources that we take into consideration . Our proposed model performs SUccessive DOwnsampling and Resampling of Multi-Resolution Features ({https://github.com/etzinis/sudo\_rm\_rf}}.      In this study, we have introduced the  \usepackage{amsmath,graphicx,mlspconf}    Copyright notices.   ------------------   Select one of the four copyright notices below. Only required for the camera-ready paper submission.      * For papers in which all authors are employed by the US government:     * For papers in which all authors are employed by a Crown government : 2020 Crown}    * For papers in which all authors are employed by the European Union: 2020 European Union}    * For all other papers: 2020 IEEE}    Header \toappear{2020 IEEE International Workshop on Machine Learning for Signal Processing, Sept.\ 21--24, 2020, Espoo, Finland}    Example definitions.   --------------------  \def\x{{\mathbf x}}  \def\L{{     Double-blind peer review.   -------------------------   Anonymize your paper for the double-blind peer-review process using the    following author and affiliation. } \address{Anonymous}    Single address.   ---------------  }  \address{Author Affiliation}    For example:   ------------  \address{       School \\      Department \\      Address  }     Two addresses.   --------------  \twoauthors{       A. Author-one, B. Author-two  }{       School A-B \\      Department A-B \\      Address A-B \\      Email A-B  }{      C. Author-three, D. Author-four  }{       School C-D \\      Department C-D \\      Address C-D \\      Email C-D  }      Two or more addresses .   -----------------------------------------   If you need to list more than 2 authors or the option for two options above    produces a poor author block, please use the following structure:  ^{$ Affiliation Number Two   }        
"," In this paper, we present an efficient neural network for end-to-end general purpose audio source separation. Specifically, the backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features  as well as their aggregation which is performed through simple one-dimensional convolutions. In this way, we are able to obtain high quality audio source separation with limited number of floating point operations, memory requirements, number of parameters and latency. Our experiments on both speech and environmental sound separation datasets show that \sudo performs comparably and even surpasses various state-of-the-art approaches with significantly higher computational resource requirements.",228
"  Today's state-of-the-art in language modeling for ASR relies on neural Language Models , capable of handling continuous space and thereby outperforming traditional Back-off N-gram LMs . BNLMs cannot exploit long context based syntactic dependencies and are also less flexible in terms of generalization for unseen cases, as semantic knowledge  is not captured while training them.   % Neural LMs however have an undesired property, they are computationally very heavy in decoding, so neural LMs cannot be effectively used in a single decoding pass, they are rather exploited by rescoring lattices obtained from a first decoding pass with a BNLM. It is obvious, but can also be shown, that information is lost during the first decoding pass, as the pruning of the recognition network is based only on short context syntax, discarding both longer context syntactic and quasi all semantic knowledge. Another problem arising is the increased latency of the system through the two decoding passes, which hampers exploitation in strict online requirements.  To reduce these limitations in exploiting neural LMs for ASR, several solutions have been proposed. In it was shown that using the neural LM to generate an augmented training corpus to train an improved BNLM is the best performing strategy. Such a BNLM trained on augmented corpus can be used in a single pass or in the first pass of decoding. Sometimes these are called approximative models as they try to capture the knowledge of the neural model through their augmented training corpus. %Recently several studies concentrated on this approach. Suzuki et al. uses a domain balanced mixture of the training corpora to train a shallow RNNLM for text generation, and improve speech recognition results for Japanese, Korean and English. Wang et al. report using general domain pre-trained Transformer to augment text corpora used to train LMs. They demonstrate that the pre-trained and fine-tuned Transformer performs significantly better in data augmentation than LSTMs or simple in-domain Transformer models. %Both and underline that this approach is particularly useful if in-domain training data is relatively scarse .  % Another burden of language modeling for morphologically rich languages are the different syntactic properties of the language compared to English. Heavy agglutination results in much larger vocabularies, which is a problem in itself, but causes other problems too: %beside handling more words,  individual word forms occur less often and hence, the size of the training corpus should accordingly be augmented to maintain the predictive power of the dataset. Moreover, as suffixes express grammatical relations usually provided by word order in English, morphologically rich languages tend to be more permissive in choosing word order, leading to higher variation. This impairs BNLM estimation badly, but may also cause that word embeddings become less powerful in terms of syntactic and semantic consistency, even despite using long context windows. %%%BLIND VERSION: %This impairs BNLM estimation badly, but may also cause that word embeddings become less powerful in terms of syntactic and semantic consistency~[BLIND], even despite using long context windows.  To alleviate these problems linked to the different organization of morphologically rich languages, subword unit modeling is an often used alternative. Subword unit based ASR has been demonstrated to improve WER for several morphologically rich languages. Suzuki et al. use subword approach for data augmentation to enrich text corpora to train BNLM, but compose these subwords back into words to prepare the final LM, unlike our approach that retokenizes words into subword units in the final LM.    % In this paper we aim to improve LM for an online call center ASR system in the morphologically rich Hungarian. We use parliamentary text to pre-train a GPT-2 structure Transformer LM, and fine-tune it on the target domain. With this model we generate training text for a BNLM. We demonstrate that such Transformer based data augmentation is efficient in morphologically rich Hungarian, %which improves ASR  if vocabulary is large enough and a large BNLM is used. Retokenizing the augmented training corpus to subword units, and training a subword-based BNLM on it, we demonstrate that  the ASR accuracy further improves compared to the word based baseline augmented BNLM, and  the footprint and complexity of the resulting subword unit augmented BNLM significantly decrease. As subword unit LMs are known to perform better on a wide range of morphologically rich languages, we hypothesize that our approach is transferable to other such languages. We consider as novelties of our paper the following:  we propose the retokenization of the Transformer augmented LM training corpus; % and use it in online ASR with single pass decoding;  we are the first to use the GPT-2 Transformer structure to augment LM training corpora;  we are the first to apply a Transformer based LM for a Hungarian ASR task;  we demonstrate that the subword-based neural text augmentation can be exceptionally efficient in modeling OOV words.       In this paper, we extended the neural text generation based data augmentation method presented in We introduced an approach called subword-based neural text augmentation that is the extension of the Transformer based LM augmentation method presented in for morphologically rich languages. With this new approach we managed to improve the WER of our online ASR system on Hungarian call center conversions by more than 10\  relative . Our solution also outperforms the original, word-based data augmentation technique in terms of WER and OOV recognition capability while keeping the vocabulary size and memory requirements of the system quite low. Besides, to the best of our knowledge this is the first paper applying GPT-2 Transformer to generate augmentation data for an ASR language model.    A rekurrens 鑼卻 transformer modellek egyes閾唗鑼卾el tov璋゜bi 1\  鑼卹het鑹 el.   
"," Recently Deep Transformer models have proven to be particularly powerful in language modeling tasks for ASR. Their high complexity, however, makes them very difficult to apply in the first  pass of an online system. Recent studies showed that a considerable part of the knowledge of neural network Language Models  can be transferred to traditional n-grams by using neural text generation based data augmentation. In our paper, we pre-train a GPT-2 Transformer LM on a general text corpus and fine-tune it on our Hungarian conversational call center ASR task. We show that although data augmentation with Transformer-generated text works well for isolating languages, it causes a vocabulary explosion in a morphologically rich language. Therefore, we propose a new method called subword-based neural text augmentation, where we retokenize the generated text into statistically derived subwords. We compare Morfessor and BPE statistical subword tokenizers and show that both methods can significantly improve the WER while greatly reducing vocabulary size and memory requirements. Finally, we also demonstrate that subword-based neural text augmentation outperforms the word-based approach not only in terms of overall WER but also in recognition of OOV words.",229
" Deep neural networks  play a central role in state-of-the-art automatic speech recognition  systems. When designing these systems, a set of DNN structure design decisions such as the hidden layer dimensionality and connectivity need to be made. These decisions are largely based on expert knowledge or empirical choice. As explicitly training and evaluating the performance of different network architectures is highly expensive, it is preferable to use automatic architecture design techniques.   To this end, neural architecture search  approaches have gained increasing interests in recent years. The key objectives of NAS methods are three fold. First, it is crucial to produce an accurate performance ranking over different candidate neural architectures to allow the best system to be selected. Second, when operating at the same level of accuracy performance target, preference should be given to simpler architectures with fewer parameters in order to minimize the risk of overfitting to limited data. Furthermore, to ensure scalability and efficiency on large data sets, a search space containing all candidate systems of interest needs to be defined.   Earlier forms of NAS techniques were based on neural evolution, where genetic algorithms were used to randomly select architecture choices at each iteration of mutation and crossover. Bayesian NAS methods based on Gaussian Process was proposed in. Reinforcement learning  based NAS approaches have also been developed. In these techniques, explicit system training and evaluation are required. In addition, as the architecture hyper-parameters and actual DNN parameters are separately learned, e.g., within the RL controller and candidate systems, a tighter integration of both is preferred during NAS.   Alternatively, differentiable architectural search  techniques can be used. Architectural search is performed over an over-parameterized parent super-network containing paths connecting all candidate DNN structures to be considered. The search is transformed into the estimation of the weights assigned to each candidate neural architecture within the super-network. The optimal architecture is obtained by pruning lower weighted paths. This allows both architecture selection and candidate DNN parameters to be consistently optimized within the same super-network model.  In contrast to the rapid development of NAS techniques in the machine learning and computer vision communities, there has been very limited research of applying these to speech recognition systems so far. In this paper, a range of DARTS based NAS techniques are used to automatically learn two architecture hyper-parameters that heavily affect the performance and model complexity of state-of-the-art factored time delay neural network  acoustic models: i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These include the standard DARTS method fully integrating the estimation of architecture weights and TDNN parameters in lattice-free Maximum Mutual Information  training; Gumbel-Softmax DARTS that reduces the confusion between candidate architectures; pipelined DARTS that circumvents the overfitting of architecture weights using validation data; and penalized DARTS that further incorporates resource constraints to flexibly adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures was also used to facilitate efficient search over a large number of TDNN systems. Experiments conducted on a 300-hour Switchboard conversational telephone speech recognition task suggest the NAS configured TDNN-F systems consistently outperform the baseline LF-MMI trained TDNN-F systems using manually designed configurations. Absolute word error rate reductions up to 1.0\% and model size reduction of 28\% relative were obtained. In order to further evaluate the performance of the proposed NAS techniques, they were applied to automatically configure two sets of hyper-parameters of a state-of-the-art disordered speech recognition task based on the UASPEECH corpus: skip connection between layers and dimensionality of factored TDNN weight matrices.  To the best of our knowledge, this paper is among the first to apply neural architecture search techniques to TDNNs in speech recognition tasks. In contrast, the vast majority of previous NAS research has been focused on computer vision applications. Existing NAS works in the speech community investigated non-TDNN based  architectures.  % Once the paper is accepted, we will release our code.  The rest of this paper is organized as follows. Section 2 presents a set of differentiable NAS techniques. Section 3 discusses the search space of TDNN-F models and necessary parameter sharing to improve search efficiency. Section 4 presents the experiments and results. Finally, the conclusions are drawn in Section 5.      In this paper, a range of neural architecture search  techniques is investigated to automatically learn three hyper-parameters that heavily affect the performance and model complexity of state-of-the-art factored time delay neural network  acoustic models: i) the left and right splicing context offsets; ii) the dimensionality of the bottleneck linear projection. Experimental results obtained from Switchboard and UASPEECH suggest NAS techniques can be used for the automatic configuration of DNN based speech recognition systems and allow their wider application to different tasks.  
"," Deep neural networks  based automatic speech recognition  systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search  techniques are used to automatically learn two types of hyper-parameters of state-of-the-art factored time delay neural networks : i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These include the DARTS method integrating architecture selection with lattice-free MMI  TDNN training; Gumbel-Softmax and pipelined DARTS reducing the confusion over candidate architectures and improving the generalization of architecture selection; and Penalized DARTS incorporating resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures allows efficient search over up to $7^{28}$ different TDNN systems. Experiments conducted on the 300-hour Switchboard corpus suggest the auto-configured systems consistently outperform the baseline LF-MMI TDNN systems using manual network design or random architecture search after LHUC speaker adaptation and RNNLM rescoring. Absolute word error rate  reductions up to 1.0\% and relative model size reduction of 28\% were obtained. Consistent performance improvements were also obtained on a UASpeech disordered speech recognition task using the proposed NAS approaches. % Deep neural networks  based automatic speech recognition  systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search  techniques are used to automatically learn three hyper-parameters that heavily affect the performance and model complexity of state-of-the-art factored time delay neural network  acoustic models: i) the left and right splicing context offsets; ii) the dimensionality of the bottleneck linear projection and iii) skip connections at each hidden layer. These include the standard DARTS method fully integrating the estimation of architecture weights and TDNN parameters in lattice-free MMI  training; Gumbel-Softmax DARTS that reduces the confusion between candidate architectures; Pipelined DARTS that circumvents the overfitting of architecture weights using held-out data; and Penalized DARTS that further incorporates resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures was also used to facilitate efficient search over up to $7^{28}$ different TDNN systems. Experiments conducted on a 300-hour Switchboard task suggest the NAS auto-configured TDNN-F systems consistently outperform the baseline LF-MMI trained TDNN-F systems using manual expert configurations. Absolute word error rate reductions up to 1.0\% and relative model size reduction of 28\% were obtained. Consistent performance improvements were also obtained in the disorder UASPEECH task.",230
"   A speech enhancement system aims at restoring the quality and intelligibility of noisy speech. The state-of-the-art speech enhancement systems are commonly built with deep neural network  based vector-to-vector regression models, where inputs are context-dependent log power spectrum  features of noisy speech and outputs correspond to either clean or enhanced LPS features. Although deep neural network  based speech enhancement  has demonstrated the state-of-the-art performance under a single-channel setting, it can also be extended to scenarios of multi-channel speech enhancement with even better-enhanced speech qualities . The process of both single and multi-channel speech enhancement can be taken as a DNN based vector-to-vector regression aiming at bridging a functional relationship  such that the input noisy speech  can be mapped to the corresponding clean speech . In, DNNs with feed-forward fully-connected  hidden layers were proposed to attain the state-of-the-art performance of speech enhancement on the target tasks and the related theorems were later set up in. In some follow-up studies, recurrent neural networks , and convolutional neural networks  were further investigated to boost speech enhancement quality. Moreover, a deep bidirectional RNN with LSTM gates was instead used in, and a generative adversarial network  was attempted for speech enhancement tasks in. In particular, CNN is a tensor-to-vector regression model because it is capable of dealing with 3D/4D tensorized input data. Besides, the recent works suggest that CNN can outperform both DNN and RNN counterparts for speech enhancement. Similarly, a tensor-to-vector regression model can also be built by directly employing the proposed tensor-train network . Besides, TT-DNN is a compact representation for a fully-connected  layers of DNN into a tensor-train  format. In, we were the first to attempt a tensor-train deep neural network  to tackle the multi-channel speech enhancement task and also demonstrate that the TT representation of a DNN does not cause the quality degradation of the enhanced speech, and it also results in a significant reduction of the model parameters. More importantly, the quality of speech enhancement can be improved over the DNN counterpart by allowing the TT-DNN parameters to grow.  A significant advantage of tensor-to-vector regression, such as CNN and TT-DNN, is its compact architecture to observe stringent hardware constraints, where computational resources are often limited. Therefore, it is worth investigating the models in terms of the representation power, and experimentally comparing them by considering the trade-off between enhancement performance and the number of model parameters. On one hand, CNN is a powerful model to learn spatial-temporal features and extract semantically meaningful aspects in higher hidden layers. On the other hand, TT-DNN can maintain baseline results of the corresponding DNN by applying the TT transformation to the FC hidden layers. Hence, in this work, we focus on a tensor-to-vector model to take advantage of both CNN and TT-DNN. More specifically, we propose a novel hybrid architecture, namely CNN-TT, with convolutional layers stacked at the bottom and one TT hidden layer on the top. To highlight the advantages of CNN-TT, we compare different deep tensor-to-vector models for speech enhancement. The used models in this work include  DNN;  CNN;  TT-DNN;  CNN-TT. In more detail, we first explain the fundamental mechanisms of tensor-to-vector regression based on our theorems of DNN based vector-to-vector regression. Then, we validate our CNN-TT models in speech enhancement tasks.  %we know that CNNs are very powerful models that can learn salient features of the input domain in the lower layers, and semantically meaningful aspect of the problem domain in the higher layer . CNN-based architectures have reported top results in several machine learning tasks, e.g.,  On the other hand, a TT-DNN can significantly reduce the number of parameters while keeping the original performance of the seeding DNN by applying a TT transformation to the fully-connected hidden layers. That is because TT-DNN only stores the TT-format of DNN, i.e., the set of low-rank core tensors, which can be used to approximately reconstruct the original DNN. Moreover, the running complexities of a DNN and corresponding TT-DNN are on the same order . In this work, we are interested in finding the best tensorized architecture that allows the best trade-off between model size and speech enhancement quality, and propose a novel hybrid architecture based on the convolutional layer in the bottom part to take advantage of feature extraction capabilities of the CNNs, and parameter reduction capabilities of TT-DNNs. We refer to this architecture as CV-TT-NN. All neural models investigated in this work are meant for tensor-to-vector regression, and three architectures are compared and contrasted:  CNNs,  TT-DNNs, and  hybrid CV-TT-NNs. We first explain the fundamental mechanisms of tensor-to-vector regression by reformulating a convolutional layer as a multiplication of two core tensors, which allows us to extend our theorems proposed in to tensor-to-vector regression neural models. Next, we move to the  speech enhancement experiments.  Our experimental results show that in single-channel speech enhancement on the Edinburgh noisy speech corpus , CNN outperforms the best DNN with a small increment of parameter sizes. Moreover, our proposed CNN-TT slightly outperforms CNN with only 32\% of the CNN model size. A further improvement can be attained if the size of the CNN-TT model is increased up to 44\% of the CNN model size. Finally, the experiments of a multi-channel speech enhancement task on a simulated noisy WSJ0 corpus  show the same trend that our proposed hybrid CNN-TT architecture can be favorably compared to both DNN and CNN models to achieve better-enhanced speech qualities and utilize much smaller model sizes.       We compare several tensor-to-vector regression models for speech enhancement. These models include CNN, DNN-TT, and the hybrid models composed of convolutional and TT layers, namely CNN-TT. We first discuss the representation power by linking tensor-to-vector regression to our earlier theories on DNN based vector-to-vector regression. Next, we evaluate these models for single-channel speech enhancement on the Edinburgh noisy speech database. Finally, we conduct multi-channel speech enhancement on a synthesized WSJ noisy corpus. Our experimental results suggest that CNN can outperform both DNN-TT and DNN with smaller regression errors and higher PESQ scores. Moreover, when the fully-connected output layer of CNN is replaced with a TT layer to generate a hybrid regression network, we achieve even better performances by gradually increasing the model size of the TT layer. In future work, we will investigate different tensor representations to reduce the parameters of the hidden convolutional layers.  \clearpage      
"," This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train  output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network  based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32\% of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44\% of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes. %Finally, experimental results on the multi-channel speech enhancement on the in-house noisy corrupted Wall Street Journal  corpus confirm our claims.  %This work applies various deep hybrid tensor-to-vector regression models for speech enhancement. In particular, we are mainly concerned with two goals:  Providing new insights into the tensor models based on approximation power of deep neural network  based tensor-to-vector functions from both theoretical and practical points of view, and  Finding different deep hybrid tensor architectures in terms of multiple metrics, i.e., the number of model parameters and speech enhancement performance. In particular, we focus on convolutional neural networks  and tensor-train neural networks  in dealing with tensorized speech data since the two architectures belong to the tensor-to-vector framework. Moreover, different hybrid architectures based on convolutional and tensor-train layers are evaluated. Our experiments of speech enhancement include both single-channel and multi-channel speech enhancement, and the empirical results suggest the following three findings:  CNNs attain better results than TT-NNs;  better speech enhancement results can be obtained by deploying neural models having convolutional and TT layers. % this work also investigates the use of Tucker decomposition to further reduce the number of CNN parameters, and strike a balance between speech enhancement performance and model complexity.",231
"  Subjective listening studies are the most reliable form of speech quality assessment for many applications, including speech enhancement and audio source separation. Listeners often rate the perceptual quality of testing materials using categorical or multi-stimuli rating protocols. The test materials are often artificially created by additively or convolutionally mixing clean speech with noise or reverberation at prescribed levels, to simulate real environments. Unfortunately, the simulated data does not capture all the intricate details of real environments , so it is not clear if these assessments are consistent with assessment results from real-world environments. Many investigations conclude that more realistic datasets and scenarios are needed to improve real-world speech processing performance. However, the cost and time-consuming nature of subjective studies also hinders progress.  Computational objective measures enable low cost and efficient speech quality assessment, where many intrusive, non-intrusive, and data-driven approaches have been developed. Intrusive measures, such as the perceptual evaluation of speech quality , signal-to-distortion ratio  and perceptual objective listening quality analysis , generate quality scores by calculating the dissimilarities between a clean reference speech signal and its degraded counterpart . These measures, however, do not always correlate well with subjective quality results.   Several non-intrusive  objective quality measures have been developed, including the ITU-T standard P.563, ANSI standard ANIQUE+, and the speech to reverberation modulation energy ratio . These approaches use signal processing concepts to generate quality-assessment scores. These approaches, however, rely on signal properties and assumptions that are not always realized in real-world environments, hence the assessment scores are not always consistent with human ratings. More recent work uses data-driven methods to estimate speech quality. The authors in  combine hand-crafted feature extraction with a tree-based regression model to predict objective PESQ scores. Quality-Net provides frame-level quality assessment by predicting the utterance-level PESQ scores that are copied as per-frame labels using a bidirectional long short-term memory  network. Similarly, NISQA estimates the per-frame POLQA scores using a convolutional neural network . It subsequently uses a BLSTM to aggregate frame-level predictions into utterance-level objective quality scores. These data-driven approaches perform well and increase the practicality of real-world assessment. However, the usage of objective quality scores as training targets is a major limitation, since objective measures only approximate human perception. Alternatively, the model developed in  predicts the mean opinion score  of human ratings, but the ratings are collected on simulated speech data. This approach advances the field, but it is not enough to ensure good performance in real environments. A complete approach is needed that predicts human quality ratings of real recordings.  In this study, we conduct a large-scale listening test on real-world data and collect 180,000 subjective quality ratings through Amazon's Mechanical Turk  using two publically-available speech corpora. This platform provides a diverse population of participants at a significantly lower cost to facilitate accurate and rapid testing. These corpora have a wide range of distortions that occur in everyday life, which reflect varying levels of noise and reverberation. Our listening tests follow the MUltiple Stimuli with Hidden Reference and Anchor  protocol. To the best of our knowledge, a large publically-available dataset that contains degraded speech and human quality ratings does not currently exist. We additionally develop an encoder-decoder model with attention mechanism to non-intrusively predict the perceived speech quality of these real-world signals. The encoder consists of stacked pyramid BLSTMs that convert low-level speech spectra into high-level features. This encoder-decoder architecture reduces the sequential size of the latent representation that is provided to an attention model. The key difference between this proposed approach and related approaches, is that our approach predicts mean-opinion scores of real-world signals using a novel deep-learning framework. The following sections discuss the details and results of our approach.        In this paper, we present a data-driven approach to evaluate speech quality, by directly predicting human MOS ratings of real-world speech signals. A large-scale speech quality study is conducted using crowdsourcing to ensure that our prediction model performs accurately and robustly in real-world environments. An attention-based pyramid recurrent model is trained to estimate MOS. The experimental results demonstrate the superiority of the proposed model in contrast to the baseline models and several state-of-the-art methods in terms of speech quality evaluation. The collected dataset will also be made available to facilitate future research efforts.        
"," The real-world capabilities of objective speech quality measures are limited since current measures  are developed from simulated data that does not adequately model real environments; or they  predict objective scores that are not always strongly correlated with subjective ratings. Additionally, a large dataset of real-world signals with listener quality ratings does not currently exist, which would help facilitate real-world assessment. In this paper, we collect and predict the perceptual quality of real-world speech signals that are evaluated by human listeners. We first collect a large quality rating dataset by conducting crowdsourced listening studies on two real-world corpora. We further develop a novel approach that predicts human quality ratings using a pyramid bidirectional long short term memory  network with an attention mechanism. The results show that the proposed model achieves statistically lower estimation errors than prior assessment approaches, where the predicted scores strongly correlate with human judgments.",232
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. %  % form to use if the first word consists of a single letter: % {A}{demo} file is .... %  % form to use if you need the single drop letter followed by % normal text : % {A}{}demo file is .... %  % Some journals put the first two words in caps: % {T}{his demo} file is .... %  % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word. {T}{ext} classification -- the procedure of designating pre-defined labels for text -- is an essential and significant task in many Natural Language Processing  applications, such as sentiment analysis  , topic labeling   , question answering   and dialog act classification .  In the era of information explosion, it is time-consuming and challenging to process and classify large amounts of text data manually.  Besides, the accuracy of manual text classification can be easily influenced by human factors, such as fatigue and expertise.  It is desirable to use machine learning methods to automate the text classification procedure to yield more reliable and less subjective results.  Moreover, this can also help enhance information retrieval efficiency and alleviate the problem of information overload by locating the required information.  Fig. illustrates a flowchart of the procedures involved in the text classification, under the light of shallow and deep analysis. Text data is different from numerical, image, or signal data. It requires NLP techniques to be processed carefully. The first important step is to preprocess text data for the model. Shallow learning models usually need to obtain good sample features by artificial methods and then classify them with classic machine learning algorithms. Therefore, the effectiveness of the method is largely restricted by feature extraction. However, different from shallow models, deep learning integrates feature engineering into the model fitting process by learning a set of nonlinear transformations that serve to map features directly to outputs.     % Generally, text classification approaches principally are separated into two branches: shallow learning and deep learning techniques, as shown in Fig..  % The task can be summarized as follows:  % the original input text should be preprocessed to obtain the vector representation for shallow learning and deep learning models . % For shallow learning models, the preprocessed data should extract features, representing a vector by representation learning methods.  % Then, a classifier learns the classification features. % The performance of the classification model is improved through iterative training with training sets and evaluation on validation sets.  % When pre-defined termination conditions are reached, it is time to predict the final label. % However, different from shallow models, deep learning integrates feature engineering into the model fitting process by learning a set of nonlinear transformations that serve to map features directly to outputs.         There have been several works reviewing text classification and its subproblems recently.  Two of them are reviews of text classification. Kowsari et al.  surveyed different text feature extraction, dimensionality reduction methods, basic model structure for text classification, and evaluation methods.  Minaee et al.  reviewed recent deep learning based text classification methods, benchmark datasets, and evaluation metrics.  Unlike existing text classification reviews, we conclude existing models from shallow to deep learning with works of recent years. % and compare among the deep learning models.  Shallow learning models emphasize the feature extraction and classifier design.  Once the text has well-designed characteristics, it can be quickly converged by training the classifier.  DNNs can perform feature extraction automatically and learn well without domain knowledge.  We then give the datasets and evaluation metrics for single-label and multi-label tasks and summarize future research challenges from data, models, and performance perspective.  Moreover, we summarize various information in three tables, including the necessary information of classic deep learning models, primary information of main datasets, and a general benchmark of state-of-the-art methods under different applications.  In summary, this study's main contributions are as follows:         %     The rest of the survey is organized as follows.  Section summarizes the existing models related to text classification, including shallow learning and deep learning models, including a summary table.  Section introduces the primary datasets with a summary table and evaluation metrics on single-label and multi-label tasks.  We then give quantitative results of the leading models in classic text classification datasets in Section.  Finally, we summarize the main challenges for deep learning text classification in Section before concluding the article in Section.       This paper principally introduces the existing models for text classification tasks from shallow learning to deep learning.  Firstly, we introduce some primary shallow learning models and deep learning models with a summary table.  The shallow model improves text classification performance mainly by improving the feature extraction scheme and classifier design.  In contrast, the deep learning model enhances performance by improving the presentation learning method, model structure, and additional data and knowledge. Then, we introduce the datasets with a summary table and evaluation metrics for single-label and multi-label tasks.  Furthermore, we give the quantitative results of the leading models in a summary table under different applications for classic text classification datasets.  Finally, we summarize the possible future research challenges of text classification.              if have a single appendix:  [Proof of the Zonklar Equations]   or      for no appendix heading   do not use 
"," Text classification is the most fundamental and essential task in natural language processing.  The last decade has seen a surge of research in this area due to the unprecedented success of deep learning.  Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey.  This paper fills the gap by reviewing the state of the art approaches from 1961 to 2020, focusing on models from shallow to deep learning.  We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions.  A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey.  Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.",233
"   Humour is an essential part of everyday communication, particularly in social media, yet it remains a challenge for computational methods.  Unlike conventional language, humour requires complex linguistic and background knowledge to understand, which are difficult to integrate with NLP methods.  An important step in the automatic processing of humour is to recognize its presence in a piece of text.  However, its intensity may be present or perceived to varying degrees to its human audience.  This level of appreciation  can vary according to the text's content and structural features, such as nonsense or disparagement or, in the case of puns, contextual coherence and the cognitive effort required to recover the target word~.  While previous work has considered mainly binary classification approaches to humorousness, the \HAHA shared task also focuses on its gradation.  This latter task is important for downstream applications such as conversational agents or machine translation, which must choose the correct tone in response to humour, or find appropriate jokes and wordplay in a target language.  The degree of creativeness may also inform an application whether the semantics of a joke can be inferred from similar examples.  This paper describes the OFAI--UKP system that participated in both subtasks of the \HAHA evaluation campaign: binary classification of tweets as humorous or not humorous, and the quantification of humour in those tweets.  Our system employs a Bayesian approach---namely, a variant of Gaussian process preference learning  that infers humorousness scores or rankings on the basis of manually annotated pairwise preference judgments and automatically annotated linguistic features.  In the following sections, we describe and discuss the background and methodology of our system, our means of adapting the \HAHA data to work with our system, and the results of our system evaluation on this data.      This paper has presented the OFAI--UKP system for predicting both binary and graded humorousness.  It employs Gaussian process preference learning, a Bayesian system that learns to rank and rate instances by exploiting pairwise preference judgments.  By providing additional feature data , the method can learn to predict scores for previously unseen items.  Though our system had previously achieved good results with rudimentary, task-agnostic linguistic features on two English-language tasks , its performance on the Spanish-language Twitter data of \HAHA was less impressive.  We tentatively attribute this to the information loss involved in the  conversion between the numeric annotations used in the task and the preference judgments required as input to our method, and to the fact that we do not normalize the Twitter data to match our linguistic resources.  Possible future work would include mitigating the above two problems , and by using additional, humour-specific features, including some of those used in past work as well as those inspired by the prevailing linguistic theories of humour.  The benefits of including word frequency also point to possible further improvements using -grams, tf--idf, or other task-agnostic linguistic features.    This work has been supported by the German Federal Ministry of Education and Research  under the promotional reference 01UG1816B , by the German Research Foundation  as part of the QA-EduInf project , by the DFG-funded research training group ``Adaptive Preparation of Information from Heterogeneous Sources'' , and by the Austrian Science Fund  under project M\,2625-N31.  The Austrian Research Institute for Artificial Intelligence is supported by the Austrian Federal Ministry for Science, Research and Economy.  
"," %   Most humour processing systems to date make at best discrete, coarse-grained distinctions between the comical and the conventional, yet such notions are better conceptualized as a broad spectrum.  In this paper, we present a probabilistic approach, a variant of Gaussian process preference learning , that learns to rank and rate the humorousness of short texts by exploiting human preference judgments and automatically sourced linguistic annotations.  We apply our system, which had previously shown good performance on English-language one-liners annotated with pairwise humorousness annotations, to the Spanish-language data set of the \HAHA evaluation campaign.  We report system performance for the campaign's two subtasks, humour detection and funniness score prediction, and discuss some issues arising from the conversion between the numeric scores used in the \HAHA data and the pairwise judgment annotations required for our method.",234
"     It is a common tendency among multilingual people who are non-native English speakers to code-mix in their speech using English-based phonetic typing. This linguistic phenomenon, particularly in social media like Twitter\footnote{https://twitter.com/}, poses a great challenge to the conventional Natural Language Processing  study area.      Within the context of the Sentiment Analysis, the study of the phenomenon of code-mixed language is important to the research community because this behavior is more common today. The interest in this area has grown due to the volume of data that social networks generate, and also by the value that this information has to understand people opinions when they are expressed in written texts.          In this paper, we explain our methodology to predict sentiment in tweets, describing how our method is based on a combination of the latest language models, and also how such models contributed to a great advance in this task. This configuration was employed and evaluated in the SemEval 2020 challenge , in which the goal is to predict the sentiment in code-mixed texts written in English and Hindi languages of a tweet . The models used in this combination are: MultiFiT  that an evolution of ULMFiT , BERT , ALBERT  and XLNet .      This work is organized as follows: Section  explains some related works, Section  describes the dataset used, Section  addresses the methodology applied in the task, Section  presents the results, and finally Section  expose our final considerations as well as possible future works.         In this paper, we propose a combination of four models for the Semeval 2020 , and our team got 72.7\  on F1 score in the competition. All of these models are based on using language models and transfer learning. They alone performed well, but together in an ensemble combination, they performed even better.          In some applications, it is difficult to use an ensemble consisted of four models, especially because of the overhead coming from time spent on inference, culminating in an approach that sometimes will not perform well. On the other hand, the individual results of these four models are very close, meaning that for this task, any model can be used.          It is important to note that MultFit has the worst result, but the difference is very small, and this specific model takes a lot less time to train, being the lightest model of the ensemble.          As future works, we intend to explore these models for Sentiment Analysis in other multilingual and monolingual scenarios.      include your own bib file like this: 
","     In this paper, we describe a methodology to predict sentiment in code-mixed tweets . Our team called verissimo.manoel in CodaLab\footnote{https://competitions.codalab.org/competitions/20654} developed an approach based on an ensemble of four models . The final classification algorithm was an ensemble of some predictions of all softmax values from these four models. This architecture was used and evaluated in the context of the SemEval 2020 challenge , and our system got 72.7\% on the F1 score.",235
"  Keyphrases are short pieces of text that summarize the key points discussed in a document. They are useful for many natural language processing and information retrieval tasks, such as, text summarization , question answering ,  % information extraction ,  sentiment analysis , document retrieval , document categorization or clustering , contextual advertisement , and more. In the automatic keyphrase generation task, the input is a document, and the output is a set of keyphrases that can be categorized as  or  keyphrases. Present keyphrases appear exactly in the target document, while absent keyphrases are only semantically related and have partial or no overlap to the target document. We provide an example of a target document and its keyphrases in Figure . % document categorization , contextual advertisement     Automatic keyphrase generation methods in literature can be broadly divided into  and  methods. A large pool of prior works have been devoted to extracting keyphrases by selecting  text spans or phrases directly from the target document   % and ranking based on their importance  . % mihalcea2004textrank However, due to their design principle, these approaches cannot predict absent keyphrases. In recent years, the neural sequence-to-sequence  framework  has become the fundamental building block in neural keyphrase generation models with its widespread usage in natural language generation tasks. The first deep neural keyphrase generation model, CopyRNN  adopts the Seq2Seq framework  with copy mechanism . % The copy attention mechanism enables the decoder to select words either according to a language model over the predefined vocabulary or according to a probability distribution computed over the input text sequence. % Thus, the copy enabled Seq2Seq methods are capable of generating both present and absent keyphrases. With the copy attention mechanism, the Seq2Seq models are capable of generating both present and absent keyphrases. A few subsequent works  extended CopyRNN to enhance keyphrase generation.   Although these generative approaches are capable of generating both present and absent phrases, they ignore the advantages of the extractive solutions, e.g., extracted keyphrases indicate the essential segments of the target document. %  recently proposed to combine an extractor that selects text spans as present keyphrases and a generator that generates the absent keyphrases word by word.  % One advantage of such a combined approach is the extractor can help the generator to identify important segments of the target document.  To generate a comprehensive set of keyphrases that summarizes the key points conveyed in the target document, reading the full document content is necessary. However, to the best of our knowledge, none of the previous neural methods are provisioned to read the full content of a document as it can be thousands of words long .  Processing such long documents through deep neural networks requires high computational resources. Hence, the existing neural methods truncate the target document; take the first few hundred words as input and ignore the rest of the document that may contain salient information.  To address the aforementioned challenges, in this paper, we propose SEG-Net  that has two major components,  a  that selects the salient sentences in a document and  an  that predicts the present keyphrases and generates the absent keyphrases jointly. The primary motivation to design the sentence-selector is to decompose a long target document into small segments, e.g., sentences, paragraphs, and identify the salient ones for keyphrase generation. % One potential solution is to decompose the target document into small segments, e.g., sentences, paragraphs, and identify the salient ones for keyphrase generation. For example, as shown in Figure , we split a document into a list of sentences and classify them with salient and non-salient labels. In this context, we consider a sentence as salient if it contains present keyphrases or overlaps with absent keyphrases.  In Figure , the sample document consists of six salient  and five non-salient sentences . A similar notion is adopted in prior works on text summarization  and question answering .   We employ   as the backbone of the extractor-generator in SEG-Net. We chose Transformer as it completely relies on a self-attention mechanism that is capable of capturing longer range dependencies. We equip the extractor-generator with a novel  coverage attention and an  copy attention such that the generated keyphrases summarize the entire target document. The layer-wise coverage attention keeps track of the target document segments that are covered by previously generated phrases to guide the self-attention mechanism in Transformer while attending the encoded target document in future generation steps.   We revise the standard copy mechanism  and propose an ``informed'' copy attention for keyphrase generation. Our revision is based on the observation that a word has a different meaning in the context of two different keyphrases. For example, in Figure , the word ``learning'' has a different meaning when used in ``computer assisted language learning'', ``integrated e learning'', and ``learning of foreign languages''.  Hence, we revise the copy mechanism so that SEG-Net does not copy a word from a present keyphrase while generating an absent keyphrase. Another motivation behind such a copy mechanism is to encourage the model to generate absent keyphrases that summarize the other segments of the target document that are not covered by the present keyphrases. % add 1 line   We train SEG-Net via multi-task learning to predict keyphrases as well as their part-of-speech  tags. We exploit the multi-layer structure of Transformer to perform both POS tagging and keyphrase generation. We evaluate SEG-Net on five benchmarks from scientific articles and two benchmarks from web documents to demonstrate its effectiveness over the state-of-the-art neural generative methods on both domains. We perform thorough ablation and analysis to present noteworthy findings such as  selecting salient sentences significantly improve present keyphrase extraction,  the layer-wise coverage attention and informed copy mechanism facilitates absent keyphrase generation,  jointly learning POS tagging and phrase prediction reduces duplicate and overlapping keyphrase generation.  %    [1]{\Note{Purple}{[1]{\Note{Orange}{[1]{\Note{Red}{[1]{\Note{Blue}{ % \theoremstyle{plain} {Task}        %% These commands are for a PROCEEDINGS abstract or paper. {June 03--05, 2018}{Woodstock, NY}      %% %% end of the preamble, start of the body of the document source.    %% %% The ""title"" command has an optional parameter, %% allowing the author to define a ""short title"" to be used in page headers. \title{Select, Extract and Generate: Neural Keyphrase Generation with Syntactic Guidance}  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research.  % \author{Wasi Uddin Ahmad, Xiao Bai, Soomin Lee, Kai-Wei Chang} % \affiliation{% %    % } %   \author{Wasi Uddin Ahmad} \affiliation{%    }   \author{Xiao Bai} \affiliation{%    }   \author{Soomin Lee} \affiliation{%    }   \author{Kai-Wei Chang} \affiliation{%    }   %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose.    %  %  %  %  \\  Generating a set of keyphrases that summarizes the core ideas discussed in a document has a significant impact on many applications, including document understanding, retrieval, advertising, and more. In recent years, deep neural sequence-to-sequence framework has demonstrated promising results in keyphrase generation. However, processing long documents using such deep neural networks requires high computational resources. To reduce the computational cost, the documents are typically truncated before given as inputs. As a result, the models may miss essential points conveyed in a document. Moreover, most of the existing methods are either extractive  or generative , and hence they do not benefit from the advantages of both modeling techniques. To address these challenges, we propose , a neural keyphrase generation model that is composed of two major components,  a selector that selects the salient sentences in a document, and  an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses a self-attentive architecture, known as,  as the building block with a couple of uniqueness. First, SEG-Net incorporates a novel  coverage attention to summarize most of the points discussed in the target document. Second, it uses an  copy attention mechanism to encourage focusing on different segments of the document during keyphrase extraction and generation. Besides, SEG-Net jointly learns keyphrase generation and their part-of-speech tag prediction, where the later provides syntactic supervision to the former. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin in both domains.  %Combining these two prevailing approaches in jointly learning to extract and generate can enable us to predict a comprehensive set of keyphrases.  % Relying on only one type of these prevailing approaches risks to miss important keyphrases.  % SEG-Net uses a self-attentive architecture, known as,  to learn document representations for its ability to capture long-range dependencies.  % SEG-Net has a couple of uniqueness compared to the vanilla Transformer.  % during keyphrase generation to emphasize less on the document segments, from where the keyphrases are extracted. % such that the extracted and generated keyphrases remain mutually exclusive.    %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %%   %  % <ccs2012> %  <concept> %   <concept_id>10010520.10010553.10010562</concept_id> %   <concept_desc>Computer systems organization~Embedded systems</concept_desc> %   <concept_significance>500</concept_significance> %  </concept> %  <concept> %   <concept_id>10010520.10010575.10010755</concept_id> %   <concept_desc>Computer systems organization~Redundancy</concept_desc> %   <concept_significance>300</concept_significance> %  </concept> %  <concept> %   <concept_id>10010520.10010553.10010554</concept_id> %   <concept_desc>Computer systems organization~Robotics</concept_desc> %   <concept_significance>100</concept_significance> %  </concept> %  <concept> %   <concept_id>10003033.10003083.10003095</concept_id> %   <concept_desc>Networks~Network reliability</concept_desc> %   <concept_significance>100</concept_significance> %  </concept> % </ccs2012> %   %  %  %  %    %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.              %% %% The acknowledgments section is defined using the ""acks"" environment %% . This ensures the proper %% identification of the section in the article metadata, and the %% consistent spelling of the heading.   %  % To Robert, for the bagels and explaining CMYK and color spaces. %   %% %% The next two lines define the bibliography style to be used, and %% the bibliography file. % \newpage %     %% %% If your work has an appendix, this is the place to put it. % \appendix     \endinput %% %% End of file `sample-sigconf.tex'.   In this paper, we present SEG-Net, a keyphrase generation model that identifies the salient sentences in a target document to utilize maximal information for keyphrase prediction. SEG-Net jointly learns to predict both present and absent keyphrases from the target document.  In SEG-Net, we incorporate novel layer-wise coverage and an informed copy attention to cover all the critical points in a document and diversify the present and absent keyphrases. We jointly train SEG-Net for keyphrase generation and their part-of-speech tags prediction. We evaluate SEG-Net on five benchmarks from scientific articles and two benchmarks from web documents. The experiment results demonstrate that it is effective over the state-of-the-art neural generative methods on both scientific and web domains.  
"," %  %  %  %  \\  Generating a set of keyphrases that summarizes the core ideas discussed in a document has a significant impact on many applications, including document understanding, retrieval, advertising, and more. In recent years, deep neural sequence-to-sequence framework has demonstrated promising results in keyphrase generation. However, processing long documents using such deep neural networks requires high computational resources. To reduce the computational cost, the documents are typically truncated before given as inputs. As a result, the models may miss essential points conveyed in a document. Moreover, most of the existing methods are either extractive  or generative , and hence they do not benefit from the advantages of both modeling techniques. To address these challenges, we propose , a neural keyphrase generation model that is composed of two major components,  a selector that selects the salient sentences in a document, and  an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses a self-attentive architecture, known as,  as the building block with a couple of uniqueness. First, SEG-Net incorporates a novel  coverage attention to summarize most of the points discussed in the target document. Second, it uses an  copy attention mechanism to encourage focusing on different segments of the document during keyphrase extraction and generation. Besides, SEG-Net jointly learns keyphrase generation and their part-of-speech tag prediction, where the later provides syntactic supervision to the former. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin in both domains.  %Combining these two prevailing approaches in jointly learning to extract and generate can enable us to predict a comprehensive set of keyphrases.  % Relying on only one type of these prevailing approaches risks to miss important keyphrases.  % SEG-Net uses a self-attentive architecture, known as,  to learn document representations for its ability to capture long-range dependencies.  % SEG-Net has a couple of uniqueness compared to the vanilla Transformer.  % during keyphrase generation to emphasize less on the document segments, from where the keyphrases are extracted. % such that the extracted and generated keyphrases remain mutually exclusive.",236
"   {) systems additionally provide formative feedback to guide revision. Although neural networks currently generate state-of-the-art AES results , non-neural AES create feature representations more easily useable by AWE . We believe that neural AES can also provide useful information for creating feature representations, e.g., by exploiting information in the intermediate layers.     Our work focuses on a particular source-based essay writing task called the response-to-text assessment  . Recently, an RTA AWE system  was built by extracting rubric-based features related to the use of {[t] { {|p{20cm}|}  $ prompt and an essay with score of 3.}        This paper proposes , a method for  using the attention scores in a neural AES model  taking essay scores into account  to capture the importance of essays, sentences, and words when  to automatically extract the Topical Components of a source text. Evaluations show the potential of  for eliminating expert effort without degrading  performance or the feature representations themselves.    outperforms  baselines and  generates comparable or even better results than a manual approach.  Although  outperforms all baselines and requires no human effort on TC extraction, annotation of essay evidence scores is still needed. This leads to an interesting future investigation direction, which is training the  using the gold standard that can be extracted automatically.   One of our next steps is to investigate the impact of TC extraction methods on a corresponding AWE system, which uses the feature values produced by  to generate formative feedback to guide essay revision.  The future work will focus on more experiments on the AWE system.  Currently, the  are trained on student essays, while the  only works on the source article. However,  uses both student essays and the source article for TC generation. It might be hard to say that the superior performance of  is due to the neural architecture and attention scores rather than the richer training resources. Therefore, a comparison between  and a model that uses both student essays and the source article is needed.    
"," While  automated essay scoring  can reliably grade essays at scale,  %a grade without feedback  is not enough to guide students in essay revision.  automated writing evaluation   additionally provides formative feedback  to guide essay revision. However,   %the feedback produced by  %generating AWE feedback is often not driven by %AES. %For example,  a neural AES typically does not provide useful  feature representations for supporting AWE. %AWE information such as rubric-based grading. %explanations. This paper presents  a method for linking AWE and  neural AES, by extracting Topical Components  representing evidence from a source text  using the intermediate output of attention layers. %of a neural AES. %for source-based AES.  %Specifically,  %to generate TCs, %we  use the  outputs of the attention layers of a hierarchical neural AES as criteria for filtering irrelevant content from essays. %We then use TCs  to support feature-based AES and AWE systems.  We  evaluate  performance   using a feature-based AES  %for a grading rubric expressed in terms of  requiring TCs.  Results show that performance is comparable whether using automatically  or manually constructed TCs for 1) representing essays as rubric-based features, 2) grading essays.   %Recently, automated essay scoring  systems were developed in order to grade essays on large scale meanwhile reduce human effort significantly. However, an essay score usually does not have the ability to provide advice to students on how to revise their essays. Therefore, automated writing evaluation systems  are also required which provides feedback for essay revision. Although the neural network generates the state-of-the-art AES results, an end-to-end neural network could not provide enough information to the AWE system which requires more feature engineering. This paper presents an investigation of using the intermediate output of the neural network of AES for extracting keyword or keyphrase  to reduce the expert effort which is useful in the AWE system. Specifically, we use outputs of attention layers of a hierarchical neural network explicitly as criteria for filtering irrelevance content from student essays. This paper shows that TCs generated by our model could support an existing AWE for providing formative feedback. We evaluate our model on source-based AES task, and results show that our model is promising.",237
"  %\url{}, accessed \printdate{}  Reading is increasingly carried out by means of online multiple texts, which can simultaneously consist of  texts of diverse genres, registers, authorships, credibilities etc.\ .  % That is, learning takes place, so to speak, on the basis of  whose components are gathered from a constantly growing, nowadays mostly web-based information landscape  or space . % \textcolor{black}{Consequently,  speak of  reading as an intertextual process.}     assume that intertext models represent selected constituents of multiple texts as  together with entity-related information .  % This is supplemented by three types of links: % IM-related source-to-source , MM-related content-to-content and source-to-content links . % A prediction of the DM, which is crucial for our work, is that the probability of generating an intertext model as a result of reading a multiple text is a function of the number of the texts involved, their authors, the perspectives they provide on the described situation , the tasks to be accomplished and other contextual factors . % This suggests to speak of the intertext model as a kind of  cognitive map  of the underlying multiple text, where the MM abstracts from this textbase : % that is, readers produce intertext models as cognitive maps of multiple texts as parts of the underlying IL, while groups or communities of readers produce distributed cognitive maps  of larger sections of the IL or the IL as a whole. % This duality of small- and large-scale reading processes leads to the object of this article.  % That is, we can ask how the IL looks like from the perspective of these distributed cognitive maps or vice versa, how it presents itself to different reader communities. % The latter question will be in the focus of this article.  Although the DM takes the necessary step of generalizing the CIM towards modeling multiple texts, it is largely single reader-oriented. % To broaden this focus, we generalize the DM conceptually in two steps:       % In a first step, which is still within the boundaries of the DM, we consider intertext models as reader-centered approximations of parts of the IL with varying degrees of explicitness . % This predicts that different readers can approximate different parts  of the IL just as they can align  their intertext models of overlapping parts depending on their interaction\textcolor{black}{, which according to  is a characteristic of non-academic online reading} and also regards online collaborative learning . % Starting from the context model of ,  Figure  illustrates this alignment scenario in terms of a situation semantic adaptation : % two readers  and  read not necessarily different multiple texts in related contexts  to solve the same or related tasks  in order to achieve the same or related goals. % We assume that the texts originate from an IL in whose context they describe a situation  and that  and  refer to the same resource situation  to understand which situation their multiple texts actually describe, where all references to contextual units are indirect:  % they are mediated through mental representations  of multiple texts , described situations , reading contexts , task contexts  and resource situations  or knowledge space). % As a result of collaborative, cooperative or simply parallel reading processes, the representations of the readers may align with each other, in the short-term  or long-term . % That is, as illustrated in Figure ,  and  have the possibility to align their mental representations so that they understand the same or similar multiple texts as descriptions of the same or similar situations.\footnote{Adding the notion of a described situation to the context model of  allows for distinguishing communication scenarios such as misunderstandings , disinformation  or misinformation   and related phenomena.} % Evidently, such an alignment requires many things, but at least the chance that both readers have access to the same or semantically sufficiently similar texts from which they can extract the same or sufficiently similar multiple texts.  % But do they? % This question brings us to the second step of generalizing the DM: %   % From the perspective of reader communities, reading is a distributed process that approximates a multifaceted IL, so that both the IL and its distributed representation by innumerable intertext models jointly develop. % Obviously, parallel to the diversity of the IL, communities of readers are also diverse as a result of a wide range of factors : % for example, membership in different language communities , ethnicities, cultures, age groups, social groups , residency in different places  or practice of different social roles. % In any event, in analogy to Step 1, we may expect that different communities dealing with the same or semantically similar parts of the IL should be able to align their corresponding intertext models among each other. % In shorter terms:  % different groups should be able to represent similar parts of the IL in a similar way. % But do they actually have access to the same or at least sufficiently similar parts of the IL -- especially under the condition that they deal with the same topic?    {}  -- even in the case of algorithmically  linked documents . % But how uniformly does the IL present itself to its  readers?  % Obviously this question is currently outside the scope of the framework of the DM and its relatives. %   Step 2 concerns precisely the viewpoint of this article. % That is, we are concerned with a central prerequisite for alignable intertext models among readers as members of large communities.  % This refers to the intertextual shape of the IL from the perspective of different communities who may have different accesses to it or  different landscapes, even in situations where the opposite would be assumed. % The DM and related approaches do not model what the multiple texts are extracted from and what countless intertext models in their distributed totality ultimately represent, that is, an underlying multifaceted, highly dynamic IL, its numerous document nodes and their relational, intertextual embeddings.  According to , reading research mostly considers small amounts of offline texts pre-selected by the experimenter rather than open ILs in which users decide what to read. % But if reading is a kind of problem solving that involves multiple search and decision processes  , then the question arises as to the limits of these processes as imposed by the IL and how they differ for which reader communities. % Apparently, approaches to multiple texts focus on micro-models that leave the corresponding macro-models, which inform about the shape of the IL and its organizational laws, under-specified. % The present paper takes a step in the direction of filling this gap: % it develops a macroscopic model of the IL and examines how its shape appears from the perspective of certain large-scale reader communities. % Our aim is, so to speak, to impart knowledge about the  in which the sort of reading takes place which according to  is to become the subject of reading research. % Thus, our approach is complementary to current research on the intertext model: % we study the IL underlying the construction of intertext models from a macroscopic perspective, in contrast to reading research, which starts from a microscopic perspective of small groups or individual readers . % \textcolor{black}{In terms of the integrated framework of multiple texts  we are concerned with the intertextuality of those information units to which the cognitive strategies and behavioral skills of  readers are related.} % That is, in modification of the fourth goal of future research on the use of multiple sources according to , we deal with the phenomenon that different communities are offered different information, especially in the context of the same topic. % The extent to which this phenomenon applies to different language communities will be examined using the example of the most frequently used knowledge resource on the Web, that is Wikipedia \rrid{RRID:SCR\_004897}.  %\textcolor{red}{Our contributions are:}  The article is organized as follows: % Section  explains the relevance of Wikipedia for educational science and gives an overview of related research. % Section  explains our research questions and describes in detail the methods we have developed to answer them.  % In Section , we describe our experiments and discuss their results. % Finally, in Section  we give a conclusion and an outlook on future work.       We introduced a three-level topic model in combination with a graph-theoretical model for measuring the intra- and intertextual similarities of article networks from different language editions of Wikipedia.    This has been done to examine a sort of linguistic relativity of Wikipedia according to which the choice of the language has a great influence about the information   In this way we built a bridge between reading research, educational science and computational linguistics.   To this end, we described a new perspective for reading research that focuses more on the information landscape as a limiting factor of online reading.    We have continued research showing that Wikipedia exhibits a topical coverage bias.   However, we have done this using a much more elaborate topic and text structure model in conjunction with a quantitative model of hypertext structure, a hybrid model that is more realistic from the perspective of hypertext linguistics .   In future work we will continue elaborating our computational hypertext model.    This will be done with special attention to hypertext usage to obtain graph models that integrate browsing behavior into graph similarity analysis.          
","  We test the hypothesis that the extent to which one obtains information on a given topic through Wikipedia depends on the language in which it is consulted.  % Controlling the size factor, we investigate this hypothesis for a number of 25 subject areas.  % %That is, we test whether even in cases where two Wikipedias cover the same topic with approximately the same number of articles, they inform about this topic rather differently. % Since Wikipedia is a central part of the web-based information landscape, this indicates a language-related,  linguistic bias.  % The article therefore deals with the question of whether Wikipedia exhibits this kind of linguistic relativity or not. % From the perspective of educational science, the article develops a computational model of the information landscape from which multiple texts are drawn as typical input of web-based reading. % For this purpose, it develops a hybrid model of intra- and intertextual similarity of different parts of the information landscape and tests this model on the example of 35 languages and corresponding Wikipedias. % In this way the article builds a bridge between reading research, educational science, Wikipedia research and computational linguistics.  %%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details. % % %Deadline: 15.\,05.~2020\par %For full guidelines regarding your manuscript please refer to {Author Guidelines}.  %As a primary goal, the abstract should render the general significance and conceptual advance of the work clearly accessible to a broad readership. References should not be cited in the abstract. Leave the Abstract empty if your article does not require one, please see {Summary Table} for details according to article type.    \tiny     %All article types: you may provide up to 8 keywords; at least 5 are mandatory.",238
"  Context and Scope: The US healthcare system is a complex setup governed and managed by state and federal agencies. Managed Care is a health delivery system utilised by Medicaid to manage cost, utilization and quality of healthcare. The Managed Care system uses contract agreements between Medicaid agencies and Managed Care Organisations  for providing these services. Some states even utilize this system beyond traditional managed care for initiatives such as care improvement for chronic \& complex conditions, payment initiatives, etc. Contracts run the gamut from computer support to janitorial services to direct client services. HHS posts all notifications of new Request for Proposal /solicitation releases, Requests for Application and Open Enrolments. RFPs are bid requests consisting of functional and non-functional requirements for different services. These also outline model contracts and the expected format of the proposals. The requirements are mentioned in the form of different questions/queries which are answered by each proposal/response to these RFPs. The procurement of these contracts entirely depends upon the scores obtained for each response based on the predefined evaluation criteria. A contract is generally awarded to the best scoring respondent.      A typical RFP bid consists of RFP advertisement, RFP itself, a model contract, proposals/responses from bidding entities  and scoring sheets for all the submissions. RFPs and supporting documents are publicly available information. MCOs typically utilise historical submissions to understand the requirements and respond better to improve their chances of winning a bid. Every RFP response  typically runs into several hundred pages which are spread across different websites and data stores. Manual exploration of historical bids is a time consuming and iterative process. Given the changing healthcare landscape, limited time-frame and resources to draft new responses, the current process is not comprehensive enough to extract insights and derive competitive advantage.     Challenges: Apart from being an industry specific problem statement, our work also poses a unique challenge of scoring entire documents. Most relevant efforts towards automatic scoring have dealt with with short answers  and essays . Our work deals with much larger sequence lengths, and a larger feature space to capture. Another difference with relevant literature is that RFPs are written by experts over multiple iterations, as opposed to students writing essays for evaluation. As such, this removes the need to check for superficial grammatical errors. Instead, there is a need to identify which aspects of the text enhance scores  and those which diminish it .  Our Solution:  In this paper, we propose an automated framework using interpretable natural language processing techniques to analyse RFP responses. The framework comprises of two components: Text Processing Module and an Interpretable Scoring Model.  RFP responses usually do not follow any standard template/formatting and are available in Portable Document Format or PDF for short. Moreover, to understand the content and extract insights, the text needs to be extracted at the most granular level . These issues complicate the text extraction process  and thus the need to develop a Text Processing Module. We have developed a generic Text processing module that would extract text from different formats of response. The extracted text is then analysed using our Interpretable Scoring Model. The scoring model enables us to identify terms/phrases and other auxiliary features which impact the section/question score positively and negatively. We term positively impacting features as enablers and negatively impacting ones as disablers. The framework also provides insights about auxiliary features which latently impact overall scoring. The framework also provides a single portal/platform to access historical bid responses for similar details across bidders and states.  %  Major Contribution of this work is as follows, we have:        We introduced a new problem statement to NLP researchers, automatic scoring of Request for Proposals  for the insurance industry. Using a generic pdf parser, we collected data for 1300 RFP responses across multiple states in US and preprocessed it to be analysed by Natural Language Processing Pipelines. We built a scoring system using Deep Learning approaches and introduced an interpretable system for identification of enabler and disabler words and phrases. These interpretations assist experts in writing better RFP responses. Future work includes building a multimodal system that can model the aesthetic as well as content wise qualities of proposal documents.        
"," The Managed Care system within Medicaid  uses Request For Proposals  to award contracts for various healthcare and related services. RFP responses are very detailed documents  submitted by competing organisations to win contracts. Subject matter expertise and domain knowledge play an important role in preparing RFP responses along with analysis of historical submissions. Automated analysis of these responses through Natural Language Processing   systems can reduce time and effort needed to explore historical responses, and assisting in writing better responses.  Our work draws parallels between scoring RFPs and essay scoring models, while highlighting new challenges and the need for interpretability. Typical scoring models focus on word level impacts to grade essays and other short write-ups. We propose a novel Bi-LSTM based regression model, and provide deeper insight into phrases which latently impact scoring of responses. We contend the merits of our proposed methodology using extensive quantitative experiments. We also qualitatively asses the impact of important phrases using human evaluators. Finally, we introduce a novel problem statement that can be used to further improve the state of the art in NLP based automatic scoring systems.",239
"  Language model pre-training has shown great power for improving many natural language processing tasks.  Most pre-training models, despite their variety, follow the BERT architecture heavily relying on multi-head self-attention to learn comprehensive representations. It has been found that 1) though the self-attention module in BERT is a highly non-local operator, a large proportion of attention heads indeed learn local dependencies due to the inherent property of natural language;  2) removing some attention heads during fine-tuning on downstream tasks does not degrade the performance.  The two findings indicate that heavy computation redundancy exists in the current model design. In this work, we aim to resolve this intrinsic redundancy issue and further improve BERT w.r.t. its efficiency and downstream task performance. We consider such a question: can we reduce the redundancy of attention heads by using a naturally local operation to replace some of them?  We notice that convolution has been very successful in extracting local features , and thus propose to use convolution layers as  a more efficient complement to self-attention for addressing local dependencies  in natural language.      Specifically, we propose to integrate convolution into self-attention to form a  mechanism that combines the advantages of the two operations. Self-attention uses all input tokens to generate attention weights for capturing global dependencies, while we expect to perform local ``self-attention'', i.e., taking in a local span of the current token to generate ``attention weights'' of the span to capture local dependencies. To achieve this, rather than deploying standard convolution with fixed parameters shared for all input tokens,  dynamic convolution  is a good choice that offers higher flexibility in capturing local dependencies of different tokens.  As shown in Fig.b, dynamic  convolution uses a kernel generator to produce different kernels for different input tokens.  However, such dynamic convolution cannot differentiate the same tokens within different context and generate the same kernels .    We thus develop the , a novel convolution that produces more adaptive convolution kernels by receiving an input span instead of only a single token, which enables  discrimination of generated kernels for the same tokens within different context. For example, as shown in Fig.c, the proposed  produces different kernels for different ``can'' tokens. With , we build the  to improve the conventional self-attention, which brings higher efficiency for pre-training as well as better performance for capturing global and local information.    To further enhance performance and efficiency, we also add the following new architecture design to BERT.  First, a bottleneck structure is designed to reduce the number of attention heads by embedding input tokens to a lower-dimensional   space for self-attention.  This also relieves the redundancy that lies in attention heads and improves efficiency.  Second, the feed-forward module in BERT consists of two fully connected linear layers with an activation in between, but the dimensionality of the inner-layer is set much higher  than that of input and output, which promises good performance but brings large parameter number and computation.  Thus we devise a grouped linear operator for the feed-forward module, which reduces parameters without hurting representation power.  Combining these novelties all together makes our proposed model, termed ConvBERT, small and efficient.      Our contributions are summarized as follows. 1) We propose a new  to replace the self-attention modules in BERT, which leverages the advantages of convolution to better capture local dependency.  To the best of our knowledge, we are the first to explore convolution for enhancing BERT efficiency. 2) We introduce a novel  operation to utilize multiple input tokens to dynamically generate the convolution kernel.  3) Based on the proposed , we build ConvBERT model. On the GLUE benchmark, ConvBERTbase achieves 86.4 GLUE score which is 5.5 higher than BERTbase and 0.7 higher than ELECTRAbase while requiring less training cost and parameters. 4) ConvBERT also incorporates some new model designs including the bottleneck attention and grouped linear operator that are of independent interest for other NLP model development.    \documentclass{article}  % if you need to pass options to natbib, use, e.g.:     \PassOptionsToPackage{numbers, compress}{natbib} % before loading neurips_2020  % ready for submission % \usepackage{neurips_2020}  % to compile a preprint version, e.g., for submission to arXiv, add add the % [preprint] option:     % \usepackage[preprint]{neurips_2020}  % to compile a camera-ready version, add the [final] option, e.g.:     \usepackage[final]{neurips_2020}  % to avoid loading the natbib package, add option nonatbib: % \usepackage[nonatbib]{neurips_2020}  \usepackage[utf8]{inputenc} % allow utf-8 input \usepackage[T1]{fontenc}    % use 8-bit T1 fonts \usepackage{hyperref}       % hyperlinks \usepackage{url}            % simple URL typesetting \usepackage{booktabs}       % professional-quality tables \usepackage{amsfonts}       % blackboard math symbols \usepackage{nicefrac}       % compact symbols for 1/2, etc. \usepackage{xcolor} \usepackage{microtype}      % microtypography \usepackage{xspace} \usepackage{subcaption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{pifont} \usepackage{floatrow} \floatsetup[table]{capposition=top} \usepackage{float} \usepackage{wrapfig} \usepackage{comment} \usepackage{caption} \definecolor{mypink}{RGB}{251,228, 234}    {span-based dynamic convolution} {mixed attention}  \title{ConvBERT: Improving BERT with Span-based Dynamic Convolution}  % The \author macro works with any number of authors. There are two commands % used to separate the names and addresses of multiple authors: \And and \AND. % % Using \And between authors leaves it to LaTeX to determine where to break the % lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4 % authors names on the first line, and the last on the second line, try using % \AND instead of \And before the third author name.   \author{Zihang Jiang\thanks{Work done during an internship at Yitu Tech.}~~\thanks{Equal contribution.}~~, Weihao Yu\samethanks~~, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan \\   National University of Singapore,    Yitu Technology\\ 	@gmail.com,} \\ 	, ~ ,  ~  \\ }           %   % \newpage     We present a novel span-based dynamic convolution operator and integrate it into the self-attention mechanism to form our mixed attention block for language pre-training.  We also devise a bottleneck structure applied to the self-attention module and a grouped linear operation for the feed-forward module. Experiment results show that our ConvBERT model integrating above novelties achieves consistent  performance improvements while costing much less pre-training computation.  
"," Pre-trained language models like BERT and its variants have recently achieved impressive performance in various natural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers large memory footprint and computation cost. Although all its attention heads query on the whole input sequence for generating the attention map  from a global perspective, we observe some heads only need to learn local dependencies, which means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to replace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the rest self-attention heads, form a new mixed attention block that is more efficient at both global and local context learning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that ConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training costs and fewer model parameters. Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than ELECTRAbase, using less than $1/4$ training cost. \footnote{Code and pre-trained model will be released at \url{https://github.com/yitu-opensource/ConvBert}.}",240
"    One of the defining characteristics of human languages is that they are productive. % We can combine together concepts in novel ways to express ideas that have never been thought of before. % This is for a good reason: as children, we observe very little of our world before we must speak to others, meaning that even mundane language is novel and not just parroting back something already expressed for us. % Similarly, even with massive data collection efforts, deep models can only have an opportunity to observe a small subset of the possible utterances and worlds. % This problem becomes especially acute when those models must drive the behavior of a robot, because misunderstanding a command may pose a serious safety hazard.  Recently, there have been a number of attempts to probe the understanding of deep networks trained to perform linguistic and robotic tasks. %  point out that generalization to novel compositions of concepts is rather limited. % This is not a matter of the amount of data available; for example,  find that even networks with the same test set performance can have very different generalization abilities. % Most recently,  released gSCAN, a dataset for testing the generalization abilities of grounded robots. % In gSCAN, a robotic agent must follow a natural-language command in a 2D environment. % Commands of specific types are systematically held out; for example, no command with a particular adjective-noun combination might appear in the training set. % When all concepts appear in the training set via a random split between the training and test set, performance is phenomenal: 97\% of commands are executed correctly. % Yet, when even the simplest combinations are missing from the training set, such as holding out an adjective-noun pair like ``yellow squares'', only 35\% to 55\% of commands are executed correctly.  Guided by the notion that compositionality is the central feature of human languages which deep networks are failing to internalize, we construct a compositional deep network to guide the behavior of robots, thus building on the work of . % Given a command, a command-specific network is assembled from previously-trained components. % Components are automatically discovered in the training set without any annotation. % The structure that combines those components is derived from the linguistic structure of the command. %  In this way, the compositional structure of language is reflected in the compositional structure of the computations executed by the network. % This approach generalizes where non-compositional networks fail to do so thereby addressing many of the limitations pointed out in gSCAN and prior work. % For example, our compositional approach correctly executes 95\% of commands involving novel adjective-noun pairs, compared with 35\% to 55\% reported in prior work.    Compositionality is not specific to any one dataset -- it is a general principle -- and the implementation we provide here is not specific to gSCAN. % Even though our base network achieves the same 97\% performance as the state-of-the-art approaches in gSCAN, it generalizes significantly better in a number of ways. % It outperforms the state of the art in most test conditions explored by gSCAN, generally by a large margin. % Where this approach shines is predicted well by the types of compositionality that exist in the network. % For example, novel combinations of concepts related to individual objects perform well. % However, no compositionality exists at the level of the map or directions, just in how concepts are combined, meaning that novel combinations of directions are not well understood; but even in this case, our approach is outperforming the state of the art by 5\%.  An additional benefit of compositional approaches appears to be that they open the door to naturally including other general linguistic principles. % For example, it appears that not all parses are made equal. % In our case, network structures derived from a semantic parser lead to better performing agents compared to structures derived from a constituency parser and a dependency parser. % We show another example of this idea by incorporating the lexical semantics of words as an additional loss while training the network. % This incorporates general notions such as big and small being antonyms of one another.  Our approach forgoes the most popular mechanism for increasing the generalization performance of neural networks: data augmentation. % Work on gSCAN shows that the gains from data augmentation appear to be low, but still significant. % Data augmentation has substantial drawbacks: it is arbitrary, it slows down runtime, and it is dataset and problem specific. % In addition, data augmentation introduces many parameters that must be tuned by hand and much knowledge that must be provided by humans. % Instead, we show that the generic principle of compositionality can replace data augmentation without any of these drawbacks: no human annotation is required; no additional systems or parameters need to be introduced;  and training time is not affected. % It remains an open question whether every data augmentation approach has a corresponding compositional structure that can supplant and generalize it. % Compositional approaches could be combined with data augmentation, potentially raising their performance even further.  % % The state of the art performance on that dataset demonstrates stunning % performance drops., even novel adjective-noun combinations drop % command-following performance from 97\% to 38\%.  %  the agent needs to understand what each word means, disentangle the meaning of each word %  combine the learned words to interpret a new command %  only need few data to acquire the meanings of new words  Our work makes four contributions.    	 %===============================================================================      We have presented a model that addresses many of the compositionality challenges found in gSCAN, a dataset specifically designed to challenge neural networks.   We show that there is nothing inherently wrong with seq2seq models: they are able to generalize to new compositional concepts; they just require a helping hand to do so.    When the compositionality inherent in a problem is reflected in the computation of a neural network, it appears that the resulting network is far better able to understand the target domain.   This is only critical at test time, when generalizing to new combinations, but of course, this is the case that is most relevant to grounded robotics.   An important caveat is that a mechanism for making representations and sub-networks compatible with one another is key.   Here we do this by constraining all communication to go through attention maps.   Other mechanisms may exist.  Performance of compositional approaches depends on what is being composed and how.   When the compositionality does not capture part of a problem domain, such as condition D here, it has no role and does not meaningfully improve results.   When compositionality is relevant, it appears that it can supplant data augmentation and provide a faster, principled, and dataset-agnostic method with fewer parameters to achieve even better results.   When compositionality is derived from language, it enables us to include linguistic ideas as a fundamental part of our models, like the notion of synonyms and antonyms.  The most suggestive and admittedly tenuous implication of this work is that perhaps, as grounded robotics develops, we can use this approach to test linguistic representations.   Many formalisms exist in linguistics for encoding the semantics of sentences and centuries of debate have not brought us closer to agreement.   Without an independent test for which formalism is better, convergence to one notion of semantics is unlikely.   It appears that when grounded compositional models are trained to perform tasks, some representations are significantly better than others.   It also appears that the more semantic those representations are, i.e., the more they abstract away the surface syntax of language, the better the resulting robotic model is.   Perhaps in the future grounded robotics will come full circle, starting from borrowing ideas from linguistics, to contributing to our understanding of language by pinpointing what language actually means.  In the meantime, robots will continue to be deployed.   As well as conversational agents.   It is critical that we have confidence in our systems and that input merely being out of the training set does not cause catastrophic failure.   We demonstrate one step toward achieving this goal: a principled way to enable networks to generalize out of the training set.   Many open problems remain, key among them: is there a way to convert a data augmentation approach into a network architecture that sees through the problem and generalizes better for a principled reason without the data augmentation.   This would be a powerful tool, which we suspect exists, but have not yet found.    The maximum paper length is 8 pages excluding references and acknowledgements, and 10 pages including references and acknowledgements      no 
","   Humans are remarkably flexible when understanding new sentences that include   combinations of concepts they have never encountered before. Recent work has   shown that while deep networks can mimic some human language abilities when   presented with novel sentences, systematic variation uncovers the limitations   in the language-understanding abilities of neural networks. We demonstrate that these   limitations can be overcome by addressing the generalization challenges in a   recently-released dataset, gSCAN, which explicitly measures how well a robotic   agent is able to interpret novel ideas grounded in vision, e.g., novel   pairings of adjectives and nouns. The key principle we employ is   compositionality: that the compositional structure of networks should reflect   the compositional structure of the problem domain they address, while allowing   all other parameters and properties to be learned end-to-end with weak   supervision. We build a general-purpose mechanism that enables robots to   generalize their language understanding to compositional domains. Crucially,   our base network has the same state-of-the-art performance as prior work, 97\%   execution accuracy, while at the same time generalizing its knowledge when   prior work does not; for example, achieving 95\% accuracy on novel   adjective-noun compositions where previous work has 55\% average   accuracy. Robust language understanding without dramatic failures and without   corner causes is critical to building safe and fair robots; we demonstrate the   significant role that compositionality can play in achieving that goal.",241
"  The proliferation of disinformation online, commonly known as ``fake news'', has given rise to a lot of research on automatic fake news detection. However, most of the efforts have focused on checking whether a piece of information is factually correct, and little attention has been paid to the propaganda techniques that malicious actors use to spread their message.  SemEval-2020 Task 11  aims to bridge this gap. It focused on detecting the use of propaganda techniques in news articles, creating a dataset that extends , and offering two subtasks:   detecting the propaganda spans in an article;  detecting the type of propaganda used in a given text span.    Below, we describe the systems we built for these two subtasks. At the core of our systems is RoBERTa , a pre-trained model based on the Transformer architecture . However, we improved over RoBERTa by adding extra layers in the neural network architecture, and we further added some post-processing steps. We further applied transfer learning between the two subtasks, and finally, we combined different models into an ensemble.  %The rest of this paper is organized as follows. Section discusses related work. Section describes the general architecture of our models. Section provides further details about our experimental setup. Section discusses the results. Finally, Section offers a conclusion.       We described the system we developed for the SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. We developed ensembles of RoBERTa-based neural architectures, additional CRF layers, transfer learning between the two subtasks, and advanced post-processing to handle the multi-label nature of the task, the consistency between nested spans, repetitions, and labels from similar spans in training. We achieved sizable improvements over baseline RoBERTa models, and the official evaluation ranked our system   out of 36 teams on the span identification subtask, and   out of 31 teams on the techniques classification subtask.  In future work, we plan to explore other neural architectures such as T5  and GPT-3 . We further want to explore transfer learning from other tasks such as argumentation mining  and offensive language detection .       An incremental analysis of our system on the development set.}      Results on the test set .}   
","   We describe our system for SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. We developed ensemble models using RoBERTa-based neural architectures, additional CRF layers, transfer learning between the two subtasks, and advanced post-processing to handle the multi-label nature of the task, the consistency between nested spans, repetitions, and labels from similar spans in training. We achieved sizable improvements over baseline fine-tuned RoBERTa models, and the official evaluation ranked our system $\mathbf{3^{rd}}$  out of 36 teams on the span identification subtask with an F1 score of 0.491, and $\mathbf{2^{nd}}$  out of 31 teams on the technique classification subtask with an F1 score of 0.62.",242
"  The 2019--2020 coronavirus pandemic has disrupted lives, societies and economies across the globe. Its classification as a pandemic highlights its global impact, touching people of all languages. Digital content of all types  have focused for many weeks predominantly on the sanitary crisis and its effects on infected people, their families, healthcare workers and the society and economy at large. This calls not only for a large set of tools to help during the pandemic , but also for tools to help digest and analyze this data after it ends. By analyzing the representation and reaction across countries with different guidelines or global trends, it might be possible to inform policies in prevention of and reaction to future epidemics. % AB: ""span countries""? and weird formulation Several institutions and groups have already started to take snapshots of the digital content shared during these weeks~.  However, because of its global scale, all this digital content is accessible in a variety of different languages, and most existing NLP tools remain English-centric~. In this paper we describe the release of a multilingual neural machine translation model  that can be used to translate biomedical text. The model is both multi-domain and multilingual, covering translation from French, German, Spanish, Italian and Korean to English.  Our contributions consist in the release of:        This paper is structured as follows: in Section we overview previous work upon which we build; Section details the model and data settings, and the released test set; and Section compares our model to other public models and to state-of-the-art results in academic competitions.  The model can be downloaded at  \url{https://github.com/naver/covid19-nmt}: the repository consists in a model checkpoint that is compatible with Fairseq~, and a script to preprocess the input text.     We describe the release of a multilingual translation model that supports translation in both the general and biomedical domains. Our model is trained on more than 350M sentences, covering French, Spanish, German, Italian and Korean . Benchmarks on public test sets show its strength across domains. In particular, we evaluated the model in the biomedical domain, where it performs near state-of-the-art, but with the advantage of being a single model that operates on several languages. To address the shortage of Korean-English data, we also release a dataset of 758 sentence pairs covering recent biomedical text about COVID-19.  Our aim is to support research studying the international impact that this crisis is causing, at a societal, economical and healthcare level.    \todo{run: NEWSTEST.it.en for our model, our model and Helsinki [DONE] on new test data}          
"," We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages  into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news  and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies. We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19.",243
"  Infants' speech perception changes in the first year of their life. For example, at the age of 6--8 months, English-learning and Japanese-learning infants are equally able to detect the difference between sounds \textipa{[\*r]}  and \textipa{[l]} , whereas by the age of 10--12 months, the two groups diverge, showing attunement to the phonetic contrasts present in their input language . Similar results have been reported for many other languages . A number of theoretical accounts explaining such early phonetic learning have been proposed , but until recently no computational models could explain  how the specific speech input to which infants are exposed leads to the observed changes in those infants' discrimination of phonetic contrasts. Models  been evaluated on their ability to learn phonetic categories, rather than to predict patterns of discrimination , but none has yet succeeded in that task when learning from non-idealized natural speech input .   In a recent study,  were the first to present such a computational model, which correctly predicted the documented cross-linguistic difference in infants' discrimination of \textipa{[\*r]} and \textipa{[l]} after learning from unsegmented speech. They explicitly simulated the learning process for Japanese and American English infants by  training their model on naturalistic speech recordings either in Japanese or in American English.  They then measured the trained models' ability to discriminate \textipa{[\*r]} and \textipa{[l]} with the machine ABX task, a flexible measure of discrimination that can be applied to model representations in essentially any format. To obtain a model capable of handling realistic input, they selected an algorithm for unsupervised learning from naturalistic speech that had been proposed in the context of engineering applications. The success of their model raises the question of whether other learning algorithms recently proposed in this context may lead to equally good, or even better, models of infants' early phonetic learning.  In this paper we apply the approach introduced in  to test a variety of models on multiple data sets of infant phone discrimination.  Doing so allows us  to gain insight into the kinds of representations and learning mechanisms that infants are likely to employ. We consider five different learning algorithms developed in the speech technology community, focusing on those that implement cognitively plausible learning mechanisms. We test the models on three crosslinguistic phone discrimination tasks grounded in infant studies from different languages.  Our two goals are  to test whether 's result is specific to their particular model applied to a particular American English phonetic contrast, and  to identify which computational models can best explain the existing infant data. We find that two models show infant-like crosslinguistic discrimination patterns for American English \textipa{[\*r]}--\textipa{[l]} and another contrast in Mandarin Chinese, while three other models appear less successful as models of early phonetic learning. We perform additional analyses to assess whether the two most successful models---which substantially differ in their learning mechanisms and representation formats---also differ in what they learn. The results suggest that it should be possible to find an empirical test to decide between these two models.  %  % Annual Cognitive Science Conference % Sample LaTeX Paper -- Proceedings Format %   % Original : Ashwin Ram        04/01/1994 % Modified : Johanna Moore       03/17/1995 % Modified : David Noelle           03/15/1996 % Modified : Pat Langley    01/26/1997 % Latex2e corrections by Ramin Charles Nakisa        01/28/1997  % Modified : Tina Eliassi-Rad   01/31/1998 % Modified : Trisha Yannuzzi  12/28/1999  % Modified : Mary Ellen Foster  12/11/2000 % Modified : Ken Forbus                              01/23/2004 % Modified : Eli M. Silk             05/24/2005 % Modified : Niels Taatgen          10/24/2006 % Modified : David Noelle      11/19/2014 % Modified : Roger Levy      12/31/2018  %% Change ""letterpaper"" in the following line to ""a4paper"" if you must.  \documentclass[10pt,letterpaper]{article}  \usepackage{cogsci}   \usepackage{apacite} \usepackage{float} % Roger Levy added this and changed figure/table                    % placement to [H] for conformity to Word template,                    % though floating tables and figures to top is                    % still generally recommended! \usepackage{graphicx} \usepackage{xcolor} \usepackage[tone]{tipa} \usepackage{natbib} \usepackage{multirow} \usepackage{siunitx} \usepackage{threeparttable} \usepackage{url} \usepackage[shortlabels]{enumitem} \usepackage{microtype} \usepackage{arydshln} \usepackage[hidelinks]{hyperref}   \usepackage{color,soul} }}  %\usepackage[none]{hyphenat} % Sometimes it can be useful to turn off %hyphenation for purposes such as spell checking of the resulting %PDF.  Uncomment this block to turn off hyphenation.    % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 4.5cm . %%If you do, we reserve the right to require you to change it back in %%the camera-ready version, which could interfere with the timely %%appearance of your paper in the Proceedings.  \title{Evaluating computational models of infant phonetic learning across languages}  \author{{\large  \\   Department of Linguistics \& UMIACS, University of Maryland \\   \AND {\large  \\   Department of Linguistics \& UMIACS, University of Maryland \\   \AND {\large [1]{\textcolor{blue}{To-do: #1}} [1]{\textcolor{orange}{#1}} [1]{#1} }        In the first year of life, infants' speech perception becomes attuned to the sounds of their native language. Many accounts of this early phonetic learning exist, but computational models predicting the attunement patterns observed in infants from the speech input they hear have been lacking. A recent study presented the first such model, drawing on algorithms proposed for unsupervised learning from naturalistic speech, and tested it on a single phone contrast. Here we study five such algorithms, selected for their potential cognitive relevance.  We simulate phonetic learning with each algorithm and perform tests on three phone contrasts from different languages, comparing the results to infants' discrimination patterns. The five models display varying degrees of agreement with empirical observations, showing that our approach can help decide between candidate mechanisms for early phonetic learning, and providing insight into which aspects of the models are critical for capturing infants' perceptual development.  Keywords:  early phonetic learning; representation learning; phone discrimination; computational model            Using computational modeling on realistic input, we compared possible mechanisms for early phonetic learning in their ability to predict the changes in discrimination empirically observed in infants. We tested five models on three phone contrasts from different languages, using a phone discrimination task. In our study, 's  DPGMM model showed the infant-like crosslinguistic pattern of discrimination both for English \textipa{[\*r]}--\textipa{[l]} and for Mandarin \textipa{[C]}--\textipa{[\texttctclig --\textipa{[E]}, the fact that two of them make correct predictions on the other two contrasts is promising. This result supports the idea that models learning representations directly from unsegmented natural speech can correctly predict some of the infant phone discrimination data. Based on the results of this study, the DPGMM and the CAE-RNN show some promise as models of early phonetic learning, although their failure to capture the effect on the Catalan contrast warrants further investigation to determine whether the failure is a problem with the models or with the amount and/or quality of the training and the test data.   Here we are making predictions about infants' phone discrimination. Adults show many cross-linguistic discrimination differences as well, and one question for future research is how the presented models of infant speech perception relate to predictions one might make about adult perception. There are likely to be additional learning mechanisms later in childhood beyond those we have modeled here, but what changes over development, and how, remains an open question.  The DPGMM and the CAE-RNN both aim to model primarily unsupervised infant phonetic learning, although they make very different assumptions about the learning mechanism and the types of representations: the DPGMM is an unsupervised bottom-up model that learns by clustering frame-level representations, while the CAE-RNN encodes chunks of speech holistically, learns by sequentially reconstructing an acoustic word, and uses weak top-down guidance from the word level . Importantly, our analysis shows that the two models do not always make identical predictions about discriminability: for example, vowel length contrasts are generally easier to discriminate for the CAE-RNN than for the DPGMM. Such differences between the models allow us to make predictions---both for native and non-native listeners---which can then be tested in experimental studies with infants to differentiate between the models.  
","  In the first year of life, infants' speech perception becomes attuned to the sounds of their native language. Many accounts of this early phonetic learning exist, but computational models predicting the attunement patterns observed in infants from the speech input they hear have been lacking. A recent study presented the first such model, drawing on algorithms proposed for unsupervised learning from naturalistic speech, and tested it on a single phone contrast. Here we study five such algorithms, selected for their potential cognitive relevance.  We simulate phonetic learning with each algorithm and perform tests on three phone contrasts from different languages, comparing the results to infants' discrimination patterns. The five models display varying degrees of agreement with empirical observations, showing that our approach can help decide between candidate mechanisms for early phonetic learning, and providing insight into which aspects of the models are critical for capturing infants' perceptual development.  Keywords:  early phonetic learning; representation learning; phone discrimination; computational model",244
"   Natural language has the potential to be the most effective and convenient way to issue commands to a robot.  % However, machine acquisition of language is difficult due to the context- and speaker-specific variations that exist in natural language. % For instance, English usage differs widely throughout the world: between children and adults, and in businesses vs. in homes. % This does not pose a significant challenge to human listeners because we acquire language by observing how others use it and then attempting to use it ourselves. % Upon observing the language use of other humans, we discover the latent structure of the language being spoken. % We develop a similar approach for machines.  Our grounded semantic parser learns the latent structure of natural language utterances. % This knowledge is executable and can be used by a planner to run commands. % The parser only observes how language is used: what was said  indicates a condition that must persist, while dim the lights indicates a goal that must be met at least once, but need not be repeated. % % % In this work, we present a grounded semantic parser that maps natural language to programs in Linear Temporal Logic , a formalism that is equipped with operators that can be composed to represent complex temporal properties. % % % Increasing the ability of machines to understand these temporal properties is relatively unexplored territory in the field of semantic parsing and brings us closer to robotic assistants that can smoothly handle natural language commands.  Our approach performs well on both machine and human generated sentences. % In both cases, it is able to execute around 80\% of commands successfully. % A traditional fully-supervised approach on the machine-generated sentences outperforms ours with 95\% accuracy, but requires the ground-truth LTL formulas. % Where this approach of discovering latent structures shines is on real-world data. % We asked human subjects, unconnected with this research and without knowledge of robotics or ML, to produce sentences that describe the behavior of robots. % This behavior was produced according to an LTL formula that we randomly generated, but the humans were free to describe whatever they wanted with whatever words they desired as long as it was true. % Despite the human sentences being much more varied and complex, including structures which our formalism cannot exactly represent, our method still finds whatever latent structure is present and required to execute the natural-language commands produced by humans with nearly the same accuracy as those produced by machines.  Executing LTL commands and understanding the kinds of temporal relations that require LTL is particularly difficult due to the richness and openness of natural language . % But LTL is just a stepping stone. % It remains an important open question in grounded robotics: what representation will be enough to capture the richness of how humans use language? % For instance, notions such as modal operators to reason about hypothetical futures will likely be required, but what else is unclear. % Having a general-purpose mechanism for learning to execute commands is extremely helpful under these conditions; we can experiment with different logics and domains with the same agents by changing the priors and planners while leaving the rest of the system intact.   % In typical weakly supervised parsing frameworks, the reward depends on the final state of execution, as evaluated by an executor. % % % However, learning temporal constraints requires a reward that is sensitive to behavior across time, not just at the final state. % % % We propose a reward that is sensitive to behavior across time: each hypothesized parse is input to the pre-trained neural planner from , and given a reward based on the likelihood of the observed executions. % % % Since the planner is trained to produce correct executions for LTL programs, it assigns a high reward to programs that are more likely to give rise to the observed behavior. % % % This allows the model to learn meaning representations that lead to correct behavior across time, rather than behavior which simply reaches the correct destination. % %  % Training using weak supervision has many benefits.  % % % It removes the need for an annotated dataset of natural language and program pairs, which is labor intensive to produce by hand. % % % Moreover, due to the ambiguity in natural language, a sentence may have many interpretations, and each interpretation may have many equivalent representations. % % % For this reason, training using weak supervision avoids over-fitting to biases in human annotation which might result in poor generalization. % % % Finally, because our model uses weak supervision, our system  paves the way for a system in which a semantic parser and planner can be trained jointly. % %  % Learning in this weak supervision setting is difficult because the space of possible meaning representations is large and many spurious forms may receive a non-zero reward even though they only coincidentally result in the correct behavior. % % % For instance, consider a command that requires the robot's proximity to a flag . % % % If the flag happens to be adjacent to an apple, then a candidate program with the meaning pick up the apple will receive an erroneous reward. % % % These challenges are made more difficult by the fact that the executor is itself a neural model, so uncertainty is introduced through noisy perception and imperfect planning. % % % We address spuriousness by encouraging our model to produce embeddings that more closely attend to the semantics of the input by using multi-task learning. % % % These embeddings are used to reconstruct the original command as well as the output parse .  % To obtain data for training and evaluation, we use a synchronous context free grammar to generate pairs of sentences and LTL programs. % % % An oracle produces  demonstration executions for each program. % % % We then use human annotators to write a command for each set of demonstrations, which gives us a dataset of natural language sentences paired with execution traces. % % % We test on a dataset of held out natural language commands.    The main contributions of this work are:   suited for grounded   semantic parsing experiments, and       We created a grounded semantic parser that, given only minimal knowledge about its environment and formalism, was able to discover the structure of an input language and produce executable formulas to command a robot.   Its performance is competitive with a state of the art supervised approach, even though we provide no direct supervision.   We were able to get similar performance on a challenging dataset produced by humans that could use any word and sentence construction to describe the actions of robots, even those that our formalism cannot completely capture.   This model has virtually no knowledge of its domain or target logical formalism; it merely requires a planner and a method to reject syntactically invalid formulas.   Many problems in robotics and NLP could be tackled by such an approach because of its low requirements for annotations.   For example, data already exists to guide agents to reproduce the actions of customer service agents in response to queries.   And in the robotic domain, future work might involve observing what humans say to one another and then acquiring a domain-specific semantic parser to guide robots on a worksite for example.   Being able to adapt to variations in language use and to changes in the environment is crucial to building useful robots, because the same language may carry very different meanings in different contexts.   In the long term, we hope that this line of research leads both to robots that understand us and to robotic systems that can be used to probe how children acquire language, bringing robotics and linguistics closer together.        Our model is able to generate programs that elicited the desired behavior from the planner at a level comparable to the ground truth programs.       Under weak supervision, our model learned to produce programs that accurately captured the behavior of the observed execution traces at a rate comparable to the fully supervised baseline.       Although training the planner module was outside the scope of this work, it is clear that any future advances in planner accuracy will result in a better supervision signal for our parser.       A natural extension of this work would involve training a parser and planner jointly from scratch.      The maximum paper length is 8 pages excluding references and acknowledgements, and 10 pages including references and acknowledgements        no     .bib   
","   Children acquire their native language with apparent ease by    observing how language is used in context and attempting to use it   themselves.   %   They do so without laborious annotations, negative examples, or even   direct corrections.   %   We take a step toward robots that can do the same by training a grounded   semantic parser, which discovers latent linguistic representations that can be used for the execution of  natural-language commands.   %   In particular, we focus on the difficult domain of commands with   a temporal aspect, whose semantics we capture with    Linear Temporal Logic, LTL.   %   Our parser is trained with pairs of sentences and executions as well as an   executor.   %   At training time, the parser hypothesizes a meaning representation for the input as a formula in LTL.   %   Three competing pressures allow the parser to discover  meaning from   language.   %   First, any hypothesized meaning for a sentence must be permissive enough to reflect all the annotated execution   trajectories.   %   Second, the executor --- a pretrained end-to-end LTL planner --- must find that the observed   trajectories are  likely executions of the meaning.   %   Finally, a generator, which reconstructs the original input, encourages the model to find representations that conserve knowledge about the command.   %   Together these ensure that the meaning is neither too general nor too   specific.   %   Our model generalizes well, being able to parse and execute both   machine-generated and human-generated commands, with near-equal accuracy,   despite the fact that the human-generated sentences are much more varied and complex with   an open lexicon.   %   The approach presented here is not specific to LTL: it can be applied to any domain   where sentence meanings can be hypothesized and  an executor can   verify these meanings, thus opening the door to many applications for robotic agents.        % %     % We generate executable programs in Linear Temporal Logic  from natural language commands by training a semantic parser in a weak supervision setting where ground truth programs are not observed.      % %     % Our parser learns by observing execution traces for each input natural language command.     % %     % During training, the parser proposes candidate programs and receives a supervision signal from a pre-trained planner, which executes these programs against a set of observed traces and assigns a reward based on their likelihood.     % %     % We generate programs for commands that contain temporal constraints, a domain which other semantic parsing approaches often neglect.     % %     % Learning using weak supervision is difficult in this domain, due to the large exploration space and the fact that many spurious forms may incorrectly receive a reward.     % %     % Despite these challenges, our weakly supervised approach is able to produce meaning representations that correctly reflects the demonstration traces 85\% of the time, compared to 16\% under random chance and 95\% under full supervision.",245
"  In recent years, introduction of Recurrent Neural Networks , in particular, Long Short Term Memory  RNNs  have been shown to outperform feed-forward neural networks  and significantly improve the performance of automatic speech recognition  systems. More recently, it has been shown that LSTMs when trained with CTC loss followed by sequence discriminative training can achieve state-of-the-art performance on various large-scale acoustic modelling tasks . However, a primary bottleneck of these frameworks is their dependence on availability of large amounts of labeled data. Since collection and transcription of large amounts of speech data is costly and time-consuming, techniques to leverage large amounts of unlabelled data for acoustic modeling are explored under the framework of semi-supervised learning .  SSL for ASR involves automatic generation of labels for unlabelled data and leveraging them along with labelled data for building ASR systems. Self-training has been shown to be an effective SSL framework where an initial system is trained on labelled data to generate machine transcriptions for unlabelled data . However, the effectiveness of such systems gets constrained by the ``goodness"" of the initial system and ``quality"" of machine transcripts. Teacher-Student based learning  originally proposed in the context of knowledge distillation  for model compression is another approach used for SSL. KD involves using a strong teacher model trained on labelled data to generate  labels which are used to train a student model to match teacher output distribution.   The effectiveness of KD has been well established in speech recognition tasks . However, conventional frame-level based KD approaches may not be directly feasible for CTC trained models due to `alignment-free' nature of CTC loss function. Modifications have been proposed in recent studies which either attempt to align teacher and student models' spike timings  or propose KD at the level of sequences instead of frames . The cost associated with aligning output spikes from teacher and student model make the frame-level based approaches computationally expensive when using large amounts of unlabelled data. Existing studies on sequence-level KD for CTC entail a certain degree of complexity in sampling n-best hypothesis  or computing forward-backward posteriors from teacher model . A simplified version of sequence-level KD has been studied in the context of neural machine translation  , which essentially collapses the entire label sequence space to a single 1-best sequence. In this work, we simplify this assumption further by approximating the entire label sequence space by a single 1-best sequence obtained by concatenating frame-level argmax output from teacher model. Our results demonstrate that such a framework provides better WER as compared to a standard self-training based approach.  %The efficiency of our method can be attributed to 3 assumptions: 1) For a given label sequence, sum over all corresponding paths can be approximated by its maximum value, 2) Greedy decoding approximation,  3) Teacher posterior distribution can be replaced by a Dirac delta function at its mode.  In a recent study in KD , it was shown that augmenting labelled data with 1 million hours of randomly selected unlabelled data improves relative WER by -\%. However, processing such large quantities of unlabelled data and subsequent training requires access to powerful compute and storage resources and leads to significant increase in training time. The prime motivation for this work is to seek answer to the following question: Instead of large amounts of randomly selected unlabelled data, is it possible to design an intelligent data selection scheme which can achieve similar WER gains but with lesser data?  Majority of data selection approaches studied in the past have relied on confidence scores, entropy based confidence measure and local-global entropy minimization among others for bootstrapping initial seed model with additional unlabelled data  in the self-learning framework. To the best of our knowledge, data selection in the context of teacher-student learning has not been explored extensively.  We present an empirical study of the role of different data selection mechanisms based on confidence and Natural Language Understanding  based domain distribution as well as speaker and content diversity on WER of CTC-SSL based student model. Overall, our CTC-SSL framework achieves 17\% relative WER improvement over a baseline CTC system with labelled data only and on-par performance with a CTC-SSL model trained on an order of magnitude higher randomly sampled unlabelled data.  %This paper is organized as follows, Section 2 describes the knowledge distillation based method used in this study for semi-supervised learning in CTC acoustic models. Section 3 discusses details about different attributes that we have used for data selection in this work. Section 4 describes our CTC-SSL experimental setup and its results with different data selection techniques. Finally, Section 5 concludes our work and suggests future directions of work.      This paper presents an empirical study on large-scale semi-supervised learning for CTC acoustic models where a strong offline teacher model is used to generate pseudo-labels for unlabelled data. The unlabelled data is selected based on confidence and domain distribution as well as speaker and content variability. Experimental results on two different dialects reinforce the efficacy of teacher generated pseudo labels and the importance of intelligent data selection methods. It is observed that domain-specific unlabelled data has a strong impact on corresponding WER with little cross-domain impact signifying the importance of such a sampling strategy in boosting the performance of low resource domains. Future work in this direction would be to devise a strategy to leverage both confidence as well as domain diversity in a combined data sampling strategy for SSL. Another important future direction will be to study the impact of word level decoding which incorporates both lexicon and strong language model in improving the quality of teacher generated pseudo-labels.       
"," Semi-supervised learning  is an active area of research which aims to utilize unlabelled data in order to improve the accuracy of speech recognition systems. The current study proposes a methodology for integration of two key ideas: 1) SSL using connectionist temporal classification  objective and teacher-student based learning 2) Designing effective data-selection mechanisms for leveraging unlabelled data to boost performance of student models. Our aim is to establish the importance of good criteria in selecting samples from a large pool of unlabelled data based on attributes like confidence measure, speaker and content variability. The question we try to answer is: Is it possible to design a data selection mechanism which reduces dependence on a large set of randomly selected unlabelled samples without compromising on Word Error Rate ? We perform empirical investigations of different data selection methods to answer this question and quantify the effect of different sampling strategies. On a semi-supervised ASR setting with 40000 hours of carefully selected unlabelled data, our CTC-SSL approach gives 17\% relative WER improvement over a baseline CTC system trained with labelled data. It also achieves on-par performance with CTC-SSL system trained on order of magnitude larger unlabeled data based on random sampling.",246
"  Probabilistic topic models assume words are generated from latent topics which can be inferred from word co-occurrence patterns taking a document as global context. In recent years, various neural topic models have been proposed. Some of them are built on the Variational Auto-Encoder   which utilizes deep neural networks to approximate the intractable posterior distribution of observed words given latent topics . However, these models take the bag-of-words  representation of a given document as the input to the VAE and aim to learn hidden topics that can be used to reconstruct the original document. They do not learn word embeddings concurrently.  Other topic modeling approaches explore the pre-trained word embeddings for the extraction of more semantically coherent topics since word embeddings capture syntactic and semantic regularities by encoding the local context of word co-occurrence patterns. For example, the topic-word generation process in the traditional topic models can be replaced by generating word embeddings given latent topics  or by a two-component mixture of a Dirichlet multinomial component and a word embedding component . Alternatively, the information derived from word embeddings can be used to promote semantically-related words in the Polya Urn sampling process of topic models  or generate topic hierarchies . However, all these models use pre-trained word embeddings and do not learn word embeddings jointly with topics.  While word embeddings could improve the topic modeling results, but conversely, the topic information could also benefit word embedding learning. Early word embedding learning methods learn a mapping function to project a word to a single vector in an embedding space. Such one-to-one mapping cannot deal with word polysemy, as a word could have multiple meanings depending on its context. For example, the word `' has two possible meanings `' and `'. When analyzing reviews about restaurants and health services, the semantic meaning of `' could be inferred depending on which topic it is associated with. One  solution is to first extract topics using the standard Latent Dirichlet Allocation  model and then incorporate the topical information into word embedding learning by treating each topic as a pseudo-word .   Whereas the aforementioned approaches adopt a two-step process, by either using pre-trained word embeddings to improve the topic extraction results in topic modeling, or incorporating topics extracted using a standard topic model into word embedding learning,  developed a Skip-Gram based model to jointly learn topics and word embeddings based on the Probabilistic Latent Semantic Analysis , where each word is associated with two matrices rather than a vector to induce topic-dependent embeddings. This is a rather cumbersome setup.  used the Skip-Gram to imitate the probabilistic topic model that each word is represented as an importance vector over topics for context generation.  In this paper we propose a neural generative model built on VAE, called the Joint Topic Word-embedding  model, for jointly learning topics and topic-specific word embeddings. More concretely, we introduce topics as tangible parameters that are shared across all the context windows. We assume that the pivot word is generated by the hidden semantics encoding the local context where it occurred. Then the hidden semantics is transformed to a topical distribution taking into account the global topics, and this enables the generation of context words. Our rationale is that the context words are generated by the hidden semantics of the pivot word together with a global topic matrix, which captures the notion that the word has multiple meanings that should be shared across the corpus. We are thus able to learn topics and generate topic-dependent word embeddings jointly. The results of our model also allow the visualization of word semantics because topics can be visualized via the top words and words can be encoded as distributions over the topics\footnote{Our source code is made available at \url{http://github.com/somethingx02/topical\_wordvec\_models}.}.  In summary, our contribution is three-fold: [noitemsep]           Driven by the motivation that combining word embedding learning and topic modeling can mutually benefit each other, we propose a probabilistic generative framework that can jointly discover more semantically coherent latent topics from the global context and also learn topic-specific word embeddings, which naturally address the problem of word polysemy. Experimental results verify the effectiveness of the model on word similarity evaluation and word sense disambiguation. Furthermore, the model can discover latent topics shared across documents, and the encoder of JTW can generate the topical distribution for each word. This enables an intuitive understanding of word semantics. We have also shown that our proposed JTW can be easily integrated with deep contextualized word embeddings to further improve the performance of downstream tasks. In future work, we will explore the discourse relationships between context windows to model, for example, the semantic shift between the neighboring sentences.  
"," We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents, a word is generated by a hidden semantic vector encoding its contextual semantic meaning, and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared with existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification.",247
" 	 	Relational databases are prevalent in many real-world applications. Typically, a structured query language such as SQL is required to interact with such databases. However, mastering SQL queries is generally difficult. It has been a long standing goal to enable users interacting with relational databases via human natural languages. The general problem was known as ``Natural Language Interface to Databases~'' in database areas~. In recent years, there has been a surge of interest on deep learning-based approaches to a crucial problem of NLIDBs: translating a natural language query to SQL, often referenced as ``NL-to-SQL'' or ``Text-to-SQL''~.  	 	In this paper, we focus on the Text-to-SQL problem and experiment with the WikiSQL~~ dataset~. WikiSQL is the first large-scale dataset for Text-to-SQL, with about 80K human annotated pairs of NL question and SQL query. WikiSQL constrained the problem by two factors -- each question is only addressed by a single table, and the table is known. This constrained setting has guided research to focus on the core elementary problem. Even though the scope is constrained, the dataset is still very challenging because the tables and questions are very diverse. Notably, there are about 24K different tables associated with this dataset. 	 	In the past, several approaches were proposed for the WikiSQL dataset. They primarily share the similar encoder-decoder architecture. The encoder encodes information from both the NL question and the table schema into some hidden representation. Some of them encode an NL question with the full table schema, e.g., concatenating the NL question with all the column names~, while others encode an NL question with each column separately~. There are also some work do both at different layers~. The decoder decodes the hidden representation to a SQL query. Some early work tried the ``to sequence'' style one step decoding~ however it is found challenging to guarantee the output correct in syntax. Later more work breaks down a SQL query to several parts like SELECT column, WHERE column, WHERE operator, WHERE value, etc. and have sub-decoders for them~. In this way, the chance of output violating syntax is reduced. Beyond improving the Text-to-SQL task models, recently there are some work study how to leverage pre-trained language models~ and get promising results~. All the previous work reveal several major challenges in Text-to-SQL on WikiSQL:  How to fuse information from both NL question and table schema, which is handled by encoder;  How to make sure the output SQL query executable and accurate, which is handled by decoder;  How to leverage pre-trained language models. 	 	This paper is primarily motivated by the above challenge , however, the proposed approach contributes to solving all the above challenges. We argue that the previous approaches~ did not align the task model well with the base language model, and hence, the power of the base language model is compromised by the task model. Specifically, both  and  concatenate the full table schema with the NL question and feed into a BERT-alike base language model. However, in their decoding stage, both need the hidden representation for each individual column. Thus, they have to apply adhoc pooling on the tokens of a column name to get a vector for the column.  simply pick the vector for the first token in their ``shallow layer'' and added another LSTM layers in their ``decoder-layer'' and ``NL2SQ-Layer''.  apply weighted average over vectors of all tokens. These ad-hoc pooling operations and additional LSTM layers caused information loss and bring in unnecessary complexity. To solve the dilemma encountered in previous work, in this paper, we choose to encode a pair of NL question and one individual column via the exact BERT/RoBERTa model structure. Then we have multiple sub-tasks in decoder stage: SELECT \& WHERE column ranking, condition operator, and condition value span. Since the decoder does not output final SQL query, we apply straightforward rules to assemble the decoder outputs to a final SQL query. Our approach has two benefits. First, the inputs, i.e., a question and column pair align perfectly with the original sentence pair training tasks of BERT/RoBERTa, and hence, we believe the power of the BERT/RoBERTa is utilized best. Second, as we only encode one column, the ``[CLS]'' vector in BERT/RoBERTa output captures all fused information from the question and the column, which is exactly the ``column vector'' needed for the decoder. So we don't need to apply any further pooling or additional complex layers. This makes our model structure very simple and efficient. Since the main philosophy in our approach is ranking on columns but with multi-task outputs, we name it Hybrid Ranking Network . 	 	In summary, our contributions are in three folds. First, we propose a simple and efficient network structure which perfectly aligns the Text-to-SQL task with the base language models, and hence, the power of the base language models are best utilized. Second, the base language model as encoder directly encodes an NL question and a column without any additional pooling operations which is believed to be the best encoder capturing the question-column relation in Text-to-SQL. Third, the proposed hybrid ranking mechanism and execution-guided decoding handle column-column relations and effectively boost the accuracy. In other words, our approach resolved all the aforementioned challenges at the same time. Our approach achieves the best result on WikiSQL dataset, which verifies its effectiveness and contributions. 	 	 	Note that the final point above is not trivial, once we introduce a complex one-to-one mapping between Spider SQL query  and HydraNet tasks. This is significantly different from other grammar-based approach, and will also be different from slot filling approach as there will be more ranking involved.  	 	 	 	There is no doubt that state-of-the-art models for text-to-SQL task must leverage pre-trained deep Transformer models like BERT, MT-DNN or RoBERTa. However, the above input construction does not fit the BERT famework very well due to the following reasons:  	 	 	 	We argue that, in the WikiSQL task, the columm-column interdependency  is quite rare. So, the main relationship to learn is that between the columns and the NL question. So, we propose a two stage approach in this paper:\\ 	 In the first stage, we learn the column-question relationships. We consider one column at a time, form the input as question-column pair and train a model to classify whether a column is a SELECT or WHERE column. \\ 	 In the second stage, we keep the most relevant columns from the first step via ranking and use execution-guided decoding  	to construct the SQL query. Execution guided decoding takes the necessary column-column relationships into account.  	 	Our approach addresses the issues as  it leverages BERT with recommended tasks setting and input setting and  the input length does not grow with number of candidate columns, hence it is able to scale to even large databases. 	 	 	 	 	: The most recent approaches  leverage pre-trained deep Transformer models like BERT and MT-DNN which has proven extremely effective for most tasks involving unstructured data. These approaches first perform a table-aware BERT-based or MT-DNN-based encoding. Then they have different slot filling models for  and . 	 	 	There is no doubt that state-of-the-art models for text-to-SQL task must leverage pre-trained deep Transformer models like BERT, MT-DNN or RoBERTa. However, both the previous approaches that leverage pre-trained contextualized word representation concatenate all the column names of the table along with the NL question in the encoding layer in order to learn the interdependencies between columns in SELECT clause and WHERE conditions as well as columns in different WHERE conditions . This has several serious limitations: 	\\ 	 $ 	 	  	In this paper, we study how to leverage pre-trained language models like BERT on WikiSQL task. We formulate text-to-SQL as a column-wise hybrid ranking problem and propose a neat network structure called HydraNet which best utilizes the pre-trained language models. The proposed model has a simple architecture but achieves No.1 result on the WikiSQL leaderboard. We believe it will bring in deep insight to the Text-to-SQL problem on how to better utilize pre-trained language models. In the future, we will extend the capability of HydraNet to support full SQL grammar. 	 	
"," 		In this paper, we study how to leverage pre-trained language models in Text-to-SQL. We argue that previous approaches under utilize the base language models by concatenating all columns together with the NL question and feeding them into the base language model in the encoding stage. We propose a neat approach called Hybrid Ranking Network~ which breaks down the problem into column-wise ranking and decoding and finally assembles the column-wise outputs into a SQL query by straightforward rules. In this approach, the encoder is given a NL question and one individual column, which perfectly aligns with the original tasks BERT/RoBERTa is trained on, and hence we avoid any ad-hoc pooling or additional encoding layers which are necessary in prior approaches. Experiments on the WikiSQL dataset show that the proposed approach is very effective, achieving the top place on the leaderboard.",248
"   } NVIDIA is working on significant performance improvements for \sockeye's Transformer  implementation through fused operators and an optimized beam search. This paper discusses \sockeyeTwo's streamlined \gluon implementation , support for state of the art architectures and efficient decoding , and improved model training .      \sockeyeTwo provides out-of-the-box support for quickly training strong Transformer models for research or production. Extensive configuration options and the simplified \gluon code base enable rapid development and experimentation. As an open source project, we invite the community to contribute their ideas to \sockeyeTwo and hope that the new programming model and various performance improvements enable others to conduct effective and successful research.  \small  
","   We present \sockeyeTwo, a modernized and streamlined version of the \sockeye neural machine translation  toolkit.   New features include a simplified code base through the use of \mxnet's \gluon API, a focus on state of the art model architectures, distributed mixed precision training, and efficient CPU decoding with 8-bit quantization.   These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production.",249
"  Nature language understanding plays an critical role in machine intelligence and it includes many challenging NLP tasks such as reading comprehension , machine translation , question answering  and etc.. Amongst a wide spectrum of NLP tasks, text classification is considered as the foundation for its measuring the semantic similarities between texts. Traditional machine learning methods employ hand-crafted features to model the statistical properties of syntactical elements , which are further fed into the classification algorithms such as k-Nearest Neighbor , Random Forests, Support Vector Machines , or its probabilistic versions . However, such hand-crafted features often suffered from the loss of semantic information and scalability. To solve the drawbacks of the hand-crafted features, automatic learning of representation using the neural networks was introduced into NLP fields. Word embedding is a foretype of automatic representation learning , which outperforms the traditional methods for alleviating the sparsity problem and enhancing the semantic representation.  In recent years, the NLP community has conducted extensive investigations on the neural-based approaches . There exist a diversity of deep neural network architectures with different modeling capabilities. The RNN is a widely-used neural network architecture for NLP tasks owing to its capability to model sequences with long-term dependencies . When modeling texts, a RNN sequentially processes word by word and generates a hidden state at each time step to represent all previous words. However, although the purpose of RNNs is to capture the long-term dependencies, theoretical and empirical studies have revealed that it is difficult for RNNs to learn very long-term information. To address this problem, the long short-term memory network   and other variants such as gated recurrent unit  , simple recurrent unit   were proposed for better remembering and memory accesses. Another roadblock for RNNs is that when they are used to process a long sequence, the latest information is more dominant than the earlier one, however, which might be the real significant part of the sequence. In fact, the most important information can appear anywhere in a sequence rather than at the end. Consequently, some researchers proposed to assign the same weight to all hidden states and average the hidden states of all time steps to equally spread the focus to all the sequence.  Inspired by the biological ability to focus on the most important information and ignore the irrelevant ones, the attention mechanism was introduced to assign different weights to the elements at different positions in a sequence and select the informative ones for the downstream tasks . Nowadays, the attention mechanism has become an integral part of sequence modeling, especially with RNNs . The attention mechanism enables RNNs to maintain a variable-length memory and compute the outputs based on the importance weights of different parts in a sequence. The attention mechanism has been empirically proven to be effective in NLP tasks such as neural machine translation . However, the attention mechanism cannot capture the relationships between words and the word ordering information, which contains important semantic information for downstream tasks. Taking the sentences ``Tina likes Bob."" and ``Bob likes Tina."" as examples, the weighted sum of their hidden states encoded by RNN are almost the same. Nevertheless, the two sentences have different meanings.  %Nowadays, it has become the mainstream for NLP tasks to learn distributed representations of words through neural language models and perform a combination of learned word representation with attention mechanism .  The ConvNet is another widely-adopted neural architecture for NLP tasks. The modeling power of ConvNets relies on four key factors: local connections, shared weight, pooling and multi-layers. The fundamental assumption behind the ConvNet approaches is that locally grouped data in natural signals are often high correlated and the compositional hierarchies in natural signals can be exploited by the stacked convolutional layers. As a result, ConvNets have been believed to be good at extracting informative semantic representations from the salient N-gram features of input word sequences by utilizing convolutional filters in a parallel way. For the above example, 2-gram features of `` Tina likes"" and ``likes Bob"" that contain the word ordering information can be captured by ConvNets. These features are more representative for the original sentence than the weighted sum of the hidden states. Therefore, ConvNets have been employed for a variety of NLP tasks and achieved impressive results in sentence modeling , semantic parsing , and text classification . Moreover, ConvNets can operate on different levels of lexical structures such as characters, words, sentences, or even the whole document. For instance, some research has shown that the character-level text classification using ConvNets can achieve competitive results in comparison with the state-of-the-arts . However, basic ConvNets apply a fixed-width window to slide over the input sequences, which limits the created representations to local semantic pattern, failing to capture long-term dependencies.  To take full advantage of both the ConvNet and the RNN, and complement the superiorities of different neural architectures, researchers explored to introduce the hybrid structure of the ConvNets and the RNNs. For instance, the recurrent convolutional neural network  proposed a recurrent structure of convolutional filters to enhance the contextual modeling ability to avoid the problem of fixed-width sliding windows. This work also claimed to apply a max-pooling layer to automatically determine the key components for text classification. However, even though this approach managed to reduce noise by replacing the fixed-width sliding window of ConvNets with a recurrent mechanism, it still depend on the max-pooling to determine the discriminative features and lacks the mechanism to selectively choose the dominant component as the attention mechanism can do. Similarly, Wang et al. proposed the convolutional recurrent neural network that stacked four types of neural layers: word embedding, Bidirectional RNN layer, convolutional layer, and max-pooling layer. This approach functions very similarly to the one in , but with disparate applications in sentence classification and answer selection. Also, this work bypassed the attention mechanism when integrating the ConvNet and the RNN structures.  As discussed above, any neural architecture has its own pros and cons, it is reasonable to conjecture that consistently combing different architectures can benefit extracting of different aspects of linguistic information from texts. However, to the best of our knowledge, there are still no efforts in integrating entirely the ConvNet, RNN and attention architectures. Inspired by proposition by LeCun et al. , we hypothesize that the attention mechanism can function as an adhesive that seamlessly integrate the ConvNet and the RNN architecture, where the RNN layer is used to represent the input word sequences and the ConvNet layer is used for classification. Furthermore, we assume that, besides attending to elements  at syntactical or symbolic level, coarser-grained attentions at the hidden state vectorial space can improve the local N-gram coherence for ConvNets, as the attentions on hidden state vectors can select the salient dimensions that represent most informative latent semantics, hence reducing the noise perturbation to the ConvNet layer and enhancing the classification performance.  Based on the above motivations, we propose a hybrid architecture based on a novel hierarchical multi-granularity attention mechanism, named Multi-granularity Attention-based Hybrid Neural Network . In MahNN, two types of attentions are introduced: the syntactical attention and the semantical attention. The syntactical attention computes the importance of the syntactic elements  at the lower symbolic level and the semantical attention is used to compute the importance of the embedded space dimension corresponding to the upper latent semantics. We adopt the text classification as an exemplifying way to illustrate the ability of MahNN to understand texts. The experimental results on a variety of datasets demonstrate that MahNN outperforms most of the state-of-the-arts for text classification.    The main contributions of our work are listed as follows:         This paper is organized as follows. Section  introduces the related work about ConvNet and attention mechanisms. Section  introduces the proposed MahNN in detail. And Section  introduces datasets, baselines, experiments, and analysis. Finally, Section  concludes this paper.      In this paper, we attempt to develop a hybrid architecture that can extract different aspects of semantic information from the linguistic data with diverse types of neural structures. Intriguingly, we propose a novel hierarchical multi-granularity attention mechanism, consisting of the syntactical attention at the symbolic level and the semantical attention at the embedding level, respectively. The experimental results show that the MahNN model achieves impressive performances on a variety of benchmark datasets for the text classification task. Moreover, visualization of attention distribution illustrates that the hierarchical multi-granularity attention mechanism is effective in capturing informative semantics from different perspectives. We can draw the following conclusions from our work:     There are several future directions to extend this work. First, we would investigate on applying a generative model to obtain multichannel representations of texts. Data augmentation in this way can represent much richer semantics. Second, ConvNets require the fixed-length inputs and perform some unnecessary convolution operations for NLP tasks. It is worthwhile to explore the novel ConvNet architecture processing with variable length. Moreover, we use simple calculating methods for the attention weights and this might not be able to demonstrate the full potential for the hierarchical multi-granularity attention mechanism. It would be intriguing to compute the attention weights with more advanced approaches such as transfer learning and reinforcement learning to further improve the performance.   
"," Neural network-based approaches have become the driven forces for Natural Language Processing  tasks. Conventionally, there are two mainstream neural architectures for NLP tasks: the recurrent neural network  and the convolution neural network . RNNs are good at modeling long-term dependencies over input texts, but preclude parallel computation. ConvNets do not have memory capability and it has to model sequential data as un-ordered features. Therefore, ConvNets fail to learn sequential dependencies over the input texts, but it is able to carry out high-efficient parallel computation. As each neural architecture, such as RNN and ConvNets, has its own pro and con, integration of different architectures is assumed to be able to enrich the semantic representation of texts, thus enhance the performance of NLP tasks. However, few investigation explores the reconciliation of these seemingly incompatible architectures. To address this issue, we propose a hybrid architecture based on a novel hierarchical multi-granularity attention mechanism, named Multi-granularity Attention-based Hybrid Neural Network . The attention mechanism is to assign different weights to different parts of the input sequence to increase the computation efficiency and performance of neural models. In MahNN, two types of attentions are introduced: the syntactical attention and the semantical attention. The syntactical attention computes the importance of the syntactic elements  at the lower symbolic level and the semantical attention is used to compute the importance of the embedded space dimension corresponding to the upper latent semantics. We adopt the text classification as an exemplifying way to illustrate the ability of MahNN to understand texts. The experimental results on a variety of datasets demonstrate that MahNN outperforms most of the state-of-the-arts for text classification.",250
" In natural language processing, pre-trained contextual representations have been widely used to help downstream tasks that lack sufficient labeled training data.  Previous works develop various self-supervised tasks to obtain pre-trained contextual representations. Taking the classic masked language modeling  task used by BERT  as an example, it first randomly chooses a small number of positions in a sentence, mask the words on the position and then learns an encoder to restore them. As such tasks require no human supervision, the size of available training data could easily amount to the scale of billions of words. Pre-training over such large-scale data consumes exceptionally huge computational resources .   In this paper, we tackle the training efficiency issue and develop a novel variance-reduced algorithm for better language pretraining. In particular, we observe that all previous works use uniformly sampled positions to mask when constructing their self-supervised tasks, and this is inevitably inefficient from the optimization perspective. For instance, in BERT training, we find that commonly used words and punctuation are easy to learn, i.e., those words  can be correctly predicted by the model in just a few thousands of training steps. Meanwhile, some rare words and phrases are difficult to predict even at the end of the training. If we always uniformly sample the positions to mask, intuitively to say,  the variance of the stochastic gradient  can be large since some positions gradually provide less informative signals while some do not. Usually, learning with a large-variance gradient estimator will be inefficient and ineffective.   To formally characterize the variance of the stochastic gradient, we first introduce a principled gradient variance decomposition theorem. The theorem shows that the gradient variance can be naturally decomposed into two parts. One part concerns about the variance of sentences sampled in a batch, and the other part concerns about the variance of the masked positions. Our focus is on the variance reduction of the second part. Importance sampling is a standard way for variance reduction, which suggests that we can sample the masks using a proposal distribution instead of the uniform distribution. According to the theory, the variance is minimized if the probability of a mask sampled from the proposal distribution is proportional to the gradient norm. However, this brings the chicken-egg problem: We will never know the gradient norm unless we mask the positions, feed the masked sentence to the model and back-propagate the loss. As the number of possible masks is huge, feeding all the possibilities to the network to obtain their gradients is time expensive which significantly slows down the training process.  To address this challenge, we introduce a meta-learning approach by introducing a MAsk Proposal Network  which takes the whole sentence as input and outputs a probability distribution over positions to sample masks. Both MAP-Net and the pretraining model are jointly optimized in an adversarial manner. Given a masked sentence sampled from the MAP-Net, the model is optimized to recover the masked sentence. At the same time, the MAP-Net receives signals from the performance of the model on this masked sentence, and improve itself. Instead of using reinforcement learning, we decouple the learning objective and make the training of the MAP-Net easier. We show that for language generation tasks, we can use the value of the loss instead of the value of the gradient norm. Therefore, the goal of the MAP-Net is to find ``tough'' masked positions with high losses to challenge the model, while the model attempts to fulfill the tasks generated by the MAP-Net. As we obtain the loss of many masked positions, the MAP-Net can be efficiently optimized from the pair-wise preference of different positions.    To demonstrate the advantage of our proposed method, we conduct several experiments by using the MAP-Net to help the training of BERT, and evaluate them over GLUE natural language understanding benchmark . Experiment results first indicate that the masked words generated by MAP-NET are meaningful and informative during training. Furthermore, as the variance is sufficiently reduced, the BERT model learned with MAP-Net achieves better accuracy than the baselines on most of the tasks.    [t]                In this work, we propose MAP-Net, which uses a mask proposal network to reduce the gradient variance of pre-training. In particular, given a sentence, the MAP-Net outputs a probability distribution over positions to sample masks. Then, the BERT model is trained to recover the masked sentence sampled from the MAP-Net, instead of uniform distribution. Extensive experiments demonstrate MAP-Net can do better on downstream tasks. It outperforms BERT on GLUE tasks with less training steps. In the future, we will continue exploring more methods to reduce the variance in pretraining, e.g., how to smartly select batched sentences.   
"," Self-supervised learning, a.k.a., pretraining, is important in natural language processing. Most of the pretraining methods first randomly mask some positions in a sentence and then train a model to recover the tokens at the masked positions. In such a way, the model can be trained without human labeling, and the massive data can be used with billion parameters. Therefore, the optimization efficiency becomes critical.  In this paper, we tackle the problem from the view of gradient variance reduction. In particular, we first propose a principled gradient variance decomposition theorem, which shows that the variance of the stochastic gradient of the language pretraining can be naturally decomposed into two terms: the variance that arises from the sample of data in a batch, and the variance that arises from the sampling of the mask. The second term is the key difference between self-supervised learning and supervised learning, which makes the pretraining slower. In order to reduce the variance of the second part, we leverage the importance sampling strategy, which aims at sampling the masks according to a proposal distribution instead of the uniform distribution. It can be shown that if the proposal distribution is proportional to the gradient norm, the variance of the sampling is reduced. To improve efficiency, we introduced a MAsk Proposal Network , which approximates the optimal mask proposal distribution and is trained end-to-end along with the model. According to the experimental result, our model converges much faster and achieves higher performance than the baseline BERT model.",251
" Written Chinese has no explicit word boundary, so Chinese word segmentation  serves as an upstream disambiguation step for Chinese language processing. The task is often viewed as sequence labeling, where each character receives a label indicating its relative position in a segmented sentence. While traditional machine learning methods have attained strong results, recent research focuses on neural networks.  first treat CWS as neural machine translation . Nonetheless,  point out that without extra resources, all previous neural methods are not yet comparable with the non-neural state of the art  from , and the NMT method is even behind.  We note two advantages of NMT: the entire sentence is encoded before making any decision, and the model jointly trains character embeddings with sequence modeling. Thus, we try to bridge the gap between the NMT approach and SOTA, using low-resource techniques such as regularization and data augmentation. Then, we explore more techniques commonly seen in NMT. The translation-based method is easy to adopt without the hassle of feature engineering and model design. In the constrained test condition, our method reaches the top on the MSR dataset and achieves a strong result on the PKU dataset without tuning. As a generic approach it can also be used for other languages.     Our low-resource translation approach to Chinese segmentation achieves strong performance and is easy to adopt. Data augmentation, objective weighting and ensembling are the most beneficial. In future, it is worth extending this to other languages.    
"," % Supervised Chinese word segmentation has been widely approached as sequence labeling or sequence modeling. Recently, some researchers attempted to treat it as character-level translation, but there is still a performance gap between the translation-based approach and other methods. In this work, we apply the best practices from low-resource neural machine translation to Chinese word segmentation. We build encoder-decoder models with attention, and examine a series of techniques including regularization, data augmentation, objective weighting, transfer learning and ensembling. When benchmarked on MSR corpus under closed test condition without additional data, our method achieves 97.6\% F1, which is on a par with the state of the art and remarkably better than previous translation-based methods. Supervised Chinese word segmentation has entered the deep learning era which reduces the hassle of feature engineering. Recently, some researchers attempted to treat it as character-level translation which further simplified model designing and building, but there is still a performance gap between the translation-based approach and other methods. In this work, we apply the best practices from low-resource neural machine translation to Chinese word segmentation. We build encoder-decoder models with attention, and examine a series of techniques including regularization, data augmentation, objective weighting, transfer learning and ensembling. Our method is generic for word segmentation, without the need for feature engineering or model implementation. In the closed test with constrained data, our method ties with the state of the art on the MSR dataset and is comparable to other methods on the PKU dataset.",252
" In the era of mobile reading, a lot of self-media platforms based on the user-generated-content mode have emerged. People are accustomed to spending fragmented time reading online articles published on self-media platforms to conveniently get information and knowledge via mobile devices. Different from traditional documents such as essays, academic papers or Wikipedia documents, self-media online articles have more diverse multimedia elements, in addition to text, usually existing pictures, videos, etc. The organization of these elements jointly affects users' perception. Besides, since the creation forms of self-media online articles are more free, these articles do not have a unified format and layout, and usually vary in diverse categories, styles and content. Therefore, it is necessary to integrate different multimedia elements more comprehensively to jointly process self-media online articles.  The openness of self-media platforms, where each user can be a producer, however, results in uneven quality of online articles. Assessing the quality of self-media online articles is a critical issue for many applications such as recommender systems and online search to find high-quality articles and filter low-quality articles. It is very helpful to increase user stickiness to propose an efficient solution for the automatic evaluation of the self-media online article quality. Considering the nature of self-media platforms, in order to engage users, the quality of self-media online articles is reasonably defined as the level of the reading experience that articles give users. This can be reflected in the article's content, writing norms, user perception, etc., and each factor also contains complicated elements, making the self-media online article quality assessment a much more complex and challenging task. The following question lies to be addressed: How to establish a unified framework to effectively solve the multivariate representation learning of self-media online article quality? However, current studies on the document quality assessment mainly focus on textual features . To the best of our knowledge, no prior work has studied the automatic self-media online article quality assessment.   %  %     In this work, we propose a novel method for the self-media online article quality classification. We design a joint network CoQAN to decouple the modeling of the layout organization, writing characteristics and text semantics, finally merge them into a unified model. We innovatively propose to explicitly learn the presentational quality of online articles, together with the text quality to predict the final quality of online articles. The proposed framework can integrate different features of the online article quality assessment, and the specially designed high-order interactive feature learning and the mobile browsing habits modeling can solve the problems in this field well. Evaluation results based on the real-world online article dataset demonstrate the effectiveness and superiority of our proposed CoQAN. Since most self-media articles convey the main content and core ideas mainly through text, and considering the complexity of this task and the efficiency in practical applications, our proposed network focuses more on understanding the semantics of the text. More generalized assessments may need to introduce the semantic judgment of pictures and we leave this into our future work.        If your work has an appendix, this is the place to put it.   
","   The automatic quality assessment of self-media online articles is an urgent and new issue, which is of great value to the online recommendation and search. Different from traditional and well-formed articles, self-media online articles are mainly created by users, which have the appearance characteristics of different text levels and multi-modal hybrid editing, along with the potential characteristics of diverse content, different styles, large semantic spans and good interactive experience requirements. To solve these challenges, we establish a joint model CoQAN in combination with the layout organization, writing characteristics and text semantics, designing different representation learning subnetworks, especially for the feature learning process and interactive reading habits on mobile terminals. It is more consistent with the cognitive style of expressing an expert's evaluation of articles. We have also constructed a large scale real-world assessment dataset. Extensive experimental results show that the proposed framework significantly outperforms state-of-the-art methods, and effectively learns and integrates different factors of the online article quality assessment.",253
" Dialogue state modules are a central component to a task-oriented dialogue system. Given a user utterance and existing dialogue history, a dialogue system typically extracts dialogue states, according to which a system response is generated. An example is shown in Figure, given two turns of a dialogue, the first user utterance is ``I want an expensive restaurant that serves Turkish food.'', and the dialogue states consist of the slot-value pairs . As the dialogue proceeds, the dialogue state is updated at each turn. After tow dialogue turns, the dialogue state becomes , where inform represents the search constraints expressed by user and request represents the search target that the user is asking for. In this example, the user intention is to reserve a restaurant. The business domain is restaurant customer service. The dialogue state represents what the user is looking for at the current turn of the conversation.   % While traditional dialogue state tracking modules extract user intention and then slot values in a pipeline [], recent work performances end-to-end DST by extracting slot-value pairs only. {, which contains more than three million tweets and replies between customer support agents of some big companies and their customers and the Ubuntu dialogue corpus . %In such occasions, to gather dialogue state information solely based on conversation utterances is more realistic.     To address this issue, it can be useful to automatically induce dialogue states from raw dialogues. We assume that there is a large set of dialogue records of many different domains, but without manual labeling of dialogue states. Such data are relatively easy to obtain, for example from customer service call records from different businesses. Consequently, we propose the task of { is regarded as the slot and area is regarded as the value. During training, DST relies on both a dialogue record and manual labeling of slot-value pairs on the dialogues. In contrast, our task does not rely on manual labeling and can generate slot-value pairs over raw dialogues automatically.  % During testing, existing methods rely on an ontology and a trained model to label slot values on dialogues, while our method does not rely on the ontology. {  % Both models are trained with variational inference.  We introduce two neural latent variable models for DSI by treating the whole state and each slot as latent variables, from which values observed in dialogue data are generated. The goal is to induce slots according to those frequently co-occurring values and the dialogue contexts. In particular, each value  is represented by using both a sparse one-hot representation and a dense contextualized embedding representation. Both models are generative probabilistic models, which generate a value by first generating a latent dialogue state vector, and then generating a slot. The difference between the two models is the modeling of service domains. We observe that different service domains may contain slots with similar contexts or values. For example, both { domains can have the same slot {. We refer to the basic model {.  % We build an unsupervised generative model for DSI. {.  % We want highlight four points:  our model can solve the unknown slot values problem and the multi-domain problem.  our model can effectively reduce the Inference Time Complexity  since it is not constrained by the number of slots and values in a pre-defined ontology list.   although we do not use external knowledge in our unsupervised model, it can be easily extended from external sources like slot values from backend  or SLU system outputs.  Annotated training data can be easily added so that our models become semi-supervised.  % { and , we construct a slot value candidate set given the dialogue history until certain turn. We build a model ... % unknown slot values People tend to solve some small but realistic research problems in this area. This is the infinite slot value problem. To summarize related work aimed to solve this problem. One is the ACl paper from Chumenwenwen, many papers are aimed to solve this, but maybe only those top conference papers can be selected and shown. % Multi-domain Infinite slot value problem can be useful for solving the realistic application problem based on current researches but is not enough. Schema-Guided Dialogue State Tracking has been proposed to further solve this problem.  This can be applied to similar domain DST, however, for complete new domains, large semantic barrier can hinder its application. Besides, its additional annotation of sentence description can also be expensive and its quality can affect the results obviously. some other related work % However, it is still far away from realistic application. The origin of these problems lie on the annotation difficulty and its cost. Thus, task-oriented dialogue corpus is limited in its scale compared to open-domain dialogue corpus. Thus, most existing corpus are based on a small ontology, which is still far away from real world user cases. % comparable results compared to supervised methods. % Useful for response generation. For the SOTA models without dialogue state, adding our DSI results can get improvements. % Existing task-oriented dialogue corpus domains include restaurant, flight, etc. Our method is especially helpful for those totally new domains, like online shopping client service, which have plenty of dialogue records, but a huge number of slots and values make it even impossible to annotate. why not few shot learning, maybe still difficult to split into categories and annotate. And our methods are based on large amount of dialogues to find their similarity.  We proposed a novel task of dialogue state induction, which is to automatically identify dialogue state slots and values over a large set of dialogue records. Compared with existing research, our task is practically more useful for handling the large variety of services available in the industry, which disallows scalable manual labeling of dialogue states. We further built two neural generative models with latent variables. Results on standard DST datasets show that the models can effectively induce meaningful dialogue states from raw dialogue data, and further improve the results of a dialogue system compared to without using dialogue states. Our methods can serve as baselines for further research on the task.    We release our code and models at .     }         ijcai20.tex  \typeout{IJCAI--PRICAI--20 Instructions for Authors}    These are the instructions for authors for IJCAI-20.  \documentclass{article} \pdfpagewidth=8.5in \pdfpageheight=11in   The file ijcai20.sty is NOT the same than previous years' \usepackage{ijcai20}    Use the postscript times font! \usepackage{times} \usepackage{soul} \usepackage{url}  \usepackage[hidelinks]{hyperref} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{multirow} \usepackage{subfigure} \usepackage{amsmath} \usepackage{amsthm} \usepackage{booktabs} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{amsfonts} \usepackage{color} \urlstyle{same} [1]{~}    the following package is optional:  \usepackage{latexsym}     See https://www.overleaf.com/learn/latex/theorems_and_proofs   for a nice explanation of how to define new theorems, but keep   in mind that the amsthm package is already included in this   template and that you must *not* alter the styling. {Example} {Theorem}    Following comment is from ijcai97-submit.tex:   The preparation of these files was supported by Schlumberger Palo Alto   Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.   Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.   Patel-Schneider, of AT\&T Bell Laboratories collaborated on their   preparation.    These instructions can be modified and used in other conferences as long   as credit to the authors and supporting agencies is retained, this notice   is not changed, and further modification or reuse is not restricted.   Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as   contacts for providing assistance without their prior permission.    To use for other conferences, change references to files and the   conference appropriate and use other authors, contacts, publishers, and   organizations.   Also change the deadline and address for returning papers and the length and   page charge instructions.   Put where the files are available in the appropriate places.  \title{Dialogue State Induction Using Neural Latent Variable Models}     Multiple author syntax (remove the single-author syntax above and the ^3^{1,2}^{4}^{1,2}^1^2^3^4$School of Computer Science and Technology, Beijing Institute of Technology 	@westlake.edu.cn, 	lbqin@ir.hit.edu.cn, 	xiaoliu@bit.edu.cn }    \maketitle                
"," Dialogue state modules are a useful component in a task-oriented dialogue system. Traditional methods find dialogue states by manually labeling training corpora, upon which neural models are trained. However, the labeling process can be costly, slow, error-prone, and more importantly, cannot cover the vast range of domains in real-world dialogues for customer service. We propose the task of dialogue state induction, building two neural latent variable models that mine dialogue states automatically from unlabeled customer service dialogue records. Results show that the models can effectively find meaningful dialogue states. In addition, equipped with induced dialogue states, a state-of-the-art dialogue system gives better performance compared with not using a dialogue state module.",254
"   Spoken dialog systems such as those utilized in voice assistants such as Alexa, Siri and Google Home, typically consists of a sequential chain of sub-systems, including Spoken Language Understanding , Dialog Management, Natural Language Generation and Text-to-Speech. Generally, these sub-systems perform cloud-based processing of speech, following on-device wakeword detection. First, the SLU system extracts natural language semantics such as utterance intent as well as associated named entities or slot values from the speech segment. The appropriate application is then invoked for further execution and finally responses are processed by a text-to-speech system and relayed to the user. An example of intents and slots for an utterance is in Table.\ . Conventionally, the SLU system comprises two distinct stages:  An Automatic Speech Recognition  system obtains the transcript or a text representation of the raw audio segment,  A Natural Language Understanding  system subsequently consumes the transcript or alternatively n-best hypotheses of the ASR system and extracts semantics, in particular, the domain, intent and slots.  In this work, we consider neural end-to-end  SLU models that produce semantics of intents and slots from audio. A primary motivation for the work arises from deployment of SLU systems to devices that are more resource limited than cloud servers. For such devices, a neural E2E SLU model can be customized under the given resource constraints to satisfy a limited set of intents or use cases , and deployed. Moving SLU computation from cloud to devices allows  offline use, e.g. in cars or emergency situations  latency gains from placing computations closer to the user  cost and carbon savings from reduced fleet sizes and reducing communication payloads.   E2E SLU provides an alternative paradigm to the conventional approach of compressing individual components of the ASR or NLU systems to satisfy on-device resource constraints. In the latter approach, the NLU subsystem is not exposed to audio information such as prosody, or ambiguity in the ASR decoding beyond n-best hypotheses. Errors from the ASR system cascade down to NLU tasks. Our approach to developing E2E SLU systems leverages models developed in ASR and NLU communities by replacing the text interface between them with a neural network hidden interface layer. We term this interpretable subclass of E2E SLU models Joint SLU models that produce intermediate transcript as well as NLU annotations. We show that NLU metrics improve with exposure to this richer interface and also that ASR metrics improve from NLU feedback with joint training. The multitask training of these models can make use of datasets with only transcribed audio as well as audio with NLU annotations.     {| p{0.2\linewidth} | p{0.7\linewidth} |}  an|Other alarm|NotificationType for|Other six|Time a.m.|Time \\ \hline Slots & NotificationType - alarm, Time - six a.m. \\ \hline      [h]                    A few prior works have considered the E2E SLU problem. In the work from Google , authors first develop the problem and note that having an intermediate text representation improves performance. They consider encoder-decoder sequence networks to predict transcript and a serialized form of the semantics in a multitask model where decoders are separate, a two-stage model where the transcript is obtained first, and a joint model where a single decoder predicts both jointly. In contrast, we formalize distinctions between ASR and NLU subsystems and study the impact of end-to-end training, and interfaces such as text or hidden layers between the two. In , a CTC based network is used to extract named entities from French speech while we use attention based networks and train on larger corpora. Finally,  are works that obtain a single label  directly from speech segments. Our work goes beyond this and performs slot filling as well.   % TODO: is such a detailed description of E2E ASR systems needed?  E2E SLU models are similar to other multitask speech systems such as speech translation systems or multilingual ASR systems. This work has been enabled by advancements in E2E ASR, where such systems have been shown to outperform conventional RNN-HMM hybrid ASR systems  when trained on large acoustic datasets. Connectionist Temporal Classification networks  was the first all neural E2E ASR model that trained a Recurrent Neural Network  on audio input features with a transcript label sequence of a different length by considering all possible alignments between inputs and labels. In Recurrent Neural Network-Transducer , authors extend CTC by also modelling interdependencies between input-output and output-output distributions using an added prediction network. In both cases, an efficient forward-backward computation enables loss computation and backpropagation over all alignments. In contrast to the aforementioned streaming architectures, in attention based sequence-to-sequence networks such as Listen Attend Spell  , input features are processed by encoder networks that produce a hidden representation output for each feature. The decoder estimates an element of the label sequence at each step using an attention network to focus on a fraction of the encoder network outputs.  Extracting intent and slots from transcript is a long running problem in NLU . In survey , authors compare DNN and earlier feature engineering approach coupled with conditional random fields or softmax layers for the purpose of named entity recognition. The interface between ASR and NLU systems has traditionally been the best hypothesis sequence although richer interfaces such as lattices and word confusion networks have also been well studied . In this work, we develop a simple joint intent and slot prediction network and study the impact of text vs hidden layer interfaces between ASR and NLU.       In Sec.\ , we first present a low-resource streaming model that extracts utterance intent directly from speech without intermediate text output. We then present a compositional model that is similar to a non-streaming pipelined two-stage SLU architecture, where a LAS based ASR system produces a transcript which is then consumed by an independently trained Neural NLU system. Finally, we present the aforementioned E2E differentiable Joint SLU models where the interface between ASR and NLU is a shared hidden layer. We restrict ourselves to 1-best interfaces between ASR and NLU and leave n-best hidden layer interfaces to future work.   We present experimental results, baselines, and metrics on a variety of datasets in Sec.\  and answer the following questions: [leftmargin=*]        We developed models for the problem of spoken language understanding of extracting natural language intent and named-entities or slots directly from speech. Such an end-to-end model can be customized and deployed on resource constrained device enabling new offline and privacy focussed use cases. We first developed an audio to intent model of small footprint. We then developed a compositional model with a pretrained LAS ASR model whose outputs, the transcription of the audio, is fed to a pre-trained neural NLU model. Finally, an end-to-end fully differentiable, interpretable Joint SLU model was presented where the NLU system consumes not the transcript output of the LAS system but a neural network interface that encodes ASR word confusion. These models were trained on multiple datasets and that affirmed the following points:  intent classification performance improves when ASR outputs are also produced  replacing a text or wordpiece interface between compositional ASR and NLU systems with a neural network hidden and joint training leads to a 2.7+\  relative improvement to NLU metrics for intents, and slots and mitigates the downstream impact of ASR errors  joint training reduces ASR WER by 3.8\  through backpropagation of NLU losses to the ASR layers.    
"," We consider the problem of spoken language understanding  of extracting natural language intents and associated slot arguments or named entities from speech that is primarily directed at voice assistants. Such a system subsumes both automatic speech recognition  as well as natural language understanding . An end-to-end joint SLU model can be built to a required specification opening up the opportunity to deploy on hardware constrained scenarios like devices enabling voice assistants to work offline, in a privacy preserving manner, whilst also reducing server costs.   % removed word ""composed"" from joint system description We first present models that extract utterance intent directly from speech without intermediate text output. We then present a compositional model, which generates the transcript using the Listen Attend Spell ASR system and then extracts interpretation using a neural NLU model. Finally, we contrast these methods to a jointly trained end-to-end joint SLU model, consisting of ASR and NLU subsystems which are connected by a neural network based interface instead of text, that produces transcripts as well as NLU interpretation. We show that the jointly trained model shows improvements to ASR incorporating semantic information from NLU and also improves NLU by exposing it to ASR confusion encoded in the hidden layer.",255
" Modularized task-oriented dialogues systems are the core of the current smart speaker generation . The main modules of such systems are Natural Language Understanding , Dialogue State Tracking , Dialogue Policy  and Natural Language Generation , each of which is trained separately using supervised and/or reinforcement learning. Thus a data collection process is required, which for some of the tasks can be laborious and expensive. For example, dialogue policy annotation has to be done by an expert, better by a professional linguist. Therefore, having a model that requires only few samples to actually perform well in the tasks is essential.     The most successful approach in few-shot learning for task-oriented dialogue systems is notably transfer learning, where a large model is firstly pre-trained on a large corpus to be then fine-tuned on specific tasks. For task-oriented dialogue systems, ~ proposed {TOD-BERT} a large pre-trained model which can achieve better performance than BERT in few-shots NLU, DST and DP.  proposed a two-step classification for few-shot slot-filling, a key task for the NLU module. Similarly,  introduced a benchmark for few-shot NLG and a pre-trained language model  specialized for the task. Further, a template rewriting schema based on T5 was developed by  for few-shot NLG in two well-known datasets.  proposed a pre-trained language model  for end-to-end pipe-lined task-oriented dialogue systems. In their experiments, they showed promising few-shot learning performance in MWoZ. Finally, several meta-learning approaches have been proposed for DP, NLG/ACT, pipelined end-to-end models and personalized dialogue systems.    For performing few-shot learning, existing methods require a set of task-specific parameters since the model is fine-tuned with few samples. Differently, in this paper, we perform few-shot learning by priming LMs with few-examples. In this setting, no parameters are updated, thus allowing a single model to perform multiple tasks at the same time. In this paper, we evaluate the few-shot ability of LM priming on the four task-oriented tasks previously mentioned . Currently, GPT-3 is not available to the public; thus we experiment on different sizes GPT-2 models such as SMALL , LARGE , and XL . All the experiments are run on a single NVIDIA 1080Ti GPU.    In this paper, we demonstrate the potential of LM priming few-shot learning in the most common task-oriented dialogue system tasks . Our experiments show that in most of the tasks larger LMs are better few-shot learners, confirming the hypothesis in~ and, in some cases, they can also achieve similar or better results than the weakest finetuning-based baseline. Finally, we unveil two limitations of the current LM priming few-shot learning the computational cost and the limited word context size. In future work, we plan to benchmark dialogue-specific models  and LM with longer context size . We also plan to investigate adversarial triggers for improving the few-shot ability of LMs, and to benchmark end-to-end dialogue tasks.        
"," Task-oriented dialogue systems use four connected modules, namely, Natural Language Understanding , a Dialogue State Tracking , Dialogue Policy  and Natural Language Generation . A research challenge is to learn each module with the least amount of samples  given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning, where large language models, either pre-trained on text or task-specific data, are fine-tuned on the few samples. These methods require fine-tuning steps and a set of parameters for each task. Differently, language models, such as GPT-2~ and GPT-3~, allow few-shot learning by priming the model with few examples. In this paper, we evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current limitations of this approach, and we discuss the possible implication to future work.",256
" Modern search engines provide search services specialized across various domains . Users come to a search engine to look for information with different possible intents: choosing favorite restaurants, checking opening hours, or restaurant addresses on Yelp; searching for people, finding job opportunities, looking for company information on LinkedIn, etc.     Understanding the intent of a searcher is crucial to the success of search systems. Queries contain rich textual information provided explicitly by the searcher, hence a strong indicator to the searcher閳ユ獨 intent.  %For example, given a LinkedIn query that contains keywords machine learning, the user could be looking for machine learning jobs, connections with machine learning skills, or online courses to learn machine learning, etc.   Understanding the underlying searcher intent from a query, is referred to the task of query intent modeling.   Query intent is an important component in the search engine ecosystem . As shown in Figure , when the user starts typing a query, the intent is predicted based on the incomplete character sequence; when the user finishes typing the whole query, a more accurate intent is predicted based on the completed query.  Understanding the user intent accurately allows the search engine to trigger corresponding vertical searches, as well as to better rank the retrieved documents based on the intent , so that users do not have to refine their searches by explicitly navigating through the different facets in the search engine.  %For example, in Figure , query intent is identified for the typeahead blending, when the query is incomplete; when user hit          Traditional methods rely on bag-of-words representation and rule based features to perform intent classification . Recently, deep learning based models  show significant improvement, which can handle similar words/word sense disambiguation well.  However, developing deep learning based query intent models for productions requires considering several challenges.  Firstly, production models have very strict latency requirements, and the whole process needs to be finished within tens of milliseconds. Secondly, queries, usually with two or three words in a complete query or several characters in a incomplete one, have limited contexts.  % In this paper, we design models customized for the specific task.   This paper proposes a practical deep learning framework to tackle the two challenges, with the goal of improving LinkedIn's commercial search engine. Two search result blending components were identified where query intent is useful: incomplete query intent for typeahead blending, and complete query intent for SERP blending .   The common part of both systems is to use query intent to assist the ranking of retrieved documents of different types. Meanwhile, the two products have their unique challenges. Typeahead blending has strong latency requirements; the input is an incomplete query ; and it is okay to return a fuzzy prediction, since users will continue to type the whole query if he/she does not find the results relevant. On the other hand, SERP blending has less latency constraint compared to typeahead but a higher accuracy requirement as it directly affects the search result page.  %Because of the different requirements, we exploit many aspects to find the best solutions for each of the two productions, such as algorithms , token units , combining traditional features, and multilinguality.   Based on the characteristics of production applications, we propose different solutions. For typeahead blending, character-level query representation is used as the resulting models are compact in terms of the number of parameters. Meanwhile, it can handle multilinguality well due to the small vocabulary size. For SERP blending, the complete query intent model is word level. Since accuracy is a high standard, BERT is explored to extract query representations which lead to a more robust model.  This paper is motivated by tackling the challenges in query intent prediction, while satisfying production needs in order to achieve real-world impact in search systems. The major contributions are:           %proxIn the rest of this paper, we first introduce how query intent is defined and utilized in LinkedIn search products. This is followed by the design of different granularity  models to adapt to different stages of intent prediction.  The experiments and results show that the proposed deep learning based character/word-level representation models are effective at understanding users' search intents as well as efficient for productionalization in online search systems.   % algorithm analysis]: BERT Experience: in-domain data, better performance/latency with smaller model. LSTM experience: capture long range distance.  CNN fast. % [token granularity analysis]:  character level model benefits: 1. compact model. 2. suitable for multilingual % [product impact analysis]: Larger impact of deep learning models on typeahead blending than serp blending  % Wide feature experience     This paper proposes a comprehensive framework for modeling the query intent in search systems for different product components. The proposed deep learning based models are proven to be effective and efficient for online search applications.  Discussions about the challenges for deploying these models to  production as well as our insights in making these decisions are provided. We hope the framework as well as the experiences during our journey could be useful for readers designing real-world query understanding and text classification tasks.           The next two lines define the bibliography style to be used, and    the bibliography file.     \endinput
","     Understanding a user's query intent behind a search is critical for modern search engine success.      Accurate query intent prediction allows the search engine to better serve the user's need by rendering results from more relevant categories.      This paper aims to provide a comprehensive learning framework for modeling query intent under different stages of a search. We focus on the design for 1) predicting users' intents as they type in queries on-the-fly in typeahead search using character-level models; and 2) accurate word-level intent prediction models for complete queries. Various deep learning components for query text understanding are experimented. Offline evaluation and online A/B test experiments show that the proposed methods are effective in understanding query intent and efficient to scale for online search systems.     % The proposed approach models query intent by 1) predicting users' intents as they type in queries on-the-fly in typeahead search with character-level models; as well as 2) word-level intent prediction models after the queries are completed. Character-level and word-level deep learning based models are proposed and thoroughly analyzed. Offline evaluation and online A/B test experiments show that the proposed methods are effective in predicting query intent, which assists search engine success.          % The proposed character-level model based on recurrent neural networks  is effective in terms of predicting search verticals on incomplete queries, while also being efficient as the online inference latency meets production requirements.      % For word-level query understanding, convolutional neural networks  based model provides an effective and efficient solution for query intent classification, while Bidirectional Encoder Representations from Transformers  outperforms CNN in terms of accuracy but introduces larger latency at the same time.      % We also focus on the challenges and efforts of putting the BERT models into real life products at LinkedIn.",257
"  A dialog system should correctly understand speakers閳 utterances and respond in natural language. Dialog act recognition  and sentiment classification are two correlative tasks to realize the former. The goal of DAR is to attach semantic labels to each utterance in a dialog and identify the underlying intentions . Meanwhile, sentiment classification can detect the sentiments which are implicated in utterances and can help to capture speakers閳 intentions .      This paper focuses on explicitly establishing the bi-directional interrelated connections for dialog act recognition and sentiment information. We propose a deep relation network to jointly model the interaction and relation between the two tasks, which adopts a stacked co-interactive relation layer to incorporate mutual knowledge explicitly. In addition, we explore three different relation layers and make a thorough study on their effects on the two tasks. Experiments on two datasets show the effectiveness of the proposed models and achieve the state-of-the-art performance.  Extensive analysis further confirms the correlation between two tasks and reveals that modeling the relation explicitly can boost their performance. Besides, we analyze the effect of incorporating strong pre-trained BERT model in our joint model. With BERT, the result reaches a new state-of-the-art level.  
","         In dialog system, dialog act recognition and sentiment classification are two correlative tasks to capture speakers闁 intentions, where dialog act and sentiment can indicate the explicit and the implicit intentions separately  . Most of the existing systems either treat them as separate tasks or just jointly model the two tasks by sharing parameters in an implicit way without explicitly  modeling mutual interaction and relation. To address this problem, we propose a Deep Co-Interactive Relation Network  to explicitly consider the cross-impact and model the interaction between the two tasks by introducing a co-interactive relation layer. In addition, the proposed relation layer can be stacked to gradually capture mutual knowledge with multiple steps of interaction. Especially, we thoroughly study different relation layers and their effects. Experimental results on two public datasets  show that  our model outperforms the state-of-the-art joint model by 4.3\% and 3.4\% in terms of F1 score on dialog act recognition task, 5.7\% and 12.4\% on sentiment classification respectively. Comprehensive analysis empirically verifies the effectiveness of explicitly modeling the relation between the two tasks and the multi-steps interaction mechanism. Finally, we employ the Bidirectional Encoder  Representation from Transformer  in our framework, which can further boost our performance in both tasks.",258
" Over the past decades, sentiment analysis has delivered promising results on stock market prediction. The sentiment-based approaches explore the sentiment or event signals from text corpus such as tweets, news, or financial announcements and attempt to relate such signals to the stock price variation. These works can be regression~ that predicts the  in next time step, or classification~ that predicts the  of the stock movement.   In early studies, bag-of-words , n-grams, or other discrete word features such as noun phrase are used to represent the text corpus~. The word features are selected by pre-defined dictionaries or statistical metrics. Although, such approaches facilitate the alignment between the linguistic features and numerical data and mitigate the dimensionality problem, they can hardly preserve the contextual information.  Lately, neural approaches have been applied to the realm.  extracted event tuples  from the news articles and trained the event embeddings. Then the daily events are averaged with the event vectors.  represented each tweet using Continuous Bag of Words Model  with word2vec for prediction. Moreover, many neural network based models such as RNN and text-CNN are proposed~ and achieve considerable improvement as compared to traditional methods.  However, some issues are less addressed by existing approaches:  Most of the time the stock prices fluctuate within a narrow range and unnecessarily reflect the market sentiment, which complicates relating sentiment signals to price signals.  The polarity of a certain word, especially an entity or a term, can change over time according to different events. For instance, the tone of word ``venezuela"" fluctuates with the oil trade situations between the U.S. and Venezuela.  The quality or value varies in different texts. Extracting sentiment signals from a text can only be valid if the text is relevant to the market.   To address these issues, we propose a two-stage system and utilize both the neural representation and discrete features for stock trend prediction. As illustrated in Figure, our system consists of a  to extract the sentiment score for the future market trend and a  that predicts the direction of the index movement of next week given the sentiment scores of news over the week. The main architecture we use for the sentiment extractor is the vanilla BERT~ with multitask learning~ - one additional prediction head that predicts the worthiness of the news. We propose a metric called  to extract the word polarity among different event periods and use it as a supplemental feature. We present a weekly-Monday prediction framework in which the Monday index variations are predicted with all news articles over the past week. A new dataset, the 10-year Reuters financial News , is also proposed.     The experimental results on the 10-year Reuters financial news dataset show that our system achieves significant improvement as compared to the baseline methods. We illustrate the weekly-Monday basis is appropriate for sentiment-based stock prediction. We show that our model that uses word polarity features and additionally learns the worthiness of the news can better predict the stock market index.       In this paper, we propose a sentiment-based stock index prediction system which contains a sentiment extractor that distills the polarity of the news articles towards the market and a summarizer that sums up the overall sentiment of the week to predict the index change of next week. We propose a discrete word feature called Polarity-Over-Time  which captures the sentiment changes of words according to certain events at different periods of time. Both the POT feature and multi-task learning are used to improve the performance of the sentiment extractor. We show that our model on the 10-year Reuters news dataset achieves considerable improvements as compared to other baselines. In particular, we demonstrate that the weekly-Monday framework provides space for the market to react on sentiment signals and therefore, is appropriate for sentiment-based stock prediction.  In the future, we will explore the following directions:  The adaptation of our model to daily price prediction. We will explore how to adapt our model to predict the daily price variations in which stronger denoising methods may be applied to sentiment extraction.   The merge of the sentiment extractor with the summarizer as one integrated neutral architecture such as BERT with hierarchical CNN so that the parameters can be jointly learned and layers instead of probability scores can be fed to the summarizer.   
","   Sentiment-based stock prediction systems aim to explore sentiment or event signals from online corpora and attempt to relate the signals to stock price variations. Both the feature-based and neural-networks-based approaches have delivered promising results. However, the frequently minor fluctuations of the stock prices restrict learning the sentiment of text from price patterns, and learning market sentiment from text can be biased if the text is irrelevant to the underlying market. In addition, when using discrete word features, the polarity of a certain term can change over time according to different events. To address these issues, we propose a two-stage system that consists of a sentiment extractor to extract the opinion on the market trend and a summarizer that predicts the direction of the index movement of following week given the opinions of the news over the current week. We adopt BERT with multitask learning which additionally predicts the worthiness of the news and propose a metric called Polarity-Over-Time to extract the word polarity among different event periods. A Weekly-Monday prediction framework and a new dataset, the 10-year Reuters financial news dataset, are also proposed.",259
"  Neural modeling approaches are prominent in research on both task-oriented and open-domain dialog. Traditional sequence-to-sequence models have been used for encoding the dialog history and predicting domains, intents, slot types, spans and more generally decoding full-fledged system responses. In recent years, large pre-trained Transformer-based models for natural language understanding  and natural language generation  have become ubiquitous, leading to tremendous advances by fine-tuning towards these dialog tasks.  %   In task-oriented speech-based dialog systems, the effect of ASR hypotheses has been widely studied and techniques have been devised to minimize the resulting downstream NLU errors. More recently, end-to-end spoken language understanding approaches have been attempted to sidestep this problem. % There is work to address such issues, such as using n-best lists. %It is not clear how these models would serve the speech modality, when ASR output contains recognition errors, and does not have structural information such as sentence boundaries and punctuation. On the other hand, research in open-domain dialog is increasingly focusing on large, monolithic end-to-end neural models like Google's Meena that are built using written data and evaluated on written interactions. Several written textual datasets have been created recently to tackle various problems in open-domain dialog, including persona-grounding, knowledge-grounding and reasoning, and state-of-the-art chatbots have been built using them. However, it is not clear whether these written text-based open-domain chatbots would seamlessly integrate with ASR models to serve the speech modality, which is popular due to the ubiquity of voice assistants like Alexa, Google Assistant and Siri.  Collecting large-scale written text-based dialog datasets is cheaper and more practical than collecting audio-based dialog datasets in many ways. But speech-robustness should be a factor of consideration when designing any  dialog system intended to be deployed to the speech modality, even in the absence of audio-based training data. To bring attention to this important aspect in the open-domain dialog community, we empirically study the effects of various types of synthetic and actual ASR hypotheses in the dialog history on TransferTransfo , a state-of-the-art neural open-domain dialog system based on the Generative Pre-trained Transformer  from the NeurIPS ConvAI2 Conversational Intelligence Challenge. We build off the Topical-Chat dataset and perform two augmentations in our study: one creating simulated ASR hypotheses for the entire dataset, and another creating actual ASR hypotheses with a smaller audio-based analogue of the Topical-Chat test sets.  We observe that TF2 trained on written textual data is very sensitive to synthetic and actual ASR hypotheses introduced to the dialog history during inference time, with the sensitivity being particularly prominent for the task of response selection. As a baseline mitigation strategy, we introduce synthetic ASR hypotheses to the dialog history during training and observe marginal improvements, demonstrating the need for further research into techniques to make end-to-end open-domain chatbots fully speech-robust. Figure shows a sample snippet with responses from TF2 models trained on written text and synthetic ASR hypotheses when fed speech-distorted dialog history.  A close work to ours in spirit is, which shows that Transformer-based generative dialog models are insensitive to unrealistic perturbations like token-shuffling of the dialog history. Our work is more focused on evaluating the effects of introducing realistic perturbations to the dialog history in the form of synthetic and actual ASR hypotheses. Our augmentation of Topical-Chat, dubbed the Topical-Chat ASR dataset, is open-sourced\footnote{https://github.com/alexa/Topical-Chat/tree/master/TopicalChatASR/} to enable open-domain dialog researchers to perform speech-robustness evaluation and fuel research into novel techniques to make monolithic neural open-domain dialog models more speech-robust.  % report the impact on automated metrics such as perplexity, unigram F1, recall and diversity.     We empirically studied the effects of synthetic and actual ASR hypotheses in the dialog history on TF2, a large state-of-the-art text-based neural open-domain dialog system from the NeurIPS ConvAI2 challenge. We observed that TF2 trained on written data is very sensitive to such hypotheses introduced to the dialog history during inference time, demonstrating that text-based neural open-domain chatbots may not be very effective at serving the speech modality as-is. We observed that training TF2 with synthetic ASR hypotheses makes it more robust to both synthetic and actual ASR hypotheses during inference time, with considerable room for improvement left for future work. Our augmentation of Topical-Chat, dubbed the Topical-Chat ASR dataset, is open-sourced and we hope our work sparks discussion and further research into modality-centric and modality-agnostic open-domain dialog systems.  
","   Large end-to-end neural open-domain chatbots are becoming increasingly popular. However, research on building such chatbots has typically assumed that the user input is written in nature and it is not clear whether these chatbots would seamlessly integrate with automatic speech recognition  models to serve the speech modality. We aim to bring attention to this important question by empirically studying the effects of various types of synthetic and actual ASR hypotheses in the dialog history on TransferTransfo, a state-of-the-art Generative Pre-trained Transformer  based neural open-domain dialog system from the NeurIPS ConvAI2 challenge. We observe that TransferTransfo trained on written data is very sensitive to such hypotheses introduced to the dialog history during inference time.   %with the sensitivity being particularly prominent for the task of response selection.   As a baseline mitigation strategy, we introduce synthetic ASR hypotheses to the dialog history during training and observe marginal improvements, demonstrating the need for further research into techniques to make end-to-end open-domain chatbots fully speech-robust. To the best of our knowledge, this is the first study to evaluate the effects of synthetic and actual ASR hypotheses on a state-of-the-art neural open-domain dialog system and we hope it promotes speech-robustness as an evaluation criterion in open-domain dialog.",260
"  Knowledge Graph  consists of facts in the form of triplet  using the corresponding relation and tail  embeddings, and calculate the plausibility of the triplet by measuring the difference between the original and the reconstructed embeddings.  These works either model this relationship in an explainable way , or utilize the black-box but expressive convolution operations . Another strand of works considers link prediction as a semantic matching problem.  They take the embeddings of the head, relation and tail as input, and output a matching score for the elements in each triplet using bi-linear transformation , convolution  and etc.  These works vary a lot in situations that they are suitable for, like the relation types in the KG, the sparsity of the KG, etc. Therefore, choosing a suitable architecture for a specific KG often requires careful analysis of both the dataset and the model. To tackle this issue,  proposes to use AutoML to greedily search for optimal score functions for distinct KGs. %To tackle this issue,  proposes to use AutoML to find an optimal structure for a specific KG. However, their work is constrained to bilinear semantic matching models and does not include the reconstruction-based models into their search space.  In this paper, we propose a novel Neural Architecture Search  framework to search for the most effective architecture for a given dataset. The framework entails a more general search space that contains both semantic matching models and reconstructive models. Therefore, it has the potential to combine the strength of the two model families.  Instead of searching over a discrete set of candidate architectures, we relax the search space to be continuous, so that the architecture can be optimized using the efficient gradient-based search algorithm.  As shown in Fig., our NAS framework contains two search modules. The representation search module aims to refine the embeddings  of the head, relation, and tail respectively through multiple representation layers. The score function search module is responsible for selecting a shallow architecture to calculate a plausibility score for the input triplet. While the operators in each module have a broad range of choices, in this work, we primarily focus on a proof-of-concept of this two-level search space by constraining the operators to architectures that are representatives of existing link prediction models. Specifically, we constrain the search space of the representation search module to be reconstructive models, whose input and output have good compatibility of this module. As for the score function search module, we select representative models from mainstream model families in the link prediction task. To avoid overfitting, we also add the identity operation in the representation search module so that the NAS algorithm could choose to use the original , or even degenerate to the basic models in the score function search module when necessary.  On the one hand, we can consider the representation search module refines  in a black-box way. On the other hand, the output of the representation search module may also embed the constraints modeled in the reconstruction-based models. Therefore, the final score could potentially benefit from the cross-validation of multiple models, which will likely lead to better prediction results.  We evaluate our approach on several popular benchmark datasets. Extensive experiments demonstrate that our approach has good generalization ability over different datasets, and achieves better performance than strong baseline models in most of the datasets.   %% %% This is file `sample-sigconf.tex', %% generated with the docstrip utility. %% %% The original source files were: %% %% samples.dtx   %%  %% IMPORTANT NOTICE: %%  %% For the copyright see the source file. %%  %% Any modified versions of this file must be renamed %% with new filenames distinct from sample-sigconf.tex. %%  %% For distribution of the original source see the terms %% for copying and modification in the file samples.dtx. %%  %% This generated file may be distributed as long as the %% original source files, as listed above, are part of the %% same distribution.  %% %% The first command in your LaTeX source must be the \documentclass command. \documentclass[sigconf]{acmart}  %%%% As of March 2017, [siggraph] is no longer used. Please use sigconf  for SIGGRAPH conferences.  %%%% As of May 2020, [sigchi] and [sigchi-a] are no longer used. Please use sigconf  for SIGCHI conferences.  %%%% Proceedings format for SIGPLAN conferences  % \documentclass[sigplan, anonymous, review]{acmart}  %%%% Proceedings format for conferences using one-column small layout % \documentclass[acmsmall,review]{acmart} %% %% \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{%   \providecommand\BibTeX{{%           %% These commands are for a PROCEEDINGS abstract or paper. %{June 03--05, 2018}{Woodstock, NY} % % %   %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%  %% %% The majority of ACM publications use numbered citations and %% references.  The command  switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%  %% %% end of the preamble, start of the body of the document source.  \usepackage{microtype} \usepackage{url} \usepackage{mathrsfs} \usepackage[linesnumbered,ruled,vlined]{algorithm2e} \usepackage{multirow} \usepackage{graphicx}  \usepackage{float}  \usepackage{subfigure}  \usepackage{color} \usepackage{enumerate} \usepackage{booktabs} \usepackage[normalem]{ulem} \usepackage[toc,page,title]{appendix}  [2]{ {@{}#1@{}}#2}   \usepackage{xcolor} [1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}  \usepackage{xspace}      %% %% The ""title"" command has an optional parameter, %% allowing the author to define a ""short title"" to be used in page headers. \title{NASE: Learning Knowledge Graph Embedding for Link Prediction via Neural Architecture Search}  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research. \author{Xiaoyu Kou} % \authornotemark[1] %\authornote{Both authors contributed equally to this research.}  %\orcid{1234-5678-9012} \affiliation{%          }  \author{Bingfeng Luo}  \affiliation{%      }  \author{Huang Hu}  \affiliation{%       }   \author{Yan Zhang}  \affiliation{%         }  %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose.  %\renewcommand{  %% %% The abstract is a short summary of the work to be presented in the %% article.  %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %%  <ccs2012> <concept> <concept_id>10010147.10010178.10010187</concept_id> <concept_desc>Computing methodologies~Knowledge representation and reasoning</concept_desc> <concept_significance>500</concept_significance> </concept> </ccs2012>      %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.   %% %% This command processes the author and affiliation and title %% information and builds the first part of the formatted document.         In this paper, we propose a novel NAS framework for the link prediction task, which can combine the strength of both reconstruction based and semantic matching based models.  Experimental results show that NASE outperforms several state-of-the-art human-designed models and AutoML based models in most of the datasets. In future work, we would like to explore the possibility of more general search spaces to include more strong architectures.  
"," Link prediction is the task of predicting missing connections between entities in the knowledge graph .  While various forms of models are proposed for the link prediction task, most of them are designed based on a few known relation patterns in several well-known datasets. Due to the diversity and complexity nature of the real-world KGs, it is inherently difficult to design a model that fits all datasets well. To address this issue, previous work has tried to use Automated Machine Learning  to search for the best model for a given dataset. However, their search space is limited only to bilinear model families. In this paper, we propose a novel Neural Architecture Search  framework for the link prediction task.  First, the embeddings of the input triplet are refined by the Representation Search Module. Then, the prediction score is searched within the Score Function Search Module. This framework entails a more general search space, which enables us to take advantage of several mainstream model families, and thus it can potentially achieve better performance.  We relax the search space to be continuous so that the architecture can be optimized efficiently using gradient-based search strategies. Experimental results on several benchmark datasets demonstrate the effectiveness of our method compared with several state-of-the-art approaches.",261
"   The capacity of a neural network influences its ability to model complex functions.  In particular, it has been argued that deeper models are conducive to more expressive features . Very deep neural network models have proved successful in computer vision  and text classification . In neural machine translation , however, current state-of-the-art models such as the Transformer typically employ only 6-12 layers .  Previous work has shown that it is difficult to train deep Transformers, such as those over 12 layers . This is due to optimization challenges: the variance of the output at each layer compounds as they get deeper, leading to unstable gradients and ultimately diverged training runs.   In this empirical study, we re-investigate whether deeper Transformer models are useful for NMT.  We apply a recent initialization technique called ADMIN , which remedies the variance problem. This enables us train Transformers that are significantly deeper, e.g. with 60 encoder layers and 12 decoder layers.\footnote{We choose to focus on this layer size since it results in the maximum model size that can fit within a single GPU system. The purpose of this study is to show that it is feasible for most researchers to experiment with very deep models; access to massive GPU budgets is not a requirement.}   In contrast to previous research, we show that it is indeed feasible to train the standard\footnote{Note there are architectural variants that enable deeper models , discussed in Sec . We focus on the standard architecture here.} Transformer  with many layers.  These deep models significantly outperform their 6-layer baseline, with up to 2.5 BLEU improvement. Further, they obtain state-of-the-art on the WMT'14 EN-FR and WMT'14 EN-DE benchmarks.% !TEX encoding = UTF-8 % !TEX Root = Main.tex        We show that it is feasible to train Transformers at a depth that was previously believed to be difficult.  Using ADMIN initialization, we build Transformer-based models of 60 encoder layers and 12 decoder layers. On WMT'14 EN-FR and WMT'14 EN-EN, these deep models outperform the conventional 6-layer Transformers by up to 2.5 BLEU, and obtain state-of-the-art results.  We believe that the ability to train very deep models may open up new avenues of research in NMT, including:  Training on extremely large but noisy data, e.g. back-translation  and adversarial training , to see if it can be exploited by the larger model capacity.   Analyzing the internal representations, to see if deeper networks can indeed extract higher-level features in syntax and semantics .  Compressing the very deep model via e.g. knowledge distillation , to study the trade-offs between size and translation quality.  Analyzing how deep models work  in theory.
","  We explore the application of very deep Transformer models for Neural Machine Translation .  Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers.  These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on  WMT14 English-French  and WMT14 English-German . To facilitate further research in Very Deep Transformers for NMT, we release the code and models: \url{https://github.com/namisan/exdeep-nmt}.",262
" Non-autoregressive transformer~ has attracted wide attention in neural machine translation~, which generates sentences simultaneously rather than sequentially. To enable parallel decoding, NAT imposes a conditional independence assumption among words in the output sentences, which leads to significantly faster inference speed~ than the autoregressive Transformer~. However, NAT still falls behind autoregressive Transformer~ in the quality of output sentences, such as BLEU~ for machine translation. %\zhouh{move BLEU into the citations} We blame it for the imposed conditional independence assumption, which prevents  NAT models from explicitly learning the word dependencies in the output sentence. Note that such word dependency is crucial, and it is explicitly learned in the AT model through the autoregressive language models~.  Recently,  propose to employ the Masked Language Model~ in NAT, which includes word dependency modeling in an iterative fashion~, therefore yielding quite competitive results compared to AT. Specifically, such iterative models randomly mask words in the reference and predict these masked words conditioned on unmasked ones during training.  In this manner, iterative models are trained to explicitly capture the dependencies between masked words and unmasked words. However, these iterative approaches still produce poor results with one decoding iteration and have to perform multiple iterations during inference, namely iteratively refining the generated outputs of the previous iteration. Such iterative process is quite time-consuming, which partly sacrifices the speed merit of NAT. To date, it remains an open question as to how the iterative process can be abandoned, while still preserving the benefits of explicitly modeling word dependencies in NAT.  In this paper, we argue that the major culprit of the problem that mask language models have to be used together with iterative inference, is the sampling strategy of masking words in MLM.  In particular, MLM employs a fixed uniform strategy for randomly masking words during training, which prevents the model from effectively learning word dependencies for one-iteration generation.   For example, at the beginning of training when the NAT model is still poorly tuned, we should mask fewer words. If not, it would be difficult for the NAT model to correctly predict the masked words.  % In the worst case, the NAT model may stuck in some local minimums during training, which prevents the NAT model from well fitting the training data. On the contrary, if we mask too little words at the end phase of training, the resulting NAT model is rarely trained to predict the whole sentences, and can only predict some sentence fragments.  In such a case, to accurately generate the whole sentence in inference, the NAT model has to generate the sentence fragments iteratively.  To this end, the sampling strategy is crucial for the training of NAT.  To address the above issues, we propose a simple yet effective approach called Glancing Transformer~, which is equipped with the proposed Glancing Language Model~ for non-iterative parallel text generation, achieving significant improvements upon strong baselines. Intuitively, GLM adopts a adaptive glancing sampling strategy, which glances at some fragments of the reference if the reference is too difficult to fit in the training of NAT. Correspondingly, when the model is well tuned, it will adaptively reduce the percentage of glancing sampling, making sure that the resulting model could learn to generate the whole sentence in the one-iteration fashion.  Specifically, our proposed GLM differs from MLM in two aspects. Firstly, GLM proposes an adaptive glancing sampling strategy, which enables \method to generate sentences in a one-iteration way, working by gradual training instead of iterative inference~. % Intuitively, GLM works like the way of  Generally, GLM is quite similar to curriculum learning~ in spirit, namely first learning to generate some fragments and gradually moving to learn the whole sentences~. To achieve the adaptive glancing sampling, GLM performs decoding twice in training. The first decoding is the same as the vanilla NAT, and the prediction accuracy indicates whether current reference is ``difficult'' for fitting. In the second decoding, GLM gets words of the reference via glancing sampling according to the first decoding, and learn to predict the remaining words that are not sampled.  Note that only the second decoding will update the model parameters. Secondly, instead of using the  token, GLM directly use representations from the encoder at corresponding positions, which is more natural and could enhance the interactions between sampled words and signals from the encoder.  Experimental results show that \method obtains significant improvements  on standard benchmarks compared to the vanilla NAT, without losing inference speed-up.  \method achieves competitive results against iterative approaches like Mask-Predict~, even outperforming the Mask-Predict model on WMT14 DE-EN and WMT16 RO-EN. % Considering the fully NAT models with the one-iteration simultaneous generation, \method achieves the best BLEU scores. Compared to the strong AT baseline, \method can still close the performance gap within 1 BLEU point while keeping  speed-up. Empirically, we find that \method outperforms AT when the source input length is less than 20 on WMT14 DE-EN. We speculate this is because GLM could capture bidirectional context while left-to-right LM is unidirectional, which indicates the potential of parallel generation models.        In non-autoregressive models, learning is under strong conditional independence assumption and lacks explicit target language modeling, which brings a challenge for training. In this paper, we propose Glancing Transformer with a glancing language model to improve the performance of non-iterative NAT.  With the glancing language model, the model starts from learning the generation of sequence fragments and gradually moving to whole sequences.  Experimental results show that our approach significantly improves the performance of non-autoregressive machine translation with one-iteration generation.  As non-autoregressive models are efficient and have great potential in multiple tasks, we plan to apply our approach to other tasks.             
"," %Non-autoregressive models generate all the tokens of the sequence in parallel.  Although non-autoregressive models with one-iteration generation achieve remarkable inference speed-up, they still fall behind their autoregressive counterparts in prediction accuracy. The non-autoregressive models with the best accuracy currently rely on multiple decoding iterations, which largely sacrifice the inference speed of non-autoregressive models.  Inspired by the way of learning word dependencies in autoregressive and iterative-decoding models, we propose Glancing Transformer~ with a glancing language model~, which learns to capture the word dependency gradually. Experiments on three benchmarks demonstrate that our approach can significantly improve the accuracy of non-autoregressive models without multiple decoding iterations. In particular, \method achieves state-of-the-art results among non-iterative models and even outperforms top iterative counterparts in some specific benchmarks.",263
"  % 1. Introduce the importance of emotion detection  Understanding human emotions is considered as the key to building engaging dialogue systems . However, most works on emotion understanding tasks treat individual emotions independently while ignoring the fuzziness nature and the interconnections among them. A psychoevolutionary theory proposed by  shows that different emotions are actually correlated, and all emotions follow a circular structure. For example, ``optimism'' is close to ``joy'' and ``anticipation'' instead of ``disgust'' and ``sadness''. Without considering the fundamental inter-correlation between them, the understanding of emotions can be unilateral, leading to sub-optimal performance. These understanding can be particularly important for low resource emotions, such as ``surprise"" and ``trust"" whose training samples are hard to get.  % Therefore, in this paper, we investigate how emotion correlations can be captured and help emotion classification tasks. Therefore, the research question we ask is, how can we 	extbf{obtain and incorporate the emotion correlation to improve emotion understanding tasks, such as classification?}  To obtain emotion correlations, a possible way is to take advantage of a multi-label emotion dataset. Intuitively, emotions with high correlations will be labeled together, and therefore, emotion correlations can be extracted from the label co-occurrences. Recently, a multi-label emotion classification competition  with 11 emotions has been introduced to promote research into emotional understanding. To tackle this challenge, the best team  first pre-trains on a large amount of external emotion-related datasets and then performs transfer learning on this multi-label task. However, they still neglect the correlations between different emotions. % leaving the research question of how emotion correlations can be helpful for emotion tasks.  % Many other methods that leverage label co-occurrences, such as Classifier Chains , CNN-RNN , and Sequence Generation Model , can be applied to capture emotion correlations. However, these models consider the multi-label classification as a sequential task while the emotions are intrinsically non-sequential, which makes the captured emotion correlations uninterpretable %  and sub-optimal.    % 2. Traditional ways to solve emotion detection problem, the problem of these methods and we need to leverage the relation between emotions. % Recently,  introduced a multi-label emotion classification dataset, which added new challenges into the emotion detection task by turning it into a multi-label problem . The challenge comes from the vagueness of human emotions, which hinders model performance on multiple emotion predictions. One way to tackle this challenge is by pre-training on a large amount of external emotion-related dataset  and then do transfer learning on this multi-label task . However, collecting a huge dataset is expensive and time-consuming for researchers, which makes this method not scalable.  % Furthermore, most related work towards multi-label emotion classification Intuitively, interconnects exist in emotions, for example, if you are surp by your friends, at this time, you also feel happy and excited. Therefore, leveraging the relations among emotions would be an effective and yet unexplored approach in multi-label emotion classification.  % 3. Some recent paper about leveraging relations in other dataset but might not be suitable for emotion % An easiest way is to apply the method which models the relations among multiple labels into this emotion classification challenge.   % 4. In this paper, introduce our method and list the contribution In this paper, we propose EmoGraph which leverages graph neural networks to model the dependencies between different emotions.  We take each emotion as a node and first construct an emotion graph based on the co-occurrence statistics between every two emotion classes. Graph neural networks are then applied to extract the features from the neighbours of each emotion node. We conduct experiments on two multi-label emotion classification datasets. Empirical results show that our model outperforms strong baselines, especially for macro-F1 score. The analysis shows that low resource emotions, such as ``trust"", can particularly benefit from the emotion correlations. An additional experiment illustrates that the captured emotion correlations can also help the single-label emotion classification task.  % The codes will be released.     In this paper, we proposed EmoGraph which leverages graph neural networks to model the dependencies among different emotions. We consider each emotion as a node and construct an emotion graph based on the co-occurrence statistics.   between different emotion classes.  Our model with EmoGraph outperforms the existing strong multi-label classification baselines. Our analysis shows EmoGraph is especially helpful for low resource emotions and large emotion space. An additional experiment shows that it can also help a single-label emotion classification task.  
"," Most emotion recognition methods tackle the emotion understanding task by considering individual emotion independently while ignoring their fuzziness nature and the interconnections among them. In this paper, we explore how emotion correlations can be captured and help different classification tasks. We propose EmoGraph that captures the dependencies among different emotions through graph networks. These graphs are constructed by leveraging the co-occurrence statistics among different emotion categories. Empirical results on two multi-label classification datasets demonstrate that EmoGraph outperforms strong baselines, especially for macro-F1. An additional experiment illustrates the captured emotion correlations can also benefit a single-label classification task.",264
"  Neural NLP models typically embed the sequence of input tokens using a lookup table of learnable parameters, where each row represents a token type as a dense vector . The same embedding matrix is often reused to predict the output in language models . How essential are embeddings to the model's success? Intuitively, one would expect them to be critical, given the ubiquitous use of embeddings layers and the vast amount of parameters they typically consume. In this work, we show that machine translation models can be trained , and that they can rival and sometimes even outperform standard embedding-based models.  % Does it play an essential role in those models ability to preform as well as they do? In this work we put this to the test by removing the trainable embedding matrix from Neural Machine Translation models and instead use a fixed one hot encoding of the vocabulary.  % Embedding is a widely used component of modern NLP models, it usually takes the form of a learnable dense matrix, , where  is the vocabulary, and  is the hidden dimension, that makes it the most parameter consuming layer of the models architectures. Does it play an essential rule in those models ability to preform as well as they do? In this work we put this to the test by removing the trainable embedding matrix from Neural Machine Translation models and instead use a fixed one hot encoding of the vocabulary.  We remove the trainable embedding matrix from a standard transformer machine translation model, and use a constant one-hot encoding of the vocabulary instead. To limit the dimensionality, we use byte tokenization by reading the text as a unicode  byte stream, which can represent virtually every text in any language in under 256 dimensions per token. Byte vocabularies obviate the need to preprocess the text with hand-crafted language-specific tokenizers  and subword induction algorithms, such as BPE .    Machine translation experiments on 10 language pairs show that models without a trainable embedding matrix perform on par with the best embedding-based baselines.  We find that embeddingless models consistently achieve higher BLEU scores than their byte baselines, and even yield slightly better performance than embedding-based character models in 80\% of the cases. % We observe a similar yet weaker trend when comparing to character level models, where embeddingless gets a higher BLEU score in 16 out of the 20 cases, with a smaller average gap. Although the recent literature on character-based transformers demonstrates the superiority of subword tokenization when controlling for network depth , our experiments show that  the embedding matrix from byte-to-byte models makes them perform at least as well as standard subword models in 9 out of 20 cases. Overall, our results suggest that highly-parameterized embedding matrices might not be as essential as commonly perceived.  % Interestingly, despite existing work in MT showing sub word models consistently outperforming character and byte level ones , our Embeddingless models are able to get equal or higher results then subword models in 9 out of 20 cases despite having significantly smaller number of parameters.   % Machine translation experiments on 10 language pairs show that models without a trainable embedding matrix can can be comparative with the best embedding based baseline. Moreover, We find that embeddingless models can achieve higher scores then byte and character level baselines in 19 and 16 cases out of 20 respectively. Interestingly, despite existing work in MT showing sub word models consistently outperforming character and byte level ones , our Embeddingless models are able to get equal or higher scores then sub word models in 9 out of 20 cases despite having significantly smaller number of parameters.    % of  outperform their byte baselines, where they achieve a higher BLEU score in 19 out of 20 cases and an equal score for ru-en. We can observe a similar but weaker trend when comparing to character level models, where embeddingless gets a higher BLEU score in 16 out of the 20 cases, with a smaller average gap.  % \uri{new version - end}  % Machine translation experiments on 10 language pairs show that removing the embedding matrix consistently improves the performance of byte-to-byte models. % When translating from English into a foreign  language, embeddingless byte-to-byte models outperform character-based models in 9 out of 10 languages, and subword models in 7 out of 10 languages. % This last result is particularly surprising given recent literature in machine translation, which demonstrates a consistent superiority of subword models over character models of the same size . \omer{we need to make this last sentence a bit more subtle...} % \uri{Maybe: This last result is particularly surprising given recent literature in machine translation, which shows sub word models consistently achieve higher BLEU scores then character models of the same size? }  % However, we observe that subword models are consistently better when translating into English. % % \uri{However, we observe that subword models are able to achieve higher BLEU scores when translating into English. ?} % \omer{We need to explain this... maybe also blame BLEU in some sense? We're using ""better"" and ""outperform"" quite a bit here, but the diffs are really not that huge, and BLEU is also super-dependent on the tokenizer. Maybe we want to soften the whole tone in the paper and say something like ""the models are competitive"", ""very similar perfomance with a slight edge to X""?} % \uri{I agree} % \omer{we need to end on a positive note...}   % \uri{the following old version of last sentence is a bit crooked but is built in a different way. It doesnt say: ""we are bad in translating into English"", and then have to both explain and end with a positive tune. Instead it says ""we are good under some conditions also in subwords"":   % While recent work in machine translation demonstrates that character and byte level models often underperform sub word models , we observe that embeddingless models close the gap and even surpass sub word models in some cases, particularly when the tokenizer that post-processes the sub words model output is not optimized for the target language. % }         %While recent work in machine translation demonstrates that character and byte level models often underperform sub word models , we observe that embeddingless models close the gap and even surpass sub word models in some cases, particularly when the tokenizer that post-processes the sub words model output is not optimized for the target language.          %   % Tokenization is the task of segmenting a string into a sequence of tokens that a model can process. % The current standard practice is to split a string into subwords using whitespaces, language-specific heuristics  \uri{maybe we can also add this to the this list :https://arxiv.org/pdf/2008.05055.pdf}, and Byte-Pair Encoding   or its variants . % While subword tokenization works well in practice for many languages, it also loses orthographic information that may be critical for morphologically rich languages. % Moreover, the assumption that subword units must be contiguous segments does not hold for languages with non-concatenative morphology such as Arabic and Hebrew. % Intuitively, sufficiently parameterized models should be able to leverage pure  tokenization to access all the information in the original string; however, recent work in machine translation demonstrates that character-level models often underperform subword models in practice .  % We hypothesize that part of the reason character-based models underperform is the embedding layer. This layer represents each vocabulary item as a vector, implicitly capturing some similarity function between the token types. % While similarity between words and subwords is potentially useful, it is not quite clear how character similarity could benefit the model. % We conjecture that, for many languages, the embedding layer allows the character-based models to learn a misleading concept of character similarity, and propose to remove it.  % In this work we omit the embedding matrix for character-to-character machine translation models, using orthogonal one-hot representations instead. % Since some languages  have thousands of characters, we use  representations based on UTF-8, which can represent all the information in any language in under 256 dimensions per token.  % Machine translation experiments on 10 language pairs show that removing the embedding matrix consistently improves the performance of byte-to-byte models. When translating from English into a foreign language, embeddingless byte-to-byte models outperform character-based models in 9 out of 10 languages, and subword-based models in 7 out of 10 languages \omer{one is equal...} \uri{we win against BPE in: zh,ar,ru,ja,tr,fa,he tie in es and lose in de,vi}. However, we observe that subword models are consistently better when translating into English.  % We suspect that the opposing trends arise from English-centric heuristics in the tokenizer. \omer{We show that byte-based tokenization can effectively learn the ...} % \omer{I want to say something about the Jieba experiment, and then finish off with how byte-to-byte can potentially learn language-specific tokenization without manually-engineered heuristics. Problem is that we still see a gap, and we need to say something about why this is happening .} \uri{Maybe we can say that we believe the gap is due to the first sentence of this paragraph, linguistic knowledge injected with Moses? }    % Byte-to-byte machine translation models also have the theoretical advantage that they will never encounter unknown token types .             % Tremendous progress was made in Neural Machine Translation  in recent years [\uri{*cite*}]. In NMT, a model gets as input text in a source language, and outputs a translation in a target language. An important and well studied question in NMT is how should the text be segmented when fed into the models as a sequence of tokens. A common approach is to build a sub-words vocabulary with Byte-Per Encoding   or one of its variants. BPE is a data compression algorithm that given a corpus and a wanted vocabulary size , merges greedily common bi-grams from the data into sub-words, and extracts a vocabulary of  frequent sub-words from the corpus.  As the use of sub-words currently allows to achieve state-of-the-art results in NMT [\uri{*cite*}], it comes with some costs, the main one being the lost of some orthographic information, especially in morphologically rich languages such as \uri{X}, \uri{Y} and \uri{Z}, consider for example [\uri{*example*}].\\  % One way of allowing the models access to all of the information the raw text holds is to use characters instead of sub words as base units. This choice has additional advantages over the use of words or sub-words, presented by . First, as opposed to sub-words based models, a character based one will not suffer from out of the vocabulary tokens during inference, second, there is no need for data preprocessing with schemes like BPE that also tend to require hyper-parameters tuning themselves. Despite the theoretical advantages, previous work [\uri{*cite*}] has shown that performance drops when adopting current NMT models to use character-to-character in a naive plug-and-play technique. That is surprising, as these models should be sophisticated enough to learn the patterns when given the opportunity to. A part of the reason lies in the fact that BPE creates shorter sequences, and current models do not handle long sequences as good as they do shorter ones . Recently, a lot of work was done in order to build models that can handle better long dependencies ,  and impressive progress was made in that area. In this work we tackle a different aspect of the current models and show an improvement in their ability to work on non-sub-word [\uri{non-sub-word is a bit weird, but I wasn't sure if I should write ""byte"" or ""char"" here, as the model is byte, but in the story we are at the ""char"" stage}] data, up to outperforming BPE in some cases, while using the same architecture that tend to struggle with longer sequences. \\  % [\uri{not sure where to put the following sentence}\\ % Recent improvements of non-sub-words models suggested adding different new components to the existing models , we on the contrary, suggest doing the opposite.]\\  % One way of allowing the models access to all of the information the raw text holds, is to use characters as tokens, but previous work [\uri{*cite*}] has shown that performance drops when adopting current NMT models to use character-to-character in a naive plug-and-play technique. That is surprising, as these models should be sophisticated enough to learn the patterns when given the opportunity to. An important difference of the char-to-char form of a corpus as opposed to its word or sub-word form is the length of the sequences, where a sequence in characters is usually much longer then its sub-words form in sub-word units, and A widely common conjuncture for why char-to-char models underpreform is that current models do not handle long sequences as good as they do with shorter sequences. A lot of recent work was done in order to build models that can handle better long dependencies , , but though that might be a part of the reason the current models struggle with characters as tokens, in this work we tackle a different part of the modern models that we believe to be also a culprit of that performance drop, and show improvement of non-sub-word models. \\  % The in recent years, NMT and in Natural Language Processing in general, is the one-model-fits-all approach, which means building models that preform well on many tasks and datasets, as opposed to designing different task specific algorithms and architectures.   % Typically, word and sub-word embeddings in language models reflect interesting relations such as lexical substitution, semantic similarity, etc. [\uri{*cite*}], and though these are great qualities of sub-word embeddings, we argue that trying to find such a similarity function for characters has no real justification, as in most languages character do not encapsulate semantic meaning by themselves. Therefore, such a function can be misleading for the rest of the model [\uri{someone might say, similarity of vowels for instance does makes sense isn't it?}] and hurt the its ability to generate high quality translations. \\   [ht] c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c} \toprule %     & Tokenized   \\ % \midrule Original Text  & \multicolumn{22}{c}{{russian}琚樿鍐欒 锜瑰啓鑺鑺儊.}\\ Subwords   & \multicolumn{4}{@{}c@{}|}{{russian}琚樿@} & \multicolumn{5}{@{}c@{}|}{{russian}鍐欒} & \multicolumn{6}{@{}c@{}|}{{russian}锜瑰啓鑺疈} & \multicolumn{6}{@{}c@{}|}{{russian}瑜夎姱鑳亇 & .\\ Characters & \multicolumn{2}{@{}c@{}|}{{russian}琚榼 & \multicolumn{2}{@{}c@{}|}{{russian}瑜峿 & \multicolumn{2}{@{}c@{}|}{{russian}鍐檥 & \multicolumn{2}{@{}c@{}|}{{russian}瑜渳 &   & \multicolumn{2}{@{}c@{}|}{{russian}锜箎 & \multicolumn{2}{@{}c@{}|}{{russian}鍐檥 & \multicolumn{2}{@{}c@{}|}{{russian}鑺瘆 & \multicolumn{2}{@{}c@{}|}{{russian}瑜墋 & \multicolumn{2}{@{}c@{}|}{{russian}鑺瘆 & \multicolumn{2}{@{}c@{}|}{{russian}鑳亇 & .  \\ Bytes  & \ D0 \  & \ 91 \ & \ D1 \ & \ 83 \ & \ D0 \ & \ B4 \ & \ D1 \ & \ 8C \ & \ 20 \ & \ D0 \ & \ B7 \ & \ D0 \ & \ B4 \ & \ D0 \ & \ BE \ & \ D1 \ & \ 80 \ & \ D0 \ & \ BE \ & \ D0 \  & \  B2 \  &\  2E  \\  琚樿鍐欒 锜瑰啓鑺鑺儊.'' UTF-8 uses two bytes to represent each character in the Cyrillic script, making the byte sequence longer than the number of characters.}         This work tests the importance of the embedding matrix in neural machine translation models.  Experiments on 10 different languages show that, despite its ubiquitous usage, competitive models can be trained without any embeddings. Future work may investigate the potential of embeddingless models for different NLP tasks, and explore new methods to improve training in byte-level models. \pdfoutput=1           File eacl2021.tex          Based on the style files for ACL 2020, which were      Based on the style files for ACL 2018, NAACL 2018/19, which were      Based on the style files for ACL-2015, with some improvements       taken from the NAACL-2016 style      Based on the style files for ACL-2014, which were, in turn,      based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,      EACL-2009, IJCNLP-2008...      Based on the style files for EACL 2006 by      e.agirre@ehu.es or Sergi.Balari@uab.es      and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{eacl2021} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily \usepackage{multirow} \usepackage{microtype} \usepackage{graphicx} \usepackage{subfigure} \usepackage{booktabs}  \usepackage{hyperref} \usepackage{amsmath} {\arabic{algorithm}} \usepackage{amsbsy} \usepackage{bm} \usepackage{float} \usepackage{lipsum} \usepackage{hhline}   This is not strictly necessary, and may be commented out,   but it will improve the layout of the manuscript,   and will typically save some space. \usepackage{microtype}   \usepackage[T2A,T1]{fontenc} \usepackage[utf8]{inputenc} \usepackage[russian,english]{babel}   \usepackage{cleveref}     Enter the acl Paper ID here      You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.            {Uri: {#1}}]}   {Omer: {#1}}]}    \title{Neural Machine Translation without Embeddings}   \author{Uri Shaham \quad Omer Levy$^    \maketitle                     
"," Many NLP models follow the embed-contextualize-predict paradigm, in which each sequence token is represented as a dense vector via an embedding matrix, and fed into a contextualization component that aggregates the information from the entire sequence in order to make a prediction.  Could NLP models work without the embedding component? To that end, we omit the input and output embeddings from a standard machine translation model, and represent text as a sequence of bytes via UTF-8 encoding, using a constant 256-dimension one-hot representation for each byte. Experiments on 10 language pairs show that removing the embedding matrix consistently improves the performance of byte-to-byte models, often outperforms character-to-character models, and sometimes even produces better translations than standard subword models.\footnote{Our code is publicly available at: \url{https://github.com/UriSha/EmbeddinglessNMT}}",265
"  Propagandist news articles are misleading in nature and aim at biasing their audience towards a particular point of view by using psychological and rhetorical techniques, including loaded language, name calling, repetition, exaggeration, minimization, etc. With the rapid growth in the number of online sources of information and the speed with which information spreads online, manual flagging of propagandist news articles has become untenable, leading to an ongoing need for new research on methods for identifying these articles automatically to mitigate the negative influence they might have on users.  Until very recently, most of the work in this area has focused on article-level detection. However, in 2019, Da San Martino et al.~ published a corpus of English news articles with individual spans of propaganda annotated that addresses the problem at a more granular level. This corpus is used in shared tasks at NLP4IF-2019 and at SemEval-2020. The 2020 shared task is a modified version of the prior year's task and includes two subtasks:       Given a plain-text document, identify those specific fragments which contain at least one propaganda technique. This is a binary sequence tagging task.           Given a text fragment identified as propaganda and its document context, identify the applied propaganda technique in the fragment.   We present our models for both tasks alongside discussions of our results and ablations.    We investigated several models and combinations of features to identify propaganda spans in text, and classify the techniques used within the span. For the span identification task, we found that our LSTM-based model combining BERT predictions with our original features gives the highest F1 score of 39.2\
","   This paper presents our systems for SemEval 2020 Shared Task 11: Detection of Propaganda Techniques in News Articles. We participate in both the span identification and technique classification subtasks and report on experiments using different BERT-based models along with handcrafted features. Our models perform well above the baselines for both tasks, and we contribute ablation studies and discussion of our results to dissect the effectiveness of different features and techniques with the goal of aiding future studies in propaganda detection.",266
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Propaganda techniques need to attach importance to arouse the emotions of the receivers, sometimes even by temporarily bypassing the intellectual defenses of the receivers  . Propaganda uses psychological and rhetorical techniques to achieve its purpose. Such techniques include using logical fallacies and appealing to the emotions of the audience. Logical fallacies are usually hard to spot since the argumentation, at first, might appear correct and objective  . However, careful analysis shows that the conclusion cannot be drawn from the premise without misusing logical rules. Another set of techniques uses emotional language to induce the audience to agree with the speaker only based on the emotional bond that is being created, provoking the suspension of any rational analysis of the argumentation.  The traditional NLP task generally classifies and detects propaganda techniques at the article level, which often fails to meet more detailed requirements. This fact has also been confirmed by previous iterations of the SemEval competition, where leading solutions used convolutional neural networks , long short-term memory     and transfer learning techniques  . The main features of an article are extracted by using the feature capture and pooling of the CNN model, but these methods can only be used at the article level and are coarse-grained detection methods. However, limited research has focused on text classification .  News articles have also been classified using the Bi-LSTM-CNN model  .   However, there are often many propaganda techniques in one article, and most of these techniques are efficient for propaganda classification but lack the ability to detect categories of propaganda techniques. Thus, they cannot achieve good results and are less efficient in practice.  Now the difficulty is to detect propaganda techniques at the fine-grained level. The SemEval-2020 Task 11, ``Detection of Propaganda Techniques in News Articles'', is designed to promote research on this task. We used the word embedded representation of the pretrained model and LSTM model to detect the news article propaganda techniques at a fine-grained level, and we also evaluate the performance among different neural network models on this task.  The task consists of two subtasks.  	  The rest of the paper is organized as follows. Section 2 describes the details of the LSTM used in our system. Section 3 presents the experimental results. Conclusions and future works are described in Section 4.       In this paper, we presented our system for the SemEval-2020 Task 11, which leverages LSTM and pretrained word embeddings without using human-engineered features for representation learning. Our experimental results show that the LSTM model with GloVe word embeddings can get better performance according to the scores of different neural network models and integration models on this task. The main goal of this task is to detect propaganda techniques in news articles at a fine-grained level, not just to make coarse judgments about whether the news articles use propaganda techniques.  It is known that neural networks perform well on large training sets, but sometimes a large, accurately labeled dataset cannot be obtained. For future work, the development of propaganda technology detection in news articles can be greatly improved in the pretraining model and the integrated model architecture.  
","   This paper summarizes our studies on propaganda detection techniques for news articles in the SemEval-2020 task 11. This task is divided into the SI and TC subtasks. We implemented the GloVe word representation, the BERT pretraining model, and the LSTM model architecture to accomplish this task. Our approach achieved good results for both the SI and TC subtasks. The macro-$F_{1}$-$score$ for the SI subtask is 0.406, and the micro-$F_{1}$-$score$ for the TC subtask is 0.505. Our method significantly outperforms the officially released baseline method, and the SI and TC subtasks rank 17th and 22nd, respectively, for the test set. This paper also compares the performances of different deep learning model architectures, such as the Bi-LSTM, LSTM, BERT, and XGBoost models, on the detection of news promotion techniques. The code of this paper is availabled at: \url{https://github.com/daojiaxu/semeval_11}.",267
" Machine Reading Comprehension  has become a popular task in NLP, aiming to understand a given passage and answer the relevant questions. With the wide availability of MRC datasets~ and deep learning models~ , significant progress has been made.  Despite the success, a majority of MRC research has focused on open domains. For specific domains, however, the construction of high-quality MRC datasets, together with the design of corresponding models is considerably deficient~. The causes behind this phenomenon are threefold. Take the medical domain as an example. i) Data annotators are required to have medical backgrounds with high standards. Hence, simple crowd-sourcing~ often leads to poor annotation results. ii) Due to the domain sensitivity, people are more concerned about the reliability of the information sources where the answers are extracted, and the explainability of the answers themselves~. This is fundamentally different from the task requirements of open-domain MRC. iii) From the perspective of model learning, it is difficult for pre-trained language models to understand the meaning of the questions and passages containing a lot of specialized terms~. Without the help of domain knowledge, state-of-the-art models can perform poorly. As shown in Figure, BERT~ and MC-BERT ~ only predict part of the correct answer, i.e.,~``torso"" and ``buttocks"", instead of generating the complete answer to the medical question. %Compared with the strong baseline model, our model can predict a complete answer after injecting a lot of medical knowledge. % Our CMedBERT model can extract the complete answer after fusing a lot of medical knowledge. % Only when we fuse medical knowledge into pre-trained models, complete answers are be extracted.    In this paper, we present a comprehensive study on Chinese medical MRC, including i) how the task is formulated, ii) the construction of the Chinese medical dataset and iii) the MRC model with rich medical knowledge injected. To meet the requirements of medical MRC, we aim to predict both the answer spans to a medical question, and the support sentence from the passage, indicating the source of the answer. The support sentences provide abundant evidence for users to learn medical knowledge, and for medical professionals to assess the trustworthiness of model output results.  %two MRC tasks in our Chinese medical dataset. %The first task is to predict the answer content of the question and the second task is to predict the index of sentence supporting the answer. For the dataset, we construct a highly-quality Chinese medical MRC dataset, named the Multi-task Chinese Medical MRC dataset . It contains 12,172 question, passage, answer, support sentence quads. Based on the analysis of CMedMRC, we summarize four special challenges for Chinese medical MRC, including long-tail terminologies, synonym terminology, terminology combination and paraphrasing. In addition, we find that comprehensive skills are required for MRC models to answer medical questions correctly. For answer extraction in CMedMRC, direct token matching is required for answering 31\% of the questions, co-reference resolution for 11\%, multi-sentence reasoning for 18\% and implicit causality for 22\%. In addition, the answers to the remaining questions  are extremely difficult to extract without rich medical background knowledge.  To address the medical MRC task, we propose the multi-task dynamic heterogeneous fusion network  based on MC-BERT~ model and Chinese medical knowledge base . The technical contributions of CMedBERT are twofold:  : We mimic humans' approach of reading comprehension~ by learning attentively  aggregated representations of multiple entities in the passage. Different from the knowledge fusion method used by KBLSTM ~ and KT-NET ~, we propose a two-level attention and a gated-loop mechanism to replace the knowledge sentinel, so that the rich knowledge representations can be better integrated into the model. % Unimportant information is then filtered by two types of attention mechanisms and the gated-loop layer. : The model parameters of CMedBERT are dynamically learned by capturing the relationships between the two tasks via multi-task learning. We regard the semantic similarities between support sentences and answers to questions as the task similarities.   In the experiments, we compare CMedBERT against four strong baselines. For answer prediction, compared to the strongest competitor, the EM  and F1 scores are increased by +3.88\% and +1.46\%, respectively. Meanwhile, the support sentence prediction task result is increased by a large margin, i.e., +7.81\% of EM and +4.07\% of F1. The contributions are summarized as follows:      % [] %  %  % \toprule % Challenges & Characteristics  & Example \\  %  \multirow{3}{*}{Lexical-Level}&  Long-tail terminologies & ...閸愬牅绗傞懖宀冨㈤懙杈ㄦ焽鐟佸倽鐦灞炬Ц鐎电懓鍞告稉濠呭㈤懖宀冨彙閺勵垰鎯佺涙ê婀弬顓☆棁鏉╂稖顢戝Λ閺屻儯鍌氬敻娑撳﹨鍊㈤懖宀冨彙閺傤叀顥囨径姘礈闂傚瓨甯存径鏍у閹甸懛杈剧礉\\閸ョ姷娲块幒銉﹀ⅵ閸戞槒鍋愰柈銊╃姵鍨氶懓鍛毌鐟... \\  \\  %  & Synonyms terminologies  & \makecell*[l]{...閺堫剝宓傞崫浣割嚠鏉╁洦鏅遍幀褔钃熼悙搴℃嫲娑撳﹤鎳犻崥鎼佷壕閹扮喐鐓嬪鏇℃崳閻ㄥ嫰钃熼崗鍛邦攨閺堝鏅ラ敍灞藉讲閻€劋绨幇鐔峰晪閹存牠钃熺粣锔惧... \\ } \\  %  &fdsafd& \makecell[l]{dafdsafd} \\  %  & fdasfd &fdsaf \\  %   []   \toprule Challenges & Characteristics  & Example \\   & Long-tail terminology  & \makecell*[l]{...閸愬牅绗傞懖宀冨㈤懙杈ㄦ焽鐟佸倽鐦灞炬Ц鐎电懓鍞告稉濠呭㈤懖宀冨彙閺勵垰鎯佺涙ê婀弬顓☆棁鏉╂稖顢戝Λ閺屻儯鍌氬敻娑撳﹨鍊㈤懖宀冨彙閺傤叀顥囨径姘礈闂傚瓨甯存径鏍у閹甸懛杈剧礉\\閸ョ姷娲块幒銉﹀ⅵ閸戞槒鍋愰柈銊╃姵鍨氶懓鍛毌鐟... \\ }    \\  Lexical-Level    & Synonym terminology  & \makecell*[l]{...閺堫剝宓傞崫浣割嚠鏉╁洦鏅遍幀褔钃熼悙搴℃嫲娑撳﹤鎳犻崥鎼佷壕閹扮喐鐓嬪鏇℃崳閻ㄥ嫰钃熼崗鍛邦攨閺堝鏅ラ敍灞藉讲閻€劋绨幇鐔峰晪閹存牠钃熺粣锔惧... \\ }  \\       & Terminology combination  & \makecell*[l]{...缁牕缈遍惀鍛嗩潒缂冩垼鍟橀惀閺勵垳纭哥亸璺ㄦ⒕閹冧簳鐞涚粻锛勬⒕閸欐ü鑵戦張闁插秷顩﹂惃鍕冮悳甯礉閺勵垯绔寸粔宥呭徔閺堝　\閻楃懓绱撻幀褎鏁奸崣妯兼畱閻厧绨抽惀鍛綁閿涘瞼纭哥亸璺ㄦ⒕閻ㄥ嫪寮楅柌宥呰嫙閸欐垹妫佹稊瀣╃... \\ }} is the most important manifestation of diabetic microangiopathy. It is a fundus \\disease with specific changes, one of the severe complications of diabetes...)}   \\  Sentence-Level   & Paraphrasing  & \makecell*[l]{Passage:\tiny ...婵″倹鐏夐崷銊ユЬ鐟欐帞鍎屾禍鍡樺灗缂佹挾姊婚惃鍕勾閺傜绻樼悰灞藉枎閺佸嚖绱濇稉閺傚綊娼伴崘閿嬫殾閻椻晛鎼ф稉宥呭叡閸戦惃鍕樈娴兼岸鐘冲灇閹扮喐鐓嬮敍娑樺綗娑撻弬褰掓桨鐏為柈銊︿刊鎼达箓妾锋担搴濈啊\\娑斿鎮楅敍灞藉冀閼板奔绱板鍓佺处娴笺倕褰涢惃鍕墹閸氬牄...\\\\Question:\tiny 娑撹桨绮堟稊鍫濇Ь鐟欐帞鍎屾禍鍡樺灗缂佹挾姊绘稉宥呯紦鐠侇喛绻樼悰灞藉枎閺佸嚖绱 \\} \\        In this work, we address the medical MRC problem with a new dataset CMedMRC constructed. An in-depth analysis of the dataset is conducted, including statistics, characteristics, required MRC skills, etc. Moreover, we propose the CMedBERT model, which can help the pre-trained model better understand domain terms by retrieving entities from medical knowledge bases. Experimental results confirm the effectiveness of our model. In the future, we will further explore how knowledge can improve the performance of models. \clearpage     
"," Machine Reading Comprehension  aims to extract answers to questions given a passage. It has been widely studied recently, especially in open domains. However, few efforts have been made on closed-domain MRC, mainly due to the lack of large-scale training data. In this paper, we introduce a multi-target MRC task for the medical domain, whose goal is to predict answers to medical questions and the corresponding support sentences from medical information sources simultaneously, in order to ensure the high reliability of medical knowledge serving. A high-quality dataset is manually constructed for the purpose, named Multi-task Chinese Medical MRC dataset , with detailed analysis conducted. We further propose the Chinese medical BERT model for the task , which fuses medical knowledge into pre-trained language models by the dynamic fusion mechanism of heterogeneous features and the multi-task learning strategy. Experiments show that CMedBERT consistently outperforms strong baselines by fusing context-aware and knowledge-aware token representations.",268
" Nowadays, the volume of biomedical literature and biomedical web pages continues to increase rapidly.  Lots of new articles and web pages containing biomedical discoveries and new insights are continuously published.  Indeed, there is an increasingly high demand for biomedical text mining.                Recent progress in the biomedical text mining approach is made possible by the development of deep learning techniques used in natural language processing . For example, pre-trained language models such as   BERT , ERNIE , XLNet  and  RoBERTa  have demonstrated remarkable successes in modeling contextualized word representations by utilizing the massive amount of training text. As a fundamental technique in natural language processing , the language models pre-trained on text could be easily transferred to learn downstream NLP tasks with finetuning, which achieve the state-of-the-art performances on many tasks including named entity recognition, paraphrase identification, question answering and information retrieval.   However,  it has limitations to apply state-of-the-art NLP methodologies to biomedical text mining directly. Firstly, since recent representation models such as BERT  are trained and tested mainly on general domain datasets such as  Wikipedia, it is difficult to adapt to biomedical datasets without losing the performance. Moreover, the word distributions of general and biomedical text are quite different, which can be a problem for biomedical text mining. In addition, there exist long-tail concepts and terminologies in biomedical texts which are difficult to be learned via language models. For the Chinese biomedical text, it is somewhat more difficult due to its complex structure and the variety of phrase combinations. To this end, recent biomedical text mining models rely mostly on adapted versions of word representations  .  Considering whether it is possible to automatically inject biomedical knowledge to the language representation learning for Chinese medical corpus, we hypothesize that current state-of-the-art word representation models such as BERT should be trained on biomedical corpora with prior biomedical knowledge to be effective in biomedical text mining tasks. However,  there exist two problems:  how to retrieve the biomedical domain knowledge;  how to leverage such knowledge to the representation learning.   In this paper, we propose a conceptualize representation learning approach   for Chinese biomedical language understanding. Specifically, we propose coarse-to-fine masking strategies to inject entity and linguistic domain knowledge into representation learning. As there are no benchmarks for the Chinese Biomedical Language Understanding Evaluation, we release the first large scale benchmark  including name entity recognition, paraphrase identification, question answering, information retrieval, intent detection, and text classification.  Experiments show that our approach achieves state-of-art results.    In this article, we introduce MC-BERT, which is a pre-trained language representation model with domain knowledge for biomedical text mining. We show that pretraining BERT on biomedical corpora is crucial in applying it to the biomedical domain. The ChineseBLUE and MC-BERT will soon be available to the BioNLP community. Our motivation is to develop a universal, GLUE-like, and open platform for the Chinese BioNLP community and a  composable and generalized representation algorithm to inject domain knowledge.  Our work is but a small step in this direction.    
"," Biomedical text mining is becoming increasingly important as the number of biomedical documents and web data rapidly grows. Recently, word representation models such as  BERT has gained popularity among researchers. However, it is difficult to estimate their performance on datasets containing biomedical texts as the word distributions of general and biomedical corpora are quite different. Moreover, the medical domain has long-tail concepts and terminologies that are difficult to be learned via language models. For the Chinese biomedical text, it is more difficult due to its complex structure and the variety of phrase combinations. In this paper, we investigate how the recently introduced pre-trained language model BERT can be adapted for Chinese biomedical corpora and propose a novel conceptualized representation learning approach.  We also release a new  Chinese Biomedical Language Understanding Evaluation benchmark .  We examine the effectiveness of Chinese pre-trained models: BERT, BERT-wwm, RoBERTa, and our approach. Experimental results on the benchmark show that our approach could bring significant gain. We release the pre-trained model on GitHub: \url{https://github.com/alibaba-research/ChineseBLUE}.",269
"   Digitalization facilitates management and manipulation of large-scale data sets, such as large collections of documents, audio recordings, or images. In such large collections, finding specific objects efficiently is only possible with computational tools. The predominant form of searching is based on the similarity of objects, where an algorithm would identify and rank a list of objects from the collection based on their similarity to a given query object. Similarity search requires object type specific similarity measures. For instance, some form of textual similarity may be used for searching document collections whereas a measure for time-series similarity would be employed for searching collections of audio recordings. 	 A particularly valuable type of information object are tables, which only recently have received appropriate attention. Tables are used to present structured information in a two-dimensional matrix, and are extensively used in scientific articles, business reports, product specifications, web pages etc. Research on tables as first class objects started roughly 10 years ago with the availability of large table collections, mostly extracted from web pages or from Wikipedia. %Research on tables from document collections has lagged behind due to the difficulty to extract tables from document formats like PDF. This situation, however, has recently improved considerably, as more and more document collections have introduced proper table mark-up.   This work is concerned with table similarity : Given a pair of tables, e.g. a query table and a table from a table corpus, compute an accurate estimate of their semantic similarity.  TS is a fundamental operation and a prerequisite for many further applications, such as table clustering and classification, table auto-completion, table fusion or filling missing values in databases. Despite the importance of TS, it has received only little attention as an operation in its own right so far. Existing table similarity functions are all tightly integrated into their downstream application and were not compared to other TS methods. For instance, previous works on table augmentation, table union, table extension or table imputation all incorporate specific TS algorithms whose individual quality is unknown. Note that TS, as we define it and as necessary for such applications, is different from the related field of table-keyword similarity. %cafarella2008webtables  What is lacking is a general and robust method to assess the similarity of two tables. Compared to similarity of other types of objects, TS has its own, specific properties. In contrast to pure texts, where the sequence of words, sentences, and paragraphs conveys meaning, tables impose meaning through the arrangement of values in columns and rows, often augmented with header information. In contrast to image similarity, where the relative positions of pixels is extremely important, table similarity often is independent of the order of rows or columns - two tables of patients from two hospitals will be considered similar irrespective of the order in which patients appear as rows, or the order in which metadata of the patients is recorded.  In this paper, we present TabSim, a TS method which employs deep learning techniques to achieve two main objectives: a) to generate suitable table representations, and b) to use these representations to learn an accurate similarity function for pairs of tables. It is based on Siamese neural networks, which are known to be able to learn a similarity model given only few samples. TabSim does not require any hand-crafted features, but learns a similarity function directly from a gold standard corpus. TabSim's network first generates a representation of each table as a concatenation of embeddings of its caption and of its tabular content. For these, we apply two different networks to properly reflect their diverging structures: A Bidirectional LSTM  layer capable of modeling sequences is utilized to capture semantic information from captions because the order of words in the caption carry semantic information. Tabular data is represented by an order-invariant self-attention neural network, since the order of cells within a column and the order of columns within a table most often does not carry meaning. The two representations are shared by both compared tables to guarantee the symmetry of the similarity score. Model parameters are optimized with a contrastive loss function that relies on tables distances.  To train and evaluate TabSim, we created a novel corpus consisting of 1500 table pairs extracted from biomedical articles and manually scored regarding their pairwise degree of similarity. To the best of our knowledge, this is the first publicly available gold standard corpus for TS. We also evaluated our approach on two other corpora originally developed for a different yet similar task which allowed adaptation: a) tables extracted from arXiv articles and b) tables in Wikipedia pages. Our evaluation on these three corpora shows that, on average, TabSim outperforms baselines by app. 7\% pp F1-score in a binary similarity classification setting and by app. 1.5\% pp in a ranking scenario using NDCG.   The paper is organized as follows.  In Section 2, we review existing techniques for TS. We explain TabSim and its neural architecture in Section 3. Section 4 presents data preparation, the used baselines, and evaluation settings and metrics. In Section 5, we provide the results of our evaluation and conclude in Section 6.   %--------------------------------------------------------------------------   We presented TabSim, a new method for assessing table similarity which uses Siamese neural networks to learn a similarity measure from a gold standard corpus of table pairs. We showed that, in comparison to five other methods of which three are also rooted in applications based on table similarity,  TabSim attains considerably higher precision, recall, F1-score, and accuracy measures on three different corpora. Our results also demonstrate that, among different configurations of TabSim, the model which uses self-attention neural networks achieve the highest performance, probably because it is, different from the 2d-based CNN or the sequence-based Bi-LSTM, invariant to row or column permutations. As part of our research, we also created the first specific gold standard corpus for table similarity research, containing 1500 table pairs manually scored regarding their semantic similarity. Although the corpus was created in a way that gives methods relying on cosine similarity a competitive advantage, TabSim also leads the field on this corpus.   A disadvantage of TabSim is its high execution time; it takes, on average, about 5 ms to classify a pair of tables . This high runtime is certainly not appropriate when using TabSim as a similarity function for a table similarity search engine, where the query table would be compared to every table from the corpus at search time. In future work, we plan to focus on designing scalable table search engines that use TabSim at the core but apply additional techniques for early search space pruning.  
"," Tables are a popular and efficient means of presenting structured information. They are used extensively in various kinds of documents including web pages. Tables display information as a two-dimensional matrix, the semantics of which is conveyed by a mixture of structure , headers, caption, and content. Recent research has started to consider tables as first class objects, not just as an addendum to texts, yielding interesting results for problems like table matching, table completion, or value imputation. All of these problems inherently rely on an accurate measure for the semantic similarity of two tables. We present TabSim, a novel method to compute table similarity scores using deep neural networks. Conceptually, TabSim represents a table as a learned concatenation of embeddings of its caption, its content, and its structure. Given two tables in this representation, a Siamese neural network is trained to compute a score correlating with the tables' semantic similarity. To train and evaluate our method, we created a gold standard corpus consisting of 1500 table pairs extracted from biomedical articles and manually scored regarding their degree of similarity, and adopted two other corpora originally developed for a different yet similar task. Our evaluation shows that TabSim outperforms other table similarity measures on average by app. 7\% pp F1-score in a binary similarity classification setting and by app. 1.5\% pp in a ranking scenario.",270
"   This paper deals with estimating the humor of edited  English news headlines. The illustration of tasks is in Figure . The original text sequence is given, which represents a title, with the annotated part that is edited along with the edit itself. Our responsibility is to determine how funny this change is in the range from 0 to 3 . This is called Sub-Task 1. We also participate in the Sub-Task 2, in that we should decide which from both given edits is the funnier one.  For the second task, we used the approach of reusing the model from the first task as it is described in section . So we are focusing the description on Sub-Task 1.    Official results were achieved with a Convolutional Neural Networks  , but we also tested numerous other approaches such as SVM and pre-trained transformer model.  The humor is a very subjective phenomenon, as can be seen from the inter-agreement on label annotation in Sub-Task 1 dataset\footnote{The inter-annotator agreement measured with Krippendorff's interval metric is just 0.2~ .}. The given data labels do not allow us to learn a sense of humor of a human annotator because the dataset does not specify from whom the grade comes. So, for example, if we have one annotator that likes dark humor and all the others not, we will be considering such a replacement as not humorous no meter if it is excellent dark humor or not. In other words, we may say that we are searching for some most common kind of humor.  The dominant theory of humor is the Incongruity Theory . It says that we are finding humor in perceiving something unexpected  that violates expectations that were set up by the joke.  There are samples, in the provided dataset, that uses the incongruity to create humor. Moreover, according to  Hossain et al.  , we can see a positive influence of incongruity on systems results for the dataset.   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %       % space normally used by the marker This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: \url{http://creativecommons.org/licenses/by/4.0/}. }     The system description was provided, and we compare the achieved results of the official model with several other models, including the baseline and the best team in the competition.   In future work, it should be more investigated if imbalanced dataset and small inter-annotator agreement caused that the JokeMeter model was more focused on the prior probabilities of grades and not on the input itself .   
"," This paper describes our system that was designed for Humor evaluation within the SemEval-2020 Task~7. The system is based on convolutional neural network architecture. We investigate the system on the official dataset, and we provide more insight to model itself to see how the learned inner features look.",271
"  Having a clear picture of students' perception on their classes, professors, and university facilities enables educational institutions to propose strategies to improve in many areas. It has been suggested by many studies that positive students' perception on the learning environment is correlated with higher academic achievement. Therefore, not only can universities improve the quality of their professors, their class content as well as learning facilities, but they also can improve --as a consequence-- their students' academic achievement, leading to an overall improvement of the education quality.  The call for action is clear. However, in order to propose and implement effective improvement strategies, one needs to measure the students' perception. Typical ways of doing this is through evaluations carried out at the final stage of each academical period where students grade their professors in several aspects. These evaluations normally consist of an online questionnaire with closed questions, and some open questions where students give their opinions about the class and their professors. Closed questions questionnaire can be tedious for students, leading to low response rates. Closed questions are helpful for the fast interpretation of results with statistical tools. These questions are designed to measure professors' performance on specific topics such as how engaging the class is, punctuality, among others. On the other hand, open questions provide students with a free space to express their opinions. Of course, gathering and interpreting data from open questions responses is a much more challenging task than making statistics from closed questions. Nonetheless, the amount of useful information found in students' opinions is a valuable source that is rarely exploited.  The latest advances in machine learning and natural language processing  techniques can be used to build tools that facilitate the analysis of large amounts of opinions generated by students. Particularly, sentiment analysis is suited to identify and quantify how positively or negatively students feel about their professors. These machine learning applications have only been recently explored. For instance, Na鑼倂e Bayes has been used to classify students' opinions in social media. Also, Latent Dirichlet Allocation  has been used to model topics along with sentiment analysis to explore opinions from students. Some studies using tools from machine learning have been conducted in the field of students' perception analysis. The majority of them have addressed the issue of performing sentiment analysis of the students' comments, and others have tried to identify topics in suggestions and opinions left by students , thus, we develop a joint approach were state of the art tools from NLP are used to perform both sentiment analysis and identify topics of interest in the students' comments.   Nonetheless, we must stress that researchers have for long worked on similar problems of assessing customer satisfaction from written opinions including public election forecasting, sales and trading prediction, marketing price prediction, among others. The common pipeline for performing opinion mining consists of the following general steps: i) retrieval of opinions from public databases, ii) cleaning of the opinions , iii) prediction of a quantity of interest such as polarity, sentiment strength, among others.  In this paper, we combine state-of-the-art methods in an NLP-based pipeline for classifying the sentiment of students' opinions. We then use these results to predict the ratings given to the professors by the students by means of supervised learning algorithms. Furthermore, we perform LDA to discover latent topics that are central in students' opinions. With the power of question answering systems, we envision students' perceptions surveys having only open questions that are fast to answer, reaching high levels of response rates, and also extracting the most relevant information, which comes from the students' opinions. These opinions are then mined with methods like the one we propose to analyse how students truly feel about their professors and classes.  The structure of this paper is as follows. In section methods and materials a brief description of the data and its prepossessing is presented. Next, in the results the analysis of model performance as well as the statistical analysis of the obtained results is scrutinised. Recommendations for future perspectives in the research of the subject as well as an outlined of the conclusions are listed in the final part of the manuscript.      In this work, we present a new natural language processing methodology to automatically explore students' opinions on their professors, compared to methodologies that use other approaches of sentiment analyses . For this, we use state-of-the-art techniques such as FastText to build classifiers that are able to identify polarity in students' opinions. Furthermore, we discover latent topics in the opinions corpus through Latent Dirichlet Allocation. These two approaches are then combined to predict the score that students give to their professors, so that we can identify professors with non-excellent performance only using the information from students' opinions. We argue that such tools can reduce human burden in this analysis, and can also simultaneously take full advantage of the information found in those text opinions. Nonetheless, the experiments so far exposed in our paper indicate that the amount of information or quality of our annotations hampers the possibility of eliminating closed questions from perception surveys to assess the professors' quality.  We envision students' perception questionnaires based on asking the students' for improvement recommendations, and on opinions about negative and positive aspects of the professor and of the class. This questionnaires will probably be more friendly with students, reaching higher response rates, leaving the tedious part of extracting information to the combined work of humans and NLP algorithms, where the heavy-lifting is done by NLP, and only high-level analysis is left for the human. We hope in the future to use larger opinion databases which allow us to find and train a better model, since a larger database enables us to confidently circumvent the imbalance problem through sampling techniques such as the one presented in Ref..  
","  Students' perception of classes measured through their opinions on teaching surveys allows to identify deficiencies and problems, both in the environment and in the learning methodologies. The purpose of this paper is to study, through sentiment analysis using natural language processing  and machine learning  techniques, those opinions in order to identify topics that are relevant for students, as well as predicting the associated sentiment via polarity analysis. As a result, it is implemented, trained and tested two algorithms to predict the associated sentiment as well as the relevant topics of such opinions. The combination of both approaches then becomes useful to identify specific properties of the students' opinions associated with each sentiment label  and topic. Furthermore, we explore the possibility that students' perception surveys are carried out without closed questions, relying on the information that students can provide through open questions where they express their opinions about their classes.  % of the comments using NLP techniques.  % In the present paper Students' perception of classes measure through teaching surveys allows to identify deficiencies and problems, both in the environment and in the learning methodologies. NLP techniques   %The abstract should briefly summarize the contents of the paper in 15--250 words.",272
"   In knowledge discovery and representation, the notion of {, i.e., `abstract entity' or `abstract object' in the Fregean dichotomy of { . In Natural Language Processing , the task of {. Halliday  offers a syntactic interpretation of { or { or { .   CE is crucial for a number of downstream applications, including, e.g., language understanding, ontology population, semantic search, and question answering; it is also the key to entity linking . In generic open domain subject-neutral discourse across different  subjects, indexing the longest possible nominal chunks and their head words located in sequences of tokens between specified ``break words""  and special dictionary lookups such as {  are very common techniques. They generally reach outstanding precision, but low recall due to constant evolvement of the language vocabulary. Advanced deep learning models that already dominate CE in specialized closed domain discourse on one or a limited range of related subjects, e.g., biomedical discourse , and that are also standard in keyphrase extraction  are an alternative. However, such models need a tremendous amount of labeled data for training.  We present an operational CE model that utilizes pointer--generator networks  and bidirectional long short-term memory  units  to retrieve concepts from general discourse textual material.\footnote{We adopt Halliday's notion of classifying nominal group as definition of a concept.} Furthermore, since for a generic, domain-independent concept extraction model we need a sufficiently large training corpus that covers a vast variety of topics and no such annotated corpora are available, we opt for distant supervision to create a sufficiently large and diverse dataset. Distant supervision consists in automatic labeling of potentially useful data by an easy-to-handle  algorithm to obtain an annotation which is likely to be noisy but, at the same time, to contain enough information to train a robust model . Two labeling schemes are considered. Experiments carried out on a dataset of 250K+ Wikipedia pages show that copies of our model trained differently and joined in an ensemble significantly outperform standard techniques and, when used on top of DBpedia Spotlight, further improve its performance by nearly 10\%.     				 We presented an adaptation of the pointer--generator network model  to generic open-domain concept extraction. Due to its capacity to cope with OOV concept labels, it outperforms dictionary lookup-based CE such as DBpedia Spotlight or AIDA in terms of recall and -score. It also shows an advantage over deep models that focus on NER only since it also covers non-named concept categories. However, a combination of the pointer--generator model with DBpedia Spotlight seems to be the best solution since it takes advantage of both the neural model and the dictionary lookup. In order to facilitate a solid evaluation of the proposed model and compare it to a series of baselines, we utilized Wikipedia pages with text snippet links as a sparsely concept-annotated dataset. To ensure that our model is capable of extracting all generic concepts instead of detecting only texts of the page links, we ignored this sparse annotation during training. Instead, we compiled a large densely concept-annotated dataset for leveraging it within the distant supervision using the algorithm described above. To the best of our knowledge, no such dataset was available so far. In the future, we plan to address the problem of multilingual concept extraction, using pre-trained multi-lingual embeddings and compiling another large dataset that contains a higher percentage of non-named entity concepts.  The code for running our pretrained models is available in the following GitHub repository: . 				 
"," Concept extraction is crucial for a number of downstream applications. However, surprisingly enough, straightforward single token/nominal chunk--concept alignment or dictionary lookup techniques such as DBpedia Spotlight still prevail. We propose a generic open-domain OOV-oriented extractive model that is based on distant supervision of a pointer--generator network leveraging bidirectional LSTMs and a copy mechanism. The model has been trained on a large annotated corpus compiled specifically for this task from 250K Wikipedia pages, and tested on regular pages, where the pointers to other pages are considered as ground truth concepts. The outcome of the experiments shows that our model significantly outperforms standard techniques and, when used on top of DBpedia Spotlight, further improves its performance. The experiments furthermore show that the model can be readily ported to other datasets on which it equally achieves a state-of-the-art performance.",273
"   This paper describes our socialbot for open-domain conversation, Chirpy Cardinal, built as a research platform during the 2019 Alexa Prize competition.  During the competition, US-based Amazon Alexa users could give an invocation phrase  to be connected to one of the competing socialbots . After receiving a minimal orientation phrase at the beginning of the conversation, the user talks to the socialbot  until they decide to end the conversation -- at which point, they are invited to provide a rating and comment.  To provide a convincing user experience, an open-domain conversational agent must excel at language understanding, language generation, emotional engagement, memory, world knowledge and conversational planning, among other desirable characteristics -- an ambitious goal! Prior work within and outside the Alexa Prize competition has taken the successful strategy of pushing progress along individual skills, and forming an ensemble of sub-systems, each excelling at a singular characteristic while ignoring others.  For instance, supporting user initiative in open-domain conversations is extremely challenging, as it requires understanding the countless ways a user can take initiative, and the ability to respond to each of them with specificity.  Faced with this difficulty, when it comes to in-depth conversations, many previous dialogue systems rely primarily on bot-initiative, driving users along carefully scripted paths.  On the other hand, systems attempting higher user-initiative via non-scripted paths are likely to lead towards shallower conversations.  Thus there is a lot of room for innovation and research in trying to simultaneously achieve two or more complementary characteristics; this is a recurring theme throughout this work.  Our goal in building this socialbot was to offer a natural-sounding and emotionally engaging dialogue agent that can talk knowledgeably about a wide variety of topics, while also letting the user take as much initiative as possible.   Initiative -- the ability to drive the direction of the conversation -- has been studied extensively in the context of task-oriented dialogue.  Mixed initiative , in which the user and the bot share initiative, is an important quality of a successful dialogue system, as it provides the user a sense of agency without making them entirely responsible for suggesting new topics and directions. In order to improve on mixed initiative while still providing an acceptable conversational depth, we designed our initial system to rely heavily on system initiative, but at the same time explored several avenues to increase user initiative in a controlled fashion.  To support mixed initiative, our system has a global navigational intent classifier  and entity tracker , allowing it to track high level topic changes from both the user and the bot. Further, our response priority system  allows individual Response Generators  to interject when the user initiates a change of topic.  High-coverage world knowledge is an important component of open-domain conversation -- our bot must be able to talk about the diverse range of entities and topics that interest users, particularly if we wish to respect user initiative. We use the Alexa Knowledge Graph, The Washington Post, Reddit and Twitter as sources of up-to-date knowledge in particular domains, while ensuring high coverage by using Wikipedia and Wikidata entities as the foundation of our entity-based conversations . However, world knowledge must be delivered in a conversational style -- this is a characteristic that distinguishes a socialbot from a virtual assistant. To achieve this, we finetuned a neural generative model on the TopicalChat dataset  to obtain a conversational paraphrasing model that adapts external text into a conversational style .  A socialbot cannot focus solely on external entities -- to be truly social, it must be able to discuss personal experiences and emotions. While ELIZA-like systems  attempt this via templated repetition of user phrases, they lack the naturalness and depth of real human conversations.  Our Neural Chat module  invites the user to share their everyday experiences and current emotions, and uses a neural generative model to respond empathetically. With it, we attempt to have a deep, sustained and emotionally engaging conversation about a user's lives. In addition, our Opinion module  allows the user to express their feelings by expressing their likes and dislikes.  To foster a reciprocal atmosphere, our bot also shares its own distinct feelings, experiences and opinions.  Lastly, we note that the advent of large-scale pretrained neural generative models has substantially impacted what is possible in open-domain socialbots.  While in the last Alexa Prize competition, none of the top three socialbots used neural generation , we found current GPT-2 models  to be a key tool to support our design goals. Neural generation enables natural phrasing and emotional engagement, as well as more flexible responsiveness , supporting higher user initiative.  A limitation of neural generation methods for dialogue is deterioration in quality and consistency over a long conversation, which can be potentially overcome with symbolic constraints. We explore ways to bring the best of both worlds -- long term consistency and short term fluidity -- together.   Despite being a first-time entrant, at the end of the competition our system achieved a rating of 3.6/5.0, which is within 0.1 of the highest-ranked systems, and is capable of detailed, sustained conversations with interested users . Qualitatively, during in-person interactions with users, we observed that many innovations such as in-depth discussions of everyday life, conversational styling of informational content, and opinionated exchanges were received with expressions of pleasant surprise -- indicating our steps were in the right direction.  In , we re-examine the goals we set out to achieve, and empirically analyze our bot's successes and failures. In , we talk about the challenges we faced, the trade-offs we made, our conclusions and avenues for future work.   [t]          Most NLP research focuses on self-contained tasks.  However, an open-domain socialbot, served to a diverse range of customers in widely different contexts, is by no means a self-contained task.  Our socialbot is a tapestry of many such components, requiring a deep understanding of each component and how they should work together -- a setting we call Full Stack NLP. Often the inputs and outputs of these components are inter-dependent, leading to cascading errors.  We made many design choices which delay hard decisions in pipelines, and maximize information exchange between modules.  Moving forward, the next avenue for advancing the state-of-the-art would be research on models which perform these tasks jointly and methods which enable training over multiple interdependent tasks with only a small amount of joint supervision.     As a recurring problem, we found that many existing NLP resources didn't work well out-the-box. The main reason for this is that the training data for these resources  is misaligned with our setting . However, a deeper reason is the constantly changing nature of dialogue agents themselves. Even for an extremely related resource , domain shift was a problem. Recent advances in online- and meta-learning could provide a useful long term solution to this issue.      Bot-human conversations are fundamentally different to human-human conversations.  Users can be adversarial, deliberately testing the bot's boundaries.  As socialbot designers, we are eager to avoid a disaster like Microsoft Tay, so we apply strict but overly simplistic methods to block off sensitive topics .  However, this rules out sincere conversation about difficult topics.  We observed that users are actually quite resilient to conflict, and can find disagreement stimulating .  We also found that emotional intimacy is reciprocal -- users are more inclined to share their feelings after the bot has shared its own .  Going forward, we should continue to take seriously the dangers of speaking inappropriately, but keep in mind the cost -- to engagement and to intimacy -- of not engaging in difficult topics.   As part of our goal to support user initiative, we focused on asking users questions to find out which topics interested them. However, this puts pressure on the user to think of a response, especially given the time constraints of Alexa devices. Thus we found that our attempts to let the user take more initiative unfortunately led to decision fatigue.  Separately, our ability to support user initiative was limited by our ability to answer followup questions, and to correctly understand long or unexpected user utterances.  On balance, we found that asking the user open-ended questions about interesting topics was a good strategy -- easier to handle than spontaneous user questions, and less pressuring than asking users to name topics. We see an opportunity for future work to build systems which listen more to the user's knowledge, rather than only providing knowledge.  
"," We present Chirpy Cardinal, an open-domain dialogue agent, as a research platform for the 2019 Alexa Prize competition. Building an open-domain socialbot that talks to real people is challenging -- such a system must meet multiple user expectations such as broad world knowledge, conversational style, and emotional connection.  Our socialbot engages users on their terms -- prioritizing their interests, feelings and autonomy.  As a result, our socialbot provides a responsive, personalized user experience, capable of talking knowledgeably about a wide variety of topics, as well as chatting empathetically about ordinary life. Neural generation plays a key role in achieving these goals, providing the backbone for our conversational and emotional tone. At the end of the competition, Chirpy Cardinal progressed to the finals with an average rating of 3.6/5.0, a median conversation duration of 2 minutes 16 seconds, and a 90$^{\text{th}}$ percentile duration of over 12 minutes.",274
"   Clinical Named Entity Recognition  extracts patient information from unstructured Electronic Health Records , which is an important task for further clinical research. The main goal of CNER is to identify clinical terminologies in EHRs, such as diseases, symptoms, treatments, exams and body parts. Accurate identification of these clinical concepts can provide effective decision support for patient care and treatment. Compared to English texts, CNER in Chinese texts is more difficult since Chinese EHRs are recorded without explicit word delimiters. In recent years, CNER has attracted considerable research efforts, and many methods have proposed in the literature. Most of them are deep learning methods.  Although many advanced models have been developed for CNER, their performance still heavily depends on the manually-annotated training data. Labeling EHRs is usually time-consuming and expensive because EHRs involve many complex clinical terminologies, and only labelers with medical background are qualified for clinical annotation. It thus becomes rather difficult to train an effective model for CNER since it requires a large number of manually-annotated clinical texts.  Active learning, which iteratively selects the most informative samples for labelers to annotate, is an effective method to reduce annotation cost. It has been widely used in many Natural Language Processing  tasks, such as text classification and event recognition. In conventional active learning, there is only one labeler and the algorithm queries the labels of the selected instances from the labeler, which always returns the ground truth of queried labels. However, in many real settings, there are multiple labelers, and they usually provide diverse quality of annotation with different costs. Obviously, a labeler which offers better overall quality will require a higher cost for each query. The overall quality of labelers can be assessed according to their previous annotation  performance. Moreover, labelers may have diverse expertise for different instances. For example, in CNER tasks, some labelers may be good at labeling diseases, while some are skilled in symptoms. Therefore, we need to consider querying which of them to annotate the selected instances so as to keep a trade-off between quality and cost.  In the past few years, active learning with multiple noisy labelers has received significant attention and achieved great success in various applications. However, many works either ignored the different expertise of multiple labelers and queried the same labeler for all instances globally or neglected the annotation costs of different labelers. Recently, two methods considering the diversity of labelers on both expertise and query costs have been proposed for classification tasks. Experimental results demonstrate the effectiveness of the two methods on selecting cost-effective queries. We thus follow the trend and focus on CNER task for the first time.  In this paper, we propose a Cost-Quality Adaptive Active Learning  method for CNER in Chinese EHRs, which selects the most cost-effective instance-labeler pairs to obtain better annotation performance with lower costs in an adaptive manner. Specifically, we first combine three sampling strategies, namely uncertainty, entropy and margin to assess the informativeness of instances. We further observe that a labeler with low quality of overall annotation can still assign accurate labels for some specific instances in real settings. Then, based on this fact, for each instance, we select a suitable labeler which offers high-quality yet cheap annotations so as to keep a balance between the annotation quality, labeling costs, and the informativeness of instances.  The main contributions of this paper can be summarized as follows:        The rest of this paper is organized as follows. Section  briefly reviews the related work on CNER and active learning. Section  presents our proposed cost-quality adaptive active learning method for Chinese CNER, followed by experimental evaluations as Section . Finally, the conclusions and potential research directions are summarized as Section .      In this paper, we propose an active learning method with multiple labelers for CNER in Chinese EHRs, namely CQAAL. CQAAL selects instance-labeler pairs, where the selected instances are informative, the costs of selected labelers are low, and the quality of annotation is high. To evaluate the informativeness of the selected instances, we take uncertainty, entropy, and margin sampling strategies into consideration. Based on the CCKS-2017 Task 2 benchmark dataset, we experimentally evaluate our proposed CQAAL. Experimental results show that compared to baseline methods, CQAAL achieves competitive performance. As future work, we plan to design a more complex function to evaluate the cost-effectiveness of instance-labeler pairs. Furthermore, we want to explore the application of our proposed method on other NLP tasks, such as relation extraction.  
","  Clinical Named Entity Recognition  aims to automatically identity clinical terminologies in Electronic Health Records , which is a fundamental and crucial step for clinical research. To train a high-performance model for CNER, it usually requires a large number of EHRs with high-quality labels. However, labeling EHRs, especially Chinese EHRs, is time-consuming and expensive. One effective solution to this is active learning, where a model asks labelers to annotate data which the model is uncertain of. Conventional active learning assumes a single labeler that always replies noiseless answers to queried labels. However, in real settings, multiple labelers provide diverse quality of annotation with varied costs and labelers with low overall annotation quality can still assign correct labels for some specific instances. In this paper, we propose a Cost-Quality Adaptive Active Learning  approach for CNER in Chinese EHRs, which maintains a balance between the annotation quality, labeling costs, and the informativeness of selected instances. Specifically, CQAAL selects cost-effective instance-labeler pairs to achieve better annotation quality with lower costs in an adaptive manner. Computational results on the CCKS-2017 Task 2 benchmark dataset demonstrate the superiority and effectiveness of the proposed CQAAL.",275
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } %\footnotemark .}   In this digital era, users express their personal thoughts and opinions regarding a wide range of topics on social media platforms such as blogs, micro-blogs , and chats .   Multilingual societies like India with a decent amount of internet penetration widely adopted such social media platforms.   However, the regional language influences the proliferation of the Hindi-English Code-Mixed  data.   Sentiment analysis of these end-user data from social media is a crucial resource for commerce and governance.  However, in contrast to the classical sentiment analysis methods, which were originally designed for dealing with well-written product reviews, CM texts from social media often contain misspellings , badly cased words, letter substitutions, ambiguities, non standard abbreviations, improper use of grammar, etc.    CM poses several unseen difficulties to natural language processing  tasks such as word-level language identification, part-of-speech tagging, dependency parsing, machine translation and semantic processing.   In the last few years, a number of workshops such as Linguistic Code-Switching Workshops\footnote{https://code-switching.github.io/2020/} and shared tasks such as Mixed Script Information Retrieval   have been organized due to the emerging popularity of code-mixing.     To promote research in this area, Task 9 of SemEval-2020 was devoted to CM sentiment analysis in Twitter.   The goal of the task was to automatically classify the polarity of a given CM Twitter post into one of the three predefined categories: positive, negative and neutral. The CM languages are English-Hindi and English-Spanish; for a more detailed description of the task see .     In this paper, we present a deep learning approach, using a Recurrent Convolutional Neural Network for the task of automatic CM sentiment classification of tweets.    % The rest of the paper is structured as follows.  Section 2 provides background in brief.  Section 3 provides the system overview and Section 4 describes our approach in detail. In Section 5, we discuss the analysis and evaluation results for our system. We conclude our work in Section 6.      { This paper describes the approach we proposed for SemEval-2020 Task 9: Sentiment Analysis for CM Social Media Text .  In our approach, we pre-processed the CM tweets and proposed a Recurrent Convolutional Neural Network for the sentiment analysis of CM tweets. We submitted two runs and obtaining promising results: our best run obtained 0.691 of F1 averaged across the positives, negatives and the neutral.  We observed that the proposed architecture occasionally strives to separate the positive and negative polarities from the neutral and vice versa.    For future work, we will explore the performance of our model with larger corpora against the testing set. Also, we would like to investigate other embedding choices such as BERT . Moreover, due to the impact that irony and sarcasm have on sentiment analysis  it would be interesting to apply deep learning techniques to detect irony  but in a code-mixed scenario.      
","   This paper describes the participation of LIMSI\_UPV team in SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text. The proposed approach competed in SentiMix Hindi-English subtask, that addresses the problem of predicting the sentiment of a given Hindi-English code-mixed tweet.  We propose Recurrent Convolutional Neural Network that combines both the recurrent neural network and the convolutional network to better capture the semantics of the text, for code-mixed sentiment analysis.  The proposed system obtained 0.69  in terms of F1 score on the given test data and achieved the 9th place  in the SentiMix Hindi-English subtask.",276
"  The performance of many machine learning algorithms depends on their hyper-parameters. For example, the prediction accuracy of support vector machines depends on the kernel and regularization hyper-parameters  and , and deep neural networks are sensitive to a wide range of hyper-parameters, including the number of units per layer, learning rates, weight decay, and dropout rates etc.. It is well-known that hyper-parameter settings often make the difference between mediocre and state-of-the-art performance. As a result, hyper-parameter optimization has been receiving an increasingly amount of attention in the NLP and machine learning communities. However, identifying the best model configuration is often a cumbersome process because it can involve several trials and errors before an optimal hyper-parameter setting can be found. Bayesian Optimization has emerged as an efficient framework for carrying out the model selection process, achieving impressive successes. For example, in several studies, it found better instantiations of convolutional network hyper-parameters than domain experts. The common theme is to perform a set of iterative hyper-parameter optimizations. In each round, these methods fit a hyper-parameter response surface using a probabilistic regression function such as Gaussian Process or tree-based models, where the response surface maps each hyper-parameter setting to an approximated accuracy. The learned regression model is then used as a surrogate of the response surface to explore the search space and identify promising hyper-parameter candidates to evaluate next in order to enhance validation accuracy.    While these methods have enjoyed great success compared to conventional random search and grid search algorithms for model selection, the focus of these work have largely been on optimizing for effectiveness, while ignoring the resulting model's training efficiency. Given that both prediction accuracy and model training time are important for real-world applications, models selected for effectiveness may not meet the strict real-world efficiency requirements necessary to deploy in a production environment. In addition, most of the previous methods exclusively focus on optimizing the hyper-parameters of a given model class, while ignoring other important extrinsic hyper-parameters such as training set size which can influence both speed and accuracy. For example, model training time typically grows proportionally with respect to training set size, and the prediction accuracy can also be influenced by the amount of training data used for learning. If the tolerance for inefficient model training is low, then the amount of training data should be reduced or adjusted with the rest of intrinsic hyper-parameters to meet the stringent efficiency requirements.    Given that both model effectiveness and training time are important for real-world applications, in this work, we propose a unified Bayesian Optimization framework for jointly selecting models for prediction effectiveness and training efficiency. First, we propose an objective that captures the tradeoff between these two metrics, then we demonstrate how we can jointly optimize them in a principled Bayesian Optimization framework. In addition, we account for extrinsic hyper-parameters such as training set size in the hyper-parameter optimization space. We will demonstrate this joint optimization of both measures in an enriched hyper-parameter space leads to selecting more efficient and accurate models. It is important to point out our work is fundamentally different from previous Bayesian Optimization that considers the speed of the hyper-parameter search/model selection process -- our focus is on model training efficiency, in addition to accuracy , while their focus is on hyper-parameter search efficiency. Our work can be viewed as taking an efficiency-centric view at selecting effective models. Experiments on model selection for recommendation and question answering tasks indicate models selected this way significantly improves model training efficiency while maintaining strong effectiveness as compared to state-of-the-art Bayesian Optimization algorithms.    The remainder of the paper is organized as follows: We start with a discussion of related work. Next, in Section we propose metrics for quantifying the tradeoff between prediction accuracy and training efficiency, then discuss methods for model selection based on the tradeoff metric. Section presents experimental results under different tradeoff scenarios for recommendation and question answering tasks, before concluding in Section.       We introduced a unified Bayesian Optimization framework for jointly optimizing models for both effectiveness and training efficiency. We propose an objective that captures the tradeoff between accuracy and training efficiency and demonstrate how we can jointly optimize these measures in a principled framework. Experiments on several real-world model selection and rating prediction tasks indicate this approach significantly improves model training efficiency while maintaining strong effectiveness as compared to state-of-the-art baseline models.  
","   The performance of many machine learning models depends on their hyper-parameter settings. Bayesian Optimization has become a successful tool for hyper-parameter optimization of machine learning algorithms, which aims to identify optimal hyper-parameters during an iterative sequential process. However, most of the Bayesian Optimization algorithms are designed to select models for effectiveness only and ignore the important issue of model training efficiency. Given that both model effectiveness and training time are important for real-world applications, models selected for effectiveness may not meet the strict training time requirements necessary to deploy in a production environment. In this work, we present a unified Bayesian Optimization framework for jointly optimizing models for both prediction effectiveness and training efficiency. We propose an objective that captures the tradeoff between these two metrics and demonstrate how we can jointly optimize them in a principled Bayesian Optimization framework. Experiments on model selection for recommendation tasks indicate models selected this way significantly improves model training efficiency while maintaining strong effectiveness as compared to state-of-the-art Bayesian Optimization algorithms.",277
" Attention-based transformer networks~ are widely used for sequence modeling tasks, including language modeling and machine translation. To improve performance, models are often scaled to be either wider, by increasing the dimension of hidden layers, or deeper, by stacking more transformer blocks. For example, T5  uses a dimension of 65K and GPT-3  uses 96 transformer blocks. However, such scaling increases the number of network parameters significantly , and complicates learning, i.e., these models either require very large training corpora  or careful regularization . In this paper, we introduce a new parameter-efficient attention-based architecture that can be easily scaled to be both wide and deep.   Our ep and t-weight ransformer architecture, \arch, extends the transformer architecture of  and delivers similar or better performance with significantly fewer parameters and operations. At the heart of \arch~is the \dextra~that uses the group linear transformations  of  with an expand-reduce strategy for varying the width and depth of the \arch~block efficiently. Since GLTs are local by nature, the \dextra~uses feature shuffling, which is analogous to channel shuffling in convolutional networks , to share information between different groups. Such wide and deep representations facilitate replacing the multi-head attention and feed-forward layers in transformers with single headed attention and light-weight feed-forward layers, reducing total network parameters and operations. Importantly, unlike transformers, the \dextra~decouples the depth and width from the input size, allowing us to allocate parameters more efficiently across blocks by using shallower and narrower \arch~blocks near the input and deeper and wider \arch~blocks near the output.  We demonstrate that \arch~models achieve similar or better performance than transformer models with significantly fewer parameters and operations, on two common sequence modeling tasks,  machine translation and   language modeling. On the low resource WMT'16 En-Ro machine translation dataset, \arch~attains transformer performance using  fewer parameters. On the high resource WMT'14 En-Fr dataset, \arch~delivers better performance  with  fewer parameters than baseline transformers. Similarly, on language modeling, \arch~matches the performance of  Transformer-XL~ with  fewer parameters on the WikiText-103 dataset. Our source code is open-source and is available at: \textcolor{blue}{\url{https://github.com/sacmehta/delight}}    This paper introduces a deep and light-weight transformer architecture, \arch, that efficiently allocates parameters both within the \arch~block and across \arch~blocks. Compared to state-of-the-art transformer models, \arch~models are  deep and light-weight and  deliver similar or better performance. In the future, we plan to apply \arch~to other tasks, including language model pre-training, question answering, and language generation.    This research was supported by ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF , and an Allen Distinguished Investigator Award. Authors would also like to thank members of the UW-NLP and the H2Lab at The University of Washington for their valuable feedback and comments.    \clearpage    
"," We introduce a deep and light-weight transformer, \arch, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. \arch~more efficiently allocates parameters both  within each Transformer block using the \dextra, a deep and light-weight transformation and  across blocks using block-wise scaling, that allows for shallower and narrower \arch~blocks near the input and wider and deeper \arch~blocks near the output. Overall, \arch~networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that \arch~matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average.",278
" Search systems provide relevant documents to users who are looking for specific information through queries. A user receives a list of ranked documents ordered by search relevance, where ranking plays a crucial role to model such relevance that directly affects consequential user interactions and experience. Most search systems deal with a large amount of natural language data from queries, profiles, and documents. An effective search system requires a deep understanding of the context and semantics behind natural language data to power ranking relevance.  %Ranking is one of the most common and important components in many search and recommender systems, e.g., document retrieval , feeds recommendation , query suggestion , auto completed query ranking , etc. These applications contain rich text data, i.e., queries, user profiles and documents.  Therefore, a ranking framework that captures the text semantics can significantly boost the performance of many productions.  Traditional ranking approaches largely rely on word/phrase exact matching features, which has a limited ability to capture contextual and deep semantic information. In the recent decade, deep learning based natural language processing technologies present an unprecedented opportunity to understand the deep semantics of natural language data through embedding representation. Moreover, to enhance contextual modeling, contextual embedding such as BERT has been proposed and extensively evaluated on various NLP tasks with significant improvements over existing techniques.  However, promoting the power of BERT in ranking is a non-trivial task. The current effective approaches integrate BERT as an embedding generation component in the ranking model, with the input a concatenated string of query and document texts. BERT is then fine tuned with ranking loss. The inherent transformer layer in BERT allows direct context sharing between query words and document words, exploiting the power of contextual modeling in BERT to the greatest extent, as the query word embeddings can incorporate many matching signals in documents. This approach, in the category of interaction based models, comes with a significant challenge in online serving: a) the heavy BERT computation on the fly is not affordable in a real world search system; and b) the interaction based structure, as applied to concatenated query and document, precludes any embedding pre-computing that can reduce computation. %Since the BERT embedding generation depends on a specific online query to be concatenated with documents, the model needs to be applied to all the query/document pairs, and can only start the computation after a query is available online, xxxx %which is not feasible for industry productionalization given the strong latency concerns on most real-world search systems.  %The advantage of BERT comes from its capability of modeling strong context among text data, which is leveraged by various approached that apply BERT to the concatenation of a pair of query and document during the ranking stage.   %Previous work has investigated a two-stage ranking model that extracts embedding features through Deep NLP  components first, then incorporates the features into a ranking model. Due to the lack of end-to-end context, this approach generally has information loss that cannot fully exploit the deep semantics. In addition, embedding that requires fine-tuning with specific tasks, e.g., BERT, does not work in this approach.  % Incorporating deep NLP components, that generates non-contextual or contextual embedding, into an end-to-end ranking system efficiently for industry applications comes with several challenges. For non-contextual embeddings, xx; for contextual embedding, e.g., BERT, it works well when both source and target are processed together through the deep NLP component, a.k.a., the interaction model. However, the relevance improvement comes with the large computation time, making it hard to productionize the deep NLP ranking models.  %are able to learn embeddings of text data, and perform semantic matching. Early works  extract a text embedding by averaging the word embeddings, and then a cosine similarity between text embeddings is calculated as semantic matching features. Later, CNN  and LSTM  are applied to capture the word order information . More recently, BERT  has shown significant improvement in document ranking  by extracting deeper semantics.  However, the relevance improvement comes with the large computation time, making it hard to productionize the deep NLP ranking models. To enable an efficient BERT-based ranking model for industry use cases, we propose to use representation based structure. Instead of applying BERT to a concatenated string of query and document texts, it generates query and document embeddings independently. It then computes the matching signals based on the query and document embeddings. This approach makes it feasible for pre-computing document embedding; thus, the online system only needs to do BERT real-time computation for queries. By independently computing query and document embeddings, however, we may lose the enhancement on the direct context sharing between queries and documents at word-level. This trade-off makes it a challenge to develop a BERT-based ranking model that is both effective and efficient.  In this work, we investigated the BERT-based ranking model solution with representation-based structure, and conducted comprehensive offline and online experiments on real-world search products. Furthermore, we extended the model solution into a general ranking framework, DeText , that is able to support several state-of-the-art deep NLP components in addition to BERT. The framework comes with great flexibility to adapt to various industry use cases. For example, BERT can be applied for ranking components that have rich natural language paraphrasing; CNN can be applied when ease of deployment is a top concern for a specific system.  Beyond the ranking framework, we also summarized experience on developing an effective and efficient ranking solution with deep NLP technology, and how to balance effectiveness and efficiency for industry usage in general. We shared practical lessons of improving relevance performance while maintaining a low latency, as well as general guidance in deploying deep ranking models into search production.  %We have proposed two strategies: for heavy models such as BERT, the target side embeddings are pre-computed; for light models such as CNN, we perform real-time inference for both source and target data.  We conduct thorough experiments and deploy DeText in LinkedIn's three vertical searches: people search, job search, help center search.  %The BERT based ranking algorithm is not limited to document ranking in search; rather, with a little changes, it can be applied into many other ranking components, such as recommender systems , query auto completion ranking , etc.  Therefore, we extend our algorithm to a ranking framework.  In order to meet the requirements of various ranking systems, we enable flexibility in many modules, such as text embedding generation, learning-to-rank loss, etc.  %For example, BERT can be applied for ranking components that have rich natural language paraphrasing; CNN is available when ease of deployment is a top priority for a specific system.   %This paper focuses on designing a general deep NLP based ranking framework for productions.  Two main challenges are identified:  Flexibility.  Different productions have different requirements of ranking models. For example, in some tasks with mainly natural language queries, deep contextual understanding is essential to understand user intents; for other tasks, shallow understanding is enough.   Effectiveness and Efficiency.  The production ranking models are usually strong and robust models, where the whole process is finished in only a few dozens of milliseconds. In contrast, deep neural networks are notorious for the heavy computation: the time complexity of LSTM models grows by at least a factor of  .  %Motivated by these challenges, we design a general neural ranking framework for practical use cases in industry, which is referred to as DeText . In order to meet the requirements of various ranking productions, we enable great flexibility in DeText, with respect to input data format, language genre, latency budget, etc. For example, BERT is applied for help center ranking that has a lot of natural language queries with rich paraphrasing; CNN is applied for query auto completion which should be finished within several milliseconds.  More details regarding the model flexibility are listed in Section .    %Another important goal is to balance the efficiency and effectiveness so that DeText is a practical solution for industry usage. Firstly, we choose the representation based methods over the interaction based methods , because the latter has larger computation complexity, and does not allow document pre-computing.  Secondly, in order to outperform the strong production models, we carefully handle the powerful traditional features that are used in production models, which are combined with deep features.  In contrast, most previous work  focus on the deep neural networks without considering the hand-crafted features. Thirdly, we can choose to have multiple source/target fields to ensure robust results.  All of these three factors together enable the usage of compact neural networks to minimize the latency while still achieving strong relevance performance.  %Another important goal is to balance the efficiency and effectiveness so that DeText is a practical solution for industry usage. In this paper, we share many practical solutions, such as representation based methods to reduce latency, combining hand-crafted features with deep models to boost relevance performance.  %Firstly, we choose the representation based methods over the interaction based methods , because the latter has larger computation complexity, and does not allow document pre-computing.  Secondly, in order to outperform the strong production models, we carefully handle the powerful traditional features that are used in production models, which are combined with deep features.  In contrast, most previous work  focus on the deep neural networks without considering the hand-crafted features. Thirdly, we can choose to have multiple source/target fields to ensure robust results.  All of these three factors together enable the usage of compact neural networks to minimize the latency while still achieving strong relevance performance.  The contribution of this paper is summarized below:      }     . To meet the requirement of ranking modules in different productions, great flexibility is enabled in the DeText framework.  Meanwhile, practical solutions are incorporated to ensure the balance between efficiency and effectiveness. Since this is a general ranking framework,                %. DeText is designed to reach a balance between effectiveness and efficiency: representation based methods, traditional feature handling, multiple source/target fields, etc.      %. DeText models are deployed in three vertical searches at LinkedIn.  We use two deployment strategies: document embedding pre-computing for BERT models, and two pass ranking for CNN models.     In this paper, we propose the DeText  ranking framework with BERT/CNN based ranking model for practical usage in industry. To accommodate the requirements of different ranking productions, DeText allows flexible configuration, such as input data, text embedding extraction, traditional feature handling, etc. These choices enable us to experiment and develop scalable neural network models with strong relevance performance.  Our offline experiments show that DeText-LiBERT/DeText-CNN consistently outperforms the strong production baselines. The resulting models are deployed into three vertical searches in LinkedIn's commercial search engines.          The next two lines define the bibliography style to be used, and the bibliography file.    
"," Ranking is the most important component in a search system. Most search systems deal with large amounts of natural language data, hence an effective ranking system requires a deep understanding of text semantics. Recently, deep learning based natural language processing  models have generated promising results on ranking systems. BERT is one of the most successful models that learn contextual embedding, which has been applied to capture complex query-document relations for search ranking. However, this is generally done by exhaustively interacting each query word with each document word, which is inefficient for online serving in search product systems. In this paper, we investigate how to build an efficient BERT-based ranking model for industry use cases. The solution is further extended to a general ranking framework, DeText, that is open sourced and can be applied to various ranking productions. Offline and online experiments of DeText on three real-world search systems present significant improvement over state-of-the-art approaches.  %There are two common challenges in ranking systems: understanding the semantics of text data, and maintaining a good balance between effectiveness and efficiency.  To this end, we propose a general deep text  ranking framework to enable deep understanding of textual data that is practical for industry applications.  Our design principles are:  A general deep learning framework with the flexibility to meet requirements of different ranking systems.  For example, for search engines that require ease of deployment, CNN is available; for the tasks where deep contextual understanding is crucial, BERT can be used to extract text semantics.  To our best knowledge, this is the first work to successfully train a representation based BERT ranking model and launch to production.  Reaching a good balance between effectiveness and efficiency to meet industry requirements. Practical solutions are used, such as representation based methods, handling traditional features, etc, in order to optimize the relevance performance with a relatively small neural network.  %For example, we choose the representation based methods over interaction based methods, since the former is more computationally efficient.  %The DeText models are evaluated in two tasks: document ranking and query auto completion ranking.  In experiments, we observe DeText models are robust on different text genres, significantly outperforming strong production baselines. We designed two deployment strategies for CNN and BERT models, and successfully deployed them into three document ranking systems at LinkedIn.",279
" % Computer Society journal papers do something a tad strange with the very % first section heading . They place it % ABOVE the main text! IEEEtran.cls currently does not do this for you. % However, You can achieve this effect by making LaTeX jump through some % hoops via something like: % %[0pt][0pt]% %  {\parbox{\columnwidth}{   The conclusion goes here.        if have a single appendix:  [Proof of the Zonklar Equations]   or      for no appendix heading   do not use 
", %\boldmath %The abstract goes here. %,280
" % COMPLETED.   represent structured collections of facts describing the world in the form of typed relationships between entities. These collections of facts have been used in a wide range of applications including Web search), cancer research, and even entertainment. However, most  on the Web are far from being complete. For instance, the birth place of  of the persons in Freebase and  of the persons in DBpedia is not to be found in the respective . In addition, more than  of the scientists in DBpedia are not linked to the predicate that describes what they are known for. Identifying such missing links is referred to as link prediction.  approaches map  to continuous vector spaces and have been proven to be highly effective and efficient at addressing the task of link prediction.   In this paper, we propose \approach, a simple but effective new  approach. \approach is a complex-valued convolutional neural model that learns complex-valued vector representations of a given  by combining a 2D convolution operation with a Hermitian inner product. The motivation behind our approach lies in the following considerations:       have demonstrated recognition accuracy better than or comparable to humans in several visual recognition tasks, including image recognition, object detection and semantic segmentation. Parallel to the successful application of  in computer vision,  leverages a multi-layer  for learning continuous vector representations of  and reaches a state-of-the-art performance in link prediction.     %  have shown its usefulness in various applications --including faster learning on memorization tasks, learning equivariant 3D-rotation for cloud processing.      has been proven to be an effective technique for link prediction.   We evaluate our approach against 37 state-of-the-art approaches on four benchmark datasets often used in the literature. Overall, our results suggest that \approach outperforms current state-of-the-art approaches , in terms of  and Hits at N .% -- standard measures for the link prediction task.  %structure of the paper % The rest of this paper is structured as follows: % We provide an overview of the state of the art in   in~\Cref{sec:related work}. Thereafter, the notation and the preliminaries are presented in~\Cref{sec:preliminaries}. Next, we introduce \approach in~\Cref{sec:approach}. In~\Cref{sec:experiments}, we explicate the research question and experimental settings.~\Cref{sec:results} reports the results of conducted experiments. Finally, we conclude with a discussion in ~\Cref{sec:conclusion}.         The superior performance of \approach stems from the composition of a 2D convolution with a Hermitian inner product of complex-valued embeddings. Applying 2D convolution on complex-valued embeddings of subjects and predicates permits \approach to recognize interactions between subjects and predicates in the form of complex-valued feature maps. Through the projection of feature maps and their inclusion into a Hermitian inner product involving the conjugate-transpose of complex-valued embeddings of objects, \approach can accurately infer various types of relations. For instance, \approach is able to model composition patterns without defining the bijection mapping explicitly. This ability is suggested by since WN18RR and FB15K-237 involve antisymmetric and composite relations. Moreover, shows that \approach requires significantly fewer parameters than RotatE, QuatE and TuckER than WN18RR and FB15K-237. and two more tables in the supplemental material explicitly show that \approach is able to capture various types of relations on benchmark datasets. However, \approach inaccurately ranks entities with \_member\_of\_domain\_region and \_member\_of\_domain\_usage. This may indicate that \approach is not able model triples where subjects and objects are loosely semantically related. Overall, \approach is more expressive than approaches that solely apply 2D convolution in   and solely apply inner products in  by Hermitian Inner Products .         Completed.  In this work, we introduce a new approach  for addressing the link prediction problem by learning continuous vector representations for knowledge graphs. \approach accurately infers the various types of relations by leveraging a composition of a 2D convolution with a Hermitian inner product of complex-valued embeddings. \approach achieves state-of-the-art performances on standard link prediction datasets while requiring fewer parameters than several state-of-the-art approaches---including QuatE, RotatE and TuckER. In future work, we plan to explore combining 2D convolution with Hamilton閳ユ獨 Quaternions.             
"," In this paper, we study the problem of learning continuous vector representations of knowledge graphs for predicting missing links. We present a new approach called \approach, which infers missing links by leveraging the composition of a 2D convolution with a Hermitian inner product of complex-valued embedding vectors. We evaluate \approach against state-of-the-art approaches on the WN18RR, FB15K-237, KINSHIP and UMLS benchmark datasets. Our experimental results show that \approach achieves a  performance superior to that of state-of-the-art approaches such as RotatE, QuatE and TuckER on the link prediction task on all datasets while requiring at least 8 times fewer parameters. We ensure the reproducibility of our results by providing an open-source implementation which includes the training, evaluation scripts along with pre-trained models at {\url{https://github.com/conex-kge/ConEx}.}   %",281
" Bipolar disorder  is a recurrent chronic mental health condition which occurs in approximately 1\% of the global population . It is characterised by episodes of low and high mood which cause significant interference with everyday life. Borderline personality disorder  is characterised by a long-term pattern of constantly variable mood, self-image and behaviour. Although BD and BPD are two very different conditions they share some similar symptoms such as mood instability and impulsive behaviour . A recent study  reported the high prevalence of comorbidity between the two conditions, with up to 21.6\% of individuals with BD found to have comorbid BPD. As a result they can be difficult to distinguish, but accurate diagnosis is crucial as they require different treatment . Standard diagnostic assessment involves a psychiatrist asking a series of questions about symptoms and the person has to retrospectively describe their account of these symptoms. The success of the assessment also relies on how the psychiatrist interprets both the verbal and non-verbal cues drawn from the person's responses. In this work, we aim to develop a method that extracts cues automatically from interviews conducted in a non-clinical setting, to assist the existing assessment framework, which is expensive and subjective.  Recent studies have explored data driven approaches to automatically screen patients, incorporating features extracted from multiple modalities in clinical interviews, showing diagnostic value for mental health conditions such as depression and bipolar disorder .  finds the performance of automatic mood detection to be much better in clinical interactions than in personal conversations, and there are significant differences in the features important to each type of interaction. While existing studies of BD  have focused on recognising mood episodes, the distinction between BD and BPD remains understudied. In this paper, we aim to bridge this gap by presenting a multi-modal  dataset containing interviews in a non-clinical setting involving individuals with a diagnosis of BD or BPD, and study the automatic assessment of the two mental health conditions.   Motivated to study the interaction between the interviewer and participant during the course of an interview from different aspects , we investigate features extracted from different modalities. Path signatures, initially introduced in rough path theory as a branch of stochastic analysis, has been shown to be successful in a range of machine learning tasks involving modelling temporal dynamics . We propose to apply path signatures for summarising features extracted from each utterance, sentence and speaker-turn into interview-level feature representations, given its ability to naturally capture the order of events. By doing so, we automatically include more non-linear prior knowledge in our final feature set, which leads to effective classification, even with a simple linear classifier.   The contributions of this work are as follows:  We present a new non-clinical interview dataset involving BD and BPD patients; , We investigate different feature types and propose using path signatures as a novel approach of summarising turn-level features;  We demonstrate a good linear model can be learnt for three classification tasks, and provide insights into the distinction between BD and BPD by analysing the importance of the selected features.      In this paper, we demonstrate the potential of using features extracted from language and speech in non-clinical interviews to assist the assessment of bipolar disorder BD and borderline personality disorder BPD, which is challenging for clinicians to distinguish.    It is crucial to diagnose the two conditions accurately so the patients can have appropriate treatment. While many machine learning based studies learn from clinical interviews to automatically screen mental health conditions, the detection of BD and BPD is still understudied. We first presented a non-clinical interview dataset, named AMoSS-I, conducted partially by psychology graduates, for the task of detecting BD and BPD. We demonstrated good performance in three classification tasks using down-selected features and a new way of summarising these features based on path signatures. Lastly, we showed the importance of linguistic features in all three tasks and the benefits of feature fusion from different modalities. For future work, we plan to learn acoustic features, and investigate the effect of acoustic properties of the interviews and the impact of recording environments.   
"," Bipolar disorder  and borderline personality disorder  are both chronic psychiatric disorders. However, their overlapping symptoms and common comorbidity make it challenging for the clinicians to distinguish the two conditions on the basis of a clinical interview.  % Recent studies have explored data driven approaches to automatically screen patients, incorporating features extracted from clinical interviews, showing diagnostic value for mental health conditions such as depression and bipolar disorder.  In this work, we first present a new multi-modal dataset containing interviews involving individuals with BD or BPD being interviewed about a non-clinical topic . We investigate the automatic detection of the two conditions, and demonstrate a good linear classifier that can be learnt using a down-selected set of features from the different aspects of the interviews and a novel approach of summarising these features. Finally, we find that different sets of features  characterise BD and BPD, thus providing insights into the difference between the automatic screening of the two conditions.",282
" Mining topics from texts is significant for various applications of natural language processing, e.g., text classification, sentiment analysis, and recommender systems. As one of the most popular approaches for discovering latent topics, topic modeling  is capable of producing interpretable results. Generally, the dominant methods for parameter estimation in topic models are variational inference  and Gibbs sampling , both of which, however, require complex re-derivation when there is any minor changes to the model structure. Moreover, with the growth of data scale, the generative process is getting tricky and expensive, which leads to mathematically arduous derivation and high computational cost in training. These limitations make it difficult to extend the models to new variations flexibly.  With the development of deep learning, variational auto-encoder   has provided another promising solution for topic modeling. Benefiting from the flexibility of neural networks, the VAE framework is competent to learn complicated non-linear distributions and is convenient to be applied to various tasks. Furthermore, by using the back-propagation for optimization, VAE is highly efficient in training when compared with the models based on variational inference or Gibbs sampling. Considering the above advantages, several models built on VAE have been proposed, such as neural variational document model  , neural variation latent Dirichlet allocation  , Gaussian softmax model  , Dirichlet variational auto-encoder  , and neural variational correlated topic modeling  . Although the VAE-based models reduce the computational cost impressively, they still suffer from the feature sparsity problem in short texts. In this case, the number of word occurrences in each text is relatively small, while the vocabulary corresponding to the corpus is large and the range of topics is broad.  To alleviate the above issue, many Bayesian approaches specific to short texts have been proposed . Nonetheless, the above models all resort to Gibbs sampling or variational inference and hence incur the problems as mentioned before. In recent years, models built on VAE are also introduced for short texts, such as Graph-based inference network for the biterm topic model   and neural sparsemax topic model  . However, learning context information is still challenging in these models due to significant word non-overlap in short texts. Relatedness information between word pairs may not be fully captured owing to the lack of word-overlap between such short messages.  In this paper, we propose a VAE-based topic model for short texts, where the context information for each text is effectively enhanced. Firstly, as can be observed, a short text generally covers only a subset of topics due to the limited text length. Therefore, we propose to filter irrelevant topics by setting a  for each topic, encouraging each short text to focus on some salient topics. Through this way, the topic inference range is narrowed down and thus the topic sparsity can be achieved indirectly. Secondly, we incorporate pre-trained word embeddings into our model to explicitly enrich the context information. Specifically, we model each topic by a multivariate Gaussian distribution or a Gaussian mixture distribution in the embedding space, through which the relatedness of synonymous word pairs can be effectively inferred regardless of word non-overlap in short texts. In this way, our model can discover more interpretable topics than other topic models. We name the proposed model as Context Reinforced Neural Topic Model  and conclude the main contributions of our work as follows:   for each topic to filter irrelevant topics, CRNTM narrows down the topic inference space and achieves topic sparsity indirectly.   The rest of this paper is organized as follows. We discuss relevant research work in Section , and detail our proposed model in Section . Experimental settings and results are presented in Section . Finally, we draw the conclusion in Section .     In this paper, we propose a Context Reinforced Neural Topic Model  to address the feature sparsity problem in short texts. By introducing a  to the inference network, CRNTM infers the topic for each word in a narrow range. Besides, pre-trained word embeddings are incorporated with multivariate Gaussian distributions or Gaussian mixture distributions into our model to enrich the context information of short messages. To quantitatively validate the effectiveness of CRNTM, we conduct various experiments on two benchmark datasets in terms of perplexity, topic coherence, and text classification accuracy. The results indicate that the proposed model largely improves the performance of topic modeling by enriching the context information effectively.  
"," As one of the prevalent topic mining tools, neural topic modeling has attracted a lot of interests for the advantages of high efficiency in training and strong generalisation abilities. However, due to the lack of context in each short text, the existing neural topic models may suffer from feature sparsity on such documents. To alleviate this issue, we propose a Context Reinforced Neural Topic Model , whose characteristics can be summarized as follows. Firstly, by assuming that each short text covers only a few salient topics, CRNTM infers the topic for each word in a narrow range. Secondly, our model exploits pre-trained word embeddings by treating topics as multivariate Gaussian distributions or Gaussian mixture distributions in the embedding space. Extensive experiments on two benchmark datasets validate the effectiveness of the proposed model on both topic discovery and text classification.",283
" Knowledge Based Systems:\\ Systems that incorporate human expertise for making decisions are knowledge-based systems . Traditionally a knowledge-based system consists of a knowledge base which is data suitably collected and organised by human experts in various fields, inference engine - that relies on the knowledge base for decision making, a working memory to handle operations. The inference engine can be rule-based, case-based, etc.\\ Deep Neural Networks:\\ Deep neural networks, on the other hand, is more about statistical modelling that relies on massive amounts of data to find statistical patterns, non-linear relationships to be able to match the prediction patterns from a given training set. It relies on these patterns to infer conclusions about new data as well.\\  Time-series models:\\ A Recurrent neural network is a class of neural network that deals with the prediction of temporal sequences. Long-Short term memory , Gated Recurrent Units are some of the Recurrent neural network architectures that are used for time series forecasting.\\\\ Sequence to Sequence models:\\ Sequence to sequence models aims to translate a fixed-length input sequence to a fixed-length output sequence where the length of the input and output may differ. It mainly has three parts: the encoder, intermediate vector and the decoder. In the encoder, several stacks of recurrent units  are combined such that each unit accepts an input element from the sequence and propagates it, thus forming an intermediate hidden state. This information in the hidden state is consumed by the decoder part of the network that in turn consists of sequences of recurrent units that produce a sequence of outputs.  Although the Deep neural networks have shown promising performance in several fields, there exist areas like interpretability, reasoning in which they lack and hence needs attention. On the other hand, Expert Systems are built on top of the characteristics which the Deep neural networks lack. Hence there can be ways where we can leverage the strengths of both systems by various principles. This paper discusses some of the techniques of integrating expert knowledge to Deep Neural Networks to attain a kind of synergy between them.      In this paper, we discussed some of the techniques for integrating expert knowledge in the form of First-order Logic Rules, tuples, embeddings etc with the neural network for time-series and sequence-to-sequence models. While each technique has its own set of pros and cons, it would be optimal to come up with a scalable technique that can incorporate the positive aspects of the above-discussed techniques.     
"," In recent years, with the advent of massive computational power and the availability of huge amounts of data, Deep neural networks have enabled the exploration of uncharted areas in several domains. But at times, they under-perform due to insufficient data, poor data quality, data that might not be covering the domain broadly,  etc. Knowledge-based systems leverage expert knowledge for making decisions and suitably take actions. Such systems retain interpretability in the decision-making process. This paper focuses on exploring techniques to integrate expert knowledge to the Deep Neural Networks for sequence-to-sequence and time series models to improve their performance and interpretability.",284
" A heuristic approach to automated test case generation  from formal requirements specifications known as  {. The reliability of LBTest for producing correct test results depends crucially on the correctness of this learning algorithm. So we give a formal definition of  IKL and prove its correctness. The IKL algorithm involves a number of optimisations necessary to achieve scalability of testing for large software systems.  We discuss these optimisations from the perspective of learning and testing.  The problems of coverage, and termination criteria for black-box testing, are complex and different solutions have been proposed.  In LBT, convergence of learning can sometimes be used as a criterion to terminate testing. However, heuristics are needed  to estimate convergence in the context of black box testing. We will empirically evaluate the reliability of a simple heuristic for IKL.   In the remainder of Section 1, we discuss the general paradigm of LBT, and specific requirements on learning for  efficient testing of reactive systems. In Section 2, we review some essential mathematical preliminaries. In Section 3, we present the  architecture of the IKL learning algorithm and its main components. These three main components are  defined and analysed in detail in  Sections 4, 5 and 6. In Section 4, we consider a learning algorithm for families of DFA which  supports  incremental learning and projection . In Section 5, we consider integrating a family of DFA  into a single Kripke structure using a subdirect product construction.   In Section 6, we consider an efficient minimisation algorithm for  deterministic Kripke structures based on Hopcroft's DFA minimisation algorithm . This is needed by the IKL algorithm to produce hypothesis models that can be efficiently model checked.  In Section 7, we empirically evaluate a black box heuristic to detect convergence of IKL, that can be used as a test termination criterion. Finally, in Section 8 we draw some conclusions and suggest prospects for further research on learning and testing.     The basic LBT paradigm requires three components: \vskip 4pt SSo_{n+1}S does not satisfy ) then  was a {M_ni_{n+1} is not wasted. We return to Step 1 and apply the learning algorithm once again to  pairs  to infer a refined model  of . \vskip 4pt o_{n+1}S1 \dotsM_0M_1M_2\ldotsSni_{n+1}i_{n+1}\ReqL by  should reuse as much information as possible about the previous approximation  .  Incremental learning algorithms are necessary for two reasons. \vskip 4pt  of an SUT for testing a requirement  raises the question of the  relative efficiency of different types of queries . We have already seen that in LBT, test cases can be generated by  model checking, by active learning, or by some other process entirely such as random querying.  As indicated in  above, the overhead of SUT execution time to answer an individual query can be large compared with the execution time of learning and model checking. There are examples of industrial systems where this execution time is of the order  of minutes. So realistically, queries should be seen as ``expensive''. From the viewpoint of relevance therefore, as many queries as possible should be derived from model checking the hypothesis automaton, since these queries are all based on checking the requirements . Conversely as few queries as possible should be derived from the active learning algorithm.  %.  Active learning queries have no way to reference the requirement , and therefore can only uncover an SUT error by accident.  Furthermore, active learning queries may explore parts of the SUT which are irrelevant to checking , thereby  leading the search for errors in a fruitless direction. Ideally, { When we consider the output variables of the SUT  that appear in a specific formal black box requirement , we often see just a small subset of the set of all output variables of .  This observation points to a powerful abstraction technique for learning that can be termed  {.   Like incremental learning, projection is another abstraction method  that concentrates on learning only the relevant SUT behavior needed to test the requirement . Essentially, projection  involves learning a quotient model of the SUT by observing just the output variables appearing in . Since quotient models of  may be dramatically smaller than  itself, the time needed for learning and testing may be considerably reduced.  Therefore, projection seems to be an essential component of a scalable LBT system. Indeed, the combination of incremental learning  and projection seems to be particularly powerful. The IKL algorithm incorporates  both these features, and they will be discussed in further detail in Sections 3 and 4.   Several previous works,  have considered a combination of learning and model checking to achieve testing and/or formal verification of reactive systems.  Within the model checking community the verification approach known as  {  and other abstraction techniques specifically chosen to achieve scalable testing and faster error discovery .  In practise, most of the well-known classical regular inference algorithms such as L*  or ID   are designed for complete rather than incremental learning. Among the much smaller number of known incremental learning algorithms, we can mention the RPNII algorithm  and the IID algorithm  which learn Moore automata, and the ICGE algorithm   which learns Mealy automata over abstract data types. No algorithm which combines incremental learning and projection has been published in the literature. The problem of integrating active learning queries with model checker generated  queries  has also not been considered.  Thus:  the {, and  its {)\mathcal O.  Our generalisation of Hopcroft's DFA minimisation algorithm to deterministic Kripke structures in Section 6 is fairly simple and straightforward. Nevertheless, this algorithm has not been previously published in the literature, and represents another novel contribution.      We have defined and analysed a learning algorithm IKL for deterministic Kripke structures which is efficient for applications in software testing.  This algorithm extends active incremental learning with new features such as lazy learning and projection.  We have formally proved the correctness of the IKL algorithm and its main components. We have also empirically evaluated a black box heuristic for detecting convergence of learning, which can be used to terminate testing for small systems under test.   Incremental learning and projection combine to make IKL scalable to larger systems under test. Also, incremental and lazy learning combine  to support frequent generation of hypothesis automata with which we can discover SUT errors much faster than random testing by model checking. These claims have been empirically evaluated and supported  in  and . The IKL algorithm has been implemented in the LBTest tool  for learning based testing of  reactive systems.     We believe that the efficiency of learning-based testing can be even further improved by more research on model inference.  For example, the modular architecture of the IKL algorithm can support experiment with other incremental DFA learning algorithms  instead of  the ID learning algorithm of Section 4, . The impact of the frequency of hypothesis automata generation on testing efficiency could then be further  investigated. When hypothesis generation is very frequent the overhead of model checking is high, and this overhead can slow down the entire  LBT process. However, if generation is very infrequent, then little use is made of the model checker  to conduct a  directed search for SUT errors using queries that can falsify the user requirements. This is also inefficient.  More generally, we could consider an optimal tuning of the rate of hypothesis automata generation, e.g. based on the estimated density of SUT errors.  The relationship between computational learning and software testing has been a fruitful line of research ever since Weyuker's thesis . Many fundamental questions remain within the context of learning-based testing.  For example, the execution of any automata learning algorithm can always be associated with a {	   		  expects file ""myrefs.bib""   
"," Learning-based testing  is an emerging methodology to automate iterative black-box requirements testing of software systems. The methodology involves combining model inference with model checking techniques. However, a variety  of optimisations on model inference are necessary in order to achieve scalable testing for large systems.  In this paper we describe the IKL learning algorithm which is an active incremental learning algorithm for deterministic Kripke structures.  We formally prove the correctness of IKL. We discuss the optimisations it incorporates to achieve scalability of testing. We also evaluate a black box heuristic for test termination based on convergence of IKL learning.",285
"  	  Since early attempts that pretrain a backbone model  on large-scale dataset  and then transfer the knowledge to numerous computer vision tasks, pretraining has become a hallmark of the success of deep learning. More recently, the volume of transformer-based and Bert-style pretraining models  has grown tremendously in the research field of natural language processing and has achieved state-of-the-art performance in various NLP tasks. Likewise, the success of Bert-style pretraining techniques has been transferred to the research field of the intersection of vision and language .    %--------------------------------fig-------------------------  %--------------------------------fig end---------------------  Despite the significant progress that recent methods have made over the initiative work ViLBert , part of their success can be traced back to the introduction of in-domain pretraining datasets besides the Conceptual Caption  dataset. By in-domain, we refer to those datasets used in both pretraining and downstream tasks, such as MSCOCO , and Visual Genome .  However, out-of-domain pretraining,  datasets and transferring the learned knowledge into downstream tasks with unkown data distributions, can be an essential research topic.  In this paper, we focus on out-of-domain pretraining and learning generic representations as the ViLBert does.      A fundamental requirement for out-of-domain transfer learning is to mitigate the biases from the pretraining data , which may be useful for the in-domain testing but harmful for out-of-domain testing  due to the spurious correlation .  To verify such existence of the correlation biases, we follow  to conduct a toy experiment on Conceptual Caption dataset. We observe that the conditional probability of   given the   is large, |\textrm{instrument}) = 5.98\% can be adjusted to   with a do operator.  The essence of deconfounding is to control the condition  from being affected by other potential confounders when assessing the effect on the outcome  given the condition, .  In this way, the pure association-based pretraining becomes to the causal intervention-based pretraining.  We note that our goal is not performing theoretically causal inference but learning generic and de-biased visio-linguistic representations that can well generalize to downstream tasks with unknown data distributions.   We are particularly targeting at the Bert-style pretraining models and the context-based proxy tasks for supervision, such as masked language/object modeling . Context-based proxy tasks solely care about association, , which refers to 	extbf{Deconfounded Visio-Linguisitic Bert}. DeVLBert is designed as model-agnostic and can be easily encapsulated into any other Bert-style models.    We conduct in-depth experiments to discuss the performance of the proposed DeVLBert architectures.  Pretraining is performed on the Conceptual Caption dataset which most downstream tasks are not built on,  	} and discuss the empirical performance on several downstream tasks. The advantages of the DeVLBert are demonstrated by quantitative experiments, ablation studies, and case studies.   	  	               In this paper, we propose to mitigate the spurious correlations for out-of-domain visio-linguistic pretraining. The fact that each output token is connected with all input tokens in Bert, and the pure association nature of masked token modeling objective makes the problem more severe. We borrow the idea of back-door adjustment to propose four novel Bert-style architectures as DeVLBert for out-of-domain pretraining. We conduct extensive quantitative evaluations as well as ablation studies to discuss the empirical effectiveness of different architectures. The results show that DeVLBert can achieve promising numerical results compared to the baseline and even some in-domain visio-linguistic pretraining methods.   
","  In this paper, we propose to investigate the problem of out-of-domain visio-linguistic pretraining, where the pretraining data distribution differs from that of downstream data on which the pretrained model will be fine-tuned. Existing methods for this problem are purely likelihood-based, leading to the spurious correlations and hurt the generalization ability when transferred to out-of-domain downstream tasks. By spurious correlation, we mean that the conditional probability of one token  given another one can be high  without robust  relationships between them. To mitigate such dataset biases, we propose a Deconfounded Visio-Linguistic Bert framework, abbreviated as DeVLBert, to perform intervention-based learning. We borrow the idea of the backdoor adjustment from the research field of causality and propose several neural-network based architectures for Bert-style out-of-domain pretraining. The quantitative results on three downstream tasks, Image Retrieval , Zero-shot IR, and Visual Question Answering, show the effectiveness of DeVLBert by boosting generalization ability.",286
"  % Thanks to the development of generative modeling, algorithmic music generation is made possible. % In recent years, instead of relying on rule-based systems or plain time-series analysis, we have seen work using recurrent networks or an attention-based model to generate music that is comparable to a human professional. % Despite the capacity of these models, the lack of interpretability remains as the main obstacle for controllable music generation ---in particular polymonic music with much richer structures.  With the development of artificial neural networks, deep learning has become one of the most popular techniques for automated music generation. In particular, we see recurrent and attention-based models being able to generate creative and human-like music without heavily handcrafted rules . %compared to traditional time-series models and rule-based algorithms. % 鏉╂瑩鍣风拠纾l濮ｆ敂ulebase瀵缚顫eviewer閺璇插毊娴滃棴绱濇潻娆愮壉鐠囧瓨娲跨广垼顫囬妴  However, the main drawback of these deep generative models is that they behave like ``black boxes閳, and it is difficult to interpret the musical meaning of their internal latent variables . Consequently, it remains a challenging task to control the generation process . This limitation restricts the application scenario of the powerful deep generative models.   In this paper, we improve the model interpretability for music generation via constrained representation learning. Inspired by the content-style disentanglement idea , we enforce the model to learn two fundamental factors of polyphonic music: chord  and texture . The former refers to the representation of the underlying chord progression, and the latter includes chord arrangement, rhythmic pattern, and melody contour. The current design focuses on learning 8-beat long piano composition segments under a variational autoencoder  framework.   The core of the model design lies in the encoder. We incorporate the encoder with two inductive biases for a successful chord-texture disentanglement. The former applies a rule-based chord recognizer and embeds the information into the first half of the latent representation. The latter regards music as 2-D images and uses a chord-invariant convolutional network to extract the texture information, storing it into the second half of the latent representation. As for the decoder, we adopt the design from PianoTree VAE , an architecture that can reconstruct polyphonic music from the latent representation in a hierarchical manner.  We further show that the interpretable representations are general-purpose, empowering a wide spectrum of controllable music generation. In this study, we explore the following three scenarios: [leftmargin=*, itemsep=0pt, parsep=1ex]      by swapping the chord and texture factors of different pieces of music, which can help us re-harmonize or re-arrange a music piece following the style of another piece.      by sampling the texture factor while keeping the chord factor, which is analogous to the creation of ``Theme and Variations閳 form of composition. %      by interpolating the latent space of chord representation while keeping the texture. This task is closely related to the ``conceptual blending''  idea in harmony analysis.      by predicting the texture factor given the melody using a downstream encoder-decoder generative model. % This task is similar to the creation of ``cover songs閳.   In sum, the contributions of our paper are as follows: [leftmargin=*, itemsep=0pt, parsep=1ex,topsep=0pt]%,topsep=0pt,parsep=0pt,itemsep=0pt,partopsep=0pt]            In conclusion, we contributed an effective algorithm to disentangle polyphonic music representation into two interpretable factors, chord and texture, under a VAE framework. Such interpretable representations serve as an intuitive human-computer co-creation interface, by which we can precisely manipulate individual factors to control the flow of the generated music. In this paper, we demonstrated three ways to interact with the model, including compositional style transfer via swapping the latent codes, texture variation by sampling from the latent distribution, accompaniment arrangement using downstream conditional prediction, and there are potentially many more. We hope this work can shed light on the field of controllable algorithmic composition in general, especially on the paradox between model complexity and model interpretability.  We acknowledge that the learned music factors are still very basic. In the future, we plan to extract more abstract and longer-range features using hierarchical models. We also plan to explore more ways to control the music generation for practical usage.                         For bibtex users: 
"," % While deep generative modeling has become promising in many domains, it remains a challenging task to algorithmically compose polymeric music,  essentially hindered by its rich structure. % Inspired by the recent work of disentanglement of factors of variations, we develop a novel architecture, under the VAE framework, that not only disentangles the chord and texture of an input polymeric segment, also provides a generation pathway leading to plausible music style transfer and analogy. % Through a wide spectrum of task validations, we show that the chord-texture resulted from our model enables several tasks including compositional style transfer, texture variation, chord progression interpolation and accompaniment arrangement. % By both automatic metrical and human-based evaluation, our method achieves the state-of-the-art quality on the music generation. While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation.\!\!\footnote{Code and demos can be accessed via \url{https://github.com/ZZWaang/polyphonic-chord-texture-disentanglement}}",287
"  Humans learn to use language  over the course of their lives from the interactions they have with the world and other people. Yet, the prevailing dominant paradigm in natural language processing  research is to build a fixed dataset from which to train a model and then freeze it, without any ability for the model to interact with humans using language at training time at all. While we need such  interaction in order to study human-machine communication to its full extent, constraints usually inhibit such research.  Firstly, conducting such experiments can be costly.  %, %for example research budgets for paying crowdworkers mean that data will have a limit. Many datasets in NLP are collected with crowdsourcing, whereby one pays the crowdworkers to perform interaction and annotation tasks. This leads to several issues, not least that  research budgets for paying crowdworkers mean that data will have a limit. %collecting a large amount of data is difficult  %Secondly, distribution as they are only motivated by money, not by actual interest in the dialogues themselves. Secondly, as crowdworkers are motivated by pay, not by interest in the actual tasks themselves, the data distribution may not match the desired one .  In this work we study the ability of an open-domain.     %We show that our iterative collection-retraining/redeployment % open source everything   %Finally, it is considerably more challenging to engage unpaid humans to provide high quality dialogue when conversing with dialogue models.  %* *Never-Ending Learning:  show it閳ユ獨 improving* %    * future of ML/AI/NLP is not fixed datasets, but continual interactive learning %* game with a purpose    %* Side points: %    * Price )  %    * Distribution  %    * Deployment leads to collecting data, and natural place to evaluate and compare models %    * games as an ideal testbed for AI, w/ rich human interaction, grounding, sandbox %    * while things like alexa challenge do allow a fully deployed system, because of the proprietary nature and other privacy concerns, that research is not open and reproducible. %Collect data, evaluate models. %     [ht!] %        We have presented a fully realized system for improving  upon an open-domain dialogue task  open-domain dialogue  by utilizing a deployed game for lifelong learning.   Detailed experiments showed that the one can collect high quality data that improves both automatic offline metrics and user engagement metrics when used for training models. We find this exciting because this approach shows it is possible to build continually improving models that learn from interacting with humans in the wild , which represents a paradigm shift away from the limited static dataset  setup that is prevalent in much of the work of the community.    Future work should study the resulting publicly released data to explore other methods of  lifelong learning,  or other learning signals that could be extracted from human utterances, for example  the ideas in .  Another possible direction, for when model performance begins to saturate, is to exploit control of the game engine itself to emphasize learning on the most difficult cases or the ones with the most learning signal, such as in the work on adversarial collection .  Finally, our role-playing setup can also be applied more generally, for example incorporating both dialogue and actions,  situated in other domains.     
"," Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance.  As argued in , crowdsourced data has the issues  of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language . % %We posit that, in order to overcome these issues, machine learning must develop systems where models continually improve by interacting with humans and the world. % % In order to overcome these issues, machine learning must develop systems where models continually improve by interacting with humans and the world.  In contrast, one might hope for machine learning systems that become more useful as they interact with people. % In this work, we build and deploy a %  role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in  the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.  %We are releasing the models and data from this work.",288
"  	Recently, deep learning has witnessed a great process . 	Video question answering  has become an emerging task in computer vision and has drawn increasing interests over the past few years due to its vast potential applications in artificial question answering system and robot dialogue, video retrieval, etc. In this task, a robot is required to answer a question after watching a video. 	Unlike the well-studied Image Question Answering  task which focuses on understanding static images, video QA is more practical since the input visual information often change dynamically, as shown in Figure . 	 	 	 	 	 	Compared with image QA, video QA is much more challenging due to several reasons.  Visual content is more complex in a video since it may contain thousands of frames, as shown in Figure . More importantly, some frames may be dominated with strong background content which however is irrelevant to questions.  Videos often contain multiple actions, but only a part of them are of interest to questions. 	 Questions in video QA task often contain queries related to temporal cues, which implies we should consider both temporal location of objects and complex  interaction between them for answer reasoning. For example in Figure , to answer the question ``What does the man do before spinning bucket?"", the robot should not only recognize the actions ``spin laptop'' and ``spin bucket'' by understanding the interaction between the man and objects  for answer reasoning along time axis. 	   	 	 	Taking video frames as inputs, most existing methods  employ some spatio-temporal attention mechanism on frame features to ask the network ``where and when to look''. 	However, these methods are often not robust due to complex background content in videos. 	% However, extracting features from the whole frame makes the model be prone to over-fit the background content .  	Lei et al.  tackle this problem by detecting the objects in each frame and then processing the sequence of object features via an LSTM. However, the order of the input object sequence, which may affect the performance, is difficult to arrange. 	More importantly, processing the objects in a recurrent manner will inevitably neglect the direct interaction between nonadjacent objects. This is critical for video QA . 	 	In this paper, we introduce a simple yet powerful network named Location-aware Graph Convolutional Networks  to model the interaction between objects related to questions. We propose to represent the content in a video as a graph and identify actions through graph convolution. 	% we propose to explicitly detect the salient objects in videos and model their relationship through constructing a fully-connected graph.  	Specifically, the objects of interest are first detected by an off-the-shelf object detector. Then, we construct a fully-connected graph where each node is an object and the edges between nodes represent their relationship.  	We further incorporate both spatial and temporal object location information into each node, letting the graph be aware of the object locations. When performing graph convolution on the object graph, the objects directly interact with each other by passing message through edges. Last, the output of GCNs and question features are fed into a visual-question interaction module to predict  a answer.  	Extensive experiments demonstrate the effectiveness of the proposed location-aware graph. We achieve state-of-the-art results on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets. 	 	The main contributions of the proposed method are as follows:  we propose to explore actions for video QA task through learning interaction between detected objects such that irrelevant background content can be explicitly excluded;  we propose to model the relationships between objects through GCNs such that all objects are able to interact with each other directly;  we propose to incorporate object location information into graph such that the network is aware of the location of a specific action;  our method achieves state-of-the-art performance on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets. 	  	 	 	  	 	In this paper, we have proposed a location-aware graph to model the relationships between detected objects for video QA task. Compared with existing spatio-temporal attention mechanism, \algname is able to explicitly get rid of the influences from irrelevant background content. Moreover, our network is aware of the spatial and temporal location of events, which is important for predicting correct answer.  	Our method outperforms state-of-the-art techniques on three benchmark datasets.   	, including TGIF-QA, Toutube2Text-QA and MSVD-QA. 	    	
","           We addressed the challenging task of video question answering, which requires machines to answer questions about videos in a natural language form. Previous state-of-the-art methods attempt to apply spatio-temporal attention mechanism on video frame features without explicitly modeling the location and relations among object interaction occurred in videos. However, the relations between object interaction and their location information are very critical for both action recognition and question reasoning.  In this work, we propose to represent the contents in the video as a location-aware graph by incorporating the location information of an object into the graph construction. Here, each node is associated with an object represented by its appearance and location features. Based on the constructed graph, we propose to use graph convolution to infer both the category and temporal locations of an action.  		As the graph is built on objects, our method is able to focus on the foreground action contents for better video question answering.  Lastly, we leverage an attention mechanism to combine the output of graph convolution and encoded question features for final answer reasoning. 		Extensive experiments demonstrate the effectiveness of the proposed methods. Specifically, our method significantly outperforms state-of-the-art methods on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets. Code and pre-trained models are publicly available at:  % 		\textcolor{red}{\tt{https://github.com/SunDoge/L-GCN}}         \url{https://github.com/SunDoge/L-GCN}",289
"  With the rapid development of the online social network  such as Twitter and Facebook, people are more frequently using the OSN to express opinions and emotions. It provides researchers with a novel and effective way to detect the mood, communication, activity, and social behavior pattern of individuals . In the past decade, researchers in various fields have conducted quantitative analyses of different illnesses and mental disorders based on the OSN platform . Sina Weibo  is the most popular OSN in the Chinese community . A statistic shows the number of Weibo's monthly active users have reached more than 480 million in the second quarter of 2019\footnote{https://www.statista.com/statistics/941456/china-number-of-sina-weibo-users/}.  Major depressive disorder, referred to as depression, is a common mental disease. According to a survey of the World Health Organization \footnote{https://www.who.int/en/news-room/fact-sheets/detail/depression}, more than 300 million people worldwide suffer from depression. Depression can cause great psychological pain, even suicidal tendencies. Moreover, evidence from a health action plan of WHO\footnote{https://www.who.int/mental\_health/action\_plan\_2013/en/} shows that people suffering from depression are much more likely to end their life prematurely than the general population. Despite the current availability of psychotherapy, medical therapy, and other modalities for the treatment of depression, 76\%-85\% of patients in low- and middle-income countries remain untreated. The emergence of this phenomenon is not only the lack of medical resources but also the inability to make an accurate assessment in the early stage of depression, which leads to a large number of people with depression difficult to get diagnosis and treatment timely .  Pictures, text, videos, and other information posted on the OSN can reflect feelings of worthlessness, guilt, helplessness, and self-hatred, which can help researchers to specifically analyze and characterize depressed individuals . However, there are some insurmountable problems in online depression detection using traditional analyzing methods. They often focus on analyzing the characteristics of users with depression rather than constructing predictive models. Therefore, it is difficult to give timely prediction results of new depressed users. Moreover, they are incapable to deal with a large number of instant interactive user data.  With the rapid development of artificial intelligence technologies, machine learning approaches have made great contributions to the detection of depression . An automated depression detection model based on machine learning usually needs to analyze various information such as tweets, pictures, videos, social activity data of users. Then, it gives the classification results of the predicted objects, most of which are presented as a binary result of normal or depressive. If an individual is predicted for a potential depressive tendency, further resources and assistance can be provided, including later medical and psychological diagnoses. Such heuristic learning approaches are quite effective for helping in the early detection of depression  since they are capable of handling a large number of instant interactive user data.    However, current approaches to online depression detection still face many unresolved challenges.  Firstly, many current studies are not user-oriented modeling . Those works usually aim to analyze and model the language style of the user. Through sentiment analysis and feature engineering of the tweet text, a classification model is developed to detect whether a specific tweet has a depressive tendency. These works analyzed fine-grained features and achieved pretty good results. However, such results cannot be directly applied to user-level depression detection, or it may lead to an incorrect prediction.  Second, in several existing studies , the size of the dataset used for modeling is insufficient, with only a few hundred to a few thousand data samples being used. Because of the difficulty of accurately obtaining and labeling depressed samples, researchers usually choose to construct small datasets or directly cited datasets from other works. As a consequence, the trained model fails to reach good generalization performance, thus hard to accurately predict depressed users on the OSN.  Moreover, not enough studies of user depression detection have been proposed on Weibo compare to Twitter and Facebook. To the best of our knowledge, there is no published large Weibo user depression detection dataset available currently.  Finally, many of the existing proposed models still do not reach a high level of classification performance, i.e. an F1-Score of 90\% and above. Thus, these models need to be further improved to achieve better performance.    Given the above problems and challenges, we hereby summarize the contributions of our work as below:    }. WU3D includes more than 10,000 depressed users and more than 20,000 normal users, each of which contains enriched information fields, including tweets, the posting time, posted pictures, the user gender, etc. This dataset is labeled and further reviewed by professionals.    Different from some existing work that directly using the information fields as features, we made statistical analyzes of all the proposed features. These features show significant distribution differences between depressed and normal users in our experiments.    It implements a multitask learning strategy to process text-based word vectors and statistical features simultaneously. Experimental results show that it achieves both the highest classification performance and the best robustness to unbalanced training samples.   The subsequent sections of this paper are organized as follows. In Section \uppercase, related work and achievements in the field of depression detection on OSNs are introduced and analyzed. The proposed framework is elaborated in Section \uppercase. Furthermore, Section \uppercase gives the significance evaluation of statistical features and the performance comparison experiments of several classification models . At the last of the paper, Section \uppercase summarizes our work and discusses directions for future work.     In this work, we proposed a multitask learning-based approach to predict depressed users on Sina Weibo.  First, based on data collection and script filtering and manual labeling, we built and publish a large Weibo User depression detection dataset - WU3D. The total number of user samples reaches over 30,000 and each user has enriched information fields. This dataset will be quite sufficient to be used by subsequent researchers to complete further research.  Secondly, we summarized and manually extracted ten statistical features including text, social behavior, and picture-based features. The experimental results showed that all of them have varying degrees of distribution differences between normal users and depressed users, which can contribute positively to classification tasks. Our experimental results also proved that the feature engineering process of text information is the most vital part of depression detection on OSN.  Furthermore, we evaluated the performance of the pretrained model XLNet as the embedding model to solve downstream classification tasks. It showed that when the appropriate embedding length is selected, XLNet has excellent performance and efficiency in handling long text sequences.  Finally, we implemented a multitask learning DNN classifier, FusionNet, to simultaneously handle the word vector classification task and the statistical feature classification task. Benefit from the strategic advantages of multitask learning, FusionNet reduced the loss of feature information caused by transfer learning. Compared with the commonly used models in existing work, FusionNet has achieved a very significant performance improvement with an F1-Score of 0.9772 and showed the best classification robustness when the training samples are unbalanced. Thus, it has proven to be an ideal classification model when dealing with multiple classification tasks at the same time.  For future work, two directions will be further explored.  The size of the dataset will be further expanded. Larger datasets will be constructed for training and evaluating classifiers to achieve better generalization performance.  The characteristics and behavior patterns of depressed users will be further analyzed. We will propose more effective feature solutions for user-level depression detection on the OSN.       Bibliography 
"," In recent years, due to the mental burden of depression, the number of people who endanger their lives has been increasing rapidly. The online social network  provides researchers with another perspective for detecting individuals suffering from depression. However, existing studies of depression detection based on machine learning still leave relatively low classification performance, suggesting that there is significant improvement potential for improvement in their feature engineering. In this paper, we manually build a large dataset on Sina Weibo , namely Weibo User Depression Detection Dataset . It includes more than 20,000 normal users and more than 10,000 depressed users, both of which are manually labeled and rechecked by professionals. By analyzing the user's text, social behavior, and posted pictures, ten statistical features are concluded and proposed. In the meantime, text-based word features are extracted using the popular pretrained model XLNet. Moreover, a novel deep neural network classification model, i.e. FusionNet , is proposed and simultaneously trained with the above-extracted features, which are seen as multiple classification tasks. The experimental results show that FusionNet achieves the highest F1-Score of 0.9772 on the test dataset. Compared to existing studies, our proposed method has better classification performance and robustness for unbalanced training samples. Our work also provides a new way to detect depression on other OSN platforms.",290
" As an unsupervised approach, topic modelling has enjoyed great success in automatic text analysis. In general, a topic model aims to discover a set of latent topics from a collection of documents, each of which describes an interpretable semantic concept. Topic models like Latent Dirichlet Allocation ~ and its hierarchical/Bayesian extensions, e.g., in~ have achieved impressive performance for document analysis. Recently, the developments of Variational AutoEncoders  and Autoencoding Variational Inference ~ have facilitated the proposal of Neural Topic Models  such as in~. Inspired by VAE, many NTMs use an encoder that takes the Bag-of-Words  representation of a document as input and approximates the posterior distribution of the latent topics. The posterior samples are further input into a decoder to reconstruct the BoW representation. Compared with conventional topic models, NTMs usually enjoy better flexibility and scalability, which are important for the applications on large-scale data.  Despite the promising performance and recent popularity, there are several shortcomings for existing NTMs, which could hinder their usefulness and further extensions. i) The training and inference processes of NTMs are typically complex due to the prior and posterior constructions of latent topics. To encourage topic sparsity and smoothness, Dirichlet~ or gamma~ distributions are usually used as  the prior and posterior of topics, but reparameterisation is inapplicable to them, thus, complex sampling schemes or approximations have to be used, which could limit the model flexibility. ii) A desideratum of a topic model is to generate better topical representations of documents with more coherent and diverse topics;  but for many existing NTMs, it is hard to achieve good document representation and coherent/diverse topics at the same time. This is because the objective of NTMs is to achieve lower reconstruction error, which usually means topics are less coherent and diverse, as observed and analysed in~.  iii) It is well-known that topic models degrade their performance severely on short documents such as tweets, news headlines and product reviews, as each individual document contains insufficient word co-occurrence information. This issue can be exacerbated for NTMs because of the use of the encoder and decoder networks, which are more vulnerable to data sparsity.  To address the above shortcomings for NTMs, we in this paper propose a neural topic model, which is built upon a novel Optimal Transport  framework derived from a new view of topic modelling. For a document, we consider its content to be encoded by two representations: the observed representation, , a distribution over all the words in the vocabulary and the latent representation, , a distribution over all the topics.  can be obtained by normalising a document's word count vector while  needs to be learned by a model. For a document collection, the vocabulary size  can be very large but one individual document usually consists of a tiny subset of the words. Therefore,  is a sparse and low-level representation of the semantic information of a document. As the number of topics is much smaller than the vocabulary size,  is the relatively dense and high-level representation of the same content. Therefore, the learning of a topic model can be viewed as the process of learning the distribution  to be as close to the distribution  as possible. Accordingly, it is crucial to investigate how to measure the distance between two distributions with different supports. As optimal transport is a powerful tool for measuring the distance travelled in transporting the mass in one distribution to match another given a specific cost function, and recent development on computational OT  has shown the promising feasibility to efficiently compute OT for large-scale problems, it is natural for us to develop a new NTM based on the minimisation of OT.  Specifically, our model leverages an encoder that outputs topic distribution  of a document by taking its word count vector as input like a standard NTMs, but we minimise the OT distance between  and , which are two discrete distributions on the support of words and topics, respectively. Notably, the cost function of the OT distance specifies the weights between topics and words, which we define as the distance in an embedding space, where we embed all the topics and words to represent their semantics. By leveraging the pretrained word embeddings, the cost function is then a function of topic embeddings, which will be learned jointly with the encoder. With the advanced properties of OT on modelling geometric structures on spaces of probability distributions, our model is able to achieve a better balance between obtaining good document representation and generating coherent/diverse topics. In addition, our model eases the burden of designing complex sampling schemes for the posterior of NTMs. More interestingly, our model is a natural way of incorporating pretrained word embeddings, which have been demonstrated to be able to alleviate the issue of insufficient word co-occurrence information in short texts~. With extensive experiments, our model can be shown to enjoy the state-of-the-art performance in terms of both topic quality and document representations for both regular and short texts.        In this paper, we presented a novel neural topic model based on optimal transport, where a document is endowed with two representations: the word distribution, , and the topic distribution, . An OT distance is leveraged to compare the semantic distance between the two distributions, whose cost function is defined according to the cosine similarities between topics and words in the embedding space.  is obtained from an encoder that takes  as input and is trained by minimising the OT distance between  and . With pretrained word embeddings, topic embeddings are learned by the same minimisation of the OT distance in terms of the cost function. Our model has shown appealing properties that are able to overcome several shortcomings of existing neural topic models. extensive experiments have been conducted, showing that our model achieves state-of-the-art performance on both discovering quality topics and deriving useful document representations  for both regular and short texts. Thanks to the flexibility and simplicity of the framework, future work will be on developing extensions and variants that discover more complex topic patterns e.g, like Correlated Topic Models~ and Dynamic Topic Models~.      {section} {section} {section} 
"," Recently, Neural Topic Models  inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport . Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.",291
" Even before the advent of the COVID-19 pandemic, people across the world were turning to the internet to find answers to their medical concerns . Around 7\%  of Google閳ユ獨 daily searches were health related, equivalent to around 70,000 queries every minute . With the emergence of medical question-answering websites such as ADAM , WebMD , AskDocs and HealthTap , people now  have the opportunity to ask detailed questions and find answers, , that satisfied their needs. COVID-19 has done nothing but accelerate this trend. Almost every government agency and healthcare organization has tried to meet the informational need of users by building online FAQs that try to address as many COVID-related topics as possible   %With the ubiquity of the Internet and the emergence of medical question-answering websites such as ADAM , WebMD , and HealthTap , people are increasingly searching online for answers to their medical questions. Pew Internet Project surveys consistently find that between 75-83\% of internet users look online for health information .  The examples above already illustrate two important problems of any medical Q\&A collection:  there is a very large number of possible questions that can be formulated in different ways, and  it is not easy for a user to browse through a large collection of pre-existing questions to find the one that most resembles their need. A scalable solution to overcome both of these issues is to build a system that can automatically match  questions with semantically similar  questions, and provide those as suggestions to the users. If no similar answered questions exist, we can mark them as priority for experts to respond. This approach more directly satisfies user needs allowing them to use their own words to formulate the question. It also provides an avenue for collecting unanswered questions that users want answered, which is extremely important in a rapidly changing situation such as the currrent COVID-19 pandemic.  %However, the number of people asking medical questions online far exceeds the number of qualified experts -- i.e doctors -- answering them. A scalable solution to overcome this imbalance is to build a system that can automatically match  questions with semantically similar  questions, and provide those as suggestions to the users. When no similar answered questions exist, we can mark them as priority for doctors to respond. This approach uses doctor time more efficiently, reducing the number of unanswered questions and lowering the cost of providing online care.   %Many of the individuals seeking medical advice online are otherwise reluctant to seek medical help due to cost, convenience, or embarrassment. For these patients, an accurate online system is critical because it may be the only medical advice they receive. Of course, some medical problems require in-person care, and an online system must indicate that. Other patients use the internet in addition to in-person care either to determine when an appointment is needed or to follow up after visits when they have lingering questions. For this second group, if the answers they see online do not match those given to them by their doctors, they are less likely to follow the advice of their doctors , which can have serious consequences.  The problem of matching general  questions with semantically similar  questions has been well-studied in the context of online user forums , community QA  and question answer archives .  Typical approaches either assume a large amount of training data on which, either statistics can be computed or models can be learned. However, these approaches fall short when applied to the problem of medical question similarity. First, medical questions imbibe a large amount of medical information that a single word can completely change the meaning of the question. As an example, I閳ユ獡 pregnant and I believe I閳ユ獫e been infected with coronavirus. What should I know about going to the hospital?  and Should I visit the doctor if I am expecting and think I might have COVID-19? are similar questions with low overlap, but Is it safe to take Vitamin D3 supplements to build immunity against Coronavirus? and Is it safe to take Hydroxychloroquine to build immunity against Coronavirus? are critically different and only a couple of words apart. Second, there is no publicly available medical question-question similarity data at the scale where these differences can be effectively encoded in order to learn a reliable similarity function. In fact, we hypothesize that constructing such large datasets that cover the large functional space of nuanced variations in medical domain can be quite hard, and is not a scalable proposition.   %Coming up with an accurate algorithm for finding similar medical questions, however, is difficult. Simple heuristics such as word-overlap are ineffective because Can a menstrual blood clot travel to your heart or lungs like other blood clots can? and Can clots from my period cause a stroke or embolism? are similar questions with low overlap, but Is candida retested after treatment and Is Chlamydia retested after treatment? are critically different and only one word apart. Machine learning is a good candidate for such complex tasks, but requires labeled training data. As no widely available data for this particular task exists,  such as the ones shown in Table.   % \TODO{Can we at least add a COVID-19 related example?} [ht]    % {|p{0.005\textwidth}|p{0.15\textwidth}|p{0.15\textwidth}|p{0.06\textwidth}|} {p{0.005\textwidth}p{0.15\textwidth}p{0.15\textwidth}p{0.06\textwidth}} {|c}{}&\multicolumn{1}{|c}{{|c}{{|c|}{{l}{}&\multicolumn{1}{l}{{l}{{c}{    % \footnote{We acknowledge that this fails at edge cases. For instance, if the answers to two questions are both ""Yes, that is correct"", that does not mean the questions are similar.}  In this paper, we tackle the general problem of medical question-question similarity, assuming only a small amount of labeled data of similarity pairs. We also apply the general solution to a specific COVID-19 scenario  where many different questions from different sources are integrated into a user-friendly experience. Our proposed solution stems from two key insights: First, whether or not two questions are semantically similar is akin to asking whether or not the answer to one also answers the other. This means that the answers in the answered questions contain wealth of medical knowledge that can be distilled into the model. The second insight is that we can infuse this medical knowledge from the answers as a pretraining task within a language model, so that we can capture relatedness between words/concepts in the language. Recent success of pretrained bi-directional transformer networks for natural language processing in non-medical fields supports this insight . % In the examples above, the answer to the question Can clots from my period cause a stroke or embolism? will talk about, for instance, `menstrual blood'  and `bleeding' that establishes the relationships between `period' and `menstrual blood'. Similar connection can be established between heart, lungs and embolism. In contrast, the answers around candida treatment is likely to discuss about yeast while that around Chlamydia on bacteria. %The second insight is that we can infuse this medical knowledge from the answers as a pre-training task within a language model, so that we can capture relatedness between words/concepts in the language. Recent success of pre-trained bi-directional transformer networks for natural language processing in non-medical fields supports this insight .     % \TODO{Can we find a COVID-19 related example?}  %Given the recent success of pre-trained bi-directional transformer networks for natural language processing  outside the medical field , most research efforts in medical NLP have tried to apply general language models to medical tasks . However, these models are not trained on medical information, and make errors that reflect this.   Our approach stems from augmenting a general language model such as BERT, with medical knowledge by process of double fine-tuning that first distills medical knowledge using a large corpus of relevant in-domain task of . Subsequently, it fine-tunes on the available small corpus of . Our models pretrained on medical question-answer pairs outperform models pretrained on out-of-domain question similarity with high statistical significance. In particular, while other pretraining tasks yield an accuracy below 78.7\% on this task, our model achieves an accuracy of 82.6\% with the same number of training examples, an accuracy of 80.0\% with a much smaller training set, and an accuracy of 84.5\% when the full corpus of medical question-answer data is used. % Furthermore, the results show promise of generalizing to other domains as well. We present early results on extensibilty of our approach to another expert domain: question-question similarity in the context of community driven question and answer website for the Ubuntu operating system.  %The task of question-answer matching was specifically chosen because it is closely related to that of question similarity; one component of whether or not two questions are semantically similar is whether or not the answer to one also answers the other. We show that the performance gains achieved by this particular task are not realized by other in-domain tasks, such as medical question-categorization and medical answer completion.   %However, labeled training data is still one of the largest barriers to supervised learning, particularly in the medical field where it is expensive to get doctor time for hand-labeling data. }   The main contributions of this paper are:    } a dataset of medical question pairs generated and labeled by doctors that is based upon real, patient-asked questions, hereafter referred as  dataset. Some sample examples from this dataset is provided in Table.   The rest of the paper is structured as follows:  \S describes the methodology used in creating a dataset that will be made publicly available. \S provides the overview of the approach. \S describes how we used the model to build a service that matches user's COVID-19-related questions to FAQs published online. \S describes experimental details and the key results, % while \S gives a peek into application of the methodology for other domains. \S discusses related work and we end with a discussion on future work.    \TODO{Expand here slightly to align with cfp} \\  In this work, we release MQP, a dataset of medical question pairs generated and labeled by doctors that is based upon real, patient-asked questions. We also show that the double finetuning approach of pretraining on in-domain question-answer matching  is particularly useful for the difficult task of identifying semantically similar questions. Furthermore, we show that the choice of this in-domain task matters: choosing a task that provides ample signal to capture the domain knowledge is needed to be able to perform the final task well.  Although the QA model outperforms the out-of-domain same-task QQP model, there are a few examples where the QQP model seems to have learned information that is missing from the QA  model. In the future, we can further explore whether these two models learned independently useful information from their pretraining tasks. If they did, then we hope to be able to combine these features into one model with multi-task learning. An additional benefit of the error analysis is that we have a better understanding of the types of mistakes that even our best model is making. It is therefore now easier to use weak supervision and augmentation rules or even active learning to supplement our datasets to increase the number of training examples in those difficult regions of the data. Both of these improvements could further improve our performance on this task.    Lastly, we would note that such a system deployed live also needs to incorporate safety considerations with respect to identifying questions where the user's life is threatened  or might need immediate attention of a doctor .\documentclass[sigconf]{acmart}   \documentclass[sigconf, anonymous, review]{acmart} \pdfoutput=1        \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{    \providecommand\BibTeX{{                   These commands are for a PROCEEDINGS abstract or paper.   {June 03--05, 2018}{Woodstock, NY}            \usepackage{times} \usepackage{float} \usepackage{enumitem}   {0cm}   {1em} {5pt}  \usepackage[para]{footmisc}  \titlespacing{
"," People increasingly search online for answers to their medical questions but the rate at which medical questions are asked online significantly exceeds the capacity of qualified people to answer them. This leaves many questions unanswered or inadequately answered. Many of these questions are not unique, and reliable identification of similar questions would enable more efficient and effective question answering schema. COVID-19 has only exacerbated this problem. Almost every government agency and healthcare organization has tried to meet the informational need of users by building online FAQs, but there is no way for people to ask their question and know if it is answered on one of these pages. While many research efforts have focused on the problem of general question similarity, these approaches do not generalize well to domains that require expert knowledge to determine semantic similarity, such as the medical domain. In this paper, we show how a double fine-tuning approach of pretraining a neural network on  followed by fine-tuning on  is a particularly useful intermediate task for the ultimate goal of determining medical question similarity. While other pretraining tasks yield an accuracy below 78.7\% on this task, our model achieves an accuracy of 82.6\% with the same number of training examples, an accuracy of 80.0\% with a much smaller training set, and an accuracy of 84.5\% when the full corpus of medical question-answer data is used. We also describe a currently live system that uses the trained model to match user questions to COVID-related FAQs. %We also present early experimental evidence suggesting the applicability of our proposed approach on another completely different domain: question-question similarity in the context of community driven question and answer website for the Ubuntu operating system.",292
"  % Alternative first paragraph: %The goal of acoustic scene classification  is to identify the class of a given audio recording, e.g., park, office, library. The ASC task can be  very challenging because  sounds within certain scenes can have similar characteristics, and  sound events can overlap one another. The growing interest on solving the ASC problem, which is confirmed by the high participation of researchers from both academia and industry to the recent IEEE Detection and Classification of Acoustic Scenes and Events  challenge , is justified by the impact that a robust ASC system can have on several real-world applications. For instance, an hearing aid devices can modify its behaviour accordingly to different acoustic envijironments.   % If the above paragraph becomes the first paragraph, this could be reduced, and we can simply say that deep learning has greatly improved the performance of ASC. Although many different solutions have been proposed over the years, and the interested reader is referred to the official DCASE website, the key elements of a successful ASC system are  CNN,  data-augmentation,  attention,  mix-up. % Then you need to make clear the device mismatch problem has received less attention, and only a few proposal have been put forth, for example . You should clarify why this is a key problem and right away say how you want to address it Knowledge distillation. The third paragraph  look good but needs to be polished and perhaps trimmed a bit. Instead, our contribution must be make stronger. What is new in this work and why people should pay attention to it.   %The goal of acoustic scene classification  is to identify the class of a given audio recording, e.g., park, airport, metro station . The ASC task can be  very challenging because  sounds within certain scenes have similar characteristics, and  sound events can overlap one another. The growing interest on solving the ASC problem, as indicated by the high participation of researchers from both academia and industry to the recent IEEE Detection and Classification of Acoustic Scenes and Events  challenges , is justified by the impact that a robust ASC system to real-world applications. For instance, an hearing aid devices could modify its behaviour accordingly to different acoustic environments.  In recent years, we have witnessed a great progress in the acoustic scene classification  task, as demonstrated by the high participation in the IEEE Detection and Classification of Acoustic Scenes and Events  challenges . Top ASC systems use deep neural networks , and the main ingredient of their success is the application of deep convolutional neural networks  . Further boost in ASC performance is obtained with the introduction of advanced deep learning techniques, such as attention mechanism , mix-up , Generative Adversial Network  and Variational Auto Encoder  based data augmentation  , and deep feature learning . Nevertheless, those ASC systems yet do not work well when processing audios from mismatched domain, e.g., audios recorded with different devices .  Device mismatch is an inevitable problem in a real production, and it is therefore an important aspect to handle when deploying an ASC system. Indeed, a new sub-task, namely , has been added to DCASE 2018  to foster research in that  direction. The goal is to design a system that can attain a good performance on 10-second audios segments collected with target devices, which are either not represented at a development phase, or represented during the ASC system deployment with a scarce amount of training material compared to that available for the source device. However, Task1b attracted only a minor interest among DCASE 2018 and 2019 participants, and even fewer teams were directly concerned with the device mismatch issue.  %There exist a few approaches proposed to tackle the domain invariant problem in ASC. For example, multi-instance learning , and low-level or mid-level feature learning , which transfer knowledge across domains and thereby tackle the robustness issue in a  broader sense. In the literature, there exist a few approaches that tackle the domain invariant problem in ASC. For example, multi-instance learning , and low-level or mid-level feature learning , which however address the robustness issue in a broader sense.  Less approaches have instead been proposed to directly combat the ASC device mismatch issue, which is actually the focus of the present work. In particular, spectrum correction  and channel conversion  build a front-end module to convert speech features from the source domain to target domain before feeding them to the back-end classifier. Besides front-end features, mid-level feature based transfer systems, which uses bottleneck features  or hidden layer representations  are adopted to transfer knowledge from source to target domain. Adversarial training methods in  leverage an extra domain discriminator to solve the device mismatch problem although the key focus is on lack of labeled target data. %Although all of those mentioned techniques are beneficial to ASC robustness issue, there is yet a clear gap between source and target device classification results. %In this work, the device mismatch problem is investigated within the  Teacher-student  learning, also named as knowledge distillation , has recently been shown to be effective in ASC and other domain adaptation speech tasks, e.g., . The key idea is to minimizes the distance measurement between teacher and student model output distributions, i.e., the information is transferred at a soft-label level. %, namely class posterior probabilities, embedded with structure relationships among output classes, are usually used to transfer knowledge from the teacher model to student model. %In recent years, researchers further propose relational learning . It directly models the relationships between sample pairs of the teacher and student model. In ,  relational knowledge distillation  is demonstrated to improve the knowledge distillation process. RKD takes into account the relations of outputs rather than individual outputs themselves. %Independently of whether relationships among outputs is taken into account, TS methods require to be effective that soft labels are accurately generated; otherwise, the information encoded in those labels is meaningless. %Among all the TS learning methods, There is a necessary condition to get good effects, the soft label must be accurate enough, otherwise the information encoded in soft labels dose not make any senses. %As a consequence, the Unfortunately, conventional TS learning can be applied with success if:   source and target data is from the same or similar domain , or  source and target data come in pair although belong to different domains . Neural label embedding , recently proposed in , is an ingenues solution to distill knowledge across domains when neither of the aforementioned two requirements could be met. %NLE are embedding at a label level and encode the structural relationships among each pair of output classes in deep neural models. Structural relationships in turn  represent the measurements of similarity or dissimilarity among pairs of objects as distances between points in low-dimensional space. Label embedding can be viewed as the centroid of soft labels from the same class. NLE can be viewed as the centroid of soft labels from the same class. As to extension of soft labels, it encodes the knowledge distilled from the source domain and teacher model, which can then be transferred to the target domain. %%More information on how to build NLE is given in Section . In , NLE was applied to accent and children's adaptation for  automatic speech recognition.  %In this work, we extend the NLE design started in and deploy an NLE teacher-student adaptation approach to combat the ASC robustness problem in the presence of source and target device mismatch.  In this study, we extend the NLE adaptation scheme  by taking into account relationships among different acoustic scenes during adaptation. We achieve this goal by proposing a relational teacher student learning  approach based on NLE for ASC device mismatching problem. First, NLE is learned from a relatively large-size source data set, i.e., collected with the source devices. %The source device data encodes the structural relationships among different acoustic scenes. Next, ASC system is adapted to the target device leveraging upon target domain data only, i.e., teacher-student learning with unpaired data, and the set of NLE, one each per acoustic scene class. The proposed solution is assessed against the DCASE 2018 Task1b data. Experimental results confirm our intuitions and demonstrate that our adaptation technique generates a significant classification improvement on target domain data. Indeed, NLE-based TS adaptation outperforms both  multi-device training strategies, and  conventional TS adaptation schemes. Furthermore, an additional boost is obtained when TS adaptation is carried out leveraging structural information.    %In this work, to solve the device mismatching problem of the ASC systemss, we focus on the structural relationship among scene classes. We propose a novel NLE with relational teacher student learning  approach to solve the domain mismatch problem in ASC. At first, the label embedding are learned from the relatively large-size source domain data, which encode the structural relationship information of classes. Then the system on target domain data is trained with label embedding with criterion including relationship loss. Our proposed approached is evaluated on DCASE2018 task1b development data. The experimental results verify that our methods can obtain significant improvement on target domain data. And the visualization verify our arguments about the structural relationships.  %The rest of this work is organized as follows:  Section describes NLE, including generation and use. The relational TS learning framework is described in Section. Next, Section shows the experimental results and analysis. Finally, Section concludes this work.     In this paper, a relational teacher student learning framework with neural label embedding is proposed to resolve the device mismatch issue in acoustic scene classification. We explore the similarities or dissimilarities between pairs of classes. This structural relationship is learned and encoded into NLE and then transferred from the source device domain to the target device domain via the relational teacher-student approach. Our proposed framework is assessed against the DCASE 2018 Task1b development data set, and experimental results demonstrate not only the viability of our approach, but also that a significant improvement of the classification accuracy on the target device data can be obtained. Furthermore, a visual analysis is provided to shed light on the key characteristics of the proposed neural label embedding concept. \clearpage 
"," %The device domain mismatch issue is an important problem of acoustic scene classification  for real-world applications. To leverage this problem, we focus on the knowledge transfer of the inner structural relationships between each classes. A label embedding with relational teacher student learning approach is proposed. Embedded labels are learned from the source domain data, which encodes the structural relationships. Then a relational teacher student learning framework is used to transfer knowledge. Our proposed approach is evaluated on DCASE2018 task1b data set. And the experimental and visualized results successfully verify our augment and proposed method, which significantly improve the classification accuracy on target device data, with the knowledge transferred from the source device data.  %  Alternative 1: %In this work, we use a model adaptation approach based on  neural label embedding  and  knowledge distillation to combat the accuracy drop in acoustic scene classification with deep neural networks caused by a mismatch between  development  and production  audio recording devices. The proposed adaptation approach works with unpaired source-target data and leverages upon NLE designed to take into account the relationships among acoustic scene classes. The NLE thereby not only condenses a representation of the DNN output distribution given all audio recordings aligned with the same output class but also captures the inherent relationships among acoustic scene classes. Device adaptation is carried out using relational teacher-student learning  solely based on target data, target labels, source DNN, and NLE. The latter serve as soft targets for DNN adaptation.  The proposed approach is assessed against the DCASE 2018 task1b dataset. Experimental evidence confirm the effectiveness our our approach, which compares favourably to conventional device adaptation, and traditional teacher-student based adaptation. Moreover, we observe that NLE based on structural information lead to superior ASC  results than NLE obtained with symmetric Kullback-Leibler divergence ,which do not take into account the relationships among acoustic scene classes.  % Alternative 2 In this paper, we propose a domain adaptation framework to address the device mismatch issue in acoustic scene classification leveraging upon neural label embedding  and relational teacher student learning .  Taking into account the structural relationships between acoustic scene classes, our proposed framework captures such relationships which are intrinsically device-independent. In the training stage, transferable knowledge is condensed in NLE from the source domain. Next in the adaptation stage, a novel RTSL strategy is adopted to learn adapted target models without using paired source-target data often required in conventional teacher student learning. The proposed framework is evaluated on the DCASE 2018 Task1b data set. Experimental results based on AlexNet-L deep classification models confirm the effectiveness of our proposed approach for mismatch situations. %when training with Device A data and testing with data recorded with Devices B and C.  NLE-alone adaptation compares favourably with the conventional device adaptation and teacher student based adaptation techniques. NLE with RTSL further improves the classification accuracy.",293
" %Motivate a bit from ASR side %Introduce a bit on punctuation problem.   The output text generated from automatic speech recognition  systems is typically devoid of punctuation and sentence formatting. Lack of sentence segmentation and punctuation makes it difficult to comprehend the ASR output. For example, consider the two sentences: ``Let's eat Grandma'' vs. ``Let's eat, Grandma!''. Punctuation restoration not only helps understand the context of the text but also greatly improves the readability. Punctuated text often helps in boosting the performance of several downstream natural language understanding  tasks.%  There is a plethora of work done in punctuation prediction over the past few decades. While some early methods of punctuation prediction used finite state or hidden markov models , some other techniques have investigated probabilistic models like language modeling , conditional random fields   and maximum entropy models . As neural networks gained popularity, several approaches have been proposed based on sequence labeling and neural machine translation . These models widely used convolutional neural networks  and LSTM based architectures . More recently, attention  and transformer  based architectures which have been successfully applied to a wide variety of tasks, have shown to perform well for punctuation prediction.  Although it is a well explored problem in the literature, most of these improvements do not directly translate to all domains. In particular, punctuation prediction for conversational speech is not very well explored . Also, a number of approaches have been proposed exploiting the use of acoustic features in addition to lexical features for punctuation task, but they are rather limited and do not clearly address the gap in performance with ASR outputs. In this paper, we focus on multimodal semi-supervised deep learning approach for punctuation prediction in conversational speech by leveraging pretrained lexical and acoustic encoders.  %two set of approaches emerged. One approach tags every word with no punctuation or a following punctuation mark treating it as sequence labeling problem . The second approach uses machine  translation based sequence to sequence models to generate punctuated text from unpunctuated text .     While several methodologies used either text or acoustic only information  for predicting punctuation, many studies show that combining both the features yields the best performance . Acoustic features widely used in the literature include prosodic information such as pause duration, phone duration, and pitch related values like fundamental frequency, and energy.  shows that using acoustic information lead to increased recognition of full stops. In , a hierarchical encoder is used to encode per frame acoustic features to word level features and the results show that incorporating acoustic features significantly outperform purely lexical systems. However, when trained on a very large independent text corpus, the lexical system outperformed the multimodal system that was trained on parallel audio/text corpora. To mitigate this, the work in  introduced speech2vec embeddings but they do not vary with respect to the acoustic context in reference speech.   In general, we identify two potential shortcomings with aforementioned multimodal systems. First, the training is still suboptimal due to lack of large-scale parallel audio/text corpora. Secondly, the models trained on reference text transcripts do not perform that well on ASR outputs, although incorporating acoustic features reduced the gap to some extent.     %And tell how our approach is different from other acoustic based approaches.  %We therefore focus on investigating the benefits of exploiting semi-supervised learning approach.   In this work, we introduce a novel framework for multimodal fusion of lexical and acoustic embeddings for punctuation prediction in conversational speech. Specifically, we investigate the benefits of using lexical and acoustic encoders that are pretrained on large amounts of unpaired text and audio data using unsupervised learning.  The key idea is to learn contextual representations through unsupervised training where substantial amounts of unlabeled data is available and then improve the performance on a downstream task like punctuation, for which the amount of data is limited, by leveraging learned representations. For multimodal fusion, we explore attention mechanism to automatically learn the alignment of word level lexical features and frame level acoustic features in the absence of explicit forced alignments.  We also show the adaptation of our proposed multimodal architecture for streaming usecase by limiting the future context. We further study the effect of pretrained encoders with respect to varying data sizes and their performance when trained on very small amounts of data. Finally, we exploit the N-best lists from ASR to perform data augmentation and reduce the gap in performance when tested on ASR outputs.   % We will investigate following research questions in the paper:   % %  %The rest of the paper is organized as follows: Section  introduces semi-supervised learning approach with pre-trained lexical and acoustic encoder for punctuation prediction. Section  describes the procedure for fusion of acoustic features with lexical encoder. We discuss our experimental setup in Section  and the results are presented in Section . Finally, in Section , we summarize our conclusions.       We introduced a novel multimodal semi-supervised learning framework which leverages large amounts of unlabelled audio and text data for punctuation prediction. We proposed an alternative attention based multimodal fusion mechanism which is effective, in the absence of forced alignment word durations. Through our data sizes ablation study, we showed how our proposed model is superior in performance to lexical only models on reference transcripts. In order to address the performance gaps on ASR outputs, we presented a robust model that is less affected by ASR errors by performing data augmentation with N-best lists.       
","  In this work, we explore a multimodal semi-supervised learning approach for punctuation prediction by learning representations from large amounts of unlabelled audio and text data. Conventional approaches in speech processing typically use forced alignment to encoder per frame acoustic features to word level features and perform multimodal fusion of the resulting acoustic and lexical representations. As an alternative, we explore attention based multimodal fusion and compare its performance with forced alignment based fusion. Experiments conducted on the Fisher corpus show that our proposed approach achieves $\sim$6-9\% and $\sim$3-4\% absolute improvement  over the baseline BLSTM model on reference transcripts and ASR outputs respectively. We further improve the model robustness to ASR errors by performing data augmentation with N-best lists which achieves up to an additional $\sim$2-6\% improvement on ASR outputs. We also demonstrate the effectiveness of semi-supervised learning approach by performing ablation study on various sizes of the corpus. When trained on 1 hour of speech and text data, the proposed model achieved $\sim$9-18\% absolute improvement over baseline model.   %We also incorporate a pretrained lexical BERT encoder to further enhance the hidden representation of acoustic embedding when performed fusion with lexical embedding.",294
"   Peking Opera, also known as Beijing Opera or Jingju, is Chinese traditional performing art which combines music, vocal performance, mime, dance and acrobatics. Singing in Peking Opera has various styles, each widely different depending on different role type and music styles. Strong personal styles also make the actual singing can be different from the given music notes. Like a dialect to Mandarin, it even has its unique way of pronunciation. Moreover, melody in singing often consist of arias with variation of complex transitory and vibratos, which makes the singing very expressive and difficult to learn. Another difference from normal singing is the note length has a great variance, sometime very long note can appear . All above factors makes it very challenging to modelling and generating Peking Opera singing comparing to normal singing.   Although there are few works focusing on the synthesis of Peking Opera, or more broadly, opera, the synthesis of singing voice has been researched since 1962 when Kelly and Lochbaum used an acoustic tube model to synthesis singing voice with success. Recently, several works use deep neural networks to synthesis singing voice which, known as parametric systems, process fundamental frequency  and harmonics features  separately. As a typical case among such systems, Neural Parametric Singing Synthesizer  using a phoneme timing model, a pitch model and a timbre model each consist a set of neural networks to generate acoustic parameters of the singing. In NPSS, a Fitting Heuristic method is introduced to eliminate the mismatch between music note duration and the predicted phoneme duration. However, Fitting Heuristic method is totally rule based and it requires to locate the principal vowel before adjusting phoneme duration. This maybe acceptable in most English or Japanese singing cases, but can cause huge duration error when synthesizing Peking Opera. Different from normal speech or singing, in Peking Opera, one syllable can last very long time and contains a long sequence of phonemes, e.g. ``l-j-E-a-a-N"". More importantly, one can't simply tell which phoneme amongst all these phonemes is the principle phone. There could be multiple equally important phonemes in Peking Opera singing.  To better synthesize the expressive Peking Opera, this paper proposes a Peking Opera singing synthesis system based on Duration Informed Attention Network . The main contribution in this study lies in the two following points: 1) To tackle with rhythm mismatch between music note duration and the predicted phoneme duration, contextual based mixture density networks  followed by a Lagrange Multiplier optimization is proposed and implemented for duration modelling. This method is completely data-driven, and more importantly, skips the step of locating the principle phoneme from the conventional Fitting Heuristic method. 2) To deal with the melody mismatch between original music score and the actual singing, and also to better model the expressive variations and vibratos in Peking Opera, a pseudo music score is generated from the real singing and fed as input during DurIAN model training. Experimental Results show proposed duration modeling and prediction method outperforms the Fitting Heuristic method by a large margin. And the generated pitch contours also demonstrate our system's ability to synthesize the singing variations and vibratos in Peking Opera.  The following sections of this paper are organized as follows. Firstly, the proposed model architecture is introduced. Next, proposed Lagrange Multiplier-based duration prediction and pseudo score generation are introduced in Section 2. In section 3 experiments are conducted based on a unique Peking Opera database. Finally, a quick discussion and conclusion is given in Section 4.          Improvisation and expressiveness in Peking Opera singing makes it extremely difficult to synthesize this classical performing art. With proposed MDN-based phoneme duration generation with Lagrange Multiplier optimization, our system can generate more accurate phoneme duration compared to the Fitting Heuristic phoneme duration scaling method. Pseudo music notes are generated through the melody transcription algorithm to solve the score inconsistency problem in training. Both the objective average predicted phoneme duration error and the generated pitch contour show our system performances well in generating Peking Opera singing. And as one can see from MOS and the generated samples that there is still a gap between the generated singing and the real performance in terms of naturalness. Our further work includes collecting and labelling more Peking Opera singing data, conducting MOS test in larger scale with subjects in musical background, and improving the quality and pitch accuracy of the generated singing.   \vfill\pagebreak     References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   -------------------------------------------------------------------------      
","  Peking Opera has been the most dominant form of Chinese performing art since around 200 years ago. A Peking Opera singer usually exhibits a very strong personal style via introducing improvisation and expressiveness on stage which leads the actual rhythm and pitch contour to deviate significantly from the original music score. This inconsistency poses a great challenge in Peking Opera singing voice synthesis from a music score. In this work, we propose to deal with this issue and synthesize expressive Peking Opera singing from the music score based on the Duration Informed Attention Network  framework. To tackle the rhythm mismatch, Lagrange multiplier is used to find the optimal output phoneme duration sequence with the constraint of the given note duration from music score. As for the pitch contour mismatch, instead of directly inferring from music score, we adopt a pseudo music score generated from the real singing and feed it as input during training. The experiments demonstrate that with the proposed system we can synthesize Peking Opera singing voice with high-quality timbre, pitch and expressiveness.",295
" Many machine learning datasets have a label imbalance or dataset bias problem. In many cases, either data is harder to collect for certain classes or the data collection phase is biased itself such that bias is introduced to the collected dataset. Typical training algorithms, optimized in order to minimize error, tend to do so by exacerbating bias, e.g., by providing higher recall and precision to the majority class than to minority classes. Therefore, the label imbalance problem raises the concern about fairness of machine learning systems in general. Spoken language understanding  problems often suffer from label imbalance, in ways that may hide important errors from the designers of SLU systems.  Consider an SLU dataset such as Air Traffic Information Systems   and the speech-to-intent detection problem on this dataset.  About 75\% of the dataset carries the intent of searching for a flight, while conversely, some minority intent classes are represented by only a single training example; this is a severe label imbalance problem. Suppose that we train a model without any concerns about fairness or imbalance. The model will very likely learn to output the `flight' intent all the time, which will give us an accuracy of 75\% which is not low and could be acceptable depending on the application. Considering that there are roughly 30 classes in the whole dataset, one class will have a recall of 1.0 and precision of 0.75 and the remaining 29 classes will have both recall and precision of 0.0. In such a scenario, the F-measure, which is a harmonic average of precision and recall, will be 0.86 for the most common class and 0.0 for the rest, which will give an average of 0.03 which is not acceptable in many cases.   % Previous work on Fair ML, There has been recent interest in introducing fairness to training in the machine learning literature . Most such studies are applied to benchmark datasets related to socioeconomic problems, e.g., disparate impact or equal opportunity . In most such studies, fairness is defined to be the task of protecting against the use of explicit or implicit information about a protected attribute  in the decisions of the machine learning algorithm, for instance, framing the problem as a constrained optimization problem by introducing several penalties. In this work, we introduce fairness into a speech-related problem, namely SLU. We also propose a positive and generalized definition of fairness, in terms of the missed detection and false alarm error rates suffered by all classes, regardless of whether the class definitions are matters of socioeconomic importance or merely engineering convenience.  % Previous methods on F-measure optimization  There have been several studies on F-measure maximization . These models usually focus on binary classification using non-neural-network models: a situation in which the problem of F-measure optimization reduces to the problem of learning a threshold on the scores computed by the model to make a decision. We are aware of one study that performs F-measure optimization for convolutional neural networks, but again, using a system that generates several binary classification outputs in parallel; in this scenario, F-measure optimization reduces to the task of tuning the thresholds of individual binary classifiers in order to maximize a weighted log likelihood. However, true multi-class classification, using the softmax output of the neural network, requires a modified definition of the F-measure.  There is no threshold that can be tuned; instead, F-measure optimization requires optimizing the model itself to generate `better' scores in terms of the F-measure. Model versus threshold optimization is the fundamental difference between this study and the previous ones.  In this work, our goal is to design a loss function to maximize the F-measure instead of the accuracy for DNNs. Our methods are tested on two standard socioeconomic classification problems from the literature on fairness , and on two SLU tasks .  On the SLU  tasks,  we perform end-to-end SLU, i.e., we directly map speech input to the labels instead of performing automatic speech recognition  followed by natural language processing .  We pose the SLU problems as multi-class classification tasks and use the softmax output from the DNN, making it possible to apply the same optimization criterion to both the socioeconomic and SLU learning problems. We approximate the F-measure with a differentiable function of the softmax activations so that we can use the standard backpropagation algorithm to train the DNN.      In this work, we proposed a method to maximize the F-measure while training a DNN to deal with the label imbalance problem that is frequently encountered in many datasets. We approximated the average  using soft counts obtained from the softmax activations of the DNN. We compared our proposed method to cross-entropy based training in our experiments. We showed that this method can be applied to different types of DNNs, either fully-connected or BLSTM based, as long as their final layer is a softmax layer. In our experiments on two SLU problems, namely the ATIS speech-to-intent detection problem and the Speech-COCO speech-to-image label classification task, we showed that deep F-measure maximization performs better than the cross-entropy model in terms of micro-, average- and the coverage of classes. Especially, significantly increased coverage shows that the proposed method provides a fair way of treating minority classes.  There are several future directions for research. One direction is to deal with the coverage versus accuracy trade-off, e.g., to explore multi-task or constrained learning methods that might improve coverage and fairness without harming performance for the majority class. Another issue that we would like to address is the performance degradation for high  cases for Speech-COCO.  We also would like to perform experiments on larger datasets with real speech instead of synthesized speech.   
"," Spoken language understanding  datasets, like many other machine learning datasets, usually suffer from the label imbalance problem. Label imbalance usually causes the learned model to replicate similar biases at the output which raises the issue of unfairness to the minority classes in the dataset. In this work, we approach the fairness problem by maximizing the F-measure instead of accuracy in neural network model training. We propose a differentiable approximation to the F-measure and train the network with this objective using standard backpropagation. We perform experiments on two standard fairness datasets, Adult, and Communities and Crime, and also on speech-to-intent detection on the ATIS dataset and speech-to-image concept classification on the Speech-COCO dataset. In  all four of these tasks, F-measure maximization results in improved micro-F1 scores, with absolute improvements of up to 8\% absolute, as compared to models trained with the cross-entropy loss function.  In the two multi-class SLU tasks, the proposed approach significantly improves class coverage, i.e., the number of classes with positive recall.",296
"  Recent neural text-to-speech  systems based on the sequence-to-sequence approach, such as Tacotron~2 , brought considerable quality improvements, but require relatively large amounts of training data and computational resources to train and operate. %Recent advances in text-to-speech synthesis   have allowed integration of high-quality speech synthesis systems into products such as Alexa or Google Assistant. However, adapting the synthesis models to custom domains requires access to relatively large amounts of training data and computational resources. %Moreover, real-time synthesis may be problematic due to the size of the systems and sequential inference. Several works attempt to reduce the computational burden in various ways , but there is still a tradeoff between fast training times, fast inference, and output quality.  In this paper, we address the training efficiency of TTS systems as well as the inference speed and hardware requirements while sustaining good quality of synthesized audio. We propose a fully convolutional, non-sequential approach to speech synthesis  %based on a combination of ideas from . %Similarly to , our system  consisting of a teacher and a student network, similarly to FastSpeech . The teacher network is an autoregressive %\OD{je tu pot鑹ba 鑹￠搯kat 閳ユ竵uteregresivn閾嗛垾, kdy鍟 se tak nikdy nepou鍟搯v璋?} \todo{myslim, ze jo -- jde o zpusob, jaky modeluje to audio. Kdyby nemodeloval audio autoregresivne, tak be nemel moc motivace naucit se spravny alignment} \OD{Fair enough.} convolutional network % based on   which is used to extract correct alignments between phonemes and corresponding audio frames. The student network is a non-autoregressive, fully convolutional network  %with residual connections  % based on   which encodes input phonemes, predicts the duration  for each one, then decodes a mel-scale spectrogram based on phoneme encodings and durations. %used to synthesize spectrograms from input phonemes. The student network first encodes the input phonemes. Then a duration prediction module predicts the duration of each phoneme. Finally, the phoneme encoding vectors are expanded based on their durations and are fed to a decoder module which synthesizes the final spectrogram.   We combine our student network with a pretrained MelGAN vocoder  to achieve fast and high-quality spectrogram inversion.  Our model can be trained on the LJ~Speech data  in under 40 hours on a single 8GB GPU and generates high-quality audio samples faster than real-time on both GPU and CPU.  %\OD{d璋 se tohle rozd鑷巐it na v閾哻 bod鏆 ne鍟 2?} \todo{ano :)}\OD{d閾唊 :-)} Our contributions are as follows:  We simplify the teacher-student architecture of FastSpeech  and provide a fast and stable training procedure.  We use a simpler, smaller and faster-to-train convolutional teacher model with a single attention layer instead of Transformer  used in FastSpeech. % .  We show that self-attention layers  in the student network are not needed for %necessary in order to achieve  high-quality speech synthesis.   We describe a simple data augmentation technique that can be used early in the training to make the teacher network robust to sequential error propagation.  We show that our model significantly outperforms strong baselines while keeping speedy training and inference. %Finally, we provide results of experiments with various techniques such as batch normalization , dropout , positional encoding and style loss functions.      We presented a convolutional model for spectrogram synthesis from phonemes that supports both speedy training and inference, while maintaining significantly better output voice quality than strong baselines. Our source code and audio samples are available on GitHub.\textsuperscript{} For future work, we plan to extend the model to support multi-speaker training data.  
"," %Recent breakthrough in in the quality of text-to-speech systems can be largely accounted to neural sequence-to-sequence models . Extensive research has been conducted to improve the effectiveness of training , inference speed  and voice quality  of the speech synthesis systems.  While recent neural sequence-to-sequence models have greatly improved the quality of speech synthesis, % in the past years, %However, to our knowledge  there has not been a system capable of %fast and efficient training, speedy inference and fast training, fast inference and high-quality audio synthesis at the same time.  %However, none of the aforementioned systems excels in all of the traits.  %In this work,  We propose a student-teacher network %based on   capable of high-quality faster-than-real-time spectrogram synthesis, with low requirements on computational resources and fast training time. We show that self-attention layers are not necessary for generation of high quality audio.  %In fact,  We utilize simple convolutional blocks with residual connections in both student and teacher networks and use only a single attention layer in the teacher model. Coupled with a MelGAN vocoder, our model's voice quality was rated significantly higher than Tacotron~2. Our model can be efficiently trained on a single GPU and can run in real time even on a  %4-core  CPU. We provide both our source code and audio samples in our GitHub repository.\footnote{\url{https://github.com/janvainer/speedyspeech}\label{fn:github}}",297
" Speech enabled applications are increasingly gaining popularity across the world. This has initiated a need to build accurate automatic speech recognition  system across different languages. Also, End-to-End  ASR systems are emerging as a popular alternative to conventional hybrid ASR systems. They replace the acoustic model , language model  and pronunciation model with a single neural network . Recurrent neural network transducer   is one such E2E system that allow streaming input and is suitable for real-time ASR applications. Therefore there is a lot of interest in building accurate RNN-T models for different languages spoken across the world.  %The speech recognition accuracy largely depends upon the amount of training data available.  There is often disparity in the availability of transcribed data for different languages. In most cases, a lot more data is available for American English than other languages. The quality of ASR model depends on a number of factors including, the training data quantity and diversity, acoustic model structure, and  optimization algorithm. Furthermore, training data diversity spans a number of factors in adults,  kids, speaking rate,  accents, near-field, and far-field acoustic conditions. A low-resource locale has limited ASR training data, and may not meet the acoustic diversity needed to train a robust model that can generalize to above acoustic factors. To overcome the low-resource constraint, transfer learning has been widely used in the hybrid ASR system to transfer the knowledge from a well trained source locale to a low-resource target locale that bring significant acoustic robustness for the target locale. In our recent work, we applied TL from a large scale en-US conventional hybrid model to the corresponding models in en-IN and it-IT locales, and achieved over 8\% word error rate relative reduction . Motivated by the success of the TL methods in the hybrid ASR system, we explore TL methods to improve low-resource RNN-T models.  Besides improving the target model acoustic robustness, TL is also crucial for training large and complex deep learning architectures. RNN-T models are difficult to train  and also require significantly large amount of data to jointly train the acoustic as well as language model attributes. In our study we have noted weaker convergence or significant parameter tuning requirements for desirable E2E training outcome for low-resource locale. Therefore we expect TL techniques to be even more relevant for E2E systems to stabilize training and improve ASR accuracy.  In the hybrid ASR system, transfer learning is typically done by initializing the target AM with the source AM. In the RNN-T framework, several transfer learning strategies exist depending upon the choice of the initialization model for the encoder and prediction networks. In this paper, we compare different transfer learning strategies in the RNN-T framework. We propose two-stage TL, by first training a target initialization model bootstrapped with a pretrained source model. Subsequently, this model is used to initialize the target RNN-T model. The two-stage TL approach shows ~ WERR reduction and faster convergence in the training loss as compared to randomly initialized RNN-T model. We also study the effect of TL with different amount of training data and show the importance of transfer learning in the case of low-resource languages.       In this paper, we explore transfer learning methods for RNN-T models. Our motivation is to leverage well-trained en-US models to bootstrap hi-IN RNN-T models and also to stabilize the hi-IN RNN-T model training. We evaluated the following transfer learning methods: a) en-US CE initialization b) en-US RNN-T initialization c) Two-stage transfer learning and d) Encoder and prediction network initialization.  Based on the WER gains and training convergence, we propose Two-stage learning approach with grapheme targets as the preferred transfer learning strategy. The experiments on smaller data-sets and training loss convergence reveal the importance of transfer learning for low-resource RNN-T models. The methods discussed in this paper can be generalized to other low-resource languages as well. In future, we plan to explore other transfer learning methods and its extension to multi-lingual RNN-T models.       Recently, End-to-End  automatic speech recognition  system have gained significant popularity in the ASR community. They replace the acoustic model , language model  and the pronunciation model of conventional hybrid ASR system with a single neural network . One such E2E architecture is the recurrent neural network transducer  that allow streaming input and is suitable for online ASR applications.    The E2E ASR systems are well-suited for on-device ASR applications as their model size is much smaller than the hybrid ASR models. The popular E2E ASR system include  Connectionist Temporal Classification , Attention-based Encoder-Decoder   and recurrent neural network transducer  . The CTC and    Several works have shown the effectiveness of TL in hybrid ASR framework . In this paper, we TL can also be used in the RNN-T framework to improve the accuracy of low-resource languages.   -Several works have shown the importance of transfer learning  on hybrid ASR systems []\\  -In case hybrid ASR system, transfer learning is typically done by initializing the AM for low resource language with well-trained AM from a high resource language. \\               Merge the two paragraphs  There is often disparity in the amount of transcribed speech data available for different languages. In most cases, lot more data is available for American English  than other languages. The RNN-T model trained with en-US data, referred to hence-forth as en-US RNN-T model, in some form encapsulates the knowledge of mapping the input speech to corresponding text learnt from corresponding en-US data. TL approaches can be used to transfer this knowledge while training the models for low resource languages. In this paper, we explore TL approaches to benefit Hindi RNN-T model by using en-US models.    The knowledge of en-US data is embedded in the models trained with en-US data, referred to hence-forth as en-US model. TL can be suitably used to transfer the knowledge from models trained with American English  to model to be trained with low resources. TL enables sharing of the knowledge from HR language to a LR language by simply initializing LR model with HR model. In this paper, we study the TL methods with American accent English data, referred to henceforth as en-US data, as high resource data and Indian accented Hindi data, referred to hi-IN data, as low resource data.     - In case of the RNN-T framework, several possible combinations exist for transfer learning. TL can be applied to encoder as well as the prediction network. \\    Review of the related work  Encoder itself can be initialized in the following different ways:   The key contributions of this work are as follows:      In general TL can also be applied to medium resource locales to seek improvements in certain new and growing applications, where corresponding training data may only be present in the source locale. It can also be applied to locales in the same language family as well as locales outside the family.     
","  Transfer learning  is widely used in conventional hybrid automatic speech recognition  system, to transfer the knowledge from source to target language. TL can be applied to end-to-end  ASR system such as recurrent neural network transducer  models, by initializing the encoder and/or prediction network of the target language with the pre-trained models from source language. In the hybrid ASR system, transfer learning is typically done by initializing the target language acoustic model  with source language AM. Several transfer learning strategies exist in the case of the RNN-T framework, depending upon the choice of the initialization model for encoder and prediction networks. This paper presents a comparative study of four different TL methods for RNN-T framework. We show ~$10\%-17\%$ relative word error rate reduction with different TL methods over randomly initialized RNN-T model. We also study the impact of TL with varying amount of training data ranging from $50$ hours to $1000$ hours and show the efficacy of TL for languages with a very small amount of training data.\\",298
" With the advent of deep learning, end-to-end text-to-speech  has shown many advantages over the conventional TTS techniques . Tacotron-based approaches  with an encoder-decoder architecture and attention mechanism have shown remarkable performance. The key idea is to integrate the conventional TTS pipeline into a unified network and learn the mapping directly from the text-waveform pair . The recent progress in neural vocoder  also contributes to the improvement of speech quality.   Speech prosody includes affective prosody and linguistic prosody. Affective prosody represents the emotion of a speaker, while linguistic prosody relates to the language content. They are both crucial in speech communication. A TTS system is expected to synthesize the right prosodic pattern at the right time. However, most of the current end-to-end systems  have not explicitly modeled speech prosody. Therefore, they can't control well the melodic and rhythmic aspects of the generated speech. This usually leads to monotonous speech, even when models are trained on very expressive speech datasets. In this paper, we would like to study the way to enable Tacotron-based TTS  for expressive prosody generation.   Multi-task learning  is a learning paradigm that leverages information from multiple related tasks to help improve the overall performance . MTL is inspired by human learning activities where people often apply the knowledge learned from many tasks for learning a new task, that is called inductive transfer. For example, if we learn to read and write together, the experience in reading can strengthen the writing and vice versa. MTL has been widely used in speech enhancement , and speech recognition . It has also been used in speech synthesis , such as statistical parametric speech synthesis with GANs  and DNN-based speech synthesis with stacked bottleneck features. In this paper, we apply multi-task learning to the Tacotron-based TTS for prosody modeling.     [t]      \\                    The study on expressive speech synthesis is focused on prosody modeling , where speech prosody generally refers to intonation, stress, speaking rate, and phrase breaks. Prosodic phrasing  plays an important role in both affective and linguistic expressions. Inadequate phrase breaks may lead to misperception in speech communication. There have been recent studies on prosody modeling for end-to-end TTS system , for example, to improve the prosodic phrasing  by using contextual information , and syntactic features . They are incorporated in the stage of text preprocessing, therefore, there are not optimized as part of the synthesis processing.  We propose a novel two-task learning scheme for Tacotron-based TTS model to improve the prosodic phrasing: 1) the main task learns the prediction of the speech spectrum parameters from character-level embedding representation, and 2) the secondary task learns the prediction of a word-level prosody embedding. During training, the secondary task serves as an additional supervision for Tacotron to learn the exquisite prosody structure associated with the input text. At run-time, the prosody embedding serves as a local condition that controls the prosodic phrasing during voice generation.    The main contributions of this paper include: 1) a novel Tacotron-based TTS architecture that explicitly models prosodic phrasing; and 2) a multi-task learning scheme, that optimizes the model for high quality speech spectrum, and adequate prosodic phrasing at the same time. The proposed system achieves remarkable voice quality for both Chinese Mandarin and Mongolian. To our best knowledge, this is the first multi-task Tacotron implementation that includes an explicit prosodic model.  This paper is organized as follows. Section  recaps the Tacotron TTS framework. We propose the  multi-task Tacotron in Section and report the experiments in Section. Section  concludes the discussion.       We have proposed a novel multi-task Tacotron model to model the prosodic phrasing in speech synthesis, where a word-level prosody generator is introduced as the secondary task. The experiments show that the proposed MTL-Tacotron consistently outperforms all contrastive systems. The modeling technique for prosodic phrasing can be easily extended to the modeling of other melodic and rhythmic aspects of speech, such as intonation and stress.                                             &         &  \\       &         &  \\      \noalign{    
"," Tacotron-based end-to-end speech synthesis has shown remarkable voice quality. However, the rendering of prosody in the synthesized speech remains to be improved, especially for long sentences, where prosodic phrasing errors can occur frequently. In this paper, we extend the Tacotron-based speech synthesis framework to explicitly model the prosodic phrase breaks. We propose a multi-task learning scheme for Tacotron training, that optimizes the system to predict both Mel spectrum and phrase breaks.  To our best knowledge, this is the first implementation of multi-task learning for Tacotron based TTS with a prosodic phrasing model. Experiments show that our proposed training scheme consistently improves the voice quality for both Chinese and Mongolian systems.",299
